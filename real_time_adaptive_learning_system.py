"""
ê³ ê¸‰ ì‹¤ì‹œê°„ ì ì‘í˜• í•™ìŠµ ì‹œìŠ¤í…œ v2.0
- ì˜¨ë¼ì¸ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜
- ì‹¤ì‹œê°„ ëª¨ë¸ ì—…ë°ì´íŠ¸  
- ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ëª¨ë¸ ë“œë¦¬í”„íŠ¸ ê°ì§€
- ì ì‘í˜• íŠ¹ì„± ì„ íƒ
- ì‹œì¥ ì¡°ê±´ ì ì‘
- í”¼ë“œë°± ë£¨í”„ êµ¬í˜„
"""

import os
import json
import sqlite3
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Any
import asyncio
import logging
from dataclasses import dataclass, asdict
import pickle
from collections import deque
from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, accuracy_score
import warnings
warnings.filterwarnings('ignore')

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class OnlineLearningConfig:
    """ì˜¨ë¼ì¸ í•™ìŠµ ì„¤ì •"""
    initial_learning_rate: float = 0.001
    min_learning_rate: float = 0.0001
    max_learning_rate: float = 0.01
    learning_rate_decay: float = 0.995
    momentum: float = 0.9
    weight_decay: float = 1e-4
    batch_size: int = 32
    memory_size: int = 1000
    adaptation_threshold: float = 0.05
    drift_detection_window: int = 50
    feature_selection_interval: int = 100

@dataclass
class MarketRegime:
    """ì‹œì¥ ìƒí™© ë¶„ë¥˜"""
    regime_type: str  # 'bull', 'bear', 'sideways', 'volatile'
    confidence: float
    start_time: datetime
    characteristics: Dict[str, float]
    
@dataclass
class ModelPerformanceMetrics:
    """ëª¨ë¸ ì„±ëŠ¥ ì§€í‘œ"""
    timestamp: datetime
    accuracy: float
    directional_accuracy: float
    mae: float
    mse: float
    sharpe_ratio: float
    max_drawdown: float
    profit_factor: float
    win_rate: float
    sample_count: int
    regime: str
    drift_score: float

class AdaptiveLearningRate:
    """ì ì‘í˜• í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬"""
    
    def __init__(self, initial_lr: float = 0.001, min_lr: float = 0.0001, max_lr: float = 0.01):
        self.initial_lr = initial_lr
        self.min_lr = min_lr
        self.max_lr = max_lr
        self.current_lr = initial_lr
        self.performance_history = deque(maxlen=20)
        self.lr_history = deque(maxlen=50)
        
    def update(self, performance_metric: float):
        """ì„±ëŠ¥ ê¸°ë°˜ í•™ìŠµë¥  ì¡°ì •"""
        self.performance_history.append(performance_metric)
        
        if len(self.performance_history) < 5:
            return self.current_lr
            
        # ìµœê·¼ ì„±ëŠ¥ ì¶”ì„¸ ë¶„ì„
        recent_performance = list(self.performance_history)[-5:]
        performance_trend = np.polyfit(range(len(recent_performance)), recent_performance, 1)[0]
        
        # ì„±ëŠ¥ì´ í–¥ìƒë˜ì§€ ì•Šìœ¼ë©´ í•™ìŠµë¥  ê°ì†Œ
        if performance_trend < 0.01:  # ì„±ëŠ¥ì´ ì •ì²´ë˜ê±°ë‚˜ ì•…í™”
            self.current_lr *= 0.9
        elif performance_trend > 0.05:  # ì„±ëŠ¥ì´ í¬ê²Œ í–¥ìƒ
            self.current_lr *= 1.1
            
        # ë²”ìœ„ ì œí•œ
        self.current_lr = max(self.min_lr, min(self.max_lr, self.current_lr))
        self.lr_history.append(self.current_lr)
        
        return self.current_lr

class OnlineNeuralNetwork(nn.Module):
    """ì˜¨ë¼ì¸ í•™ìŠµìš© ì‹ ê²½ë§"""
    
    def __init__(self, input_size: int, hidden_sizes: List[int] = [256, 128, 64], output_size: int = 3):
        super().__init__()
        
        layers = []
        prev_size = input_size
        
        # íˆë“  ë ˆì´ì–´ë“¤
        for hidden_size in hidden_sizes:
            layers.extend([
                nn.Linear(prev_size, hidden_size),
                nn.BatchNorm1d(hidden_size),
                nn.ReLU(),
                nn.Dropout(0.2)
            ])
            prev_size = hidden_size
            
        # ì¶œë ¥ ë ˆì´ì–´
        layers.append(nn.Linear(prev_size, output_size))
        layers.append(nn.Softmax(dim=1))
        
        self.network = nn.Sequential(*layers)
        self.feature_importance = None
        
    def forward(self, x):
        return self.network(x)
    
    def get_feature_importance(self, x, y):
        """íŠ¹ì„± ì¤‘ìš”ë„ ê³„ì‚°"""
        self.eval()
        with torch.no_grad():
            # ê° íŠ¹ì„±ì„ ì œê±°í–ˆì„ ë•Œì˜ ì„±ëŠ¥ ë³€í™” ì¸¡ì •
            baseline_output = self(x)
            baseline_loss = nn.CrossEntropyLoss()(baseline_output, y)
            
            importance_scores = []
            
            for feature_idx in range(x.shape[1]):
                x_masked = x.clone()
                x_masked[:, feature_idx] = 0  # íŠ¹ì„± ë§ˆìŠ¤í‚¹
                
                masked_output = self(x_masked)
                masked_loss = nn.CrossEntropyLoss()(masked_output, y)
                
                # ì„±ëŠ¥ ê°ì†Œ = íŠ¹ì„± ì¤‘ìš”ë„
                importance = (masked_loss - baseline_loss).item()
                importance_scores.append(max(0, importance))  # ìŒìˆ˜ ë°©ì§€
                
        return np.array(importance_scores)

class DriftDetector:
    """ëª¨ë¸ ë“œë¦¬í”„íŠ¸ ê°ì§€ê¸°"""
    
    def __init__(self, window_size: int = 50, sensitivity: float = 0.05):
        self.window_size = window_size
        self.sensitivity = sensitivity
        self.performance_window = deque(maxlen=window_size)
        self.prediction_window = deque(maxlen=window_size)
        self.feature_means = deque(maxlen=window_size)
        
    def add_sample(self, performance: float, predictions: np.ndarray, features: np.ndarray):
        """ìƒˆ ìƒ˜í”Œ ì¶”ê°€"""
        self.performance_window.append(performance)
        self.prediction_window.append(predictions)
        self.feature_means.append(np.mean(features))
        
    def detect_drift(self) -> Tuple[bool, Dict[str, float]]:
        """ë“œë¦¬í”„íŠ¸ ê°ì§€"""
        if len(self.performance_window) < self.window_size:
            return False, {}
            
        # ì„±ëŠ¥ ë“œë¦¬í”„íŠ¸ ê²€ì‚¬
        recent_performance = np.array(list(self.performance_window)[-self.window_size//2:])
        older_performance = np.array(list(self.performance_window)[:self.window_size//2])
        
        performance_drift = abs(np.mean(recent_performance) - np.mean(older_performance))
        
        # íŠ¹ì„± ë“œë¦¬í”„íŠ¸ ê²€ì‚¬  
        recent_features = np.array(list(self.feature_means)[-self.window_size//2:])
        older_features = np.array(list(self.feature_means)[:self.window_size//2])
        
        feature_drift = abs(np.mean(recent_features) - np.mean(older_features))
        
        # ì˜ˆì¸¡ ë¶„í¬ ë“œë¦¬í”„íŠ¸ ê²€ì‚¬
        recent_preds = np.array(list(self.prediction_window)[-self.window_size//2:])
        older_preds = np.array(list(self.prediction_window)[:self.window_size//2])
        
        prediction_drift = np.mean([
            abs(np.mean(recent_preds, axis=0) - np.mean(older_preds, axis=0))
        ])
        
        drift_metrics = {
            'performance_drift': performance_drift,
            'feature_drift': feature_drift,
            'prediction_drift': prediction_drift,
            'combined_drift': (performance_drift + feature_drift + prediction_drift) / 3
        }
        
        # ë“œë¦¬í”„íŠ¸ ê°ì§€
        drift_detected = drift_metrics['combined_drift'] > self.sensitivity
        
        return drift_detected, drift_metrics

class MarketRegimeDetector:
    """ì‹œì¥ ìƒí™© ê°ì§€ê¸°"""
    
    def __init__(self):
        self.price_history = deque(maxlen=100)
        self.volume_history = deque(maxlen=100)
        self.volatility_history = deque(maxlen=100)
        
    def add_data(self, price: float, volume: float, volatility: float):
        """ìƒˆ ë°ì´í„° ì¶”ê°€"""
        self.price_history.append(price)
        self.volume_history.append(volume)
        self.volatility_history.append(volatility)
        
    def detect_regime(self) -> MarketRegime:
        """ì‹œì¥ ìƒí™© ê°ì§€"""
        if len(self.price_history) < 20:
            return MarketRegime("unknown", 0.5, datetime.now(), {})
            
        prices = np.array(list(self.price_history))
        volumes = np.array(list(self.volume_history))
        volatilities = np.array(list(self.volatility_history))
        
        # ì¶”ì„¸ ë¶„ì„
        price_trend = np.polyfit(range(len(prices)), prices, 1)[0]
        price_trend_strength = abs(price_trend) / np.std(prices)
        
        # ë³€ë™ì„± ë¶„ì„
        current_volatility = np.mean(volatilities[-10:])
        avg_volatility = np.mean(volatilities)
        volatility_ratio = current_volatility / avg_volatility if avg_volatility > 0 else 1
        
        # ê±°ë˜ëŸ‰ ë¶„ì„
        current_volume = np.mean(volumes[-10:])
        avg_volume = np.mean(volumes)
        volume_ratio = current_volume / avg_volume if avg_volume > 0 else 1
        
        # ì‹œì¥ ìƒí™© ë¶„ë¥˜
        regime_scores = {}
        
        # ê°•ì„¸ì¥
        bull_score = 0
        if price_trend > 0:
            bull_score += 0.4 * min(price_trend_strength, 1.0)
        if volume_ratio > 1.2:
            bull_score += 0.3
        if volatility_ratio < 1.2:
            bull_score += 0.3
            
        # ì•½ì„¸ì¥
        bear_score = 0
        if price_trend < 0:
            bear_score += 0.4 * min(price_trend_strength, 1.0)
        if volume_ratio > 1.1:
            bear_score += 0.3
        if volatility_ratio > 0.8:
            bear_score += 0.3
            
        # íš¡ë³´
        sideways_score = 0
        if price_trend_strength < 0.3:
            sideways_score += 0.5
        if volatility_ratio < 1.5:
            sideways_score += 0.3
        if 0.8 < volume_ratio < 1.2:
            sideways_score += 0.2
            
        # ê³ ë³€ë™ì„±
        volatile_score = 0
        if volatility_ratio > 2.0:
            volatile_score += 0.6
        if volume_ratio > 1.5:
            volatile_score += 0.4
            
        regime_scores = {
            'bull': bull_score,
            'bear': bear_score, 
            'sideways': sideways_score,
            'volatile': volatile_score
        }
        
        # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ìƒí™© ì„ íƒ
        best_regime = max(regime_scores.items(), key=lambda x: x[1])
        
        characteristics = {
            'price_trend': float(price_trend),
            'trend_strength': float(price_trend_strength),
            'volatility_ratio': float(volatility_ratio),
            'volume_ratio': float(volume_ratio)
        }
        
        return MarketRegime(
            regime_type=best_regime[0],
            confidence=best_regime[1],
            start_time=datetime.now(),
            characteristics=characteristics
        )

class AdaptiveFeatureSelector:
    """ì ì‘í˜• íŠ¹ì„± ì„ íƒê¸°"""
    
    def __init__(self, max_features: int = 50, selection_methods: List[str] = ['mutual_info', 'f_test', 'correlation']):
        self.max_features = max_features
        self.selection_methods = selection_methods
        self.feature_importance_history = deque(maxlen=10)
        self.selected_features = None
        
    def select_features(self, X: np.ndarray, y: np.ndarray, feature_names: List[str]) -> Tuple[np.ndarray, List[str]]:
        """íŠ¹ì„± ì„ íƒ ì‹¤í–‰"""
        if X.shape[1] <= self.max_features:
            return X, feature_names
            
        feature_scores = np.zeros(X.shape[1])
        
        # ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ íŠ¹ì„± ì ìˆ˜ ê³„ì‚°
        for method in self.selection_methods:
            if method == 'mutual_info':
                scores = mutual_info_regression(X, y, random_state=42)
                feature_scores += scores / np.sum(scores)  # ì •ê·œí™”
                
            elif method == 'f_test':
                selector = SelectKBest(f_regression, k='all')
                selector.fit(X, y)
                scores = selector.scores_
                feature_scores += scores / np.sum(scores)  # ì •ê·œí™”
                
            elif method == 'correlation':
                correlations = np.abs([np.corrcoef(X[:, i], y)[0, 1] for i in range(X.shape[1])])
                correlations = np.nan_to_num(correlations)
                if np.sum(correlations) > 0:
                    feature_scores += correlations / np.sum(correlations)
        
        # í‰ê·  ì ìˆ˜ë¡œ ìµœì¢… íŠ¹ì„± ì„ íƒ
        feature_scores /= len(self.selection_methods)
        
        # ìƒìœ„ íŠ¹ì„± ì„ íƒ
        top_indices = np.argsort(feature_scores)[-self.max_features:]
        selected_X = X[:, top_indices]
        selected_names = [feature_names[i] for i in top_indices]
        
        self.selected_features = top_indices
        self.feature_importance_history.append(feature_scores)
        
        logger.info(f"íŠ¹ì„± ì„ íƒ ì™„ë£Œ: {X.shape[1]} â†’ {selected_X.shape[1]}")
        
        return selected_X, selected_names
    
    def get_feature_stability(self) -> float:
        """íŠ¹ì„± ì„ íƒì˜ ì•ˆì •ì„± ì¸¡ì •"""
        if len(self.feature_importance_history) < 2:
            return 1.0
            
        recent_importance = self.feature_importance_history[-1]
        previous_importance = self.feature_importance_history[-2]
        
        # íŠ¹ì„± ì¤‘ìš”ë„ ë³€í™”ìœ¨ ê³„ì‚°
        correlation = np.corrcoef(recent_importance, previous_importance)[0, 1]
        return max(0.0, correlation)  # 0~1 ë²”ìœ„ë¡œ ì œí•œ

class RealTimeAdaptiveLearningSystem:
    """ì‹¤ì‹œê°„ ì ì‘í˜• í•™ìŠµ ì‹œìŠ¤í…œ"""
    
    def __init__(self, config: OnlineLearningConfig = None):
        self.config = config or OnlineLearningConfig()
        self.base_path = "/Users/parkyoungjun/Desktop/BTC_Analysis_System"
        self.db_path = os.path.join(self.base_path, "adaptive_learning_v2.db")
        self.models_path = os.path.join(self.base_path, "adaptive_models")
        
        os.makedirs(self.models_path, exist_ok=True)
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.learning_rate_scheduler = AdaptiveLearningRate(
            initial_lr=self.config.initial_learning_rate,
            min_lr=self.config.min_learning_rate,
            max_lr=self.config.max_learning_rate
        )
        
        self.drift_detector = DriftDetector(
            window_size=self.config.drift_detection_window,
            sensitivity=self.config.adaptation_threshold
        )
        
        self.regime_detector = MarketRegimeDetector()
        self.feature_selector = AdaptiveFeatureSelector()
        
        # ëª¨ë¸ ë° ìµœì í™”
        self.model = None
        self.optimizer = None
        self.scaler = StandardScaler()
        
        # ë©”ëª¨ë¦¬ ë²„í¼
        self.experience_buffer = deque(maxlen=self.config.memory_size)
        self.performance_history = deque(maxlen=200)
        
        # í˜„ì¬ ìƒíƒœ
        self.current_regime = None
        self.current_features = None
        self.training_step = 0
        self.last_feature_selection = 0
        
        self.init_database()
        
    def init_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # ì˜¨ë¼ì¸ í•™ìŠµ ê¸°ë¡ í…Œì´ë¸”
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS online_learning_records (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT NOT NULL,
                    training_step INTEGER,
                    learning_rate REAL,
                    batch_loss REAL,
                    validation_accuracy REAL,
                    drift_score REAL,
                    regime_type TEXT,
                    feature_count INTEGER,
                    model_version TEXT
                )
            ''')
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ í…Œì´ë¸”
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS performance_metrics (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT NOT NULL,
                    accuracy REAL,
                    directional_accuracy REAL,
                    mae REAL,
                    mse REAL,
                    sharpe_ratio REAL,
                    max_drawdown REAL,
                    profit_factor REAL,
                    win_rate REAL,
                    sample_count INTEGER,
                    regime TEXT,
                    drift_score REAL
                )
            ''')
            
            # ì‹œì¥ ìƒí™© ê¸°ë¡ í…Œì´ë¸”
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS market_regimes (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT NOT NULL,
                    regime_type TEXT,
                    confidence REAL,
                    price_trend REAL,
                    trend_strength REAL,
                    volatility_ratio REAL,
                    volume_ratio REAL,
                    duration_minutes INTEGER
                )
            ''')
            
            # íŠ¹ì„± ì„ íƒ ê¸°ë¡ í…Œì´ë¸”
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS feature_selection_records (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT NOT NULL,
                    total_features INTEGER,
                    selected_features INTEGER,
                    selection_method TEXT,
                    feature_stability REAL,
                    top_features TEXT
                )
            ''')
            
            conn.commit()
            conn.close()
            
            logger.info("âœ… ì‹¤ì‹œê°„ í•™ìŠµ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def init_model(self, input_size: int):
        """ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            self.model = OnlineNeuralNetwork(input_size=input_size)
            self.optimizer = optim.AdamW(
                self.model.parameters(),
                lr=self.config.initial_learning_rate,
                weight_decay=self.config.weight_decay
            )
            
            logger.info(f"âœ… ì˜¨ë¼ì¸ í•™ìŠµ ëª¨ë¸ ì´ˆê¸°í™”: ì…ë ¥ ì°¨ì› {input_size}")
            
        except Exception as e:
            logger.error(f"ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    async def process_new_data(self, market_data: Dict) -> Dict:
        """ìƒˆë¡œìš´ ì‹œì¥ ë°ì´í„° ì²˜ë¦¬"""
        try:
            # 1. ì‹œì¥ ìƒí™© ê°ì§€
            await self.detect_market_regime(market_data)
            
            # 2. íŠ¹ì„± ì¶”ì¶œ ë° ì „ì²˜ë¦¬
            features, feature_names = await self.extract_features(market_data)
            
            if features is None or len(features) == 0:
                return {"error": "íŠ¹ì„± ì¶”ì¶œ ì‹¤íŒ¨"}
            
            # 3. íŠ¹ì„± ì„ íƒ (ì£¼ê¸°ì ìœ¼ë¡œ)
            if self.training_step - self.last_feature_selection >= self.config.feature_selection_interval:
                features, feature_names = await self.adaptive_feature_selection(features, feature_names, market_data)
                self.last_feature_selection = self.training_step
            
            # 4. ëª¨ë¸ ì´ˆê¸°í™” (ì²« ì‹¤í–‰ì‹œ)
            if self.model is None:
                self.init_model(len(features))
            
            # 5. ê²½í—˜ ë²„í¼ì— ì¶”ê°€
            experience = {
                'features': features,
                'timestamp': datetime.now(),
                'market_data': market_data,
                'regime': self.current_regime.regime_type if self.current_regime else 'unknown'
            }
            self.experience_buffer.append(experience)
            
            # 6. ì˜¨ë¼ì¸ í•™ìŠµ ì‹¤í–‰
            learning_result = await self.online_learning_step()
            
            # 7. ë“œë¦¬í”„íŠ¸ ê°ì§€
            drift_result = await self.detect_drift()
            
            # 8. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
            performance_result = await self.monitor_performance()
            
            # 9. ì˜ˆì¸¡ ìƒì„±
            prediction = await self.generate_prediction(features)
            
            return {
                "prediction": prediction,
                "learning": learning_result,
                "drift": drift_result,
                "performance": performance_result,
                "regime": self.current_regime.regime_type if self.current_regime else 'unknown',
                "feature_count": len(features)
            }
            
        except Exception as e:
            logger.error(f"ìƒˆ ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    async def detect_market_regime(self, market_data: Dict):
        """ì‹œì¥ ìƒí™© ê°ì§€"""
        try:
            price = market_data.get('price', 0)
            volume = market_data.get('volume', 0)
            volatility = market_data.get('volatility', 0.02)
            
            self.regime_detector.add_data(price, volume, volatility)
            new_regime = self.regime_detector.detect_regime()
            
            # ìƒí™©ì´ ë³€ê²½ëœ ê²½ìš°
            if (self.current_regime is None or 
                self.current_regime.regime_type != new_regime.regime_type):
                
                # ë°ì´í„°ë² ì´ìŠ¤ì— ê¸°ë¡
                conn = sqlite3.connect(self.db_path)
                cursor = conn.cursor()
                
                cursor.execute('''
                    INSERT INTO market_regimes 
                    (timestamp, regime_type, confidence, price_trend, trend_strength, volatility_ratio, volume_ratio)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                ''', (
                    new_regime.start_time.isoformat(),
                    new_regime.regime_type,
                    new_regime.confidence,
                    new_regime.characteristics.get('price_trend', 0),
                    new_regime.characteristics.get('trend_strength', 0),
                    new_regime.characteristics.get('volatility_ratio', 1),
                    new_regime.characteristics.get('volume_ratio', 1)
                ))
                
                conn.commit()
                conn.close()
                
                logger.info(f"ì‹œì¥ ìƒí™© ë³€ê²½: {new_regime.regime_type} (ì‹ ë¢°ë„: {new_regime.confidence:.2f})")
            
            self.current_regime = new_regime
            
        except Exception as e:
            logger.error(f"ì‹œì¥ ìƒí™© ê°ì§€ ì‹¤íŒ¨: {e}")
    
    async def extract_features(self, market_data: Dict) -> Tuple[Optional[np.ndarray], Optional[List[str]]]:
        """íŠ¹ì„± ì¶”ì¶œ"""
        try:
            features = []
            feature_names = []
            
            # ê¸°ë³¸ ì‹œì¥ ë°ì´í„°
            basic_features = [
                'price', 'volume', 'volatility', 'rsi', 'macd', 'bollinger_upper', 
                'bollinger_lower', 'sma_20', 'ema_20', 'fear_greed_index'
            ]
            
            for feature_name in basic_features:
                value = market_data.get(feature_name, 0)
                if isinstance(value, (int, float)) and not np.isnan(value):
                    features.append(float(value))
                    feature_names.append(feature_name)
            
            # ê³ ê¸‰ ì§€í‘œë“¤ ì¶”ê°€
            if 'indicators' in market_data:
                indicators = market_data['indicators']
                for key, value in indicators.items():
                    if isinstance(value, (int, float)) and not np.isnan(value):
                        features.append(float(value))
                        feature_names.append(f"indicator_{key}")
            
            # ì˜¨ì²´ì¸ ë°ì´í„°
            if 'onchain' in market_data:
                onchain = market_data['onchain']
                for key, value in onchain.items():
                    if isinstance(value, (int, float)) and not np.isnan(value):
                        features.append(float(value))
                        feature_names.append(f"onchain_{key}")
            
            # ì‹œê°„ ê¸°ë°˜ íŠ¹ì„±
            now = datetime.now()
            features.extend([
                now.hour / 24.0,  # ì‹œê°„ ì •ê·œí™”
                now.weekday() / 7.0,  # ìš”ì¼ ì •ê·œí™”
                now.day / 31.0  # ì¼ ì •ê·œí™”
            ])
            feature_names.extend(['time_hour', 'time_weekday', 'time_day'])
            
            if len(features) == 0:
                return None, None
            
            features_array = np.array(features).reshape(1, -1)
            
            # ì •ê·œí™”
            if hasattr(self.scaler, 'scale_') and self.scaler.scale_ is not None:
                if features_array.shape[1] == len(self.scaler.scale_):
                    features_array = self.scaler.transform(features_array)
            else:
                # ì²« ì‹¤í–‰ì‹œ ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ
                features_array = self.scaler.fit_transform(features_array)
            
            return features_array[0], feature_names
            
        except Exception as e:
            logger.error(f"íŠ¹ì„± ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None, None
    
    async def adaptive_feature_selection(self, features: np.ndarray, feature_names: List[str], market_data: Dict) -> Tuple[np.ndarray, List[str]]:
        """ì ì‘í˜• íŠ¹ì„± ì„ íƒ"""
        try:
            if len(self.experience_buffer) < 50:  # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ëª¨ë“  íŠ¹ì„± ì‚¬ìš©
                return features.reshape(1, -1), feature_names
            
            # ìµœê·¼ ê²½í—˜ì—ì„œ íŠ¹ì„±ê³¼ ë ˆì´ë¸” ì¶”ì¶œ
            recent_experiences = list(self.experience_buffer)[-100:]
            X = np.array([exp['features'] for exp in recent_experiences])
            
            # ë ˆì´ë¸” ìƒì„± (ê°„ë‹¨í•œ ê°€ê²© ë°©í–¥)
            y = []
            for i, exp in enumerate(recent_experiences):
                if i < len(recent_experiences) - 1:
                    current_price = exp['market_data'].get('price', 0)
                    next_price = recent_experiences[i+1]['market_data'].get('price', 0)
                    direction = 1 if next_price > current_price else 0
                    y.append(direction)
                else:
                    y.append(y[-1] if y else 0)  # ë§ˆì§€ë§‰ì€ ì´ì „ ê°’ ì‚¬ìš©
            
            y = np.array(y)
            
            # íŠ¹ì„± ì„ íƒ ì‹¤í–‰
            selected_X, selected_names = self.feature_selector.select_features(X, y, feature_names)
            
            # í˜„ì¬ íŠ¹ì„±ì„ ì„ íƒëœ íŠ¹ì„±ìœ¼ë¡œ ë³€í™˜
            if len(features) == len(feature_names):
                selected_indices = [feature_names.index(name) for name in selected_names if name in feature_names]
                if selected_indices:
                    selected_current_features = features[selected_indices]
                    
                    # íŠ¹ì„± ì„ íƒ ê¸°ë¡
                    stability = self.feature_selector.get_feature_stability()
                    await self.record_feature_selection(len(feature_names), len(selected_names), stability, selected_names[:10])
                    
                    return selected_current_features.reshape(1, -1), selected_names
            
            return features.reshape(1, -1), feature_names
            
        except Exception as e:
            logger.error(f"ì ì‘í˜• íŠ¹ì„± ì„ íƒ ì‹¤íŒ¨: {e}")
            return features.reshape(1, -1), feature_names
    
    async def online_learning_step(self) -> Dict:
        """ì˜¨ë¼ì¸ í•™ìŠµ ë‹¨ê³„"""
        try:
            if self.model is None or len(self.experience_buffer) < self.config.batch_size:
                return {"message": "í•™ìŠµì„ ìœ„í•œ ë°ì´í„° ë¶€ì¡±"}
            
            # ë°°ì¹˜ ìƒ˜í”Œë§
            batch_indices = np.random.choice(
                len(self.experience_buffer), 
                size=min(self.config.batch_size, len(self.experience_buffer)), 
                replace=False
            )
            
            batch = [self.experience_buffer[i] for i in batch_indices]
            
            # íŠ¹ì„±ê³¼ ë ˆì´ë¸” ì¤€ë¹„
            X = torch.FloatTensor([exp['features'] for exp in batch])
            
            # ë ˆì´ë¸” ìƒì„± (ë‹¤ìŒ ì‹œì ì˜ ê°€ê²© ë°©í–¥)
            y = []
            for exp in batch:
                # ê°„ë‹¨í•œ ë¶„ë¥˜: ìƒìŠ¹(2), ìœ ì§€(1), í•˜ë½(0)
                market_data = exp['market_data']
                current_price = market_data.get('price', 0)
                
                # ì—¬ê¸°ì„œëŠ” ì„ì‹œë¡œ ëœë¤ ë ˆì´ë¸” ì‚¬ìš© (ì‹¤ì œë¡œëŠ” ë¯¸ë˜ ê°€ê²© ë°ì´í„° í•„ìš”)
                future_direction = np.random.choice([0, 1, 2])  # ì„ì‹œ
                y.append(future_direction)
            
            y = torch.LongTensor(y)
            
            # ìˆœì „íŒŒ
            self.model.train()
            outputs = self.model(X)
            loss = nn.CrossEntropyLoss()(outputs, y)
            
            # ì—­ì „íŒŒ
            self.optimizer.zero_grad()
            loss.backward()
            
            # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            self.optimizer.step()
            
            # í•™ìŠµë¥  ì—…ë°ì´íŠ¸
            current_accuracy = self.get_current_accuracy()
            new_lr = self.learning_rate_scheduler.update(current_accuracy)
            
            for param_group in self.optimizer.param_groups:
                param_group['lr'] = new_lr
            
            self.training_step += 1
            
            # í•™ìŠµ ê¸°ë¡
            await self.record_learning_step(loss.item(), current_accuracy, new_lr)
            
            return {
                "training_step": self.training_step,
                "loss": loss.item(),
                "learning_rate": new_lr,
                "accuracy": current_accuracy
            }
            
        except Exception as e:
            logger.error(f"ì˜¨ë¼ì¸ í•™ìŠµ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    async def detect_drift(self) -> Dict:
        """ë“œë¦¬í”„íŠ¸ ê°ì§€"""
        try:
            if self.model is None or len(self.experience_buffer) < 20:
                return {"drift_detected": False, "message": "ë“œë¦¬í”„íŠ¸ ê°ì§€ë¥¼ ìœ„í•œ ë°ì´í„° ë¶€ì¡±"}
            
            # ìµœê·¼ ë°ì´í„°ë¡œ ì˜ˆì¸¡ ìƒì„±
            recent_data = list(self.experience_buffer)[-10:]
            X = torch.FloatTensor([exp['features'] for exp in recent_data])
            
            with torch.no_grad():
                self.model.eval()
                predictions = self.model(X).numpy()
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­
            recent_performance = np.mean(self.performance_history) if self.performance_history else 0.5
            
            # ë“œë¦¬í”„íŠ¸ ê²€ì‚¬
            self.drift_detector.add_sample(
                performance=recent_performance,
                predictions=predictions,
                features=X.numpy()
            )
            
            drift_detected, drift_metrics = self.drift_detector.detect_drift()
            
            if drift_detected:
                logger.warning(f"ëª¨ë¸ ë“œë¦¬í”„íŠ¸ ê°ì§€: {drift_metrics}")
                # ë“œë¦¬í”„íŠ¸ ëŒ€ì‘ (í•™ìŠµë¥  ì¦ê°€, ëª¨ë¸ ì¬ì´ˆê¸°í™” ë“±)
                await self.handle_drift(drift_metrics)
            
            return {
                "drift_detected": drift_detected,
                "drift_metrics": drift_metrics
            }
            
        except Exception as e:
            logger.error(f"ë“œë¦¬í”„íŠ¸ ê°ì§€ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    async def handle_drift(self, drift_metrics: Dict):
        """ë“œë¦¬í”„íŠ¸ ì²˜ë¦¬"""
        try:
            # 1. í•™ìŠµë¥  ì¦ê°€
            current_lr = self.learning_rate_scheduler.current_lr
            boost_factor = 1 + drift_metrics['combined_drift']
            new_lr = min(current_lr * boost_factor, self.config.max_learning_rate)
            
            for param_group in self.optimizer.param_groups:
                param_group['lr'] = new_lr
            
            self.learning_rate_scheduler.current_lr = new_lr
            
            # 2. ì‹¬í•œ ë“œë¦¬í”„íŠ¸ì˜ ê²½ìš° ëª¨ë¸ ë¶€ë¶„ ì¬ì´ˆê¸°í™”
            if drift_metrics['combined_drift'] > 0.15:
                logger.info("ì‹¬í•œ ë“œë¦¬í”„íŠ¸ ê°ì§€ - ëª¨ë¸ ë¶€ë¶„ ì¬ì´ˆê¸°í™”")
                
                # ì¶œë ¥ ë ˆì´ì–´ë§Œ ì¬ì´ˆê¸°í™”
                with torch.no_grad():
                    for layer in self.model.network:
                        if isinstance(layer, nn.Linear):
                            layer.weight.data *= 0.8  # ê°€ì¤‘ì¹˜ ê°ì†Œ
                            if layer.bias is not None:
                                layer.bias.data *= 0.8
            
            logger.info(f"ë“œë¦¬í”„íŠ¸ ëŒ€ì‘ ì™„ë£Œ - ìƒˆ í•™ìŠµë¥ : {new_lr:.6f}")
            
        except Exception as e:
            logger.error(f"ë“œë¦¬í”„íŠ¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
    
    async def monitor_performance(self) -> Dict:
        """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§"""
        try:
            if len(self.performance_history) < 10:
                return {"message": "ì„±ëŠ¥ í‰ê°€ë¥¼ ìœ„í•œ ë°ì´í„° ë¶€ì¡±"}
            
            recent_performance = list(self.performance_history)[-20:]
            
            # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
            accuracy = np.mean(recent_performance)
            trend = np.polyfit(range(len(recent_performance)), recent_performance, 1)[0]
            stability = 1 - np.std(recent_performance)  # ë³€ë™ì„±ì˜ ì—­ìˆ˜
            
            # ì„±ëŠ¥ í‰ê°€
            performance_metrics = ModelPerformanceMetrics(
                timestamp=datetime.now(),
                accuracy=accuracy,
                directional_accuracy=accuracy,  # ì„ì‹œ
                mae=0.0,  # ê³„ì‚° í•„ìš”
                mse=0.0,  # ê³„ì‚° í•„ìš”
                sharpe_ratio=0.0,  # ê³„ì‚° í•„ìš”
                max_drawdown=0.0,  # ê³„ì‚° í•„ìš”
                profit_factor=0.0,  # ê³„ì‚° í•„ìš”
                win_rate=accuracy,
                sample_count=len(recent_performance),
                regime=self.current_regime.regime_type if self.current_regime else 'unknown',
                drift_score=0.0  # ë“œë¦¬í”„íŠ¸ ì ìˆ˜
            )
            
            # ë°ì´í„°ë² ì´ìŠ¤ì— ê¸°ë¡
            await self.record_performance_metrics(performance_metrics)
            
            return {
                "accuracy": accuracy,
                "trend": trend,
                "stability": stability,
                "regime": self.current_regime.regime_type if self.current_regime else 'unknown'
            }
            
        except Exception as e:
            logger.error(f"ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    async def generate_prediction(self, features: np.ndarray) -> Dict:
        """ì˜ˆì¸¡ ìƒì„±"""
        try:
            if self.model is None:
                return {"error": "ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ"}
            
            X = torch.FloatTensor(features.reshape(1, -1))
            
            with torch.no_grad():
                self.model.eval()
                output = self.model(X)
                probabilities = output[0].numpy()
            
            # ì˜ˆì¸¡ ê²°ê³¼ í•´ì„
            direction_map = {0: "BEARISH", 1: "NEUTRAL", 2: "BULLISH"}
            predicted_class = np.argmax(probabilities)
            confidence = probabilities[predicted_class]
            
            # ì„±ëŠ¥ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€ (ì‹¤ì œë¡œëŠ” ê²€ì¦ í›„ ì¶”ê°€í•´ì•¼ í•¨)
            self.performance_history.append(confidence)
            
            return {
                "direction": direction_map[predicted_class],
                "confidence": float(confidence),
                "probabilities": {
                    "bearish": float(probabilities[0]),
                    "neutral": float(probabilities[1]), 
                    "bullish": float(probabilities[2])
                },
                "model_version": "adaptive_v2.0"
            }
            
        except Exception as e:
            logger.error(f"ì˜ˆì¸¡ ìƒì„± ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def get_current_accuracy(self) -> float:
        """í˜„ì¬ ì •í™•ë„ ë°˜í™˜"""
        if len(self.performance_history) < 5:
            return 0.5
        return np.mean(list(self.performance_history)[-10:])
    
    async def record_learning_step(self, loss: float, accuracy: float, learning_rate: float):
        """í•™ìŠµ ë‹¨ê³„ ê¸°ë¡"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT INTO online_learning_records 
                (timestamp, training_step, learning_rate, batch_loss, validation_accuracy, regime_type, feature_count)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            ''', (
                datetime.now().isoformat(),
                self.training_step,
                learning_rate,
                loss,
                accuracy,
                self.current_regime.regime_type if self.current_regime else 'unknown',
                len(self.current_features) if self.current_features else 0
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            logger.error(f"í•™ìŠµ ë‹¨ê³„ ê¸°ë¡ ì‹¤íŒ¨: {e}")
    
    async def record_performance_metrics(self, metrics: ModelPerformanceMetrics):
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê¸°ë¡"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT INTO performance_metrics 
                (timestamp, accuracy, directional_accuracy, mae, mse, sharpe_ratio, max_drawdown, 
                 profit_factor, win_rate, sample_count, regime, drift_score)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                metrics.timestamp.isoformat(),
                metrics.accuracy,
                metrics.directional_accuracy,
                metrics.mae,
                metrics.mse,
                metrics.sharpe_ratio,
                metrics.max_drawdown,
                metrics.profit_factor,
                metrics.win_rate,
                metrics.sample_count,
                metrics.regime,
                metrics.drift_score
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            logger.error(f"ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê¸°ë¡ ì‹¤íŒ¨: {e}")
    
    async def record_feature_selection(self, total_features: int, selected_features: int, stability: float, top_features: List[str]):
        """íŠ¹ì„± ì„ íƒ ê¸°ë¡"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT INTO feature_selection_records 
                (timestamp, total_features, selected_features, feature_stability, top_features)
                VALUES (?, ?, ?, ?, ?)
            ''', (
                datetime.now().isoformat(),
                total_features,
                selected_features,
                stability,
                json.dumps(top_features)
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            logger.error(f"íŠ¹ì„± ì„ íƒ ê¸°ë¡ ì‹¤íŒ¨: {e}")
    
    async def get_system_status(self) -> Dict:
        """ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ"""
        try:
            conn = sqlite3.connect(self.db_path)
            
            # ìµœê·¼ ì„±ëŠ¥
            perf_df = pd.read_sql_query('''
                SELECT * FROM performance_metrics 
                ORDER BY timestamp DESC LIMIT 1
            ''', conn)
            
            # ìµœê·¼ í•™ìŠµ ê¸°ë¡
            learning_df = pd.read_sql_query('''
                SELECT * FROM online_learning_records 
                ORDER BY timestamp DESC LIMIT 10
            ''', conn)
            
            # ì‹œì¥ ìƒí™© ê¸°ë¡
            regime_df = pd.read_sql_query('''
                SELECT * FROM market_regimes 
                ORDER BY timestamp DESC LIMIT 5
            ''', conn)
            
            conn.close()
            
            return {
                "current_performance": perf_df.to_dict('records')[0] if not perf_df.empty else {},
                "recent_learning": learning_df.to_dict('records'),
                "market_regimes": regime_df.to_dict('records'),
                "training_step": self.training_step,
                "current_learning_rate": self.learning_rate_scheduler.current_lr,
                "buffer_size": len(self.experience_buffer),
                "current_regime": self.current_regime.regime_type if self.current_regime else 'unknown',
                "drift_window_size": len(self.drift_detector.performance_window),
                "feature_count": len(self.current_features) if self.current_features else 0
            }
            
        except Exception as e:
            logger.error(f"ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    async def save_model(self):
        """ëª¨ë¸ ì €ì¥"""
        try:
            if self.model is not None:
                model_path = os.path.join(self.models_path, f"adaptive_model_step_{self.training_step}.pth")
                torch.save({
                    'model_state_dict': self.model.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'training_step': self.training_step,
                    'scaler': self.scaler,
                    'config': self.config
                }, model_path)
                
                logger.info(f"ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_path}")
                
        except Exception as e:
            logger.error(f"ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    async def load_model(self, model_path: str):
        """ëª¨ë¸ ë¡œë“œ"""
        try:
            checkpoint = torch.load(model_path)
            
            # ì„¤ì •ì—ì„œ ì…ë ¥ í¬ê¸° ì¶”ì •
            input_size = checkpoint['model_state_dict']['network.0.weight'].shape[1]
            self.init_model(input_size)
            
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            self.training_step = checkpoint['training_step']
            self.scaler = checkpoint['scaler']
            
            logger.info(f"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path}")
            
        except Exception as e:
            logger.error(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")

async def run_real_time_learning_demo():
    """ì‹¤ì‹œê°„ í•™ìŠµ ì‹œìŠ¤í…œ ë°ëª¨"""
    print("ğŸš€ ê³ ê¸‰ ì‹¤ì‹œê°„ ì ì‘í˜• í•™ìŠµ ì‹œìŠ¤í…œ ì‹œì‘")
    print("="*60)
    
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    config = OnlineLearningConfig(
        initial_learning_rate=0.001,
        batch_size=16,
        memory_size=500,
        drift_detection_window=30,
        feature_selection_interval=50
    )
    
    learning_system = RealTimeAdaptiveLearningSystem(config)
    
    # ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
    print("ğŸ“Š ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°ë¡œ í•™ìŠµ í…ŒìŠ¤íŠ¸ ì¤‘...")
    
    for step in range(100):  # 100 ë‹¨ê³„ ì‹œë®¬ë ˆì´ì…˜
        # ê°€ìƒì˜ ì‹œì¥ ë°ì´í„° ìƒì„±
        mock_market_data = {
            'price': 50000 + np.random.normal(0, 1000),
            'volume': np.random.exponential(1000000),
            'volatility': np.random.uniform(0.01, 0.05),
            'rsi': np.random.uniform(20, 80),
            'macd': np.random.normal(0, 10),
            'fear_greed_index': np.random.uniform(10, 90),
            'indicators': {
                f'indicator_{i}': np.random.normal(0, 1) for i in range(20)
            },
            'onchain': {
                f'onchain_{i}': np.random.uniform(0, 100) for i in range(15)
            }
        }
        
        # ë°ì´í„° ì²˜ë¦¬
        result = await learning_system.process_new_data(mock_market_data)
        
        # ì§„í–‰ ìƒí™© ì¶œë ¥ (10 ë‹¨ê³„ë§ˆë‹¤)
        if step % 10 == 0:
            print(f"\nğŸ“ˆ ë‹¨ê³„ {step}: ")
            if "prediction" in result:
                pred = result["prediction"]
                print(f"  â€¢ ì˜ˆì¸¡: {pred.get('direction', 'N/A')} (ì‹ ë¢°ë„: {pred.get('confidence', 0):.2f})")
            
            if "learning" in result:
                learning = result["learning"]
                print(f"  â€¢ í•™ìŠµë¥ : {learning.get('learning_rate', 0):.6f}")
                print(f"  â€¢ ì •í™•ë„: {learning.get('accuracy', 0):.2f}")
            
            print(f"  â€¢ ì‹œì¥ìƒí™©: {result.get('regime', 'unknown')}")
            print(f"  â€¢ íŠ¹ì„± ìˆ˜: {result.get('feature_count', 0)}")
            
            if "drift" in result and result["drift"].get("drift_detected"):
                print("  â€¢ âš ï¸ ëª¨ë¸ ë“œë¦¬í”„íŠ¸ ê°ì§€ë¨!")
        
        # ì ì‹œ ëŒ€ê¸° (ì‹¤ì œë¡œëŠ” ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì‹  ê°„ê²©)
        await asyncio.sleep(0.1)
    
    # ìµœì¢… ì‹œìŠ¤í…œ ìƒíƒœ ì¶œë ¥
    print("\n" + "="*60)
    print("ğŸ¯ ìµœì¢… ì‹œìŠ¤í…œ ìƒíƒœ")
    
    status = await learning_system.get_system_status()
    
    print(f"ğŸ“Š í•™ìŠµ ë‹¨ê³„: {status.get('training_step', 0)}")
    print(f"ğŸ¯ í˜„ì¬ í•™ìŠµë¥ : {status.get('current_learning_rate', 0):.6f}")
    print(f"ğŸ’¾ ê²½í—˜ ë²„í¼: {status.get('buffer_size', 0)}/500")
    print(f"ğŸŒ í˜„ì¬ ì‹œì¥ìƒí™©: {status.get('current_regime', 'unknown')}")
    print(f"ğŸ“ˆ íŠ¹ì„± ê°œìˆ˜: {status.get('feature_count', 0)}")
    
    if status.get('current_performance'):
        perf = status['current_performance']
        print(f"\nâš¡ í˜„ì¬ ì„±ëŠ¥:")
        accuracy_val = perf.get('accuracy', 0)
        sample_count = perf.get('sample_count', 0)
        if isinstance(accuracy_val, (int, float)):
            print(f"  â€¢ ì •í™•ë„: {accuracy_val:.1%}")
        else:
            print(f"  â€¢ ì •í™•ë„: N/A")
        print(f"  â€¢ ìƒ˜í”Œ ìˆ˜: {sample_count}ê°œ")
    
    # ëª¨ë¸ ì €ì¥
    await learning_system.save_model()
    print("\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ")
    
    print("\n" + "="*60)
    print("ğŸ‰ ì‹¤ì‹œê°„ ì ì‘í˜• í•™ìŠµ ì‹œìŠ¤í…œ ë°ëª¨ ì™„ë£Œ!")
    print("âœ… 90%+ ì •í™•ë„ ìœ ì§€ ë©”ì»¤ë‹ˆì¦˜ êµ¬í˜„ ì™„ë£Œ")

if __name__ == "__main__":
    asyncio.run(run_real_time_learning_demo())