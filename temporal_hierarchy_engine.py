#!/usr/bin/env python3
"""
ğŸ¯ Temporal Hierarchy Modeling Engine
ì‹œê°„ ê³„ì¸µ ëª¨ë¸ë§ ì—”ì§„ - ì¥/ì¤‘/ë‹¨ê¸° íŠ¸ë Œë“œ ë¶„ì„ ë° ë…¸ì´ì¦ˆ í•„í„°ë§

ì£¼ìš” ê¸°ëŠ¥:
1. Multi-Scale Temporal Analysis - ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ì‹œê°„ ë¶„ì„
2. Noise Filtering - ê³ ê¸‰ ë…¸ì´ì¦ˆ í•„í„°ë§
3. Cross-Horizon Consistency - ì‹œê°„ëŒ€ ê°„ ì¼ê´€ì„± ì œì•½
4. Hierarchical Feature Extraction - ê³„ì¸µì  íŠ¹ì„± ì¶”ì¶œ
5. Adaptive Time Weighting - ì ì‘í˜• ì‹œê°„ ê°€ì¤‘ì¹˜
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional
from scipy import signal
from scipy.stats import pearsonr
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import logging
from datetime import datetime, timedelta
from dataclasses import dataclass
import json
import matplotlib.pyplot as plt

@dataclass
class TemporalLevel:
    """ì‹œê°„ ê³„ì¸µ ë ˆë²¨ ì •ì˜"""
    name: str
    window_size: int
    sampling_rate: int
    weight: float
    noise_threshold: float

class NoiseFilter:
    """ê³ ê¸‰ ë…¸ì´ì¦ˆ í•„í„°"""
    
    def __init__(self):
        self.filter_methods = {
            'savgol': self._savitzky_golay_filter,
            'butterworth': self._butterworth_filter,
            'kalman': self._simple_kalman_filter,
            'wavelet': self._wavelet_denoise,
            'median': self._median_filter
        }
    
    def _savitzky_golay_filter(self, data: np.ndarray, window: int = 11, poly_order: int = 3) -> np.ndarray:
        """Savitzky-Golay í•„í„°"""
        if len(data) < window:
            return data
        return signal.savgol_filter(data, window, poly_order)
    
    def _butterworth_filter(self, data: np.ndarray, cutoff: float = 0.1, order: int = 4) -> np.ndarray:
        """Butterworth ì €ì—­ í†µê³¼ í•„í„°"""
        if len(data) < 10:
            return data
        
        nyquist = 0.5
        normal_cutoff = cutoff / nyquist
        
        b, a = signal.butter(order, normal_cutoff, btype='low')
        filtered_data = signal.filtfilt(b, a, data)
        
        return filtered_data
    
    def _simple_kalman_filter(self, data: np.ndarray) -> np.ndarray:
        """ê°„ë‹¨í•œ ì¹¼ë§Œ í•„í„°"""
        if len(data) == 0:
            return data
        
        # ì¹¼ë§Œ í•„í„° ë§¤ê°œë³€ìˆ˜
        Q = 1e-5  # í”„ë¡œì„¸ìŠ¤ ë…¸ì´ì¦ˆ ë¶„ì‚°
        R = 0.1   # ì¸¡ì • ë…¸ì´ì¦ˆ ë¶„ì‚°
        
        # ì´ˆê¸°ê°’
        x_hat = data[0]  # ì´ˆê¸° ì¶”ì •ê°’
        P = 1.0          # ì´ˆê¸° ì˜¤ì°¨ ê³µë¶„ì‚°
        
        filtered_data = np.zeros_like(data)
        
        for i, measurement in enumerate(data):
            # ì˜ˆì¸¡ ë‹¨ê³„
            x_hat_minus = x_hat
            P_minus = P + Q
            
            # ì—…ë°ì´íŠ¸ ë‹¨ê³„
            K = P_minus / (P_minus + R)  # ì¹¼ë§Œ ê²Œì¸
            x_hat = x_hat_minus + K * (measurement - x_hat_minus)
            P = (1 - K) * P_minus
            
            filtered_data[i] = x_hat
        
        return filtered_data
    
    def _wavelet_denoise(self, data: np.ndarray) -> np.ndarray:
        """ì›¨ì´ë¸”ë¦¿ ë””ë…¸ì´ì¦ˆ (ë‹¨ìˆœ ë²„ì „)"""
        # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ ì´ë™í‰ê· ìœ¼ë¡œ ëŒ€ì²´
        window = min(5, len(data) // 4)
        if window <= 1:
            return data
        
        return np.convolve(data, np.ones(window)/window, mode='same')
    
    def _median_filter(self, data: np.ndarray, window: int = 5) -> np.ndarray:
        """ë¯¸ë””ì–¸ í•„í„°"""
        if len(data) < window:
            return data
        return signal.medfilt(data, kernel_size=window)
    
    def apply_adaptive_filter(self, data: np.ndarray, volatility: float) -> np.ndarray:
        """ë³€ë™ì„±ì— ë”°ë¥¸ ì ì‘í˜• í•„í„°ë§"""
        if volatility > 0.1:  # ê³ ë³€ë™ì„±
            # ê°•í•œ í•„í„°ë§
            filtered = self._butterworth_filter(data, cutoff=0.05)
            filtered = self._median_filter(filtered, window=7)
        elif volatility > 0.05:  # ì¤‘ê°„ ë³€ë™ì„±
            # ì¤‘ê°„ í•„í„°ë§
            filtered = self._savitzky_golay_filter(data)
        else:  # ì €ë³€ë™ì„±
            # ê°€ë²¼ìš´ í•„í„°ë§
            filtered = self._simple_kalman_filter(data)
        
        return filtered

class TrendAnalyzer:
    """íŠ¸ë Œë“œ ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.trend_methods = ['linear', 'polynomial', 'exponential']
    
    def detect_trend_strength(self, data: np.ndarray) -> Dict[str, float]:
        """íŠ¸ë Œë“œ ê°•ë„ ê°ì§€"""
        if len(data) < 2:
            return {'strength': 0.0, 'direction': 0, 'consistency': 0.0}
        
        x = np.arange(len(data))
        
        # ì„ í˜• íŠ¸ë Œë“œ
        linear_coef = np.polyfit(x, data, 1)[0]
        linear_pred = np.polyval(np.polyfit(x, data, 1), x)
        linear_r2 = 1 - np.sum((data - linear_pred)**2) / np.sum((data - np.mean(data))**2)
        
        # ë‹¤í•­ì‹ íŠ¸ë Œë“œ (2ì°¨)
        if len(data) >= 3:
            poly_coef = np.polyfit(x, data, 2)
            poly_pred = np.polyval(poly_coef, x)
            poly_r2 = 1 - np.sum((data - poly_pred)**2) / np.sum((data - np.mean(data))**2)
        else:
            poly_r2 = linear_r2
        
        # íŠ¸ë Œë“œ ê°•ë„ (RÂ² ê¸°ë°˜)
        trend_strength = max(0, max(linear_r2, poly_r2))
        
        # íŠ¸ë Œë“œ ë°©í–¥
        trend_direction = 1 if linear_coef > 0 else -1 if linear_coef < 0 else 0
        
        # íŠ¸ë Œë“œ ì¼ê´€ì„± (ë³€ê³¡ì  ê°œìˆ˜ ê¸°ë°˜)
        diff_sign_changes = np.sum(np.diff(np.sign(np.diff(data))) != 0)
        consistency = max(0, 1 - diff_sign_changes / max(1, len(data) - 2))
        
        return {
            'strength': float(trend_strength),
            'direction': int(trend_direction),
            'consistency': float(consistency),
            'linear_slope': float(linear_coef),
            'r2_score': float(trend_strength)
        }
    
    def decompose_trend_seasonality(self, data: np.ndarray, period: int = 24) -> Dict[str, np.ndarray]:
        """íŠ¸ë Œë“œ-ê³„ì ˆì„± ë¶„í•´"""
        if len(data) < period * 2:
            return {
                'trend': data.copy(),
                'seasonal': np.zeros_like(data),
                'residual': np.zeros_like(data)
            }
        
        # ë‹¨ìˆœ ì´ë™ í‰ê· ìœ¼ë¡œ íŠ¸ë Œë“œ ì¶”ì¶œ
        trend = np.zeros_like(data, dtype=float)
        half_period = period // 2
        
        for i in range(len(data)):
            start = max(0, i - half_period)
            end = min(len(data), i + half_period + 1)
            trend[i] = np.mean(data[start:end])
        
        # ê³„ì ˆì„± ì„±ë¶„ (íŠ¸ë Œë“œ ì œê±° í›„ ì£¼ê¸°ì„± ì¶”ì¶œ)
        detrended = data - trend
        seasonal = np.zeros_like(data, dtype=float)
        
        for i in range(len(data)):
            seasonal_indices = list(range(i % period, len(data), period))
            if len(seasonal_indices) > 1:
                seasonal[i] = np.mean(detrended[seasonal_indices])
        
        # ì”ì°¨
        residual = data - trend - seasonal
        
        return {
            'trend': trend,
            'seasonal': seasonal,
            'residual': residual
        }

class CrossHorizonConsistency:
    """ì‹œê°„ëŒ€ ê°„ ì¼ê´€ì„± ì œì•½"""
    
    def __init__(self):
        self.consistency_weights = {
            (1, 4): 0.8,    # 1h-4h ë†’ì€ ì¼ê´€ì„±
            (4, 24): 0.7,   # 4h-24h ì¤‘ê°„ ì¼ê´€ì„±
            (24, 72): 0.6,  # 24h-72h ì¤‘ê°„ ì¼ê´€ì„±
            (72, 168): 0.5  # 72h-168h ë‚®ì€ ì¼ê´€ì„±
        }
    
    def compute_consistency_score(self, predictions: Dict[int, float]) -> float:
        """ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚°"""
        if len(predictions) < 2:
            return 1.0
        
        sorted_horizons = sorted(predictions.keys())
        consistency_scores = []
        
        for i in range(len(sorted_horizons) - 1):
            h1, h2 = sorted_horizons[i], sorted_horizons[i + 1]
            
            # ì˜ˆì¸¡ê°’ ì°¨ì´ ì •ê·œí™”
            pred_diff = abs(predictions[h1] - predictions[h2])
            max_pred = max(abs(predictions[h1]), abs(predictions[h2]))
            
            if max_pred > 0:
                normalized_diff = pred_diff / max_pred
                consistency = max(0, 1 - normalized_diff)
            else:
                consistency = 1.0
            
            # ì‹œê°„ëŒ€ ê°€ì¤‘ì¹˜ ì ìš©
            weight = self.consistency_weights.get((h1, h2), 0.5)
            weighted_consistency = consistency * weight
            
            consistency_scores.append(weighted_consistency)
        
        return np.mean(consistency_scores) if consistency_scores else 1.0
    
    def apply_consistency_constraint(self, predictions: Dict[int, float], 
                                   constraint_strength: float = 0.5) -> Dict[int, float]:
        """ì¼ê´€ì„± ì œì•½ ì ìš©"""
        if len(predictions) < 2:
            return predictions
        
        sorted_horizons = sorted(predictions.keys())
        adjusted_predictions = predictions.copy()
        
        # ì¸ì ‘í•œ ì‹œê°„ëŒ€ ê°„ ì¡°ì •
        for i in range(len(sorted_horizons) - 1):
            h1, h2 = sorted_horizons[i], sorted_horizons[i + 1]
            
            pred1, pred2 = adjusted_predictions[h1], adjusted_predictions[h2]
            
            # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ì¡°ì •
            weight = self.consistency_weights.get((h1, h2), 0.5) * constraint_strength
            
            adjusted_pred1 = pred1 * (1 - weight) + pred2 * weight
            adjusted_pred2 = pred2 * (1 - weight) + pred1 * weight
            
            adjusted_predictions[h1] = adjusted_pred1
            adjusted_predictions[h2] = adjusted_pred2
        
        return adjusted_predictions

class HierarchicalFeatureExtractor:
    """ê³„ì¸µì  íŠ¹ì„± ì¶”ì¶œê¸°"""
    
    def __init__(self):
        self.scalers = {}
        self.pca_models = {}
    
    def extract_multi_scale_features(self, data: np.ndarray, levels: List[TemporalLevel]) -> Dict[str, np.ndarray]:
        """ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ íŠ¹ì„± ì¶”ì¶œ"""
        features = {}
        
        for level in levels:
            level_features = self._extract_level_features(data, level)
            features[level.name] = level_features
        
        return features
    
    def _extract_level_features(self, data: np.ndarray, level: TemporalLevel) -> np.ndarray:
        """íŠ¹ì • ë ˆë²¨ì˜ íŠ¹ì„± ì¶”ì¶œ"""
        if len(data) < level.window_size:
            return np.array([])
        
        # ì‹œê°„ ìœˆë„ìš°ë³„ ìƒ˜í”Œë§
        sampled_data = data[::level.sampling_rate]
        
        if len(sampled_data) < level.window_size:
            return np.array([])
        
        features = []
        
        for i in range(len(sampled_data) - level.window_size + 1):
            window_data = sampled_data[i:i + level.window_size]
            
            # í†µê³„ì  íŠ¹ì„±
            window_features = [
                np.mean(window_data),           # í‰ê· 
                np.std(window_data),            # í‘œì¤€í¸ì°¨
                np.min(window_data),            # ìµœì†Ÿê°’
                np.max(window_data),            # ìµœëŒ“ê°’
                np.median(window_data),         # ì¤‘ì•™ê°’
                np.percentile(window_data, 25), # 1ì‚¬ë¶„ìœ„ìˆ˜
                np.percentile(window_data, 75), # 3ì‚¬ë¶„ìœ„ìˆ˜
            ]
            
            # íŠ¸ë Œë“œ íŠ¹ì„±
            if len(window_data) > 1:
                slope = np.polyfit(range(len(window_data)), window_data, 1)[0]
                window_features.append(slope)
            else:
                window_features.append(0.0)
            
            # ë³€í™”ìœ¨ íŠ¹ì„±
            if len(window_data) > 1:
                returns = np.diff(window_data) / window_data[:-1]
                window_features.extend([
                    np.mean(returns),
                    np.std(returns),
                    np.sum(returns > 0) / len(returns),  # ìƒìŠ¹ ë¹„ìœ¨
                ])
            else:
                window_features.extend([0.0, 0.0, 0.5])
            
            features.append(window_features)
        
        return np.array(features)
    
    def combine_hierarchical_features(self, level_features: Dict[str, np.ndarray], 
                                    target_length: int) -> np.ndarray:
        """ê³„ì¸µì  íŠ¹ì„± ê²°í•©"""
        combined_features = []
        
        for level_name, features in level_features.items():
            if len(features) == 0:
                continue
            
            # ê¸¸ì´ ë§ì¶¤ (ë³´ê°„ ë˜ëŠ” ìƒ˜í”Œë§)
            if len(features) > target_length:
                # ë‹¤ìš´ìƒ˜í”Œë§
                indices = np.linspace(0, len(features) - 1, target_length, dtype=int)
                resampled = features[indices]
            elif len(features) < target_length:
                # ì—…ìƒ˜í”Œë§ (ë§ˆì§€ë§‰ ê°’ ë°˜ë³µ)
                resampled = np.zeros((target_length, features.shape[1]))
                resampled[:len(features)] = features
                resampled[len(features):] = features[-1] if len(features) > 0 else 0
            else:
                resampled = features
            
            # ì •ê·œí™”
            if level_name not in self.scalers:
                self.scalers[level_name] = StandardScaler()
                normalized = self.scalers[level_name].fit_transform(resampled)
            else:
                normalized = self.scalers[level_name].transform(resampled)
            
            combined_features.append(normalized)
        
        if combined_features:
            return np.concatenate(combined_features, axis=1)
        else:
            return np.array([])

class TemporalHierarchyEngine:
    """ì‹œê°„ ê³„ì¸µ ëª¨ë¸ë§ ì—”ì§„"""
    
    def __init__(self):
        # ì‹œê°„ ê³„ì¸µ ë ˆë²¨ ì •ì˜
        self.temporal_levels = [
            TemporalLevel("short_term", window_size=24, sampling_rate=1, weight=0.2, noise_threshold=0.05),
            TemporalLevel("medium_term", window_size=72, sampling_rate=3, weight=0.3, noise_threshold=0.03),
            TemporalLevel("long_term", window_size=168, sampling_rate=6, weight=0.5, noise_threshold=0.02),
        ]
        
        # êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™”
        self.noise_filter = NoiseFilter()
        self.trend_analyzer = TrendAnalyzer()
        self.consistency_checker = CrossHorizonConsistency()
        self.feature_extractor = HierarchicalFeatureExtractor()
        
        # ìƒíƒœ ì €ì¥
        self.analysis_cache = {}
        self.setup_logging()
    
    def setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
    
    def analyze_temporal_hierarchy(self, data: np.ndarray) -> Dict:
        """ì™„ì „í•œ ì‹œê°„ ê³„ì¸µ ë¶„ì„"""
        self.logger.info(f"ğŸ•’ ì‹œê°„ ê³„ì¸µ ë¶„ì„ ì‹œì‘ - ë°ì´í„° ê¸¸ì´: {len(data)}")
        
        if len(data) < 24:
            self.logger.warning("ë°ì´í„°ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤ (24ì‹œê°„ ë¯¸ë§Œ)")
            return self._create_empty_analysis()
        
        # 1. ë…¸ì´ì¦ˆ í•„í„°ë§
        filtered_data = self._apply_hierarchical_filtering(data)
        
        # 2. ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ íŠ¸ë Œë“œ ë¶„ì„
        trend_analysis = self._multi_scale_trend_analysis(filtered_data)
        
        # 3. ê³„ì¸µì  íŠ¹ì„± ì¶”ì¶œ
        hierarchical_features = self._extract_hierarchical_features(filtered_data)
        
        # 4. ì‹œê°„ëŒ€ ê°„ ì¼ê´€ì„± ë¶„ì„
        consistency_analysis = self._analyze_cross_horizon_consistency(filtered_data)
        
        # 5. ì ì‘í˜• ê°€ì¤‘ì¹˜ ê³„ì‚°
        adaptive_weights = self._compute_adaptive_weights(trend_analysis, consistency_analysis)
        
        # 6. ë…¸ì´ì¦ˆ ìˆ˜ì¤€ í‰ê°€
        noise_analysis = self._evaluate_noise_levels(data, filtered_data)
        
        analysis_result = {
            'timestamp': datetime.now().isoformat(),
            'data_length': len(data),
            'filtered_data': filtered_data.tolist(),
            'trend_analysis': trend_analysis,
            'hierarchical_features': hierarchical_features,
            'consistency_analysis': consistency_analysis,
            'adaptive_weights': adaptive_weights,
            'noise_analysis': noise_analysis,
            'temporal_levels': [
                {
                    'name': level.name,
                    'window_size': level.window_size,
                    'sampling_rate': level.sampling_rate,
                    'weight': level.weight
                } for level in self.temporal_levels
            ]
        }
        
        self.logger.info("âœ… ì‹œê°„ ê³„ì¸µ ë¶„ì„ ì™„ë£Œ")
        return analysis_result
    
    def _apply_hierarchical_filtering(self, data: np.ndarray) -> np.ndarray:
        """ê³„ì¸µì  ë…¸ì´ì¦ˆ í•„í„°ë§"""
        # ë³€ë™ì„± ê³„ì‚°
        volatility = np.std(data) / np.mean(np.abs(data)) if np.mean(np.abs(data)) > 0 else 0
        
        # ì ì‘í˜• í•„í„°ë§ ì ìš©
        filtered = self.noise_filter.apply_adaptive_filter(data, volatility)
        
        return filtered
    
    def _multi_scale_trend_analysis(self, data: np.ndarray) -> Dict:
        """ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ íŠ¸ë Œë“œ ë¶„ì„"""
        trend_results = {}
        
        for level in self.temporal_levels:
            if len(data) >= level.window_size:
                # í•´ë‹¹ ë ˆë²¨ì˜ ë°ì´í„° ì¶”ì¶œ
                level_data = data[-level.window_size::level.sampling_rate]
                
                # íŠ¸ë Œë“œ ë¶„ì„
                trend_info = self.trend_analyzer.detect_trend_strength(level_data)
                
                # íŠ¸ë Œë“œ-ê³„ì ˆì„± ë¶„í•´
                decomposition = self.trend_analyzer.decompose_trend_seasonality(
                    level_data, period=min(24, len(level_data) // 4)
                )
                
                trend_results[level.name] = {
                    'trend_strength': trend_info,
                    'decomposition': {
                        'trend_component': decomposition['trend'].tolist()[-10:],  # ìµœê·¼ 10ê°œë§Œ
                        'seasonal_component': decomposition['seasonal'].tolist()[-10:],
                        'residual_component': decomposition['residual'].tolist()[-10:],
                    },
                    'volatility': float(np.std(level_data)),
                    'mean_level': float(np.mean(level_data))
                }
        
        return trend_results
    
    def _extract_hierarchical_features(self, data: np.ndarray) -> Dict:
        """ê³„ì¸µì  íŠ¹ì„± ì¶”ì¶œ"""
        # ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ íŠ¹ì„± ì¶”ì¶œ
        level_features = self.feature_extractor.extract_multi_scale_features(data, self.temporal_levels)
        
        # íŠ¹ì„± í†µê³„
        feature_stats = {}
        for level_name, features in level_features.items():
            if len(features) > 0:
                feature_stats[level_name] = {
                    'feature_count': int(features.shape[1]) if features.ndim > 1 else 1,
                    'sample_count': int(features.shape[0]) if features.ndim > 0 else 0,
                    'mean_values': np.mean(features, axis=0).tolist() if features.ndim > 1 else [float(np.mean(features))],
                    'std_values': np.std(features, axis=0).tolist() if features.ndim > 1 else [float(np.std(features))]
                }
            else:
                feature_stats[level_name] = {
                    'feature_count': 0,
                    'sample_count': 0,
                    'mean_values': [],
                    'std_values': []
                }
        
        return feature_stats
    
    def _analyze_cross_horizon_consistency(self, data: np.ndarray) -> Dict:
        """ì‹œê°„ëŒ€ ê°„ ì¼ê´€ì„± ë¶„ì„"""
        if len(data) < 168:  # ìµœì†Œ 1ì£¼ì¼ ë°ì´í„° í•„ìš”
            return {'consistency_score': 1.0, 'horizon_correlations': {}}
        
        horizons = [1, 4, 24, 72, 168]
        predictions = {}
        
        # ê° ì‹œê°„ëŒ€ì˜ ë‹¨ìˆœ ì˜ˆì¸¡ ìƒì„± (íŠ¸ë Œë“œ ê¸°ë°˜)
        for horizon in horizons:
            if len(data) >= horizon + 24:  # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°
                recent_data = data[-24:]  # ìµœê·¼ 24ì‹œê°„
                trend_slope = np.polyfit(range(len(recent_data)), recent_data, 1)[0]
                predictions[horizon] = float(data[-1] + trend_slope * horizon)
        
        # ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚°
        consistency_score = self.consistency_checker.compute_consistency_score(predictions)
        
        # ì‹œê°„ëŒ€ ê°„ ìƒê´€ê´€ê³„ ë¶„ì„
        horizon_correlations = {}
        for i, h1 in enumerate(horizons):
            for h2 in horizons[i+1:]:
                if len(data) >= max(h1, h2) + 24:
                    data1 = data[:-max(h1, h2)] if max(h1, h2) < len(data) else data[:1]
                    data2 = data[h2-h1:] if h2 > h1 and h2-h1 < len(data) else data[:1]
                    
                    if len(data1) > 1 and len(data2) > 1 and len(data1) == len(data2):
                        corr, _ = pearsonr(data1, data2)
                        horizon_correlations[f'{h1}h_{h2}h'] = float(corr) if not np.isnan(corr) else 0.0
        
        return {
            'consistency_score': consistency_score,
            'horizon_correlations': horizon_correlations,
            'predictions': predictions
        }
    
    def _compute_adaptive_weights(self, trend_analysis: Dict, consistency_analysis: Dict) -> Dict[str, float]:
        """ì ì‘í˜• ê°€ì¤‘ì¹˜ ê³„ì‚°"""
        weights = {}
        
        for level in self.temporal_levels:
            base_weight = level.weight
            
            if level.name in trend_analysis:
                trend_strength = trend_analysis[level.name]['trend_strength']['strength']
                consistency_factor = consistency_analysis['consistency_score']
                
                # íŠ¸ë Œë“œ ê°•ë„ì™€ ì¼ê´€ì„±ì— ë”°ë¼ ê°€ì¤‘ì¹˜ ì¡°ì •
                adjusted_weight = base_weight * (0.5 + 0.5 * trend_strength) * (0.8 + 0.2 * consistency_factor)
                weights[level.name] = float(adjusted_weight)
            else:
                weights[level.name] = base_weight
        
        # ì •ê·œí™”
        total_weight = sum(weights.values())
        if total_weight > 0:
            weights = {k: v / total_weight for k, v in weights.items()}
        
        return weights
    
    def _evaluate_noise_levels(self, original_data: np.ndarray, filtered_data: np.ndarray) -> Dict:
        """ë…¸ì´ì¦ˆ ìˆ˜ì¤€ í‰ê°€"""
        if len(original_data) != len(filtered_data):
            return {'noise_level': 0.0, 'filter_effectiveness': 0.0}
        
        # ë…¸ì´ì¦ˆ ì¶”ì • (ì›ë³¸ - í•„í„°ë§ëœ ë°ì´í„°)
        noise = original_data - filtered_data
        
        # ë…¸ì´ì¦ˆ í†µê³„
        noise_level = float(np.std(noise))
        signal_level = float(np.std(filtered_data))
        
        # ì‹ í˜¸ ëŒ€ ë…¸ì´ì¦ˆ ë¹„ìœ¨
        snr = signal_level / noise_level if noise_level > 0 else float('inf')
        
        # í•„í„° íš¨ê³¼ì„±
        original_volatility = np.std(original_data)
        filtered_volatility = np.std(filtered_data)
        filter_effectiveness = (original_volatility - filtered_volatility) / original_volatility if original_volatility > 0 else 0
        
        return {
            'noise_level': noise_level,
            'signal_level': signal_level,
            'snr': float(snr) if snr != float('inf') else 1000.0,
            'filter_effectiveness': float(filter_effectiveness),
            'noise_distribution': {
                'mean': float(np.mean(noise)),
                'std': float(np.std(noise)),
                'skewness': float(self._calculate_skewness(noise)),
                'kurtosis': float(self._calculate_kurtosis(noise))
            }
        }
    
    def _calculate_skewness(self, data: np.ndarray) -> float:
        """ë¹„ëŒ€ì¹­ë„ ê³„ì‚°"""
        if len(data) < 3:
            return 0.0
        
        mean = np.mean(data)
        std = np.std(data, ddof=1)
        
        if std == 0:
            return 0.0
        
        skewness = np.mean(((data - mean) / std) ** 3)
        return skewness
    
    def _calculate_kurtosis(self, data: np.ndarray) -> float:
        """ì²¨ë„ ê³„ì‚°"""
        if len(data) < 4:
            return 0.0
        
        mean = np.mean(data)
        std = np.std(data, ddof=1)
        
        if std == 0:
            return 0.0
        
        kurtosis = np.mean(((data - mean) / std) ** 4) - 3
        return kurtosis
    
    def _create_empty_analysis(self) -> Dict:
        """ë¹ˆ ë¶„ì„ ê²°ê³¼ ìƒì„±"""
        return {
            'timestamp': datetime.now().isoformat(),
            'data_length': 0,
            'filtered_data': [],
            'trend_analysis': {},
            'hierarchical_features': {},
            'consistency_analysis': {'consistency_score': 0.0, 'horizon_correlations': {}},
            'adaptive_weights': {level.name: level.weight for level in self.temporal_levels},
            'noise_analysis': {'noise_level': 0.0, 'filter_effectiveness': 0.0},
            'temporal_levels': []
        }
    
    def optimize_predictions(self, raw_predictions: Dict[int, float], 
                           hierarchy_analysis: Dict) -> Dict[int, float]:
        """ê³„ì¸µ ë¶„ì„ ê²°ê³¼ë¡œ ì˜ˆì¸¡ ìµœì í™”"""
        if not raw_predictions:
            return raw_predictions
        
        # ì ì‘í˜• ê°€ì¤‘ì¹˜ ì ìš©
        adaptive_weights = hierarchy_analysis.get('adaptive_weights', {})
        
        # ì¼ê´€ì„± ì œì•½ ì ìš©
        optimized_predictions = self.consistency_checker.apply_consistency_constraint(
            raw_predictions, constraint_strength=0.3
        )
        
        # íŠ¸ë Œë“œ ë¶„ì„ ê²°ê³¼ ë°˜ì˜
        trend_analysis = hierarchy_analysis.get('trend_analysis', {})
        
        for horizon, prediction in optimized_predictions.items():
            horizon_name = self._get_horizon_level_name(horizon)
            
            if horizon_name in trend_analysis:
                trend_info = trend_analysis[horizon_name]['trend_strength']
                
                # íŠ¸ë Œë“œ ê°•ë„ì— ë”°ë¥¸ ì˜ˆì¸¡ ì¡°ì •
                if trend_info['strength'] > 0.7:  # ê°•í•œ íŠ¸ë Œë“œ
                    trend_adjustment = trend_info['linear_slope'] * horizon * 0.5
                    optimized_predictions[horizon] = prediction + trend_adjustment
        
        return optimized_predictions
    
    def _get_horizon_level_name(self, horizon_hours: int) -> str:
        """ì‹œê°„ëŒ€ë¥¼ ë ˆë²¨ëª…ìœ¼ë¡œ ë³€í™˜"""
        if horizon_hours <= 24:
            return "short_term"
        elif horizon_hours <= 72:
            return "medium_term"
        else:
            return "long_term"

def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    np.random.seed(42)
    time_points = 1000
    
    # í•©ì„± BTC ê°€ê²© ë°ì´í„° (íŠ¸ë Œë“œ + ë…¸ì´ì¦ˆ)
    trend = np.linspace(50000, 55000, time_points)
    seasonal = 1000 * np.sin(2 * np.pi * np.arange(time_points) / 24)  # 24ì‹œê°„ ì£¼ê¸°
    noise = np.random.normal(0, 500, time_points)
    synthetic_data = trend + seasonal + noise
    
    # ì—”ì§„ ì´ˆê¸°í™” ë° ë¶„ì„
    engine = TemporalHierarchyEngine()
    
    print("ğŸ•’ Temporal Hierarchy Modeling Engine Test")
    print("="*60)
    
    # ê³„ì¸µ ë¶„ì„ ì‹¤í–‰
    analysis_result = engine.analyze_temporal_hierarchy(synthetic_data)
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"ğŸ“Š ë¶„ì„ ì™„ë£Œ:")
    print(f"  ë°ì´í„° ê¸¸ì´: {analysis_result['data_length']}")
    print(f"  ì¼ê´€ì„± ì ìˆ˜: {analysis_result['consistency_analysis']['consistency_score']:.3f}")
    print(f"  ë…¸ì´ì¦ˆ ë ˆë²¨: {analysis_result['noise_analysis']['noise_level']:.2f}")
    print(f"  í•„í„° íš¨ê³¼ì„±: {analysis_result['noise_analysis']['filter_effectiveness']:.3f}")
    
    print(f"\nğŸ¯ ì‹œê°„ ê³„ì¸µë³„ íŠ¸ë Œë“œ ë¶„ì„:")
    for level_name, trend_data in analysis_result['trend_analysis'].items():
        strength = trend_data['trend_strength']['strength']
        direction = "ìƒìŠ¹" if trend_data['trend_strength']['direction'] > 0 else "í•˜ë½" if trend_data['trend_strength']['direction'] < 0 else "íš¡ë³´"
        print(f"  {level_name}: {direction} íŠ¸ë Œë“œ (ê°•ë„: {strength:.3f})")
    
    print(f"\nâš–ï¸ ì ì‘í˜• ê°€ì¤‘ì¹˜:")
    for level_name, weight in analysis_result['adaptive_weights'].items():
        print(f"  {level_name}: {weight:.3f}")
    
    # ì˜ˆì¸¡ ìµœì í™” í…ŒìŠ¤íŠ¸
    raw_predictions = {1: 55100, 4: 55200, 24: 55500, 72: 56000, 168: 56500}
    optimized_predictions = engine.optimize_predictions(raw_predictions, analysis_result)
    
    print(f"\nğŸ¯ ì˜ˆì¸¡ ìµœì í™” ê²°ê³¼:")
    for horizon in sorted(raw_predictions.keys()):
        raw = raw_predictions[horizon]
        opt = optimized_predictions[horizon]
        change = opt - raw
        print(f"  {horizon}h: ${raw:,.0f} â†’ ${opt:,.0f} ({change:+.0f})")
    
    # ê²°ê³¼ ì €ì¥
    with open('temporal_hierarchy_analysis.json', 'w', encoding='utf-8') as f:
        json.dump(analysis_result, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ ë¶„ì„ ê²°ê³¼ ì €ì¥: temporal_hierarchy_analysis.json")
    
    return analysis_result

if __name__ == "__main__":
    main()