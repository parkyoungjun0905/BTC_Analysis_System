#!/usr/bin/env python3
"""
ë³€ë™ì„± ì²´ì œ ë¶„ì„ ë° ì „í™˜ í™•ë¥  ëª¨ë¸ë§ ì‹œìŠ¤í…œ
Bitcoin ë³€ë™ì„± íŒ¨í„´ì„ ë¶„ì„í•˜ì—¬ ì²´ì œ ì „í™˜ì„ ì˜ˆì¸¡í•˜ê³  í™•ë¥ ì„ ëª¨ë¸ë§

í•µì‹¬ ê¸°ëŠ¥:
1. ë³€ë™ì„± í´ëŸ¬ìŠ¤í„°ë§ ê°ì§€ (GARCH ëª¨ë¸)
2. ë³€ë™ì„± ì²´ì œ ì „í™˜ì  ì‹ë³„
3. ì²´ì œ ì§€ì† ê¸°ê°„ ì˜ˆì¸¡ 
4. ë³€ë™ì„± ì¶©ê²© ì „íŒŒ ëª¨ë¸ë§
5. ë¦¬ìŠ¤í¬ ì¡°ì • í¬ì§€ì…˜ ì‚¬ì´ì§•
"""

import os
import json
import sqlite3
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Union
import logging
from dataclasses import dataclass, asdict
import asyncio
import pickle
from collections import defaultdict, deque
import statistics
import warnings
warnings.filterwarnings("ignore")

# ì‹œê³„ì—´ ë° ë³€ë™ì„± ëª¨ë¸ë§
from scipy import stats
from scipy.signal import find_peaks, argrelextrema
from scipy.optimize import minimize
import sklearn.cluster as cluster
from sklearn.preprocessing import StandardScaler
from sklearn.mixture import GaussianMixture

# ê¸ˆìœµ ì‹œê³„ì—´ ë¶„ì„
try:
    from arch import arch_model
    from arch.unitroot import ADF
    ARCH_AVAILABLE = True
except ImportError:
    ARCH_AVAILABLE = False
    logger.warning("ARCH ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. GARCH ëª¨ë¸ ëŒ€ì‹  ê°„ë‹¨í•œ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class VolatilityRegime:
    """ë³€ë™ì„± ì²´ì œ ì •ë³´"""
    regime_id: int
    regime_name: str
    start_date: datetime
    end_date: Optional[datetime]
    duration_days: int
    avg_volatility: float
    volatility_std: float
    max_volatility: float
    min_volatility: float
    volatility_persistence: float
    clustering_coefficient: float
    regime_confidence: float

@dataclass
class RegimeTransitionProbability:
    """ì²´ì œ ì „í™˜ í™•ë¥ """
    from_regime: str
    to_regime: str
    base_probability: float
    conditional_probabilities: Dict[str, float]
    trigger_conditions: List[str]
    expected_duration_days: float
    confidence_interval: Tuple[float, float]
    last_updated: datetime

@dataclass
class VolatilityForecast:
    """ë³€ë™ì„± ì˜ˆì¸¡"""
    forecast_horizon: int
    volatility_forecast: List[float]
    confidence_bands: List[Tuple[float, float]]
    regime_probabilities: Dict[str, List[float]]
    peak_volatility_date: Optional[datetime]
    trough_volatility_date: Optional[datetime]
    forecast_accuracy_score: float

class VolatilityRegimeAnalyzer:
    """ë³€ë™ì„± ì²´ì œ ë¶„ì„ê¸°"""
    
    def __init__(self, base_path: str = "/Users/parkyoungjun/Desktop/BTC_Analysis_System"):
        self.base_path = base_path
        self.db_path = os.path.join(base_path, "volatility_regime_db.db")
        self.models_path = os.path.join(base_path, "volatility_models")
        os.makedirs(self.models_path, exist_ok=True)
        
        # ë³€ë™ì„± ì²´ì œ ì •ì˜
        self.volatility_regimes = {
            0: "ULTRA_LOW",      # < 1% ì¼ê°„ ë³€ë™ì„±
            1: "LOW",            # 1-2%  
            2: "NORMAL",         # 2-4%
            3: "HIGH",           # 4-7%
            4: "EXTREME",        # > 7%
        }
        
        # ì²´ì œ ì„ê³„ê°’
        self.regime_thresholds = [0.01, 0.02, 0.04, 0.07, float('inf')]
        
        # ë¶„ì„ ëª¨ë¸ë“¤
        self.garch_model = None
        self.regime_classifier = None
        self.transition_matrix = np.zeros((5, 5))
        self.duration_models = {}
        
        # í˜„ì¬ ì²´ì œ ì¶”ì 
        self.current_regime = None
        self.regime_history = deque(maxlen=500)
        self.volatility_history = deque(maxlen=1000)
        
        # ì˜ˆì¸¡ ëª¨ë¸
        self.forecast_models = {}
        
        self.init_database()
        self.load_models()
    
    def init_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # ë³€ë™ì„± ì²´ì œ ê¸°ë¡
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS volatility_regimes (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    regime_id INTEGER NOT NULL,
                    regime_name TEXT NOT NULL,
                    start_date TEXT NOT NULL,
                    end_date TEXT,
                    duration_days INTEGER NOT NULL,
                    avg_volatility REAL NOT NULL,
                    volatility_std REAL NOT NULL,
                    max_volatility REAL NOT NULL,
                    min_volatility REAL NOT NULL,
                    volatility_persistence REAL NOT NULL,
                    clustering_coefficient REAL NOT NULL,
                    regime_confidence REAL NOT NULL
                )
            ''')
            
            # ì²´ì œ ì „í™˜ í™•ë¥ 
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS regime_transition_probabilities (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    from_regime TEXT NOT NULL,
                    to_regime TEXT NOT NULL,
                    base_probability REAL NOT NULL,
                    conditional_probabilities TEXT NOT NULL,
                    trigger_conditions TEXT NOT NULL,
                    expected_duration_days REAL NOT NULL,
                    confidence_interval_low REAL NOT NULL,
                    confidence_interval_high REAL NOT NULL,
                    last_updated TEXT NOT NULL
                )
            ''')
            
            # ë³€ë™ì„± ì˜ˆì¸¡
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS volatility_forecasts (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    forecast_date TEXT NOT NULL,
                    forecast_horizon INTEGER NOT NULL,
                    volatility_forecast TEXT NOT NULL,
                    confidence_bands TEXT NOT NULL,
                    regime_probabilities TEXT NOT NULL,
                    peak_volatility_date TEXT,
                    trough_volatility_date TEXT,
                    forecast_accuracy_score REAL,
                    model_used TEXT NOT NULL
                )
            ''')
            
            # ë³€ë™ì„± ë°ì´í„°
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS volatility_data (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT NOT NULL,
                    realized_volatility REAL NOT NULL,
                    implied_volatility REAL,
                    garch_volatility REAL,
                    regime_id INTEGER NOT NULL,
                    regime_probability REAL NOT NULL,
                    clustering_score REAL,
                    persistence_score REAL,
                    shock_magnitude REAL
                )
            ''')
            
            conn.commit()
            conn.close()
            logger.info("âœ… ë³€ë™ì„± ì²´ì œ ë¶„ì„ê¸° ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ë³€ë™ì„± DB ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def load_models(self):
        """ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ"""
        try:
            # GARCH ëª¨ë¸ ë¡œë“œ
            garch_file = os.path.join(self.models_path, "garch_volatility_model.pkl")
            if os.path.exists(garch_file):
                with open(garch_file, 'rb') as f:
                    self.garch_model = pickle.load(f)
            
            # ì²´ì œ ë¶„ë¥˜ ëª¨ë¸ ë¡œë“œ
            classifier_file = os.path.join(self.models_path, "regime_classifier.pkl")
            if os.path.exists(classifier_file):
                with open(classifier_file, 'rb') as f:
                    self.regime_classifier = pickle.load(f)
            
            # ì „í™˜ í–‰ë ¬ ë¡œë“œ
            transition_file = os.path.join(self.models_path, "transition_matrix.npy")
            if os.path.exists(transition_file):
                self.transition_matrix = np.load(transition_file)
            
            logger.info("âœ… ë³€ë™ì„± ëª¨ë¸ë“¤ ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ë³€ë™ì„± ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    async def analyze_volatility_regime(self, price_data: List[float], 
                                       timestamps: List[datetime]) -> Dict:
        """ë³€ë™ì„± ì²´ì œ ë¶„ì„"""
        try:
            if len(price_data) < 30:
                return {"error": "ë³€ë™ì„± ë¶„ì„ì„ ìœ„í•œ ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤"}
            
            logger.info(f"ğŸ” ë³€ë™ì„± ì²´ì œ ë¶„ì„ ì‹œì‘ (ë°ì´í„°: {len(price_data)}ê°œ)")
            
            # ìˆ˜ìµë¥  ê³„ì‚°
            returns = self.calculate_returns(price_data)
            
            # ì‹¤í˜„ ë³€ë™ì„± ê³„ì‚°
            realized_vol = self.calculate_realized_volatility(returns)
            
            # GARCH ë³€ë™ì„± ëª¨ë¸ë§
            garch_results = await self.model_garch_volatility(returns)
            
            # ë³€ë™ì„± í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„
            clustering_results = self.analyze_volatility_clustering(realized_vol)
            
            # ì²´ì œ ì‹ë³„
            regime_results = await self.identify_volatility_regimes(realized_vol, timestamps)
            
            # ì§€ì†ì„± ë¶„ì„
            persistence_results = self.analyze_volatility_persistence(realized_vol)
            
            # ì¶©ê²© ì „íŒŒ ë¶„ì„
            shock_results = self.analyze_volatility_shocks(returns, realized_vol)
            
            # ì²´ì œ ì „í™˜ í™•ë¥  ì—…ë°ì´íŠ¸
            await self.update_transition_probabilities(regime_results)
            
            analysis_result = {
                "analysis_date": datetime.now().isoformat(),
                "data_points": len(price_data),
                "current_regime": regime_results.get("current_regime"),
                "current_volatility": realized_vol[-1] if realized_vol else 0,
                "garch_results": garch_results,
                "clustering_results": clustering_results,
                "regime_results": regime_results,
                "persistence_results": persistence_results,
                "shock_results": shock_results,
                "regime_forecast": await self.forecast_regime_changes(7)
            }
            
            # ê²°ê³¼ ì €ì¥
            await self.save_analysis_results(analysis_result)
            
            return analysis_result
            
        except Exception as e:
            logger.error(f"ë³€ë™ì„± ì²´ì œ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def calculate_returns(self, prices: List[float]) -> np.ndarray:
        """ìˆ˜ìµë¥  ê³„ì‚°"""
        try:
            prices = np.array(prices)
            returns = np.diff(np.log(prices))
            return returns
        except Exception as e:
            logger.error(f"ìˆ˜ìµë¥  ê³„ì‚° ì‹¤íŒ¨: {e}")
            return np.array([])
    
    def calculate_realized_volatility(self, returns: np.ndarray, window: int = 24) -> List[float]:
        """ì‹¤í˜„ ë³€ë™ì„± ê³„ì‚° (ì´ë™ ìœˆë„ìš°)"""
        try:
            if len(returns) < window:
                return []
            
            realized_vol = []
            for i in range(window, len(returns) + 1):
                window_returns = returns[i-window:i]
                vol = np.sqrt(np.var(window_returns) * 365)  # ì—°ìœ¨í™”
                realized_vol.append(vol)
            
            return realized_vol
            
        except Exception as e:
            logger.error(f"ì‹¤í˜„ ë³€ë™ì„± ê³„ì‚° ì‹¤íŒ¨: {e}")
            return []
    
    async def model_garch_volatility(self, returns: np.ndarray) -> Dict:
        """GARCH ëª¨ë¸ë¡œ ë³€ë™ì„± ëª¨ë¸ë§"""
        try:
            if not ARCH_AVAILABLE or len(returns) < 100:
                return await self.simple_volatility_model(returns)
            
            # GARCH(1,1) ëª¨ë¸ ì í•©
            model = arch_model(returns * 100, vol='Garch', p=1, q=1)
            fitted_model = model.fit(disp='off')
            
            # ì¡°ê±´ë¶€ ë³€ë™ì„± ì¶”ì¶œ
            conditional_volatility = fitted_model.conditional_volatility / 100
            
            # ëª¨ë¸ íŒŒë¼ë¯¸í„°
            params = fitted_model.params
            
            # ì”ì°¨ ë¶„ì„
            standardized_residuals = fitted_model.std_resid
            ljung_box_stat = self.ljung_box_test(standardized_residuals)
            
            # ì˜ˆì¸¡
            forecasts = fitted_model.forecast(horizon=5)
            vol_forecast = np.sqrt(forecasts.variance.values[-1] / 10000)
            
            # ëª¨ë¸ ì €ì¥
            self.garch_model = fitted_model
            
            return {
                "model_type": "GARCH(1,1)",
                "parameters": {
                    "omega": params.get("omega", 0),
                    "alpha": params.get("alpha[1]", 0),
                    "beta": params.get("beta[1]", 0)
                },
                "conditional_volatility": conditional_volatility.tolist()[-50:],  # ìµœê·¼ 50ê°œ
                "volatility_forecast": vol_forecast.tolist(),
                "model_diagnostics": {
                    "ljung_box_pvalue": ljung_box_stat,
                    "log_likelihood": fitted_model.loglikelihood,
                    "aic": fitted_model.aic,
                    "bic": fitted_model.bic
                },
                "persistence": params.get("alpha[1]", 0) + params.get("beta[1]", 0)
            }
            
        except Exception as e:
            logger.error(f"GARCH ëª¨ë¸ë§ ì‹¤íŒ¨: {e}")
            return await self.simple_volatility_model(returns)
    
    async def simple_volatility_model(self, returns: np.ndarray) -> Dict:
        """ê°„ë‹¨í•œ ë³€ë™ì„± ëª¨ë¸ (GARCH ëŒ€ì²´)"""
        try:
            if len(returns) < 20:
                return {"error": "ë°ì´í„° ë¶€ì¡±"}
            
            # ì´ë™í‰ê·  ë³€ë™ì„±
            window_sizes = [5, 10, 20]
            volatilities = {}
            
            for window in window_sizes:
                vol_series = []
                for i in range(window, len(returns)):
                    vol = np.std(returns[i-window:i]) * np.sqrt(365)
                    vol_series.append(vol)
                volatilities[f"vol_{window}d"] = vol_series[-10:]  # ìµœê·¼ 10ê°œ
            
            # EWMA ë³€ë™ì„± (lambda = 0.94)
            ewma_vol = []
            lambda_param = 0.94
            vol_estimate = np.var(returns[:20])  # ì´ˆê¸°ê°’
            
            for ret in returns[20:]:
                vol_estimate = lambda_param * vol_estimate + (1 - lambda_param) * ret**2
                ewma_vol.append(np.sqrt(vol_estimate * 365))
            
            # ê°„ë‹¨í•œ ì˜ˆì¸¡ (ë§ˆì§€ë§‰ ê°’ ê¸°ì¤€)
            forecast = [ewma_vol[-1] * (0.95 + 0.1 * np.random.random()) for _ in range(5)]
            
            return {
                "model_type": "Simple_EWMA",
                "volatilities": volatilities,
                "ewma_volatility": ewma_vol[-20:],  # ìµœê·¼ 20ê°œ
                "volatility_forecast": forecast,
                "current_vol": ewma_vol[-1] if ewma_vol else 0.0
            }
            
        except Exception as e:
            logger.error(f"ê°„ë‹¨í•œ ë³€ë™ì„± ëª¨ë¸ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def ljung_box_test(self, residuals: np.ndarray, lags: int = 10) -> float:
        """Ljung-Box í…ŒìŠ¤íŠ¸ (ì”ì°¨ ìê¸°ìƒê´€ ê²€ì •)"""
        try:
            n = len(residuals)
            autocorr = np.correlate(residuals, residuals, mode='full')
            autocorr = autocorr[len(autocorr)//2:]
            autocorr = autocorr[1:lags+1] / autocorr[0]
            
            lb_stat = n * (n + 2) * np.sum([autocorr[i]**2 / (n - i - 1) for i in range(lags)])
            p_value = 1 - stats.chi2.cdf(lb_stat, lags)
            
            return p_value
            
        except Exception as e:
            logger.error(f"Ljung-Box í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return 0.5
    
    def analyze_volatility_clustering(self, volatility: List[float]) -> Dict:
        """ë³€ë™ì„± í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„"""
        try:
            if len(volatility) < 50:
                return {"error": "í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ì„ ìœ„í•œ ë°ì´í„° ë¶€ì¡±"}
            
            vol_array = np.array(volatility).reshape(-1, 1)
            
            # Gaussian Mixture Modelë¡œ í´ëŸ¬ìŠ¤í„°ë§
            n_clusters = 3
            gmm = GaussianMixture(n_components=n_clusters, random_state=42)
            cluster_labels = gmm.fit_predict(vol_array)
            
            # í´ëŸ¬ìŠ¤í„°ë³„ íŠ¹ì„±
            cluster_stats = {}
            for i in range(n_clusters):
                cluster_data = np.array(volatility)[cluster_labels == i]
                if len(cluster_data) > 0:
                    cluster_stats[f"cluster_{i}"] = {
                        "mean": np.mean(cluster_data),
                        "std": np.std(cluster_data),
                        "size": len(cluster_data),
                        "percentage": len(cluster_data) / len(volatility) * 100
                    }
            
            # í´ëŸ¬ìŠ¤í„° ì§€ì†ì„± (ê°™ì€ í´ëŸ¬ìŠ¤í„°ê°€ ì—°ì†ìœ¼ë¡œ ë‚˜íƒ€ë‚˜ëŠ” ì •ë„)
            transitions = np.sum(np.diff(cluster_labels) != 0)
            persistence_score = 1 - (transitions / (len(cluster_labels) - 1))
            
            # í˜„ì¬ í´ëŸ¬ìŠ¤í„°
            current_cluster = cluster_labels[-1] if len(cluster_labels) > 0 else 0
            
            return {
                "n_clusters": n_clusters,
                "cluster_labels": cluster_labels.tolist()[-20:],  # ìµœê·¼ 20ê°œ
                "cluster_statistics": cluster_stats,
                "persistence_score": persistence_score,
                "current_cluster": current_cluster,
                "clustering_quality": float(gmm.aic_)
            }
            
        except Exception as e:
            logger.error(f"ë³€ë™ì„± í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    async def identify_volatility_regimes(self, volatility: List[float], 
                                        timestamps: List[datetime]) -> Dict:
        """ë³€ë™ì„± ì²´ì œ ì‹ë³„"""
        try:
            if len(volatility) < 10:
                return {"error": "ì²´ì œ ì‹ë³„ì„ ìœ„í•œ ë°ì´í„° ë¶€ì¡±"}
            
            # í˜„ì¬ ë³€ë™ì„±ìœ¼ë¡œ ì²´ì œ ê²°ì •
            current_vol = volatility[-1]
            current_regime_id = self.classify_volatility_regime(current_vol)
            current_regime_name = self.volatility_regimes[current_regime_id]
            
            # ì²´ì œ ë³€ê²½ì  íƒì§€
            regime_changes = []
            regime_sequence = []
            
            for i, vol in enumerate(volatility):
                regime_id = self.classify_volatility_regime(vol)
                regime_sequence.append(regime_id)
                
                if i > 0 and regime_sequence[i] != regime_sequence[i-1]:
                    regime_changes.append({
                        "index": i,
                        "timestamp": timestamps[i].isoformat() if i < len(timestamps) else None,
                        "from_regime": self.volatility_regimes[regime_sequence[i-1]],
                        "to_regime": self.volatility_regimes[regime_sequence[i]],
                        "volatility": vol
                    })
            
            # í˜„ì¬ ì²´ì œ ì§€ì† ê¸°ê°„
            current_regime_duration = 0
            for i in range(len(regime_sequence) - 1, -1, -1):
                if regime_sequence[i] == current_regime_id:
                    current_regime_duration += 1
                else:
                    break
            
            # ì²´ì œë³„ í†µê³„
            regime_stats = {}
            for regime_id, regime_name in self.volatility_regimes.items():
                regime_vols = [vol for i, vol in enumerate(volatility) 
                              if regime_sequence[i] == regime_id]
                if regime_vols:
                    regime_stats[regime_name] = {
                        "count": len(regime_vols),
                        "percentage": len(regime_vols) / len(volatility) * 100,
                        "avg_volatility": np.mean(regime_vols),
                        "max_volatility": np.max(regime_vols),
                        "min_volatility": np.min(regime_vols)
                    }
            
            return {
                "current_regime_id": current_regime_id,
                "current_regime": current_regime_name,
                "current_volatility": current_vol,
                "regime_duration": current_regime_duration,
                "regime_changes": regime_changes[-10:],  # ìµœê·¼ 10ê°œ ë³€ê²½ì 
                "regime_sequence": regime_sequence[-50:],  # ìµœê·¼ 50ê°œ
                "regime_statistics": regime_stats,
                "total_regime_changes": len(regime_changes)
            }
            
        except Exception as e:
            logger.error(f"ë³€ë™ì„± ì²´ì œ ì‹ë³„ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def classify_volatility_regime(self, volatility: float) -> int:
        """ë³€ë™ì„± ê°’ìœ¼ë¡œ ì²´ì œ ë¶„ë¥˜"""
        try:
            for i, threshold in enumerate(self.regime_thresholds):
                if volatility <= threshold:
                    return i
            return len(self.regime_thresholds) - 1
        except:
            return 2  # ê¸°ë³¸ê°’: NORMAL
    
    def analyze_volatility_persistence(self, volatility: List[float]) -> Dict:
        """ë³€ë™ì„± ì§€ì†ì„± ë¶„ì„"""
        try:
            if len(volatility) < 30:
                return {"error": "ì§€ì†ì„± ë¶„ì„ì„ ìœ„í•œ ë°ì´í„° ë¶€ì¡±"}
            
            vol_array = np.array(volatility)
            
            # ìê¸°ìƒê´€ ë¶„ì„
            max_lags = min(20, len(volatility) // 4)
            autocorrelations = []
            
            for lag in range(1, max_lags + 1):
                if len(vol_array) > lag:
                    corr = np.corrcoef(vol_array[:-lag], vol_array[lag:])[0, 1]
                    autocorrelations.append(corr)
            
            # ì§€ì†ì„± ì ìˆ˜ (ì²« ë²ˆì§¸ ìê¸°ìƒê´€ê³„ìˆ˜)
            persistence_score = autocorrelations[0] if autocorrelations else 0
            
            # ë°˜ê°ê¸° ì¶”ì • (ìê¸°ìƒê´€ì´ 0.5 ì´í•˜ë¡œ ë–¨ì–´ì§€ëŠ” ì§€ì )
            half_life = 1
            for i, corr in enumerate(autocorrelations):
                if corr < 0.5:
                    half_life = i + 1
                    break
            else:
                half_life = len(autocorrelations)
            
            # ë³€ë™ì„±ì˜ ë³€ë™ì„± (vol of vol)
            vol_changes = np.diff(vol_array)
            vol_of_vol = np.std(vol_changes) / np.mean(vol_array) if np.mean(vol_array) > 0 else 0
            
            # í‰ê·  íšŒê·€ ì†ë„
            mean_vol = np.mean(vol_array)
            deviations = vol_array - mean_vol
            mean_reversion_speed = -np.mean(np.diff(deviations) / deviations[:-1]) if len(deviations) > 1 else 0
            
            return {
                "persistence_score": persistence_score,
                "autocorrelations": autocorrelations[:10],  # ì²˜ìŒ 10ê°œ
                "half_life_days": half_life,
                "volatility_of_volatility": vol_of_vol,
                "mean_reversion_speed": mean_reversion_speed,
                "current_deviation": (vol_array[-1] - mean_vol) / mean_vol if mean_vol > 0 else 0
            }
            
        except Exception as e:
            logger.error(f"ë³€ë™ì„± ì§€ì†ì„± ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def analyze_volatility_shocks(self, returns: np.ndarray, volatility: List[float]) -> Dict:
        """ë³€ë™ì„± ì¶©ê²© ë¶„ì„"""
        try:
            if len(returns) < 20 or len(volatility) < 20:
                return {"error": "ì¶©ê²© ë¶„ì„ì„ ìœ„í•œ ë°ì´í„° ë¶€ì¡±"}
            
            # ë³€ë™ì„± ì¶©ê²© ì„ê³„ê°’ (ìƒìœ„ 10%)
            vol_threshold = np.percentile(volatility, 90)
            
            # ìˆ˜ìµë¥  ì¶©ê²© ì„ê³„ê°’ (ì ˆëŒ“ê°’ ê¸°ì¤€ ìƒìœ„ 5%)
            return_threshold = np.percentile(np.abs(returns), 95)
            
            # ì¶©ê²© ì´ë²¤íŠ¸ ì‹ë³„
            vol_shocks = []
            return_shocks = []
            
            for i, vol in enumerate(volatility):
                if vol > vol_threshold:
                    vol_shocks.append({
                        "index": i,
                        "magnitude": vol,
                        "z_score": (vol - np.mean(volatility)) / np.std(volatility)
                    })
            
            for i, ret in enumerate(returns):
                if abs(ret) > return_threshold:
                    return_shocks.append({
                        "index": i,
                        "magnitude": ret,
                        "z_score": ret / np.std(returns)
                    })
            
            # ì¶©ê²© ì „íŒŒ ë¶„ì„ (ì¶©ê²© í›„ ë³€ë™ì„± ë³€í™”)
            shock_propagation = []
            for shock in vol_shocks[-5:]:  # ìµœê·¼ 5ê°œ ì¶©ê²©
                shock_idx = shock["index"]
                if shock_idx < len(volatility) - 5:
                    post_shock_vol = volatility[shock_idx:shock_idx+5]
                    pre_shock_vol = np.mean(volatility[max(0, shock_idx-5):shock_idx])
                    
                    propagation = {
                        "shock_magnitude": shock["magnitude"],
                        "pre_shock_avg": pre_shock_vol,
                        "post_shock_series": post_shock_vol,
                        "decay_rate": self.calculate_decay_rate(post_shock_vol)
                    }
                    shock_propagation.append(propagation)
            
            # ì¶©ê²© ë¹ˆë„ ë¶„ì„
            shock_frequency = len(vol_shocks) / len(volatility) * 100  # ë°±ë¶„ìœ¨
            
            return {
                "volatility_shocks": vol_shocks[-10:],  # ìµœê·¼ 10ê°œ
                "return_shocks": return_shocks[-10:],
                "shock_propagation": shock_propagation,
                "shock_frequency_pct": shock_frequency,
                "avg_shock_magnitude": np.mean([s["magnitude"] for s in vol_shocks]) if vol_shocks else 0,
                "shock_clustering": self.analyze_shock_clustering(vol_shocks)
            }
            
        except Exception as e:
            logger.error(f"ë³€ë™ì„± ì¶©ê²© ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def calculate_decay_rate(self, post_shock_series: List[float]) -> float:
        """ì¶©ê²© í›„ ë³€ë™ì„± ê°ì‡ ìœ¨ ê³„ì‚°"""
        try:
            if len(post_shock_series) < 3:
                return 0.0
            
            # ì§€ìˆ˜ì  ê°ì‡  ëª¨ë¸ ì í•©
            x = np.arange(len(post_shock_series))
            y = np.array(post_shock_series)
            
            # ë¡œê·¸ ë³€í™˜ í›„ ì„ í˜• íšŒê·€
            if np.all(y > 0):
                log_y = np.log(y)
                slope = np.polyfit(x, log_y, 1)[0]
                return -slope  # ì–‘ìˆ˜ë©´ ê°ì‡ , ìŒìˆ˜ë©´ ì¦ê°€
            else:
                return 0.0
                
        except Exception as e:
            logger.error(f"ê°ì‡ ìœ¨ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
    
    def analyze_shock_clustering(self, shocks: List[Dict]) -> Dict:
        """ì¶©ê²© í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„"""
        try:
            if len(shocks) < 3:
                return {"clustering_detected": False}
            
            # ì¶©ê²© ê°„ ì‹œê°„ ê°„ê²© ê³„ì‚°
            intervals = []
            for i in range(1, len(shocks)):
                interval = shocks[i]["index"] - shocks[i-1]["index"]
                intervals.append(interval)
            
            if not intervals:
                return {"clustering_detected": False}
            
            # í´ëŸ¬ìŠ¤í„°ë§ ì„ê³„ê°’ (í‰ê·  ê°„ê²©ì˜ 50% ì´í•˜)
            avg_interval = np.mean(intervals)
            cluster_threshold = avg_interval * 0.5
            
            # í´ëŸ¬ìŠ¤í„° ì‹ë³„
            clustered_intervals = [interval for interval in intervals if interval <= cluster_threshold]
            clustering_ratio = len(clustered_intervals) / len(intervals)
            
            return {
                "clustering_detected": clustering_ratio > 0.3,
                "clustering_ratio": clustering_ratio,
                "avg_interval": avg_interval,
                "cluster_threshold": cluster_threshold,
                "total_intervals": len(intervals),
                "clustered_intervals": len(clustered_intervals)
            }
            
        except Exception as e:
            logger.error(f"ì¶©ê²© í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"clustering_detected": False}
    
    async def update_transition_probabilities(self, regime_results: Dict):
        """ì²´ì œ ì „í™˜ í™•ë¥  ì—…ë°ì´íŠ¸"""
        try:
            regime_sequence = regime_results.get("regime_sequence", [])
            if len(regime_sequence) < 10:
                return
            
            # ì „í™˜ í–‰ë ¬ ê³„ì‚°
            n_regimes = len(self.volatility_regimes)
            transitions = np.zeros((n_regimes, n_regimes))
            
            for i in range(1, len(regime_sequence)):
                from_regime = regime_sequence[i-1]
                to_regime = regime_sequence[i]
                if 0 <= from_regime < n_regimes and 0 <= to_regime < n_regimes:
                    transitions[from_regime, to_regime] += 1
            
            # í–‰ë³„ ì •ê·œí™” (í™•ë¥ ë¡œ ë³€í™˜)
            for i in range(n_regimes):
                row_sum = transitions[i].sum()
                if row_sum > 0:
                    transitions[i] /= row_sum
            
            self.transition_matrix = transitions
            
            # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
            await self.save_transition_probabilities()
            
        except Exception as e:
            logger.error(f"ì „í™˜ í™•ë¥  ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    async def forecast_regime_changes(self, horizon_days: int = 7) -> Dict:
        """ì²´ì œ ë³€ê²½ ì˜ˆì¸¡"""
        try:
            if self.current_regime is None:
                return {"error": "í˜„ì¬ ì²´ì œê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"}
            
            current_regime_id = self.current_regime
            
            # ë§ˆë¥´ì½”í”„ ì²´ì¸ìœ¼ë¡œ ë¯¸ë˜ ì²´ì œ ì˜ˆì¸¡
            current_prob = np.zeros(len(self.volatility_regimes))
            current_prob[current_regime_id] = 1.0
            
            future_probs = []
            for day in range(horizon_days):
                current_prob = current_prob @ self.transition_matrix
                future_probs.append(current_prob.copy())
            
            # ê° ë‚ ì§œë³„ ê°€ì¥ ê°€ëŠ¥ì„± ë†’ì€ ì²´ì œ
            most_likely_regimes = []
            for day_prob in future_probs:
                most_likely_id = np.argmax(day_prob)
                most_likely_regimes.append({
                    "regime_id": most_likely_id,
                    "regime_name": self.volatility_regimes[most_likely_id],
                    "probability": day_prob[most_likely_id],
                    "all_probabilities": day_prob.tolist()
                })
            
            # ì²´ì œ ì „í™˜ ê°ì§€
            regime_changes_forecast = []
            for day, regime_info in enumerate(most_likely_regimes):
                if day == 0:
                    if regime_info["regime_id"] != current_regime_id:
                        regime_changes_forecast.append({
                            "day": day + 1,
                            "from_regime": self.volatility_regimes[current_regime_id],
                            "to_regime": regime_info["regime_name"],
                            "probability": regime_info["probability"]
                        })
                else:
                    prev_regime = most_likely_regimes[day-1]["regime_id"]
                    if regime_info["regime_id"] != prev_regime:
                        regime_changes_forecast.append({
                            "day": day + 1,
                            "from_regime": self.volatility_regimes[prev_regime],
                            "to_regime": regime_info["regime_name"],
                            "probability": regime_info["probability"]
                        })
            
            return {
                "forecast_horizon": horizon_days,
                "current_regime": self.volatility_regimes[current_regime_id],
                "daily_forecasts": most_likely_regimes,
                "regime_changes": regime_changes_forecast,
                "transition_matrix": self.transition_matrix.tolist()
            }
            
        except Exception as e:
            logger.error(f"ì²´ì œ ë³€ê²½ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    async def save_transition_probabilities(self):
        """ì „í™˜ í™•ë¥  ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
            cursor.execute('DELETE FROM regime_transition_probabilities')
            
            # ìƒˆë¡œìš´ ì „í™˜ í™•ë¥  ì €ì¥
            for i in range(len(self.volatility_regimes)):
                for j in range(len(self.volatility_regimes)):
                    probability = self.transition_matrix[i, j]
                    if probability > 0.001:  # ì˜ë¯¸ ìˆëŠ” í™•ë¥ ë§Œ ì €ì¥
                        cursor.execute('''
                            INSERT INTO regime_transition_probabilities
                            (from_regime, to_regime, base_probability, conditional_probabilities,
                             trigger_conditions, expected_duration_days, confidence_interval_low,
                             confidence_interval_high, last_updated)
                            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                        ''', (
                            self.volatility_regimes[i],
                            self.volatility_regimes[j],
                            probability,
                            json.dumps({}),  # ì¡°ê±´ë¶€ í™•ë¥ ì€ ì¶”í›„ êµ¬í˜„
                            json.dumps([]),  # íŠ¸ë¦¬ê±° ì¡°ê±´ì€ ì¶”í›„ êµ¬í˜„
                            1.0 / probability if probability > 0 else float('inf'),
                            probability * 0.8,  # ì‹ ë¢°êµ¬ê°„ í•˜í•œ
                            probability * 1.2,  # ì‹ ë¢°êµ¬ê°„ ìƒí•œ
                            datetime.now().isoformat()
                        ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            logger.error(f"ì „í™˜ í™•ë¥  ì €ì¥ ì‹¤íŒ¨: {e}")
    
    async def save_analysis_results(self, results: Dict):
        """ë¶„ì„ ê²°ê³¼ ì €ì¥"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # í˜„ì¬ ë³€ë™ì„± ë°ì´í„° ì €ì¥
            regime_results = results.get("regime_results", {})
            current_vol = regime_results.get("current_volatility", 0)
            current_regime_id = regime_results.get("current_regime_id", 2)
            
            cursor.execute('''
                INSERT INTO volatility_data
                (timestamp, realized_volatility, regime_id, regime_probability,
                 clustering_score, persistence_score)
                VALUES (?, ?, ?, ?, ?, ?)
            ''', (
                datetime.now().isoformat(),
                current_vol,
                current_regime_id,
                1.0,  # í˜„ì¬ ì²´ì œ í™•ë¥ 
                results.get("clustering_results", {}).get("persistence_score", 0),
                results.get("persistence_results", {}).get("persistence_score", 0)
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            logger.error(f"ë¶„ì„ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    async def get_volatility_diagnostics(self) -> Dict:
        """ë³€ë™ì„± ë¶„ì„ê¸° ì§„ë‹¨"""
        try:
            diagnostics = {
                "analyzer_status": "active",
                "models_loaded": {
                    "garch_model": self.garch_model is not None,
                    "regime_classifier": self.regime_classifier is not None
                },
                "current_regime": self.current_regime,
                "regime_history_length": len(self.regime_history),
                "volatility_history_length": len(self.volatility_history)
            }
            
            # ì „í™˜ í–‰ë ¬ ìš”ì•½
            if self.transition_matrix.size > 0:
                most_stable_regime = np.argmax(np.diag(self.transition_matrix))
                most_volatile_regime = np.argmin(np.diag(self.transition_matrix))
                
                diagnostics["transition_analysis"] = {
                    "most_stable_regime": self.volatility_regimes[most_stable_regime],
                    "most_volatile_regime": self.volatility_regimes[most_volatile_regime],
                    "avg_persistence": np.mean(np.diag(self.transition_matrix))
                }
            
            # ìµœê·¼ ë¶„ì„ ê²°ê³¼ ì¡°íšŒ
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                SELECT realized_volatility, regime_id, timestamp
                FROM volatility_data 
                ORDER BY timestamp DESC 
                LIMIT 10
            ''')
            
            recent_data = cursor.fetchall()
            if recent_data:
                diagnostics["recent_volatility"] = [
                    {
                        "volatility": row[0],
                        "regime": self.volatility_regimes.get(row[1], "UNKNOWN"),
                        "timestamp": row[2]
                    } for row in recent_data
                ]
            
            conn.close()
            
            return diagnostics
            
        except Exception as e:
            logger.error(f"ë³€ë™ì„± ì§„ë‹¨ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}

# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
async def test_volatility_regime_analyzer():
    """ë³€ë™ì„± ì²´ì œ ë¶„ì„ê¸° í…ŒìŠ¤íŠ¸"""
    print("ğŸ“Š ë³€ë™ì„± ì²´ì œ ë¶„ì„ê¸° í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    analyzer = VolatilityRegimeAnalyzer()
    
    # ì§„ë‹¨ ì •ë³´
    diagnostics = await analyzer.get_volatility_diagnostics()
    print(f"ğŸ”§ ë¶„ì„ê¸° ìƒíƒœ: {diagnostics.get('analyzer_status')}")
    print(f"ğŸ“ˆ GARCH ëª¨ë¸: {'âœ…' if diagnostics.get('models_loaded', {}).get('garch_model') else 'âŒ'}")
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± (ê°€ìƒì˜ ë¹„íŠ¸ì½”ì¸ ê°€ê²©)
    np.random.seed(42)
    n_days = 100
    base_price = 60000
    
    # ë³€ë™ì„±ì´ ë³€í•˜ëŠ” ê°€ê²© ì‹œê³„ì—´ ìƒì„±
    prices = [base_price]
    volatilities = []
    
    for i in range(n_days):
        # ì£¼ê¸°ì ìœ¼ë¡œ ë³€í•˜ëŠ” ë³€ë™ì„±
        vol = 0.02 + 0.03 * np.sin(i * 0.1) + 0.01 * np.random.random()
        volatilities.append(vol)
        
        # ê°€ê²© ì—…ë°ì´íŠ¸
        daily_return = np.random.normal(0, vol)
        new_price = prices[-1] * (1 + daily_return)
        prices.append(max(new_price, 1000))  # ìµœì†Œ ê°€ê²© ë³´ì¥
    
    timestamps = [datetime.now() - timedelta(days=n_days-i) for i in range(n_days+1)]
    
    print(f"ğŸ§ª í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(prices)}ê°œ ê°€ê²© í¬ì¸íŠ¸")
    
    # ë³€ë™ì„± ì²´ì œ ë¶„ì„ ì‹¤í–‰
    analysis_result = await analyzer.analyze_volatility_regime(prices, timestamps)
    
    if not analysis_result.get("error"):
        print(f"âœ… ë¶„ì„ ì™„ë£Œ")
        print(f"ğŸ“Š í˜„ì¬ ì²´ì œ: {analysis_result.get('current_regime')}")
        print(f"ğŸ“ˆ í˜„ì¬ ë³€ë™ì„±: {analysis_result.get('current_volatility', 0):.1%}")
        
        # ì²´ì œ ê²°ê³¼
        regime_results = analysis_result.get("regime_results", {})
        print(f"ğŸ”„ ì²´ì œ ì§€ì†ê¸°ê°„: {regime_results.get('regime_duration', 0)}ì¼")
        print(f"ğŸ¯ ì´ ì²´ì œ ë³€ê²½: {regime_results.get('total_regime_changes', 0)}íšŒ")
        
        # GARCH ê²°ê³¼
        garch_results = analysis_result.get("garch_results", {})
        if garch_results.get("model_type"):
            print(f"ğŸ“‰ {garch_results.get('model_type')} ëª¨ë¸ ì ìš©")
            persistence = garch_results.get("persistence", 0)
            print(f"ğŸ”— ë³€ë™ì„± ì§€ì†ì„±: {persistence:.3f}")
        
        # í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼
        clustering = analysis_result.get("clustering_results", {})
        if clustering.get("persistence_score"):
            print(f"ğŸ² í´ëŸ¬ìŠ¤í„°ë§ ì§€ì†ì„±: {clustering.get('persistence_score'):.3f}")
        
        # ì²´ì œ ì˜ˆì¸¡
        forecast = analysis_result.get("regime_forecast", {})
        if forecast.get("regime_changes"):
            print(f"ğŸ”® ì˜ˆìƒ ì²´ì œ ë³€ê²½:")
            for change in forecast["regime_changes"][:3]:
                print(f"   â€¢ Day {change['day']}: {change['from_regime']} â†’ {change['to_regime']} ({change['probability']:.1%})")
    else:
        print(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {analysis_result.get('error')}")
    
    print("\n" + "=" * 50)
    print("ğŸ‰ ë³€ë™ì„± ì²´ì œ ë¶„ì„ê¸° í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

if __name__ == "__main__":
    asyncio.run(test_volatility_regime_analyzer())