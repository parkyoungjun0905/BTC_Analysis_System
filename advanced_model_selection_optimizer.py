#!/usr/bin/env python3
"""
ğŸ¯ ê³ ê¸‰ ëª¨ë¸ ì„ íƒ ë° ì•™ìƒë¸” ìµœì í™” ì‹œìŠ¤í…œ
ìœ ì „ ì•Œê³ ë¦¬ì¦˜, ê°•í™”í•™ìŠµ, ë‹¤ëª©ì  ìµœì í™”ë¥¼ í™œìš©í•œ ë™ì  ëª¨ë¸ ì„ íƒ

í•µì‹¬ ê¸°ëŠ¥:
- ìœ ì „ ì•Œê³ ë¦¬ì¦˜ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ìµœì í™”
- ê°•í™”í•™ìŠµì„ í†µí•œ ë™ì  ëª¨ë¸ ì„ íƒ
- ë‹¤ëª©ì  ìµœì í™” (ì •í™•ë„ vs ë‹¤ì–‘ì„±)
- ì˜¨ë¼ì¸ ì ì‘í˜• ì•™ìƒë¸”
- ì„±ëŠ¥ ì¼ê´€ì„± ëª¨ë‹ˆí„°ë§
"""

import numpy as np
import pandas as pd
import json
import pickle
import warnings
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Any, Optional
from pathlib import Path
import sqlite3
from concurrent.futures import ThreadPoolExecutor
import logging

# ìµœì í™” ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from deap import base, creator, tools, algorithms
    from scipy.optimize import differential_evolution, minimize
    OPTIMIZATION_AVAILABLE = True
except ImportError:
    OPTIMIZATION_AVAILABLE = False
    print("âš ï¸ ìµœì í™” ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¯¸ì„¤ì¹˜ (deap, scipy)")

# ê°•í™”í•™ìŠµ ë¼ì´ë¸ŒëŸ¬ë¦¬ (ì„ íƒì )
try:
    import gym
    from gym import spaces
    REINFORCEMENT_AVAILABLE = True
except ImportError:
    REINFORCEMENT_AVAILABLE = False
    print("âš ï¸ OpenAI Gym ë¯¸ì„¤ì¹˜ - ê°•í™”í•™ìŠµ ë¹„í™œì„±í™”")

# ë¨¸ì‹ ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

warnings.filterwarnings('ignore')

class GeneticEnsembleOptimizer:
    """
    ğŸ§¬ ìœ ì „ ì•Œê³ ë¦¬ì¦˜ ê¸°ë°˜ ì•™ìƒë¸” ìµœì í™”
    """
    
    def __init__(self, population_size: int = 50, generations: int = 100):
        self.population_size = population_size
        self.generations = generations
        
        if OPTIMIZATION_AVAILABLE:
            # DEAP ì„¤ì •
            creator.create("FitnessMax", base.Fitness, weights=(1.0,))
            creator.create("Individual", list, fitness=creator.FitnessMax)
            
            self.toolbox = base.Toolbox()
        
        self.logger = logging.getLogger(__name__)

    def create_genetic_operators(self, n_models: int):
        """ìœ ì „ ì—°ì‚°ì ìƒì„±"""
        if not OPTIMIZATION_AVAILABLE:
            return None
        
        # ê°€ì¤‘ì¹˜ ë²”ìœ„: 0.0 ~ 1.0
        self.toolbox.register("attr_weight", np.random.random)
        self.toolbox.register("individual", tools.initRepeat, creator.Individual,
                             self.toolbox.attr_weight, n_models)
        self.toolbox.register("population", tools.initRepeat, list,
                             self.toolbox.individual)
        
        # í‰ê°€ í•¨ìˆ˜ ë“±ë¡
        self.toolbox.register("evaluate", self.evaluate_ensemble)
        self.toolbox.register("mate", tools.cxBlend, alpha=0.5)
        self.toolbox.register("mutate", tools.mutGaussian, mu=0, sigma=0.2, indpb=0.1)
        self.toolbox.register("select", tools.selTournament, tournsize=3)

    def normalize_weights(self, weights: List[float]) -> List[float]:
        """ê°€ì¤‘ì¹˜ ì •ê·œí™”"""
        weights = np.array(weights)
        weights = np.maximum(weights, 0)  # ìŒìˆ˜ ì œê±°
        total = np.sum(weights)
        return (weights / total).tolist() if total > 0 else [1.0/len(weights)] * len(weights)

    def evaluate_ensemble(self, individual: List[float]) -> Tuple[float]:
        """ê°œì²´ í‰ê°€ í•¨ìˆ˜"""
        weights = self.normalize_weights(individual)
        
        # ì•™ìƒë¸” ì˜ˆì¸¡ ê³„ì‚°
        ensemble_pred = np.zeros(len(self.y_val))
        
        for i, (model_name, pred) in enumerate(self.model_predictions.items()):
            ensemble_pred += weights[i] * pred
        
        # í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚°
        # 1. ë°©í–¥ì„± ì •í™•ë„ (ì£¼ìš” ëª©í‘œ)
        direction_actual = np.diff(self.y_val) > 0
        direction_pred = np.diff(ensemble_pred) > 0
        direction_accuracy = np.mean(direction_actual == direction_pred)
        
        # 2. RÂ² ì ìˆ˜
        r2 = r2_score(self.y_val, ensemble_pred)
        r2_normalized = max(0, min(1, r2))
        
        # 3. ë‹¤ì–‘ì„± ì ìˆ˜ (ê°€ì¤‘ì¹˜ ë¶„ì‚°)
        diversity = 1 - np.var(weights)  # ê· ë“±í• ìˆ˜ë¡ ë†’ì€ ì ìˆ˜
        
        # ì¢…í•© ì ìˆ˜ ê³„ì‚°
        fitness = (
            0.6 * direction_accuracy +  # ë°©í–¥ì„±ì´ ê°€ì¥ ì¤‘ìš”
            0.3 * r2_normalized +       # ì„¤ëª…ë ¥
            0.1 * diversity            # ë‹¤ì–‘ì„±
        )
        
        return (fitness,)

    def optimize_ensemble_weights(self, model_predictions: Dict[str, np.ndarray], 
                                y_val: np.ndarray) -> Dict[str, float]:
        """
        ìœ ì „ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ìµœì í™”
        
        Args:
            model_predictions: ëª¨ë¸ë³„ ì˜ˆì¸¡ê°’
            y_val: ê²€ì¦ íƒ€ê²Ÿ
            
        Returns:
            Dict[str, float]: ìµœì í™”ëœ ê°€ì¤‘ì¹˜
        """
        if not OPTIMIZATION_AVAILABLE:
            # ê¸°ë³¸ ê· ë“± ê°€ì¤‘ì¹˜ ë°˜í™˜
            n_models = len(model_predictions)
            return {name: 1.0/n_models for name in model_predictions.keys()}
        
        self.model_predictions = model_predictions
        self.y_val = y_val
        
        n_models = len(model_predictions)
        self.create_genetic_operators(n_models)
        
        # ì´ˆê¸° ê°œì²´êµ° ìƒì„±
        population = self.toolbox.population(n=self.population_size)
        
        # í†µê³„ ì„¤ì •
        stats = tools.Statistics(lambda ind: ind.fitness.values)
        stats.register("avg", np.mean)
        stats.register("std", np.std)
        stats.register("min", np.min)
        stats.register("max", np.max)
        
        # ìœ ì „ ì•Œê³ ë¦¬ì¦˜ ì‹¤í–‰
        population, logbook = algorithms.eaSimple(
            population, self.toolbox, 
            cxpb=0.7, mutpb=0.3, ngen=self.generations,
            stats=stats, verbose=False
        )
        
        # ìµœì  ê°œì²´ ì„ íƒ
        best_individual = tools.selBest(population, 1)[0]
        best_weights = self.normalize_weights(best_individual)
        
        # ëª¨ë¸ëª…ê³¼ ë§¤ì¹­
        model_names = list(model_predictions.keys())
        optimal_weights = {}
        
        for i, name in enumerate(model_names):
            optimal_weights[name] = best_weights[i]
        
        # ì„±ëŠ¥ í‰ê°€
        best_fitness = best_individual.fitness.values[0]
        
        self.logger.info(f"ğŸ§¬ ìœ ì „ ì•Œê³ ë¦¬ì¦˜ ìµœì í™” ì™„ë£Œ - ìµœì  ì í•©ë„: {best_fitness:.4f}")
        
        return optimal_weights

class ReinforcementEnsembleAgent:
    """
    ğŸ® ê°•í™”í•™ìŠµ ê¸°ë°˜ ë™ì  ì•™ìƒë¸” ì—ì´ì „íŠ¸
    """
    
    def __init__(self, n_models: int, learning_rate: float = 0.01):
        self.n_models = n_models
        self.learning_rate = learning_rate
        
        # Q-í…Œì´ë¸” (ìƒíƒœ x ì•¡ì…˜)
        self.q_table = {}
        self.epsilon = 0.1  # íƒí—˜ ë¹„ìœ¨
        self.gamma = 0.95   # í• ì¸ íŒ©í„°
        
        # ì„±ëŠ¥ ê¸°ë¡
        self.performance_history = []
        
        self.logger = logging.getLogger(__name__)

    def get_market_state(self, recent_prices: np.ndarray, 
                        model_performances: Dict[str, float]) -> str:
        """
        ì‹œì¥ ìƒíƒœ ì¸ì½”ë”©
        
        Args:
            recent_prices: ìµœê·¼ ê°€ê²© ë°ì´í„°
            model_performances: ëª¨ë¸ë³„ ìµœê·¼ ì„±ëŠ¥
            
        Returns:
            str: ì¸ì½”ë”©ëœ ìƒíƒœ
        """
        # 1. ì‹œì¥ íŠ¸ë Œë“œ (ìƒìŠ¹/í•˜ë½/íš¡ë³´)
        price_change = (recent_prices[-1] - recent_prices[0]) / recent_prices[0]
        if price_change > 0.02:
            trend = "bull"
        elif price_change < -0.02:
            trend = "bear"
        else:
            trend = "sideways"
        
        # 2. ë³€ë™ì„± (ë†’ìŒ/ë³´í†µ/ë‚®ìŒ)
        volatility = np.std(np.diff(recent_prices) / recent_prices[:-1])
        if volatility > 0.05:
            vol_state = "high"
        elif volatility > 0.02:
            vol_state = "medium"
        else:
            vol_state = "low"
        
        # 3. ëª¨ë¸ ì„±ëŠ¥ ìƒíƒœ (ì¢‹ìŒ/ë³´í†µ/ë‚˜ì¨)
        avg_performance = np.mean(list(model_performances.values()))
        if avg_performance > 0.7:
            perf_state = "good"
        elif avg_performance > 0.5:
            perf_state = "medium"
        else:
            perf_state = "poor"
        
        return f"{trend}_{vol_state}_{perf_state}"

    def select_models(self, state: str, available_models: List[str]) -> List[str]:
        """
        í˜„ì¬ ìƒíƒœì—ì„œ ëª¨ë¸ ì„ íƒ
        
        Args:
            state: í˜„ì¬ ì‹œì¥ ìƒíƒœ
            available_models: ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡
            
        Returns:
            List[str]: ì„ íƒëœ ëª¨ë¸ë“¤
        """
        # ìƒíƒœê°€ ì²˜ìŒ ë³´ëŠ” ê²½ìš° ì´ˆê¸°í™”
        if state not in self.q_table:
            self.q_table[state] = np.random.random(len(available_models))
        
        # Îµ-íƒìš• ì •ì±…
        if np.random.random() < self.epsilon:
            # íƒí—˜: ëœë¤ ì„ íƒ
            n_select = max(1, int(len(available_models) * 0.6))  # 60% ì„ íƒ
            selected_indices = np.random.choice(
                len(available_models), size=n_select, replace=False
            )
        else:
            # í™œìš©: Qê°’ ê¸°ë°˜ ì„ íƒ
            q_values = self.q_table[state]
            n_select = max(1, int(len(available_models) * 0.6))
            selected_indices = np.argsort(q_values)[-n_select:]  # ìƒìœ„ ëª¨ë¸ë“¤
        
        selected_models = [available_models[i] for i in selected_indices]
        return selected_models

    def update_q_values(self, prev_state: str, action_indices: List[int], 
                       reward: float, current_state: str):
        """Qê°’ ì—…ë°ì´íŠ¸"""
        if prev_state not in self.q_table:
            self.q_table[prev_state] = np.random.random(self.n_models)
        
        if current_state not in self.q_table:
            self.q_table[current_state] = np.random.random(self.n_models)
        
        # Q-learning ì—…ë°ì´íŠ¸
        for action_idx in action_indices:
            current_q = self.q_table[prev_state][action_idx]
            max_future_q = np.max(self.q_table[current_state])
            
            new_q = current_q + self.learning_rate * (
                reward + self.gamma * max_future_q - current_q
            )
            
            self.q_table[prev_state][action_idx] = new_q
        
        # íƒí—˜ ë¹„ìœ¨ ê°ì†Œ
        self.epsilon = max(0.01, self.epsilon * 0.995)

    def calculate_reward(self, ensemble_performance: float, 
                        individual_performances: List[float]) -> float:
        """ë³´ìƒ ê³„ì‚°"""
        # 1. ì•™ìƒë¸” ì„±ëŠ¥ ë³´ìƒ
        performance_reward = ensemble_performance
        
        # 2. ê°œë³„ ëª¨ë¸ ëŒ€ë¹„ ê°œì„  ë³´ìƒ
        max_individual = max(individual_performances) if individual_performances else 0
        improvement_reward = max(0, ensemble_performance - max_individual)
        
        # 3. ë‹¤ì–‘ì„± ë³´ìƒ (ì„±ëŠ¥ ë¶„ì‚°ì´ í´ìˆ˜ë¡ ì¢‹ìŒ)
        diversity_reward = np.std(individual_performances) if len(individual_performances) > 1 else 0
        
        total_reward = (
            0.6 * performance_reward +
            0.3 * improvement_reward +
            0.1 * diversity_reward
        )
        
        return total_reward

class MultiObjectiveEnsembleOptimizer:
    """
    ğŸ¯ ë‹¤ëª©ì  ì•™ìƒë¸” ìµœì í™” (ì •í™•ë„ vs ë‹¤ì–‘ì„±)
    """
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)

    def calculate_diversity_metrics(self, predictions: Dict[str, np.ndarray]) -> Dict[str, float]:
        """
        ë‹¤ì–‘ì„± ë©”íŠ¸ë¦­ ê³„ì‚°
        
        Args:
            predictions: ëª¨ë¸ë³„ ì˜ˆì¸¡ê°’
            
        Returns:
            Dict[str, float]: ë‹¤ì–‘ì„± ì§€í‘œë“¤
        """
        pred_matrix = np.array(list(predictions.values()))
        
        # 1. ìƒê´€ê´€ê³„ ê¸°ë°˜ ë‹¤ì–‘ì„±
        correlations = np.corrcoef(pred_matrix)
        avg_correlation = np.mean(correlations[np.triu_indices_from(correlations, k=1)])
        correlation_diversity = 1 - abs(avg_correlation)
        
        # 2. ë¶„ì‚° ê¸°ë°˜ ë‹¤ì–‘ì„±
        pred_variance = np.var(pred_matrix, axis=0)
        variance_diversity = np.mean(pred_variance)
        
        # 3. ê±°ë¦¬ ê¸°ë°˜ ë‹¤ì–‘ì„±
        distances = []
        model_names = list(predictions.keys())
        for i in range(len(model_names)):
            for j in range(i+1, len(model_names)):
                dist = np.mean(np.abs(pred_matrix[i] - pred_matrix[j]))
                distances.append(dist)
        
        distance_diversity = np.mean(distances) if distances else 0
        
        return {
            'correlation_diversity': correlation_diversity,
            'variance_diversity': variance_diversity,
            'distance_diversity': distance_diversity,
            'combined_diversity': np.mean([correlation_diversity, 
                                         variance_diversity/np.max(pred_variance) if np.max(pred_variance) > 0 else 0,
                                         distance_diversity])
        }

    def pareto_optimal_selection(self, model_performances: Dict[str, Dict],
                               diversity_threshold: float = 0.3) -> List[str]:
        """
        íŒŒë ˆí†  ìµœì  ëª¨ë¸ ì„ íƒ
        
        Args:
            model_performances: ëª¨ë¸ë³„ ì„±ëŠ¥ ì§€í‘œ
            diversity_threshold: ë‹¤ì–‘ì„± ì„ê³„ê°’
            
        Returns:
            List[str]: íŒŒë ˆí†  ìµœì  ëª¨ë¸ë“¤
        """
        models = []
        accuracies = []
        diversities = []
        
        for name, perf in model_performances.items():
            if 'direction_accuracy' in perf:
                models.append(name)
                accuracies.append(perf['direction_accuracy'])
                
                # ë‹¤ì–‘ì„± ì ìˆ˜ (ì„ì‹œë¡œ r2 ì ìˆ˜ì˜ ì—­ìˆ˜ ì‚¬ìš©)
                diversity_score = 1.0 - abs(perf.get('r2', 0))
                diversities.append(diversity_score)
        
        if len(models) < 2:
            return models
        
        # íŒŒë ˆí†  í”„ë¡ íŠ¸ ì°¾ê¸°
        pareto_models = []
        
        for i, (model, acc, div) in enumerate(zip(models, accuracies, diversities)):
            is_dominated = False
            
            for j, (other_acc, other_div) in enumerate(zip(accuracies, diversities)):
                if i != j:
                    # ë‹¤ë¥¸ ëª¨ë¸ì´ ì´ ëª¨ë¸ì„ ì§€ë°°í•˜ëŠ”ì§€ í™•ì¸
                    if (other_acc >= acc and other_div >= div and 
                        (other_acc > acc or other_div > div)):
                        is_dominated = True
                        break
            
            if not is_dominated:
                pareto_models.append(model)
        
        return pareto_models

class OnlineEnsembleAdapter:
    """
    ğŸ”„ ì˜¨ë¼ì¸ ì ì‘í˜• ì•™ìƒë¸”
    """
    
    def __init__(self, adaptation_window: int = 100):
        self.adaptation_window = adaptation_window
        self.recent_performances = {}
        self.weight_history = []
        
        self.logger = logging.getLogger(__name__)

    def update_model_performance(self, model_name: str, accuracy: float, 
                               timestamp: datetime):
        """ëª¨ë¸ ì„±ëŠ¥ ì—…ë°ì´íŠ¸"""
        if model_name not in self.recent_performances:
            self.recent_performances[model_name] = []
        
        self.recent_performances[model_name].append({
            'accuracy': accuracy,
            'timestamp': timestamp
        })
        
        # ìœˆë„ìš° í¬ê¸° ìœ ì§€
        if len(self.recent_performances[model_name]) > self.adaptation_window:
            self.recent_performances[model_name] = \
                self.recent_performances[model_name][-self.adaptation_window:]

    def adaptive_weight_adjustment(self, current_weights: Dict[str, float]) -> Dict[str, float]:
        """
        ì ì‘í˜• ê°€ì¤‘ì¹˜ ì¡°ì •
        
        Args:
            current_weights: í˜„ì¬ ê°€ì¤‘ì¹˜
            
        Returns:
            Dict[str, float]: ì¡°ì •ëœ ê°€ì¤‘ì¹˜
        """
        adjusted_weights = current_weights.copy()
        
        # ìµœê·¼ ì„±ëŠ¥ ê¸°ë°˜ ì¡°ì •
        for model_name in current_weights.keys():
            if (model_name in self.recent_performances and 
                len(self.recent_performances[model_name]) >= 10):
                
                recent_scores = [p['accuracy'] for p in 
                               self.recent_performances[model_name][-10:]]
                
                # ì„±ëŠ¥ íŠ¸ë Œë“œ ê³„ì‚°
                if len(recent_scores) >= 5:
                    recent_avg = np.mean(recent_scores[-5:])
                    older_avg = np.mean(recent_scores[:-5])
                    trend = (recent_avg - older_avg) / older_avg if older_avg > 0 else 0
                    
                    # ê°€ì¤‘ì¹˜ ì¡°ì • (Â±20% ë²”ìœ„)
                    adjustment = min(0.2, max(-0.2, trend))
                    adjusted_weights[model_name] *= (1 + adjustment)
        
        # ì •ê·œí™”
        total_weight = sum(adjusted_weights.values())
        if total_weight > 0:
            adjusted_weights = {name: weight/total_weight 
                              for name, weight in adjusted_weights.items()}
        
        # ê°€ì¤‘ì¹˜ ë³€í™” ê¸°ë¡
        self.weight_history.append({
            'timestamp': datetime.now(),
            'weights': adjusted_weights.copy()
        })
        
        return adjusted_weights

    def detect_concept_drift(self, recent_accuracies: List[float], 
                           window_size: int = 20) -> bool:
        """
        ê°œë… ì´ë™ ê°ì§€
        
        Args:
            recent_accuracies: ìµœê·¼ ì •í™•ë„ë“¤
            window_size: ë¹„êµ ìœˆë„ìš° í¬ê¸°
            
        Returns:
            bool: ê°œë… ì´ë™ ê°ì§€ ì—¬ë¶€
        """
        if len(recent_accuracies) < 2 * window_size:
            return False
        
        # ìµœê·¼ ìœˆë„ìš°ì™€ ì´ì „ ìœˆë„ìš° ë¹„êµ
        recent_window = recent_accuracies[-window_size:]
        older_window = recent_accuracies[-2*window_size:-window_size]
        
        # t-ê²€ì • (ê°„ë‹¨í•œ ë²„ì „)
        recent_mean = np.mean(recent_window)
        older_mean = np.mean(older_window)
        
        # ì„±ëŠ¥ì´ 10% ì´ìƒ ì €í•˜ë˜ë©´ ê°œë… ì´ë™ìœ¼ë¡œ ê°„ì£¼
        performance_drop = (older_mean - recent_mean) / older_mean if older_mean > 0 else 0
        
        if performance_drop > 0.1:
            self.logger.warning(f"ğŸš¨ ê°œë… ì´ë™ ê°ì§€ - ì„±ëŠ¥ ì €í•˜: {performance_drop:.2%}")
            return True
        
        return False

class AdvancedModelSelectionSystem:
    """
    ğŸ¯ ê³ ê¸‰ ëª¨ë¸ ì„ íƒ ë° ìµœì í™” í†µí•© ì‹œìŠ¤í…œ
    """
    
    def __init__(self):
        self.genetic_optimizer = GeneticEnsembleOptimizer()
        self.rl_agent = None
        self.multi_objective_optimizer = MultiObjectiveEnsembleOptimizer()
        self.online_adapter = OnlineEnsembleAdapter()
        
        # ì„±ëŠ¥ ì¶”ì 
        self.optimization_history = []
        
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('model_selection_optimizer.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)

    def comprehensive_model_optimization(self, 
                                       model_predictions: Dict[str, np.ndarray],
                                       y_val: np.ndarray,
                                       model_performances: Dict[str, Dict]) -> Dict:
        """
        ì¢…í•©ì ì¸ ëª¨ë¸ ìµœì í™”
        
        Args:
            model_predictions: ëª¨ë¸ë³„ ì˜ˆì¸¡ê°’
            y_val: ê²€ì¦ íƒ€ê²Ÿ
            model_performances: ëª¨ë¸ë³„ ì„±ëŠ¥ ì§€í‘œ
            
        Returns:
            Dict: ìµœì í™” ê²°ê³¼
        """
        print("ğŸ¯ ê³ ê¸‰ ëª¨ë¸ ì„ íƒ ë° ìµœì í™” ì‹œì‘...")
        
        start_time = datetime.now()
        
        # 1. ìœ ì „ ì•Œê³ ë¦¬ì¦˜ ìµœì í™”
        print("ğŸ§¬ ìœ ì „ ì•Œê³ ë¦¬ì¦˜ ê°€ì¤‘ì¹˜ ìµœì í™”...")
        genetic_weights = self.genetic_optimizer.optimize_ensemble_weights(
            model_predictions, y_val
        )
        
        # 2. ë‹¤ëª©ì  ìµœì í™” - íŒŒë ˆí†  ìµœì  ëª¨ë¸ ì„ íƒ
        print("ğŸ¯ íŒŒë ˆí†  ìµœì  ëª¨ë¸ ì„ íƒ...")
        pareto_models = self.multi_objective_optimizer.pareto_optimal_selection(
            model_performances
        )
        
        # 3. ë‹¤ì–‘ì„± ë©”íŠ¸ë¦­ ê³„ì‚°
        print("ğŸ“Š ì•™ìƒë¸” ë‹¤ì–‘ì„± ë¶„ì„...")
        diversity_metrics = self.multi_objective_optimizer.calculate_diversity_metrics(
            model_predictions
        )
        
        # 4. ê°•í™”í•™ìŠµ ì—ì´ì „íŠ¸ ì´ˆê¸°í™” (ëª¨ë¸ ìˆ˜ê°€ ì¶©ë¶„í•œ ê²½ìš°)
        if len(model_predictions) >= 3 and REINFORCEMENT_AVAILABLE:
            print("ğŸ® ê°•í™”í•™ìŠµ ëª¨ë¸ ì„ íƒ ì—ì´ì „íŠ¸ ì´ˆê¸°í™”...")
            self.rl_agent = ReinforcementEnsembleAgent(len(model_predictions))
            
            # ì‹œì¥ ìƒíƒœ ê³„ì‚° (ì„ì‹œ)
            recent_prices = y_val[-20:] if len(y_val) >= 20 else y_val
            model_perf_dict = {name: perf.get('direction_accuracy', 0) 
                              for name, perf in model_performances.items()}
            
            market_state = self.rl_agent.get_market_state(recent_prices, model_perf_dict)
            rl_selected_models = self.rl_agent.select_models(
                market_state, list(model_predictions.keys())
            )
        else:
            rl_selected_models = list(model_predictions.keys())
        
        # 5. ì—¬ëŸ¬ ë°©ë²•ì˜ ê²°ê³¼ í†µí•©
        print("ğŸ”„ ìµœì í™” ê²°ê³¼ í†µí•©...")
        
        # ìœ ì „ ì•Œê³ ë¦¬ì¦˜ ì•™ìƒë¸” í‰ê°€
        genetic_ensemble = np.zeros(len(y_val))
        for model_name, pred in model_predictions.items():
            genetic_ensemble += genetic_weights.get(model_name, 0) * pred
        
        genetic_accuracy = self.calculate_direction_accuracy(y_val, genetic_ensemble)
        
        # íŒŒë ˆí†  ìµœì  ì•™ìƒë¸” (ê· ë“± ê°€ì¤‘ì¹˜)
        pareto_ensemble = np.zeros(len(y_val))
        pareto_weight = 1.0 / len(pareto_models) if pareto_models else 0
        
        for model_name in pareto_models:
            if model_name in model_predictions:
                pareto_ensemble += pareto_weight * model_predictions[model_name]
        
        pareto_accuracy = self.calculate_direction_accuracy(y_val, pareto_ensemble) if pareto_models else 0
        
        # ê°•í™”í•™ìŠµ ì„ íƒ ì•™ìƒë¸” (ê· ë“± ê°€ì¤‘ì¹˜)
        rl_ensemble = np.zeros(len(y_val))
        rl_weight = 1.0 / len(rl_selected_models) if rl_selected_models else 0
        
        for model_name in rl_selected_models:
            if model_name in model_predictions:
                rl_ensemble += rl_weight * model_predictions[model_name]
        
        rl_accuracy = self.calculate_direction_accuracy(y_val, rl_ensemble) if rl_selected_models else 0
        
        # 6. ìµœì  ë°©ë²• ì„ íƒ
        optimization_results = [
            {
                'method': 'genetic_algorithm',
                'weights': genetic_weights,
                'accuracy': genetic_accuracy,
                'selected_models': list(genetic_weights.keys())
            },
            {
                'method': 'pareto_optimal',
                'weights': {name: pareto_weight for name in pareto_models},
                'accuracy': pareto_accuracy,
                'selected_models': pareto_models
            },
            {
                'method': 'reinforcement_learning',
                'weights': {name: rl_weight for name in rl_selected_models},
                'accuracy': rl_accuracy,
                'selected_models': rl_selected_models
            }
        ]
        
        # ìµœê³  ì„±ëŠ¥ ë°©ë²• ì„ íƒ
        best_method = max(optimization_results, key=lambda x: x['accuracy'])
        
        # 7. ê²°ê³¼ ì •ë¦¬
        end_time = datetime.now()
        optimization_time = (end_time - start_time).total_seconds()
        
        result = {
            'optimization_completed': end_time.isoformat(),
            'optimization_time_seconds': optimization_time,
            'best_method': best_method,
            'all_methods': optimization_results,
            'diversity_metrics': diversity_metrics,
            'genetic_weights': genetic_weights,
            'pareto_models': pareto_models,
            'rl_selected_models': rl_selected_models,
            'total_models_evaluated': len(model_predictions)
        }
        
        # ìµœì í™” ê¸°ë¡ ì €ì¥
        self.optimization_history.append(result)
        
        # ê²°ê³¼ ì¶œë ¥
        print("\n" + "="*50)
        print("ğŸ¯ ëª¨ë¸ ì„ íƒ ìµœì í™” ì™„ë£Œ!")
        print("="*50)
        print(f"â±ï¸  ìµœì í™” ì‹œê°„: {optimization_time:.1f}ì´ˆ")
        print(f"ğŸ† ìµœì  ë°©ë²•: {best_method['method']}")
        print(f"ğŸ“ˆ ìµœì  ì •í™•ë„: {best_method['accuracy']:.3f} ({best_method['accuracy']*100:.1f}%)")
        print(f"ğŸ¤– ì„ íƒëœ ëª¨ë¸ ìˆ˜: {len(best_method['selected_models'])}ê°œ")
        print(f"ğŸ“Š ì•™ìƒë¸” ë‹¤ì–‘ì„±: {diversity_metrics['combined_diversity']:.3f}")
        
        if best_method['accuracy'] >= 0.90:
            print("ğŸ‰ 90% ëª©í‘œ ë‹¬ì„±!")
        
        return result

    def calculate_direction_accuracy(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:
        """ë°©í–¥ì„± ì •í™•ë„ ê³„ì‚°"""
        if len(y_true) <= 1:
            return 0.0
        
        direction_actual = np.diff(y_true) > 0
        direction_pred = np.diff(y_pred) > 0
        return np.mean(direction_actual == direction_pred)

    def save_optimization_results(self, results: Dict, 
                                 file_path: str = None) -> str:
        """ìµœì í™” ê²°ê³¼ ì €ì¥"""
        if file_path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            file_path = f"/Users/parkyoungjun/Desktop/BTC_Analysis_System/model_optimization_results_{timestamp}.json"
        
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)
        
        self.logger.info(f"ğŸ’¾ ìµœì í™” ê²°ê³¼ ì €ì¥: {file_path}")
        return file_path

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ¯ ê³ ê¸‰ ëª¨ë¸ ì„ íƒ ë° ì•™ìƒë¸” ìµœì í™” ì‹œìŠ¤í…œ")
    
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    optimizer = AdvancedModelSelectionSystem()
    
    print("âœ… ê³ ê¸‰ ëª¨ë¸ ì„ íƒ ìµœì í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    print("ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ê¸°ëŠ¥:")
    print("  ğŸ§¬ ìœ ì „ ì•Œê³ ë¦¬ì¦˜ ê°€ì¤‘ì¹˜ ìµœì í™”")
    print("  ğŸ¯ íŒŒë ˆí†  ìµœì  ëª¨ë¸ ì„ íƒ")
    print("  ğŸ“Š ë‹¤ëª©ì  ìµœì í™” (ì •í™•ë„ vs ë‹¤ì–‘ì„±)")
    if REINFORCEMENT_AVAILABLE:
        print("  ğŸ® ê°•í™”í•™ìŠµ ê¸°ë°˜ ë™ì  ëª¨ë¸ ì„ íƒ")
    print("  ğŸ”„ ì˜¨ë¼ì¸ ì ì‘í˜• ì•™ìƒë¸”")
    
    return optimizer

if __name__ == "__main__":
    optimizer = main()