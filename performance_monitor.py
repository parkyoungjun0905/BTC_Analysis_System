#!/usr/bin/env python3
"""
ğŸ” BTC í•™ìŠµ ì‹œìŠ¤í…œ ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
- 95% ì •í™•ë„ ë‹¬ì„± ê°ì§€
- ì§€ì†ì ì¸ ì„±ëŠ¥ ì¶”ì 
- ìë™ ì•Œë¦¼ ë° ë³´ê³ ì„œ
"""

import json
import time
import datetime
from typing import Dict, List, Optional
from dataclasses import dataclass
import logging

@dataclass
class PerformanceMetrics:
    """ì„±ëŠ¥ ì§€í‘œ ë°ì´í„° í´ë˜ìŠ¤"""
    timestamp: str
    test_point: int
    direction_accuracy: float
    price_accuracy: float
    timing_accuracy: float
    combined_accuracy: float
    confidence: float
    predicted_price: float
    actual_price: float
    price_error_rate: float

class RealTimeMonitor:
    """ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ"""
    
    def __init__(self, target_accuracy: float = 0.95):
        self.target_accuracy = target_accuracy
        self.metrics_history: List[PerformanceMetrics] = []
        self.achievement_points: List[Dict] = []
        self.current_streak = 0
        self.best_accuracy = 0.0
        self.total_tests = 0
        
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('performance_monitor.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
    def add_test_result(self, test_point: int, direction_correct: bool, 
                       price_accuracy: float, timing_accuracy: float,
                       confidence: float, predicted_price: float, 
                       actual_price: float) -> None:
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ ì¶”ê°€í•˜ê³  ë¶„ì„"""
        
        # ì¢…í•© ì •í™•ë„ ê³„ì‚° (ë°©í–¥ 50%, ê°€ê²© 30%, íƒ€ì´ë° 20%)
        direction_score = 100.0 if direction_correct else 0.0
        combined_accuracy = (
            direction_score * 0.5 + 
            price_accuracy * 0.3 + 
            timing_accuracy * 0.2
        ) / 100.0
        
        # ê°€ê²© ì˜¤ì°¨ìœ¨ ê³„ì‚°
        price_error_rate = abs(predicted_price - actual_price) / actual_price * 100
        
        # ë©”íŠ¸ë¦­ ìƒì„±
        metrics = PerformanceMetrics(
            timestamp=datetime.datetime.now().isoformat(),
            test_point=test_point,
            direction_accuracy=direction_score,
            price_accuracy=price_accuracy,
            timing_accuracy=timing_accuracy,
            combined_accuracy=combined_accuracy * 100,  # í¼ì„¼íŠ¸ë¡œ ë³€í™˜
            confidence=confidence,
            predicted_price=predicted_price,
            actual_price=actual_price,
            price_error_rate=price_error_rate
        )
        
        self.metrics_history.append(metrics)
        self.total_tests += 1
        
        # ì„±ëŠ¥ ë¶„ì„
        self._analyze_performance(metrics)
        
        # 95% ë‹¬ì„± ì²´í¬
        if combined_accuracy >= self.target_accuracy:
            self._handle_target_achievement(metrics)
            
    def _analyze_performance(self, metrics: PerformanceMetrics) -> None:
        """ì„±ëŠ¥ ë¶„ì„ ë° ë¡œê¹…"""
        
        # ìµœê³  ì •í™•ë„ ì—…ë°ì´íŠ¸
        if metrics.combined_accuracy > self.best_accuracy:
            self.best_accuracy = metrics.combined_accuracy
            self.logger.info(f"ğŸš€ ì‹ ê¸°ë¡ ë‹¬ì„±: {self.best_accuracy:.1f}%")
            
        # ì—°ì† ì„±ê³µ ìŠ¤íŠ¸ë¦­ ì¶”ì 
        if metrics.combined_accuracy >= 90.0:  # 90% ì´ìƒ
            self.current_streak += 1
            if self.current_streak >= 3:
                self.logger.info(f"ğŸ”¥ ì—°ì† ê³ ì„±ëŠ¥: {self.current_streak}íšŒ ì—°ì† 90%+")
        else:
            self.current_streak = 0
            
        # ì‹¤ì‹œê°„ ìƒíƒœ ë¡œê¹…
        status_emoji = "ğŸ¯" if metrics.combined_accuracy >= 95.0 else "ğŸ“ˆ" if metrics.combined_accuracy >= 90.0 else "ğŸ”„"
        
        self.logger.info(
            f"{status_emoji} í…ŒìŠ¤íŠ¸ {metrics.test_point}: "
            f"ì¢…í•© {metrics.combined_accuracy:.1f}% "
            f"(ë°©í–¥ì„±: {'âœ…' if metrics.direction_accuracy > 0 else 'âŒ'}, "
            f"ê°€ê²©: {metrics.price_accuracy:.1f}%, "
            f"íƒ€ì´ë°: {metrics.timing_accuracy:.1f}%, "
            f"ì‹ ë¢°ë„: {metrics.confidence:.2f})"
        )
        
    def _handle_target_achievement(self, metrics: PerformanceMetrics) -> None:
        """95% ëª©í‘œ ë‹¬ì„± ì²˜ë¦¬"""
        
        achievement = {
            "timestamp": metrics.timestamp,
            "test_point": metrics.test_point,
            "accuracy": metrics.combined_accuracy,
            "details": {
                "direction": metrics.direction_accuracy,
                "price": metrics.price_accuracy,
                "timing": metrics.timing_accuracy,
                "confidence": metrics.confidence,
                "price_error": metrics.price_error_rate
            }
        }
        
        self.achievement_points.append(achievement)
        
        self.logger.info("ğŸ‰" * 10)
        self.logger.info(f"ğŸ¯ 95% ëª©í‘œ ë‹¬ì„±! í…ŒìŠ¤íŠ¸ {metrics.test_point}")
        self.logger.info(f"ğŸ“Š ì¢…í•© ì •í™•ë„: {metrics.combined_accuracy:.1f}%")
        self.logger.info(f"   - ë°©í–¥ì„±: {metrics.direction_accuracy:.1f}%")
        self.logger.info(f"   - ê°€ê²©: {metrics.price_accuracy:.1f}%")
        self.logger.info(f"   - íƒ€ì´ë°: {metrics.timing_accuracy:.1f}%")
        self.logger.info(f"ğŸ’° ê°€ê²© ì˜ˆì¸¡: ${metrics.predicted_price:,.2f} â†’ ${metrics.actual_price:,.2f}")
        self.logger.info(f"ğŸ“‰ ì˜¤ì°¨ìœ¨: {metrics.price_error_rate:.2f}%")
        self.logger.info("ğŸ‰" * 10)
        
    def generate_performance_report(self) -> Dict:
        """ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±"""
        
        if not self.metrics_history:
            return {"error": "No data available"}
            
        # í†µê³„ ê³„ì‚°
        recent_10 = self.metrics_history[-10:] if len(self.metrics_history) >= 10 else self.metrics_history
        
        avg_accuracy = sum(m.combined_accuracy for m in recent_10) / len(recent_10)
        avg_price_error = sum(m.price_error_rate for m in recent_10) / len(recent_10)
        direction_success_rate = sum(1 for m in recent_10 if m.direction_accuracy > 0) / len(recent_10) * 100
        
        # 95% ë‹¬ì„± í†µê³„
        achievement_count = len(self.achievement_points)
        achievement_rate = (achievement_count / self.total_tests * 100) if self.total_tests > 0 else 0
        
        report = {
            "summary": {
                "ì´_í…ŒìŠ¤íŠ¸_ìˆ˜": self.total_tests,
                "95%_ë‹¬ì„±_íšŸìˆ˜": achievement_count,
                "95%_ë‹¬ì„±ë¥ ": f"{achievement_rate:.1f}%",
                "ìµœê³ _ì •í™•ë„": f"{self.best_accuracy:.1f}%",
                "í˜„ì¬_ì—°ì†_ê³ ì„±ëŠ¥": self.current_streak
            },
            "ìµœê·¼_10íšŒ_í‰ê· ": {
                "ì¢…í•©_ì •í™•ë„": f"{avg_accuracy:.1f}%",
                "ë°©í–¥ì„±_ì„±ê³µë¥ ": f"{direction_success_rate:.1f}%",
                "í‰ê· _ê°€ê²©_ì˜¤ì°¨": f"{avg_price_error:.2f}%"
            },
            "ë‹¬ì„±_ê¸°ë¡": self.achievement_points[-5:] if self.achievement_points else [],
            "ì„±ëŠ¥_íŠ¸ë Œë“œ": self._calculate_trend(),
            "ìƒì„±_ì‹œê°": datetime.datetime.now().isoformat()
        }
        
        return report
        
    def _calculate_trend(self) -> str:
        """ì„±ëŠ¥ íŠ¸ë Œë“œ ê³„ì‚°"""
        
        if len(self.metrics_history) < 10:
            return "ë°ì´í„° ë¶€ì¡±"
            
        recent_5 = self.metrics_history[-5:]
        previous_5 = self.metrics_history[-10:-5]
        
        recent_avg = sum(m.combined_accuracy for m in recent_5) / 5
        previous_avg = sum(m.combined_accuracy for m in previous_5) / 5
        
        diff = recent_avg - previous_avg
        
        if diff > 5:
            return "ğŸš€ ê¸‰ìƒìŠ¹"
        elif diff > 2:
            return "ğŸ“ˆ ìƒìŠ¹"
        elif diff > -2:
            return "â¡ï¸ ì•ˆì •"
        elif diff > -5:
            return "ğŸ“‰ í•˜ë½"
        else:
            return "âš ï¸ ê¸‰í•˜ë½"
            
    def save_metrics_to_file(self, filename: str = "performance_metrics.json") -> None:
        """ë©”íŠ¸ë¦­ì„ íŒŒì¼ë¡œ ì €ì¥"""
        
        data = {
            "metrics_history": [
                {
                    "timestamp": m.timestamp,
                    "test_point": m.test_point,
                    "direction_accuracy": m.direction_accuracy,
                    "price_accuracy": m.price_accuracy,
                    "timing_accuracy": m.timing_accuracy,
                    "combined_accuracy": m.combined_accuracy,
                    "confidence": m.confidence,
                    "predicted_price": m.predicted_price,
                    "actual_price": m.actual_price,
                    "price_error_rate": m.price_error_rate
                }
                for m in self.metrics_history
            ],
            "achievement_points": self.achievement_points,
            "summary": self.generate_performance_report()
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
            
        self.logger.info(f"ğŸ“„ ì„±ëŠ¥ ë°ì´í„° ì €ì¥: {filename}")
        
    def load_metrics_from_file(self, filename: str = "performance_metrics.json") -> bool:
        """íŒŒì¼ì—ì„œ ë©”íŠ¸ë¦­ ë¡œë“œ"""
        
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
            # ë©”íŠ¸ë¦­ íˆìŠ¤í† ë¦¬ ë³µì›
            self.metrics_history = []
            for m_data in data.get("metrics_history", []):
                metrics = PerformanceMetrics(
                    timestamp=m_data["timestamp"],
                    test_point=m_data["test_point"],
                    direction_accuracy=m_data["direction_accuracy"],
                    price_accuracy=m_data["price_accuracy"],
                    timing_accuracy=m_data["timing_accuracy"],
                    combined_accuracy=m_data["combined_accuracy"],
                    confidence=m_data["confidence"],
                    predicted_price=m_data["predicted_price"],
                    actual_price=m_data["actual_price"],
                    price_error_rate=m_data["price_error_rate"]
                )
                self.metrics_history.append(metrics)
                
            # ë‹¬ì„± ê¸°ë¡ ë³µì›
            self.achievement_points = data.get("achievement_points", [])
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self.total_tests = len(self.metrics_history)
            if self.metrics_history:
                self.best_accuracy = max(m.combined_accuracy for m in self.metrics_history)
                
            self.logger.info(f"ğŸ“‚ ì„±ëŠ¥ ë°ì´í„° ë¡œë“œ: {len(self.metrics_history)}ê°œ ê¸°ë¡")
            return True
            
        except FileNotFoundError:
            self.logger.warning(f"ğŸ“‚ íŒŒì¼ ì—†ìŒ: {filename}")
            return False
        except Exception as e:
            self.logger.error(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False

def main():
    """ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    
    print("ğŸ” BTC ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì‹œì‘")
    
    monitor = RealTimeMonitor()
    
    # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ì‹œë„
    monitor.load_metrics_from_file()
    
    # ìƒ˜í”Œ ë°ì´í„° ì¶”ê°€ (ì‹¤ì œë¡œëŠ” btc_learning_system.pyì—ì„œ í˜¸ì¶œ)
    sample_results = [
        (168, False, 86.8, 40.4, 0.67, 83015.12, 73320.95),  # ì´ˆê¸° ì €ì„±ëŠ¥
        (172, True, 60.0, 60.0, 1.00, 80235.87, 73441.35),   # ì²« ì„±ê³µ
        (214, True, 100.0, 79.0, 0.90, 73686.25, 74258.53), # 95% ë‹¬ì„±!
    ]
    
    for test_point, direction_correct, price_acc, timing_acc, confidence, pred_price, actual_price in sample_results:
        monitor.add_test_result(
            test_point=test_point,
            direction_correct=direction_correct,
            price_accuracy=price_acc,
            timing_accuracy=timing_acc,
            confidence=confidence,
            predicted_price=pred_price,
            actual_price=actual_price
        )
        time.sleep(0.5)  # ì‹œë®¬ë ˆì´ì…˜ì„ ìœ„í•œ ë”œë ˆì´
    
    # ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±
    report = monitor.generate_performance_report()
    print("\nğŸ“Š ì„±ëŠ¥ ë³´ê³ ì„œ:")
    print(json.dumps(report, ensure_ascii=False, indent=2))
    
    # íŒŒì¼ë¡œ ì €ì¥
    monitor.save_metrics_to_file()

if __name__ == "__main__":
    main()