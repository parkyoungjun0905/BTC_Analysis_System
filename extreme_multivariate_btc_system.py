#!/usr/bin/env python3
"""
ğŸš€ ê·¹í•œ ë‹¤ë°©ë©´ BTC ë¶„ì„ ì‹œìŠ¤í…œ

íŠ¹ì§•:
- 1039ê°œ ì „ì²´ ë³€ìˆ˜ í™œìš©
- 50+ ë‹¤ì–‘í•œ AI ëª¨ë¸ ì•™ìƒë¸”
- ì‹œê°„, ì£¼ê¸°, íŒ¨í„´, í†µê³„, ê²½ì œ, ì‹¬ë¦¬ ë“± ëª¨ë“  ë¶„ì„
- í•˜ì´í¼íŒŒë¼ë¯¸í„° ìë™ ìµœì í™”
- ë™ì  ê°€ì¤‘ì¹˜ ì‹¤ì‹œê°„ ì¡°ì •
- ê·¹í•œ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§

ëª©í‘œ: 99.9% ì •í™•ë„
"""

import os
import json
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Any, Union
import logging
import warnings
warnings.filterwarnings('ignore')

# ë¨¸ì‹ ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬
from sklearn.ensemble import (
    RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor,
    AdaBoostRegressor, BaggingRegressor, VotingRegressor
)
from sklearn.neural_network import MLPRegressor
from sklearn.linear_model import (
    Ridge, Lasso, ElasticNet, BayesianRidge, 
    LinearRegression, HuberRegressor, TheilSenRegressor
)
from sklearn.svm import SVR, NuSVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.preprocessing import (
    StandardScaler, RobustScaler, MinMaxScaler, MaxAbsScaler,
    PowerTransformer, QuantileTransformer, PolynomialFeatures
)
from sklearn.decomposition import PCA, FastICA, TruncatedSVD
from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.cluster import KMeans
from sklearn.mixture import GaussianMixture

# í†µê³„ ë° ì‹ í˜¸ì²˜ë¦¬
try:
    from scipy import signal, stats
    from scipy.fft import fft, fftfreq
    import scipy.optimize as optimize
    scipy_available = True
except ImportError:
    scipy_available = False

# ê³ ê¸‰ ë¶„ì„ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    import ta
    ta_available = True
except ImportError:
    ta_available = False

class ExtremeMultivariateBTCSystem:
    """ê·¹í•œ ë‹¤ë°©ë©´ BTC ë¶„ì„ ì‹œìŠ¤í…œ"""
    
    def __init__(self, data_path: str = "ai_optimized_3month_data/integrated_complete_data.json"):
        """ê·¹í•œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        self.base_path = "/Users/parkyoungjun/Desktop/BTC_Analysis_System"
        self.data_path = os.path.join(self.base_path, data_path)
        
        # ê·¹í•œ ì„±ëŠ¥ ëª©í‘œ
        self.target_accuracy = 0.999  # 99.9% ì •í™•ë„
        self.target_price_error = 0.01  # 1% ì´í•˜ ê°€ê²© ì˜¤ì°¨
        
        # ë¡œê¹… ì„¤ì • ë¨¼ì €
        self.setup_logging()
        
        # ë°ì´í„° ë¡œë“œ
        self.data = self.load_data()
        
        # ëª¨ë“  ë³€ìˆ˜ ì¶”ì¶œ
        self.all_variables = self.extract_all_variables()
        self.logger.info(f"ğŸ¯ ì „ì²´ ë³€ìˆ˜ ê°œìˆ˜: {len(self.all_variables)}")
        
        # ê·¹í•œ ëª¨ë¸ ì»¬ë ‰ì…˜ (50+ ëª¨ë¸)
        self.extreme_models = {}
        
        # ë‹¤ì–‘í•œ ì „ì²˜ë¦¬ê¸°ë“¤
        self.preprocessors = {
            'scalers': {
                'standard': StandardScaler(),
                'robust': RobustScaler(),
                'minmax': MinMaxScaler(),
                'maxabs': MaxAbsScaler(),
                'power_yj': PowerTransformer(method='yeo-johnson'),
                'power_box': PowerTransformer(method='box-cox', standardize=True),
                'quantile_uniform': QuantileTransformer(output_distribution='uniform'),
                'quantile_normal': QuantileTransformer(output_distribution='normal')
            },
            'decompositions': {
                'pca_50': PCA(n_components=50),
                'pca_100': PCA(n_components=100),
                'pca_200': PCA(n_components=200),
                'ica_50': FastICA(n_components=50),
                'ica_100': FastICA(n_components=100),
                'svd_50': TruncatedSVD(n_components=50),
                'svd_100': TruncatedSVD(n_components=100)
            },
            'feature_selectors': {
                'kbest_100': SelectKBest(f_regression, k=100),
                'kbest_200': SelectKBest(f_regression, k=200),
                'kbest_500': SelectKBest(f_regression, k=500),
                'mutual_100': SelectKBest(mutual_info_regression, k=100),
                'mutual_200': SelectKBest(mutual_info_regression, k=200)
            }
        }
        
        # ê·¹í•œ ëª¨ë¸ ì´ˆê¸°í™”
        self.initialize_extreme_models()
        
        self.logger.info("ğŸš€ ê·¹í•œ ë‹¤ë°©ë©´ BTC ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        
    def setup_logging(self):
        """ë¡œê¹… ì‹œìŠ¤í…œ"""
        log_path = os.path.join(self.base_path, "logs")
        os.makedirs(log_path, exist_ok=True)
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(os.path.join(log_path, 'extreme_multivariate_system.log')),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
    def load_data(self) -> Dict:
        """ë°ì´í„° ë¡œë“œ"""
        with open(self.data_path, 'r', encoding='utf-8') as f:
            return json.load(f)
            
    def extract_all_variables(self) -> Dict[str, List]:
        """ëª¨ë“  ë³€ìˆ˜ ì¶”ì¶œ (1039ê°œ + ì¶”ê°€ ìƒì„± ë³€ìˆ˜ë“¤)"""
        
        all_vars = {}
        
        # 1. í•µì‹¬ ì§€í‘œë“¤ (1039ê°œ)
        critical_features = self.data['timeseries_complete']['critical_features']
        for name, data in critical_features.items():
            if 'values' in data:
                all_vars[f"critical_{name}"] = data['values']
                
        # 2. ì¤‘ìš” ì§€í‘œë“¤
        important_features = self.data['timeseries_complete']['important_features']
        for name, data in important_features.items():
            if 'values' in data:
                all_vars[f"important_{name}"] = data['values']
                
        # 3. ê°€ê²© ê´€ë ¨ ë³€ìˆ˜ íŠ¹ë³„ ì²˜ë¦¬
        price_vars = {}
        for name, values in all_vars.items():
            if any(keyword in name.lower() for keyword in ['price', 'btc', 'market']):
                price_vars[name] = values
                
        # 4. íŒŒìƒ ë³€ìˆ˜ë“¤ ìƒì„± (ê¸°ì¡´ ë³€ìˆ˜ë“¤ë¡œë¶€í„°)
        if price_vars:
            first_price_var = list(price_vars.values())[0]
            prices = [float(p) if p is not None else 0.0 for p in first_price_var]
            
            # ìˆ˜ë§ì€ íŒŒìƒ ë³€ìˆ˜ë“¤
            derived_vars = self.generate_derived_variables(prices, all_vars)
            all_vars.update(derived_vars)
            
        self.logger.info(f"âœ… ì „ì²´ ë³€ìˆ˜ ì¶”ì¶œ ì™„ë£Œ: {len(all_vars)}ê°œ")
        return all_vars
        
    def generate_derived_variables(self, prices: List[float], base_vars: Dict) -> Dict[str, List]:
        """ê·¹í•œ íŒŒìƒ ë³€ìˆ˜ ìƒì„±"""
        
        derived = {}
        
        if len(prices) < 200:
            return derived
            
        # 1. ë‹¤ì–‘í•œ ê¸°ê°„ì˜ ì´ë™í‰ê·  (20ê°œ)
        ma_periods = [5, 10, 15, 20, 25, 30, 50, 60, 75, 100, 120, 150, 200, 250, 300, 360, 480, 600, 720, 1000]
        for period in ma_periods:
            if len(prices) >= period:
                ma_values = []
                for i in range(len(prices)):
                    if i >= period - 1:
                        ma = np.mean(prices[i-period+1:i+1])
                        ma_values.append(ma)
                    else:
                        ma_values.append(prices[i] if i < len(prices) else 0)
                derived[f"MA_{period}"] = ma_values
                
                # ê°€ê²© ëŒ€ë¹„ ì´ë™í‰ê·  ë¹„ìœ¨
                ratio_values = [prices[i] / ma_values[i] - 1 if ma_values[i] > 0 else 0 for i in range(len(prices))]
                derived[f"MA_{period}_RATIO"] = ratio_values
                
        # 2. ë³€ë™ì„± ì§€í‘œë“¤ (15ê°œ)
        volatility_windows = [10, 20, 30, 50, 100, 200, 250, 500, 750, 1000, 1200, 1440, 1800, 2160]
        for window in volatility_windows:
            if len(prices) >= window:
                vol_values = []
                for i in range(len(prices)):
                    if i >= window - 1:
                        window_prices = prices[i-window+1:i+1]
                        vol = np.std(window_prices) / np.mean(window_prices) if np.mean(window_prices) > 0 else 0
                        vol_values.append(vol)
                    else:
                        vol_values.append(0)
                derived[f"VOLATILITY_{window}"] = vol_values
                
        # 3. ëª¨ë©˜í…€ ì§€í‘œë“¤ (25ê°œ)
        momentum_periods = [1, 3, 6, 12, 24, 48, 72, 96, 120, 168, 240, 336, 480, 720, 1008, 1440, 1680, 2016, 2160]
        for period in momentum_periods:
            if len(prices) > period:
                momentum_values = []
                for i in range(len(prices)):
                    if i >= period:
                        momentum = (prices[i] - prices[i-period]) / prices[i-period] if prices[i-period] > 0 else 0
                        momentum_values.append(momentum)
                    else:
                        momentum_values.append(0)
                derived[f"MOMENTUM_{period}"] = momentum_values
                
        # 4. RSI ë‹¤ì–‘í•œ ê¸°ê°„ (10ê°œ)
        rsi_periods = [7, 14, 21, 28, 50, 100, 200, 300, 500, 1000]
        for period in rsi_periods:
            if len(prices) >= period + 1:
                rsi_values = []
                for i in range(len(prices)):
                    if i >= period:
                        price_changes = np.diff(prices[i-period:i+1])
                        gains = np.where(price_changes > 0, price_changes, 0)
                        losses = np.where(price_changes < 0, -price_changes, 0)
                        
                        avg_gain = np.mean(gains) if len(gains) > 0 else 0
                        avg_loss = np.mean(losses) if len(losses) > 0 else 0
                        
                        if avg_loss > 0:
                            rs = avg_gain / avg_loss
                            rsi = 100 - (100 / (1 + rs))
                        else:
                            rsi = 100 if avg_gain > 0 else 50
                            
                        rsi_values.append(rsi / 100.0)
                    else:
                        rsi_values.append(0.5)
                derived[f"RSI_{period}"] = rsi_values
                
        # 5. ë³¼ë¦°ì € ë°´ë“œ (ë‹¤ì–‘í•œ ê¸°ê°„ê³¼ í‘œì¤€í¸ì°¨)
        bb_configs = [(20, 1.5), (20, 2.0), (20, 2.5), (50, 2.0), (100, 2.0), (200, 2.0)]
        for period, std_mult in bb_configs:
            if len(prices) >= period:
                bb_position_values = []
                bb_width_values = []
                for i in range(len(prices)):
                    if i >= period - 1:
                        window_prices = prices[i-period+1:i+1]
                        bb_mean = np.mean(window_prices)
                        bb_std = np.std(window_prices)
                        
                        if bb_std > 0:
                            bb_upper = bb_mean + (bb_std * std_mult)
                            bb_lower = bb_mean - (bb_std * std_mult)
                            bb_position = (prices[i] - bb_lower) / (bb_upper - bb_lower)
                            bb_width = (bb_upper - bb_lower) / bb_mean
                        else:
                            bb_position = 0.5
                            bb_width = 0
                            
                        bb_position_values.append(bb_position)
                        bb_width_values.append(bb_width)
                    else:
                        bb_position_values.append(0.5)
                        bb_width_values.append(0)
                        
                derived[f"BB_{period}_{std_mult}_POSITION"] = bb_position_values
                derived[f"BB_{period}_{std_mult}_WIDTH"] = bb_width_values
                
        # 6. í†µê³„ì  ì§€í‘œë“¤
        stats_windows = [24, 48, 72, 168, 336, 720, 1440, 2160]
        for window in stats_windows:
            if len(prices) >= window:
                skewness_values = []
                kurtosis_values = []
                for i in range(len(prices)):
                    if i >= window - 1:
                        window_prices = prices[i-window+1:i+1]
                        if scipy_available:
                            skewness = stats.skew(window_prices)
                            kurtosis = stats.kurtosis(window_prices)
                        else:
                            skewness = 0
                            kurtosis = 0
                        skewness_values.append(skewness)
                        kurtosis_values.append(kurtosis)
                    else:
                        skewness_values.append(0)
                        kurtosis_values.append(0)
                derived[f"SKEWNESS_{window}"] = skewness_values
                derived[f"KURTOSIS_{window}"] = kurtosis_values
                
        # 7. ì‹œê°„ ê¸°ë°˜ íŠ¹ì„±ë“¤
        total_hours = len(prices)
        
        # ì‹œê°„ ì£¼ê¸°ì„±
        hour_cycle = [np.sin(2 * np.pi * i / 24) for i in range(total_hours)]
        day_cycle = [np.cos(2 * np.pi * i / 24) for i in range(total_hours)]
        week_cycle = [np.sin(2 * np.pi * i / (24 * 7)) for i in range(total_hours)]
        month_cycle = [np.cos(2 * np.pi * i / (24 * 30)) for i in range(total_hours)]
        
        derived["TIME_HOUR_SIN"] = hour_cycle
        derived["TIME_HOUR_COS"] = day_cycle
        derived["TIME_WEEK_SIN"] = week_cycle
        derived["TIME_MONTH_COS"] = month_cycle
        
        # 8. ë‹¤ë¥¸ ë³€ìˆ˜ë“¤ê³¼ì˜ ìƒê´€ê´€ê³„ ê¸°ë°˜ íŒŒìƒ ë³€ìˆ˜
        correlation_vars = []
        var_names = list(base_vars.keys())[:20]  # ìƒìœ„ 20ê°œ ë³€ìˆ˜ë§Œ ì‚¬ìš©
        
        for var_name in var_names:
            try:
                var_values = base_vars[var_name]
                if len(var_values) == len(prices):
                    numeric_values = [float(v) if v is not None else 0.0 for v in var_values]
                    
                    # ìƒê´€ê³„ìˆ˜ ê³„ì‚°
                    if scipy_available and len(numeric_values) > 10:
                        corr, _ = stats.pearsonr(prices, numeric_values)
                        correlation_vars.append(corr if not np.isnan(corr) else 0)
                    else:
                        correlation_vars.append(0)
            except:
                correlation_vars.append(0)
                
        # ìƒê´€ê´€ê³„ ê¸°ë°˜ í•©ì„± ì§€í‘œ
        if correlation_vars:
            derived["CORRELATION_COMPOSITE"] = [sum(correlation_vars)] * total_hours
            
        self.logger.info(f"ğŸ¯ íŒŒìƒ ë³€ìˆ˜ ìƒì„± ì™„ë£Œ: {len(derived)}ê°œ")
        return derived
        
    def initialize_extreme_models(self):
        """ê·¹í•œ ëª¨ë¸ ì»¬ë ‰ì…˜ ì´ˆê¸°í™” (50+ ëª¨ë¸)"""
        
        self.logger.info("ğŸ¤– ê·¹í•œ ëª¨ë¸ ì»¬ë ‰ì…˜ ì´ˆê¸°í™” ì¤‘...")
        
        # 1. ì•™ìƒë¸” ëª¨ë¸ë“¤ (15ê°œ)
        self.extreme_models['ensemble'] = {
            'rf_1': RandomForestRegressor(n_estimators=100, max_depth=10, random_state=1),
            'rf_2': RandomForestRegressor(n_estimators=200, max_depth=20, random_state=2),
            'rf_3': RandomForestRegressor(n_estimators=500, max_depth=30, random_state=3),
            'rf_4': RandomForestRegressor(n_estimators=1000, max_depth=None, random_state=4),
            
            'gb_1': GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=1),
            'gb_2': GradientBoostingRegressor(n_estimators=200, learning_rate=0.05, max_depth=10, random_state=2),
            'gb_3': GradientBoostingRegressor(n_estimators=500, learning_rate=0.01, max_depth=15, random_state=3),
            
            'et_1': ExtraTreesRegressor(n_estimators=100, max_depth=10, random_state=1),
            'et_2': ExtraTreesRegressor(n_estimators=300, max_depth=20, random_state=2),
            'et_3': ExtraTreesRegressor(n_estimators=500, max_depth=None, random_state=3),
            
            'ada_1': AdaBoostRegressor(n_estimators=50, learning_rate=1.0, random_state=1),
            'ada_2': AdaBoostRegressor(n_estimators=100, learning_rate=0.5, random_state=2),
            
            'bag_1': BaggingRegressor(n_estimators=50, random_state=1),
            'bag_2': BaggingRegressor(n_estimators=100, random_state=2),
            'bag_3': BaggingRegressor(n_estimators=200, random_state=3)
        }
        
        # 2. ì‹ ê²½ë§ ëª¨ë¸ë“¤ (10ê°œ)
        self.extreme_models['neural'] = {
            'mlp_1': MLPRegressor(hidden_layer_sizes=(100,), max_iter=500, random_state=1),
            'mlp_2': MLPRegressor(hidden_layer_sizes=(200, 100), max_iter=500, random_state=2),
            'mlp_3': MLPRegressor(hidden_layer_sizes=(300, 200, 100), max_iter=500, random_state=3),
            'mlp_4': MLPRegressor(hidden_layer_sizes=(500, 300, 200, 100), max_iter=500, random_state=4),
            'mlp_5': MLPRegressor(hidden_layer_sizes=(100, 50), activation='tanh', max_iter=500, random_state=5),
            'mlp_6': MLPRegressor(hidden_layer_sizes=(200, 100, 50), activation='relu', max_iter=500, random_state=6),
            'mlp_7': MLPRegressor(hidden_layer_sizes=(150,), solver='lbfgs', max_iter=500, random_state=7),
            'mlp_8': MLPRegressor(hidden_layer_sizes=(250, 125), solver='adam', max_iter=500, random_state=8),
            'mlp_9': MLPRegressor(hidden_layer_sizes=(400, 200), alpha=0.01, max_iter=500, random_state=9),
            'mlp_10': MLPRegressor(hidden_layer_sizes=(300, 150, 75), alpha=0.001, max_iter=500, random_state=10)
        }
        
        # 3. ì„ í˜• ëª¨ë¸ë“¤ (12ê°œ)
        self.extreme_models['linear'] = {
            'ridge_1': Ridge(alpha=0.1, random_state=1),
            'ridge_2': Ridge(alpha=1.0, random_state=2),
            'ridge_3': Ridge(alpha=10.0, random_state=3),
            'lasso_1': Lasso(alpha=0.1, random_state=1),
            'lasso_2': Lasso(alpha=1.0, random_state=2),
            'elastic_1': ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=1),
            'elastic_2': ElasticNet(alpha=1.0, l1_ratio=0.7, random_state=2),
            'bayesian_1': BayesianRidge(),
            'bayesian_2': BayesianRidge(alpha_1=1e-6, alpha_2=1e-6),
            'huber_1': HuberRegressor(epsilon=1.35),
            'huber_2': HuberRegressor(epsilon=2.0),
            'theil_1': TheilSenRegressor(random_state=1)
        }
        
        # 4. SVM ëª¨ë¸ë“¤ (8ê°œ)
        self.extreme_models['svm'] = {
            'svr_rbf_1': SVR(kernel='rbf', C=1.0, gamma='scale'),
            'svr_rbf_2': SVR(kernel='rbf', C=10.0, gamma='scale'),
            'svr_rbf_3': SVR(kernel='rbf', C=100.0, gamma='auto'),
            'svr_linear': SVR(kernel='linear', C=1.0),
            'svr_poly': SVR(kernel='poly', degree=3, C=1.0),
            'nu_svr_1': NuSVR(nu=0.5, kernel='rbf'),
            'nu_svr_2': NuSVR(nu=0.1, kernel='rbf'),
            'nu_svr_3': NuSVR(nu=0.9, kernel='linear')
        }
        
        # 5. ê¸°íƒ€ íŠ¹ìˆ˜ ëª¨ë¸ë“¤ (10ê°œ)
        self.extreme_models['special'] = {
            'knn_1': KNeighborsRegressor(n_neighbors=5),
            'knn_2': KNeighborsRegressor(n_neighbors=10, weights='distance'),
            'knn_3': KNeighborsRegressor(n_neighbors=20, weights='uniform'),
            'tree_1': DecisionTreeRegressor(max_depth=10, random_state=1),
            'tree_2': DecisionTreeRegressor(max_depth=20, random_state=2),
            'tree_3': DecisionTreeRegressor(min_samples_split=10, random_state=3),
            'gp_1': GaussianProcessRegressor(random_state=1),
            'gp_2': GaussianProcessRegressor(alpha=1e-8, random_state=2),
            'linear_1': LinearRegression(),
            'linear_2': LinearRegression(fit_intercept=False)
        }
        
        total_models = sum(len(models) for models in self.extreme_models.values())
        self.logger.info(f"âœ… ê·¹í•œ ëª¨ë¸ ì»¬ë ‰ì…˜ ì´ˆê¸°í™” ì™„ë£Œ: {total_models}ê°œ ëª¨ë¸")
        
    def extract_extreme_features(self, timepoint: int) -> np.ndarray:
        """ê·¹í•œ íŠ¹ì„± ì¶”ì¶œ - ëª¨ë“  ë³€ìˆ˜ + íŒŒìƒ ë³€ìˆ˜ë“¤"""
        
        try:
            features = []
            
            # 1. ëª¨ë“  ê¸°ë³¸ ë³€ìˆ˜ë“¤
            for var_name, var_values in self.all_variables.items():
                if timepoint < len(var_values):
                    value = var_values[timepoint]
                    features.append(float(value) if value is not None else 0.0)
                else:
                    features.append(0.0)
                    
            # 2. ê·¹í•œ í†µê³„ íŠ¹ì„±ë“¤
            if timepoint >= 168:  # 1ì£¼ì¼ ì´ìƒ ë°ì´í„°
                
                # ê°€ê²© ì‹œê³„ì—´ ì°¾ê¸°
                price_series = None
                for var_name, var_values in self.all_variables.items():
                    if 'price' in var_name.lower() and timepoint < len(var_values):
                        price_series = [float(v) if v is not None else 0 for v in var_values[:timepoint+1]]
                        break
                        
                if price_series and len(price_series) >= 168:
                    
                    # ê·¹í•œ í†µê³„ íŠ¹ì„±ë“¤
                    recent_prices = price_series[-168:]  # ìµœê·¼ 1ì£¼ì¼
                    
                    # ë¶„ìœ„ìˆ˜ë“¤
                    percentiles = [1, 5, 10, 25, 50, 75, 90, 95, 99]
                    for p in percentiles:
                        features.append(np.percentile(recent_prices, p))
                        
                    # ë¶„í¬ íŠ¹ì„±ë“¤
                    if scipy_available:
                        features.extend([
                            stats.skew(recent_prices),
                            stats.kurtosis(recent_prices),
                            stats.entropy(np.histogram(recent_prices, bins=20)[0] + 1e-10),
                            stats.variation(recent_prices)
                        ])
                    else:
                        features.extend([0, 0, 0, 0])
                        
                    # íŠ¸ë Œë“œ íŠ¹ì„±ë“¤
                    if len(recent_prices) > 10:
                        # ì„ í˜• íŠ¸ë Œë“œ
                        x = np.arange(len(recent_prices))
                        slope, intercept = np.polyfit(x, recent_prices, 1)
                        features.extend([slope, intercept])
                        
                        # 2ì°¨ íŠ¸ë Œë“œ
                        if len(recent_prices) > 20:
                            poly_coeffs = np.polyfit(x, recent_prices, 2)
                            features.extend(poly_coeffs)
                        else:
                            features.extend([0, 0, 0])
                    else:
                        features.extend([0, 0, 0, 0, 0])
                        
                    # ì£¼íŒŒìˆ˜ ë„ë©”ì¸ íŠ¹ì„±ë“¤ (FFT)
                    if scipy_available and len(recent_prices) >= 64:
                        fft_vals = fft(recent_prices)
                        fft_freqs = fftfreq(len(recent_prices))
                        
                        # ì£¼ìš” ì£¼íŒŒìˆ˜ ì„±ë¶„ë“¤
                        dominant_freqs = np.argsort(np.abs(fft_vals))[-10:]
                        for freq_idx in dominant_freqs:
                            features.extend([
                                np.abs(fft_vals[freq_idx]),
                                np.angle(fft_vals[freq_idx]),
                                fft_freqs[freq_idx]
                            ])
                    else:
                        features.extend([0] * 30)
                        
            # 3. ì‹œì¥ ì²´ì œ ê°ì§€ íŠ¹ì„±ë“¤
            regime_features = self.detect_market_regime(timepoint)
            features.extend(regime_features)
            
            # 4. ë³€ìˆ˜ê°„ ìƒí˜¸ì‘ìš© íŠ¹ì„±ë“¤
            interaction_features = self.generate_interaction_features(timepoint)
            features.extend(interaction_features)
            
            return np.array(features, dtype=np.float32)
            
        except Exception as e:
            self.logger.error(f"âŒ ê·¹í•œ íŠ¹ì„± ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return np.zeros(5000, dtype=np.float32)  # ê¸°ë³¸ê°’
            
    def detect_market_regime(self, timepoint: int) -> List[float]:
        """ì‹œì¥ ì²´ì œ ê°ì§€ íŠ¹ì„±"""
        
        features = []
        
        try:
            # ë³€ë™ì„± ì²´ì œ
            volatility_vars = [name for name in self.all_variables.keys() if 'volatility' in name.lower()]
            if volatility_vars and timepoint < len(self.all_variables[volatility_vars[0]]):
                recent_vol = []
                for i in range(max(0, timepoint-23), timepoint+1):
                    if i < len(self.all_variables[volatility_vars[0]]):
                        vol = self.all_variables[volatility_vars[0]][i]
                        recent_vol.append(float(vol) if vol is not None else 0)
                        
                if recent_vol:
                    features.extend([
                        np.mean(recent_vol),
                        np.std(recent_vol),
                        max(recent_vol) - min(recent_vol)
                    ])
                else:
                    features.extend([0, 0, 0])
            else:
                features.extend([0, 0, 0])
                
            # íŠ¸ë Œë“œ ì²´ì œ (ì—¬ëŸ¬ ì‹œê°„ í”„ë ˆì„)
            for window in [24, 72, 168]:
                trend_strength = 0
                if timepoint >= window:
                    price_vars = [name for name in self.all_variables.keys() if 'price' in name.lower()]
                    if price_vars:
                        prices = []
                        for i in range(timepoint-window+1, timepoint+1):
                            if i < len(self.all_variables[price_vars[0]]):
                                price = self.all_variables[price_vars[0]][i]
                                prices.append(float(price) if price is not None else 0)
                                
                        if len(prices) >= window:
                            # ì„ í˜• íšŒê·€ë¡œ íŠ¸ë Œë“œ ê°•ë„ ì¸¡ì •
                            x = np.arange(len(prices))
                            slope, _ = np.polyfit(x, prices, 1)
                            trend_strength = abs(slope)
                            
                features.append(trend_strength)
                
        except:
            features = [0] * 6
            
        return features
        
    def generate_interaction_features(self, timepoint: int) -> List[float]:
        """ë³€ìˆ˜ê°„ ìƒí˜¸ì‘ìš© íŠ¹ì„± ìƒì„±"""
        
        features = []
        
        try:
            # ì£¼ìš” ë³€ìˆ˜ë“¤ ê°„ì˜ ê³±ì…ˆ, ë‚˜ëˆ—ì…ˆ, ì°¨ì´ ë“±
            main_vars = list(self.all_variables.keys())[:50]  # ìƒìœ„ 50ê°œ ë³€ìˆ˜ë§Œ
            
            # í˜„ì¬ ì‹œì  ê°’ë“¤ ì¶”ì¶œ
            current_values = []
            for var_name in main_vars:
                if timepoint < len(self.all_variables[var_name]):
                    val = self.all_variables[var_name][timepoint]
                    current_values.append(float(val) if val is not None else 0)
                else:
                    current_values.append(0)
                    
            # ìƒí˜¸ì‘ìš© ìƒì„± (ìƒìœ„ 20ê°œ ë³€ìˆ˜ë¡œ ì œí•œ)
            if len(current_values) >= 20:
                selected_values = current_values[:20]
                
                # ê³±ì…ˆ ìƒí˜¸ì‘ìš©
                for i in range(len(selected_values)):
                    for j in range(i+1, min(i+6, len(selected_values))):  # ê° ë³€ìˆ˜ë‹¹ ìµœëŒ€ 5ê°œì™€ ìƒí˜¸ì‘ìš©
                        features.append(selected_values[i] * selected_values[j])
                        
                # ë‚˜ëˆ—ì…ˆ ìƒí˜¸ì‘ìš© (0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€)
                for i in range(len(selected_values)):
                    for j in range(i+1, min(i+4, len(selected_values))):
                        if selected_values[j] != 0:
                            features.append(selected_values[i] / selected_values[j])
                        else:
                            features.append(0)
                            
                # ì°¨ì´ ìƒí˜¸ì‘ìš©
                for i in range(len(selected_values)):
                    for j in range(i+1, min(i+4, len(selected_values))):
                        features.append(selected_values[i] - selected_values[j])
                        
        except:
            pass
            
        # ê³ ì • ê¸¸ì´ë¡œ íŒ¨ë”©
        while len(features) < 200:
            features.append(0)
            
        return features[:200]  # ìƒìœ„ 200ê°œë§Œ ë°˜í™˜
        
    def train_extreme_ensemble(self, training_samples: int = None):
        """ê·¹í•œ ì•™ìƒë¸” í›ˆë ¨"""
        
        self.logger.info("ğŸš€ ê·¹í•œ ì•™ìƒë¸” í›ˆë ¨ ì‹œì‘")
        
        # ëª¨ë“  ì‚¬ìš© ê°€ëŠ¥í•œ ì‹œì 
        available_timepoints = self.get_available_timepoints()
        
        if training_samples is None:
            training_samples = len(available_timepoints)
        else:
            training_samples = min(training_samples, len(available_timepoints))
            
        selected_timepoints = available_timepoints[:training_samples]
        
        self.logger.info(f"ğŸ¯ í›ˆë ¨ ë°ì´í„°: {len(selected_timepoints)}ê°œ ì‹œì ")
        
        # íŠ¹ì„± ë° íƒ€ê²Ÿ ìˆ˜ì§‘
        X_list = []
        y_list = []
        
        for i, timepoint in enumerate(selected_timepoints):
            try:
                # ê·¹í•œ íŠ¹ì„± ì¶”ì¶œ
                features = self.extract_extreme_features(timepoint)
                
                # ë¯¸ë˜ ê°€ê²© ì¡°íšŒ (72ì‹œê°„ í›„)
                future_timepoint = timepoint + 72
                current_price, future_price = self.get_prices(timepoint, future_timepoint)
                
                if current_price and future_price and current_price > 0:
                    price_change = (future_price - current_price) / current_price
                    
                    X_list.append(features)
                    y_list.append(price_change)
                    
                if (i + 1) % 200 == 0:
                    self.logger.info(f"ğŸ“Š ë°ì´í„° ìˆ˜ì§‘ ì§„í–‰ë¥ : {i+1}/{len(selected_timepoints)}")
                    
            except Exception as e:
                continue
                
        if len(X_list) < 100:
            raise ValueError(f"ë°ì´í„° ë¶€ì¡±: {len(X_list)}ê°œ")
            
        X = np.array(X_list)
        y = np.array(y_list)
        
        self.logger.info(f"ğŸ¯ ìµœì¢… ë°ì´í„°ì…‹: {X.shape[0]}ê°œ ìƒ˜í”Œ, {X.shape[1]}ê°œ íŠ¹ì„±")
        
        # ë‹¤ì–‘í•œ ì „ì²˜ë¦¬ ì ìš©í•˜ì—¬ ê° ëª¨ë¸ ê·¸ë£¹ë³„ë¡œ í›ˆë ¨
        training_results = {}
        
        # 1. ì•™ìƒë¸” ëª¨ë¸ë“¤ (RobustScaler ì‚¬ìš©)
        self.logger.info("ğŸš€ ì•™ìƒë¸” ëª¨ë¸ í›ˆë ¨...")
        X_robust = self.preprocessors['scalers']['robust'].fit_transform(X)
        
        for model_name, model in self.extreme_models['ensemble'].items():
            try:
                model.fit(X_robust, y)
                score = model.score(X_robust, y)
                training_results[f'ensemble_{model_name}'] = score
                
                if len(training_results) % 5 == 0:
                    self.logger.info(f"  âœ… ì™„ë£Œ: {len(training_results)}/15")
            except:
                training_results[f'ensemble_{model_name}'] = 0
                
        # 2. ì‹ ê²½ë§ ëª¨ë¸ë“¤ (StandardScaler ì‚¬ìš©)
        self.logger.info("ğŸš€ ì‹ ê²½ë§ ëª¨ë¸ í›ˆë ¨...")
        X_standard = self.preprocessors['scalers']['standard'].fit_transform(X)
        
        for model_name, model in self.extreme_models['neural'].items():
            try:
                model.fit(X_standard, y)
                score = model.score(X_standard, y)
                training_results[f'neural_{model_name}'] = score
            except:
                training_results[f'neural_{model_name}'] = 0
                
        # 3. ì„ í˜• ëª¨ë¸ë“¤ (PowerTransformer ì‚¬ìš©)
        self.logger.info("ğŸš€ ì„ í˜• ëª¨ë¸ í›ˆë ¨...")
        try:
            X_power = self.preprocessors['scalers']['power_yj'].fit_transform(X)
        except:
            X_power = X_standard
            
        for model_name, model in self.extreme_models['linear'].items():
            try:
                model.fit(X_power, y)
                score = model.score(X_power, y)
                training_results[f'linear_{model_name}'] = score
            except:
                training_results[f'linear_{model_name}'] = 0
                
        # 4. SVM ëª¨ë¸ë“¤ (MinMaxScaler ì‚¬ìš©)
        self.logger.info("ğŸš€ SVM ëª¨ë¸ í›ˆë ¨...")
        X_minmax = self.preprocessors['scalers']['minmax'].fit_transform(X)
        
        for model_name, model in self.extreme_models['svm'].items():
            try:
                model.fit(X_minmax, y)
                score = model.score(X_minmax, y)
                training_results[f'svm_{model_name}'] = score
            except:
                training_results[f'svm_{model_name}'] = 0
                
        # 5. íŠ¹ìˆ˜ ëª¨ë¸ë“¤ (ë‹¤ì–‘í•œ ì „ì²˜ë¦¬)
        self.logger.info("ğŸš€ íŠ¹ìˆ˜ ëª¨ë¸ í›ˆë ¨...")
        preprocessed_data = [X_robust, X_standard, X_power, X_minmax, X]
        
        for i, (model_name, model) in enumerate(self.extreme_models['special'].items()):
            try:
                X_prep = preprocessed_data[i % len(preprocessed_data)]
                model.fit(X_prep, y)
                score = model.score(X_prep, y)
                training_results[f'special_{model_name}'] = score
            except:
                training_results[f'special_{model_name}'] = 0
                
        self.logger.info("âœ… ê·¹í•œ ì•™ìƒë¸” í›ˆë ¨ ì™„ë£Œ!")
        
        # ê²°ê³¼ ìš”ì•½
        total_models = len(training_results)
        avg_score = np.mean(list(training_results.values()))
        best_score = max(training_results.values())
        
        self.logger.info(f"ğŸ“Š í›ˆë ¨ ê²°ê³¼: {total_models}ê°œ ëª¨ë¸, í‰ê·  ì ìˆ˜: {avg_score:.4f}, ìµœê³  ì ìˆ˜: {best_score:.4f}")
        
        return training_results
        
    def get_available_timepoints(self) -> List[int]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ì‹œì ë“¤"""
        if not self.all_variables:
            return []
            
        first_var = list(self.all_variables.values())[0]
        total_hours = len(first_var)
        
        return list(range(240, total_hours - 72))
        
    def get_prices(self, current_timepoint: int, future_timepoint: int) -> Tuple[Optional[float], Optional[float]]:
        """ê°€ê²© ì¡°íšŒ"""
        try:
            price_vars = [name for name in self.all_variables.keys() if 'price' in name.lower()]
            
            if not price_vars:
                return None, None
                
            price_values = self.all_variables[price_vars[0]]
            
            if current_timepoint >= len(price_values) or future_timepoint >= len(price_values):
                return None, None
                
            current = price_values[current_timepoint]
            future = price_values[future_timepoint]
            
            if current is not None and future is not None:
                return float(current) * 100, float(future) * 100
                
            return None, None
        except:
            return None, None
            
    def predict_extreme_ensemble(self, timepoint: int) -> Dict[str, Any]:
        """ê·¹í•œ ì•™ìƒë¸” ì˜ˆì¸¡"""
        
        try:
            # ê·¹í•œ íŠ¹ì„± ì¶”ì¶œ
            features = self.extract_extreme_features(timepoint)
            
            # ê° ì „ì²˜ë¦¬ë³„ ì˜ˆì¸¡
            X = features.reshape(1, -1)
            
            predictions = {}
            
            # ì „ì²˜ë¦¬ ì ìš©
            try:
                X_robust = self.preprocessors['scalers']['robust'].transform(X)
                X_standard = self.preprocessors['scalers']['standard'].transform(X)
                X_power = self.preprocessors['scalers']['power_yj'].transform(X)
                X_minmax = self.preprocessors['scalers']['minmax'].transform(X)
            except:
                X_robust = X_standard = X_power = X_minmax = X
                
            preprocessed_data = {
                'ensemble': X_robust,
                'neural': X_standard,
                'linear': X_power,
                'svm': X_minmax
            }
            
            # ê° ëª¨ë¸ ê·¸ë£¹ë³„ ì˜ˆì¸¡
            for group_name, models in self.extreme_models.items():
                X_prep = preprocessed_data.get(group_name, X)
                
                for model_name, model in models.items():
                    try:
                        pred = model.predict(X_prep)[0]
                        predictions[f"{group_name}_{model_name}"] = pred
                    except:
                        predictions[f"{group_name}_{model_name}"] = 0
                        
            # íŠ¹ìˆ˜ ëª¨ë¸ë“¤ (ë‹¤ì–‘í•œ ì „ì²˜ë¦¬ ì‚¬ìš©)
            prep_options = [X_robust, X_standard, X_power, X_minmax, X]
            for i, (model_name, model) in enumerate(self.extreme_models['special'].items()):
                try:
                    X_prep = prep_options[i % len(prep_options)]
                    pred = model.predict(X_prep)[0]
                    predictions[f"special_{model_name}"] = pred
                except:
                    predictions[f"special_{model_name}"] = 0
                    
            # ì•™ìƒë¸” ê²°í•© (ê°€ì¤‘ í‰ê· )
            ensemble_weights = {
                'ensemble': 0.35,
                'neural': 0.25,
                'linear': 0.15,
                'svm': 0.15,
                'special': 0.10
            }
            
            group_predictions = {}
            for group_name in self.extreme_models.keys():
                group_preds = [v for k, v in predictions.items() if k.startswith(f"{group_name}_")]
                if group_preds:
                    group_predictions[group_name] = np.mean(group_preds)
                else:
                    group_predictions[group_name] = 0
                    
            # ìµœì¢… ì˜ˆì¸¡
            final_prediction = sum(
                group_predictions[group] * weight 
                for group, weight in ensemble_weights.items()
            )
            
            # í˜„ì¬ ê°€ê²©
            current_price, _ = self.get_prices(timepoint, timepoint)
            if not current_price:
                current_price = 65000
                
            predicted_price = current_price * (1 + final_prediction)
            
            # ë°©í–¥ì„±
            direction = "UP" if final_prediction > 0.01 else ("DOWN" if final_prediction < -0.01 else "SIDEWAYS")
            
            # ì‹ ë¢°ë„ (ì˜ˆì¸¡ ë¶„ì‚° ê¸°ë°˜)
            pred_values = list(predictions.values())
            pred_std = np.std(pred_values) if pred_values else 0
            confidence = max(0.7, min(0.99, 0.95 - pred_std * 10))
            
            return {
                "timepoint": timepoint,
                "current_price": current_price,
                "predicted_price": predicted_price,
                "price_change_rate": final_prediction * 100,
                "direction": direction,
                "confidence": confidence,
                "model_count": len(predictions),
                "prediction_std": pred_std,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ê·¹í•œ ì•™ìƒë¸” ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            raise
            
    def run_extreme_accuracy_test(self, test_samples: int = 500):
        """ê·¹í•œ ì •í™•ë„ í…ŒìŠ¤íŠ¸"""
        
        self.logger.info("ğŸ¯ ê·¹í•œ ì •í™•ë„ í…ŒìŠ¤íŠ¸ ì‹œì‘!")
        
        # ê·¹í•œ ì•™ìƒë¸” í›ˆë ¨
        self.train_extreme_ensemble()
        
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        available_timepoints = self.get_available_timepoints()
        test_timepoints = available_timepoints[-test_samples:] if len(available_timepoints) >= test_samples else available_timepoints
        
        results = {
            "correct_predictions": 0,
            "total_predictions": 0,
            "price_errors": [],
            "direction_correct": 0
        }
        
        self.logger.info(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ì‹œì : {len(test_timepoints)}ê°œ")
        
        for i, timepoint in enumerate(test_timepoints):
            try:
                prediction = self.predict_extreme_ensemble(timepoint)
                
                # ì‹¤ì œ ê°’ í™•ì¸
                current_price, future_price = self.get_prices(timepoint, timepoint + 72)
                
                if current_price and future_price:
                    actual_change = (future_price - current_price) / current_price
                    predicted_change = prediction['price_change_rate'] / 100
                    
                    # ë°©í–¥ì„± í‰ê°€
                    actual_dir = "UP" if actual_change > 0.01 else ("DOWN" if actual_change < -0.01 else "SIDEWAYS")
                    if prediction['direction'] == actual_dir:
                        results["direction_correct"] += 1
                        
                    # ê°€ê²© ì˜¤ì°¨
                    price_error = abs(prediction['predicted_price'] - future_price) / future_price
                    results["price_errors"].append(price_error)
                    
                    # ì¢…í•© í‰ê°€
                    direction_match = prediction['direction'] == actual_dir
                    price_accurate = price_error < 0.05  # 5% ì´ë‚´
                    
                    if direction_match and price_accurate:
                        results["correct_predictions"] += 1
                        
                    results["total_predictions"] += 1
                    
                if (i + 1) % 100 == 0:
                    current_acc = results["correct_predictions"] / results["total_predictions"] * 100
                    self.logger.info(f"ğŸ“ˆ ì§„í–‰ë¥ : {i+1}/{len(test_timepoints)}, í˜„ì¬ ì •í™•ë„: {current_acc:.1f}%")
                    
            except Exception as e:
                continue
                
        # ìµœì¢… ê²°ê³¼
        if results["total_predictions"] > 0:
            final_accuracy = results["correct_predictions"] / results["total_predictions"]
            direction_accuracy = results["direction_correct"] / results["total_predictions"]
            avg_price_error = np.mean(results["price_errors"]) if results["price_errors"] else 0
            
            final_results = {
                "combined_accuracy": final_accuracy,
                "direction_accuracy": direction_accuracy,
                "average_price_error": avg_price_error,
                "total_tests": results["total_predictions"],
                "models_used": sum(len(models) for models in self.extreme_models.values()),
                "features_used": len(self.all_variables),
                "extreme_target_achieved": final_accuracy >= 0.95
            }
            
            self.logger.info("ğŸ‰" * 30)
            self.logger.info("ğŸš€ ê·¹í•œ ë‹¤ë°©ë©´ BTC ë¶„ì„ ê²°ê³¼")
            self.logger.info("ğŸ‰" * 30)
            self.logger.info(f"ğŸ¯ ì¢…í•© ì •í™•ë„: {final_accuracy*100:.2f}%")
            self.logger.info(f"ğŸ¯ ë°©í–¥ì„± ì •í™•ë„: {direction_accuracy*100:.2f}%")
            self.logger.info(f"ğŸ’° í‰ê·  ê°€ê²© ì˜¤ì°¨: {avg_price_error*100:.2f}%")
            self.logger.info(f"ğŸ¤– ì‚¬ìš© ëª¨ë¸ ìˆ˜: {final_results['models_used']}ê°œ")
            self.logger.info(f"ğŸ“Š ì‚¬ìš© ë³€ìˆ˜ ìˆ˜: {final_results['features_used']}ê°œ")
            self.logger.info(f"âœ… 95%+ ë‹¬ì„±: {'ì„±ê³µ!' if final_results['extreme_target_achieved'] else 'ë” ê°œì„  í•„ìš”'}")
            self.logger.info("ğŸ‰" * 30)
            
            return final_results
            
        return {"error": "í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨"}

def main():
    """ê·¹í•œ ë‹¤ë°©ë©´ ì‹œìŠ¤í…œ ì‹¤í–‰"""
    
    print("ğŸš€ ê·¹í•œ ë‹¤ë°©ë©´ BTC ë¶„ì„ ì‹œìŠ¤í…œ")
    print("=" * 60)
    print("ğŸ“Š ëª¨ë“  ë³€ìˆ˜ í™œìš© + 50+ AI ëª¨ë¸ ì•™ìƒë¸”")
    print("ğŸ¯ ëª©í‘œ: 99.9% ì •í™•ë„ ë‹¬ì„±")
    print("=" * 60)
    
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    system = ExtremeMultivariateBTCSystem()
    
    # ê·¹í•œ ì •í™•ë„ í…ŒìŠ¤íŠ¸
    results = system.run_extreme_accuracy_test(test_samples=300)
    
    if 'error' not in results:
        print(f"\nğŸ¯ ìµœì¢… ê·¹í•œ ì„±ëŠ¥:")
        print(f"   ì¢…í•© ì •í™•ë„: {results['combined_accuracy']*100:.2f}%")
        print(f"   ë°©í–¥ì„± ì •í™•ë„: {results['direction_accuracy']*100:.2f}%")
        print(f"   í‰ê·  ê°€ê²© ì˜¤ì°¨: {results['average_price_error']*100:.2f}%")
        print(f"   ì‚¬ìš© ëª¨ë¸: {results['models_used']}ê°œ")
        print(f"   ì‚¬ìš© ë³€ìˆ˜: {results['features_used']}ê°œ")
        
        # ê²°ê³¼ ì €ì¥
        with open('extreme_multivariate_results.json', 'w') as f:
            json.dump(results, f, indent=2, default=str)
            
        print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: extreme_multivariate_results.json")
        
    print("\nğŸ‰ ê·¹í•œ ë‹¤ë°©ë©´ ì‹œìŠ¤í…œ ì™„ë£Œ!")

if __name__ == "__main__":
    main()