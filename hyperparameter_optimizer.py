#!/usr/bin/env python3
"""
ğŸ¯ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œìŠ¤í…œ
- ë² ì´ì§€ì•ˆ ìµœì í™” (Gaussian Process, Tree-structured Parzen Estimator)
- ë‹¤ëª©ì  ìµœì í™” (Multi-objective Optimization)
- ë¶„ì‚° ìµœì í™” (Distributed Optimization)
- ì¡°ê¸° ì¤‘ë‹¨ (Early Stopping)
- í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¤‘ìš”ë„ ë¶„ì„
- ìµœì í™” ê³¼ì • ì‹œê°í™”
- ìë™ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ íŒŒì´í”„ë¼ì¸
"""

import numpy as np
import pandas as pd
import warnings
from datetime import datetime, timedelta
import json
import os
from typing import Dict, List, Tuple, Optional, Union, Callable, Any
from dataclasses import dataclass, field
import logging
from abc import ABC, abstractmethod
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
import multiprocessing as mp
import time

# ML ë¼ì´ë¸ŒëŸ¬ë¦¬
from sklearn.model_selection import cross_val_score, TimeSeriesSplit
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import Ridge, ElasticNet
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import xgboost as xgb
import lightgbm as lgb

# ë² ì´ì§€ì•ˆ ìµœì í™”
try:
    from skopt import gp_minimize, forest_minimize, gbrt_minimize
    from skopt.space import Real, Integer, Categorical
    from skopt.utils import use_named_args
    from skopt.acquisition import gaussian_ei, gaussian_pi, gaussian_lcb
    from skopt.callbacks import EarlyStopper
    SKOPT_AVAILABLE = True
except ImportError:
    SKOPT_AVAILABLE = False
    print("âš ï¸ scikit-optimize ë¯¸ì„¤ì¹˜: ë² ì´ì§€ì•ˆ ìµœì í™” ë¶ˆê°€")

# Optuna (ëŒ€ì•ˆ ë² ì´ì§€ì•ˆ ìµœì í™”)
try:
    import optuna
    from optuna.samplers import TPESampler, CmaEsSampler
    from optuna.pruners import MedianPruner, HyperbandPruner
    OPTUNA_AVAILABLE = True
except ImportError:
    OPTUNA_AVAILABLE = False
    print("âš ï¸ Optuna ë¯¸ì„¤ì¹˜: TPE ìµœì í™” ë¶ˆê°€")

# ì‹œê°í™”
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.express as px

warnings.filterwarnings('ignore')

@dataclass
class OptimizationConfig:
    """ìµœì í™” ì„¤ì •"""
    # ìµœì í™” ë°©ë²•
    optimization_method: str = 'bayesian_gp'  # 'grid', 'random', 'bayesian_gp', 'bayesian_rf', 'tpe'
    
    # ìµœì í™” ì˜ˆì‚°
    n_calls: int = 100              # ìµœì í™” í˜¸ì¶œ ìˆ˜
    n_initial_points: int = 10      # ì´ˆê¸° ëœë¤ í¬ì¸íŠ¸ ìˆ˜
    
    # êµì°¨ ê²€ì¦
    cv_folds: int = 5              # êµì°¨ ê²€ì¦ í´ë“œ
    cv_scoring: str = 'neg_mean_squared_error'  # êµì°¨ ê²€ì¦ ìŠ¤ì½”ì–´ë§
    
    # ì¡°ê¸° ì¤‘ë‹¨
    early_stopping: bool = True     # ì¡°ê¸° ì¤‘ë‹¨ ì‚¬ìš© ì—¬ë¶€
    patience: int = 20             # ì¡°ê¸° ì¤‘ë‹¨ patience
    min_improvement: float = 0.001  # ìµœì†Œ ê°œì„ ê°’
    
    # ë‹¤ëª©ì  ìµœì í™”
    multi_objective: bool = False   # ë‹¤ëª©ì  ìµœì í™” ì—¬ë¶€
    objectives: List[str] = field(default_factory=lambda: ['accuracy', 'speed'])
    
    # ë¶„ì‚° ìµœì í™”
    n_jobs: int = -1               # ë³‘ë ¬ ì²˜ë¦¬ ìˆ˜
    distributed: bool = False       # ë¶„ì‚° ì²˜ë¦¬ ì—¬ë¶€
    
    # ì •ê·œí™”
    normalize_objectives: bool = True  # ëª©ì í•¨ìˆ˜ ì •ê·œí™”
    
    # ì‹œê°í™”
    plot_convergence: bool = True   # ìˆ˜ë ´ í”Œë¡¯ ìƒì„±
    plot_evaluations: bool = True   # í‰ê°€ í”Œë¡¯ ìƒì„±
    
    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¤‘ìš”ë„
    feature_importance: bool = True # í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„
    
    # ì €ì¥ ì„¤ì •
    save_intermediate: bool = True  # ì¤‘ê°„ ê²°ê³¼ ì €ì¥

class BaseOptimizer(ABC):
    """ê¸°ë³¸ ìµœì í™”ê¸° ì¶”ìƒ í´ë˜ìŠ¤"""
    
    def __init__(self, config: OptimizationConfig):
        self.config = config
        self.optimization_history = []
        self.best_params = None
        self.best_score = float('-inf')
        
    @abstractmethod
    def optimize(self, objective_func: Callable, search_space: Dict) -> Dict:
        """ìµœì í™” ì‹¤í–‰"""
        pass
    
    def _evaluate_objective(self, params: Dict, objective_func: Callable) -> float:
        """ëª©ì í•¨ìˆ˜ í‰ê°€"""
        try:
            score = objective_func(params)
            self.optimization_history.append({
                'params': params.copy(),
                'score': score,
                'timestamp': datetime.now()
            })
            
            if score > self.best_score:
                self.best_score = score
                self.best_params = params.copy()
            
            return score
        except Exception as e:
            logging.warning(f"ëª©ì í•¨ìˆ˜ í‰ê°€ ì‹¤íŒ¨: {e}")
            return float('-inf')

class BayesianOptimizer(BaseOptimizer):
    """ë² ì´ì§€ì•ˆ ìµœì í™”ê¸°"""
    
    def __init__(self, config: OptimizationConfig):
        super().__init__(config)
        self.gp_model = None
        
    def optimize(self, objective_func: Callable, search_space: Dict) -> Dict:
        """ë² ì´ì§€ì•ˆ ìµœì í™” ì‹¤í–‰"""
        if not SKOPT_AVAILABLE:
            raise ImportError("scikit-optimizeê°€ í•„ìš”í•©ë‹ˆë‹¤")
        
        # ê²€ìƒ‰ ê³µê°„ ë³€í™˜
        dimensions = self._convert_search_space(search_space)
        
        # ëª©ì í•¨ìˆ˜ ë˜í¼
        @use_named_args(dimensions)
        def objective(**params):
            return -self._evaluate_objective(params, objective_func)  # ìµœì†Œí™”ë¥¼ ìœ„í•´ ìŒìˆ˜
        
        # ë² ì´ì§€ì•ˆ ìµœì í™” ë°©ë²• ì„ íƒ
        if self.config.optimization_method == 'bayesian_gp':
            result = gp_minimize(
                objective,
                dimensions,
                n_calls=self.config.n_calls,
                n_initial_points=self.config.n_initial_points,
                acquisition_func='EI',
                random_state=42
            )
        elif self.config.optimization_method == 'bayesian_rf':
            result = forest_minimize(
                objective,
                dimensions,
                n_calls=self.config.n_calls,
                n_initial_points=self.config.n_initial_points,
                random_state=42
            )
        else:
            result = gbrt_minimize(
                objective,
                dimensions,
                n_calls=self.config.n_calls,
                n_initial_points=self.config.n_initial_points,
                random_state=42
            )
        
        # ê²°ê³¼ ì •ë¦¬
        best_params = dict(zip([d.name for d in dimensions], result.x))
        
        return {
            'best_params': best_params,
            'best_score': -result.fun,  # ë‹¤ì‹œ ì–‘ìˆ˜ë¡œ
            'optimization_result': result,
            'n_evaluations': len(result.func_vals),
            'convergence_trace': [-val for val in result.func_vals]
        }
    
    def _convert_search_space(self, search_space: Dict) -> List:
        """ê²€ìƒ‰ ê³µê°„ì„ scikit-optimize í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        dimensions = []
        
        for param_name, param_config in search_space.items():
            if isinstance(param_config, dict):
                param_type = param_config.get('type', 'real')
                
                if param_type == 'real':
                    dimensions.append(Real(
                        param_config['low'],
                        param_config['high'],
                        name=param_name,
                        prior=param_config.get('prior', 'uniform')
                    ))
                elif param_type == 'integer':
                    dimensions.append(Integer(
                        param_config['low'],
                        param_config['high'],
                        name=param_name
                    ))
                elif param_type == 'categorical':
                    dimensions.append(Categorical(
                        param_config['choices'],
                        name=param_name
                    ))
            else:
                # ê°„ë‹¨í•œ í˜•ì‹ ì§€ì›
                if isinstance(param_config, tuple) and len(param_config) == 2:
                    dimensions.append(Real(param_config[0], param_config[1], name=param_name))
                elif isinstance(param_config, list):
                    dimensions.append(Categorical(param_config, name=param_name))
        
        return dimensions

class OptunaOptimizer(BaseOptimizer):
    """Optuna TPE ìµœì í™”ê¸°"""
    
    def __init__(self, config: OptimizationConfig):
        super().__init__(config)
        self.study = None
        
    def optimize(self, objective_func: Callable, search_space: Dict) -> Dict:
        """Optuna ìµœì í™” ì‹¤í–‰"""
        if not OPTUNA_AVAILABLE:
            raise ImportError("Optunaê°€ í•„ìš”í•©ë‹ˆë‹¤")
        
        # Optuna ìŠ¤í„°ë”” ìƒì„±
        sampler = TPESampler(seed=42)
        if self.config.early_stopping:
            pruner = MedianPruner(n_startup_trials=self.config.patience)
        else:
            pruner = optuna.pruners.NopPruner()
        
        self.study = optuna.create_study(
            direction='maximize',
            sampler=sampler,
            pruner=pruner
        )
        
        # ëª©ì í•¨ìˆ˜ ë˜í¼
        def optuna_objective(trial):
            params = self._suggest_params(trial, search_space)
            return self._evaluate_objective(params, objective_func)
        
        # ìµœì í™” ì‹¤í–‰
        self.study.optimize(
            optuna_objective,
            n_trials=self.config.n_calls,
            n_jobs=1 if self.config.n_jobs == -1 else self.config.n_jobs
        )
        
        return {
            'best_params': self.study.best_params,
            'best_score': self.study.best_value,
            'n_evaluations': len(self.study.trials),
            'study': self.study
        }
    
    def _suggest_params(self, trial, search_space: Dict) -> Dict:
        """íŒŒë¼ë¯¸í„° ì œì•ˆ"""
        params = {}
        
        for param_name, param_config in search_space.items():
            if isinstance(param_config, dict):
                param_type = param_config.get('type', 'real')
                
                if param_type == 'real':
                    params[param_name] = trial.suggest_float(
                        param_name,
                        param_config['low'],
                        param_config['high']
                    )
                elif param_type == 'integer':
                    params[param_name] = trial.suggest_int(
                        param_name,
                        param_config['low'],
                        param_config['high']
                    )
                elif param_type == 'categorical':
                    params[param_name] = trial.suggest_categorical(
                        param_name,
                        param_config['choices']
                    )
            else:
                # ê°„ë‹¨í•œ í˜•ì‹
                if isinstance(param_config, tuple) and len(param_config) == 2:
                    params[param_name] = trial.suggest_float(param_name, param_config[0], param_config[1])
                elif isinstance(param_config, list):
                    params[param_name] = trial.suggest_categorical(param_name, param_config)
        
        return params

class MultiObjectiveOptimizer(BaseOptimizer):
    """ë‹¤ëª©ì  ìµœì í™”ê¸°"""
    
    def __init__(self, config: OptimizationConfig):
        super().__init__(config)
        self.pareto_front = []
        
    def optimize(self, objective_funcs: List[Callable], search_space: Dict) -> Dict:
        """ë‹¤ëª©ì  ìµœì í™” ì‹¤í–‰"""
        if not OPTUNA_AVAILABLE:
            raise ImportError("ë‹¤ëª©ì  ìµœì í™”ëŠ” Optunaê°€ í•„ìš”í•©ë‹ˆë‹¤")
        
        # ë‹¤ëª©ì  ìŠ¤í„°ë”” ìƒì„±
        study = optuna.create_study(
            directions=['maximize'] * len(objective_funcs),
            sampler=TPESampler(seed=42)
        )
        
        def multi_objective(trial):
            params = self._suggest_params(trial, search_space)
            
            objectives = []
            for obj_func in objective_funcs:
                try:
                    score = obj_func(params)
                    objectives.append(score)
                except Exception:
                    objectives.append(float('-inf'))
            
            return objectives
        
        # ìµœì í™” ì‹¤í–‰
        study.optimize(multi_objective, n_trials=self.config.n_calls)
        
        # íŒŒë ˆí†  í”„ë¡ íŠ¸ ì¶”ì¶œ
        pareto_trials = []
        for trial in study.trials:
            if trial.state == optuna.trial.TrialState.COMPLETE:
                pareto_trials.append({
                    'params': trial.params,
                    'values': trial.values
                })
        
        return {
            'pareto_front': pareto_trials,
            'study': study,
            'n_evaluations': len(study.trials)
        }

class HyperparameterOptimizer:
    """í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ë©”ì¸ ì‹œìŠ¤í…œ"""
    
    def __init__(self, config: OptimizationConfig = None):
        self.config = config or OptimizationConfig()
        self.data_path = "/Users/parkyoungjun/Desktop/BTC_Analysis_System"
        
        # ìµœì í™”ê¸° ì´ˆê¸°í™”
        self.optimizers = self._initialize_optimizers()
        
        # ê²°ê³¼ ì €ì¥
        self.optimization_results = {}
        self.parameter_importance = {}
        
        # ë¡œê¹… ì„¤ì •
        self.setup_logging()
        
    def setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(os.path.join(self.data_path, 'hyperparameter_optimization.log'), encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
    def _initialize_optimizers(self) -> Dict:
        """ìµœì í™”ê¸°ë“¤ ì´ˆê¸°í™”"""
        optimizers = {}
        
        # ë² ì´ì§€ì•ˆ ìµœì í™”
        if SKOPT_AVAILABLE:
            optimizers['bayesian'] = BayesianOptimizer(self.config)
        
        # Optuna TPE
        if OPTUNA_AVAILABLE:
            optimizers['tpe'] = OptunaOptimizer(self.config)
            optimizers['multi_objective'] = MultiObjectiveOptimizer(self.config)
        
        return optimizers
    
    def optimize_model(self, model_class, X: pd.DataFrame, y: pd.Series, 
                      search_space: Dict) -> Dict:
        """ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”"""
        self.logger.info(f"ğŸ¯ {model_class.__name__} í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘...")
        
        # ëª©ì í•¨ìˆ˜ ì •ì˜
        def objective_function(params):
            return self._evaluate_model(model_class, params, X, y)
        
        # ìµœì í™” ë°©ë²• ì„ íƒ
        if self.config.optimization_method in self.optimizers:
            optimizer = self.optimizers[self.config.optimization_method]
        else:
            # ê¸°ë³¸ê°’: ì‚¬ìš© ê°€ëŠ¥í•œ ì²« ë²ˆì§¸ ìµœì í™”ê¸°
            optimizer = list(self.optimizers.values())[0]
        
        # ìµœì í™” ì‹¤í–‰
        start_time = time.time()
        optimization_result = optimizer.optimize(objective_function, search_space)
        optimization_time = time.time() - start_time
        
        # ê²°ê³¼ ì •ë¦¬
        result = {
            'model_class': model_class.__name__,
            'optimization_method': self.config.optimization_method,
            'optimization_time_seconds': optimization_time,
            'best_params': optimization_result['best_params'],
            'best_score': optimization_result['best_score'],
            'n_evaluations': optimization_result.get('n_evaluations', len(optimizer.optimization_history)),
            'optimization_history': optimizer.optimization_history,
            'search_space': search_space,
            'data_shape': X.shape,
            'timestamp': datetime.now().isoformat()
        }
        
        # íŒŒë¼ë¯¸í„° ì¤‘ìš”ë„ ë¶„ì„
        if self.config.feature_importance:
            importance = self._analyze_parameter_importance(optimizer.optimization_history, search_space)
            result['parameter_importance'] = importance
        
        # ê²°ê³¼ ì €ì¥
        model_key = f"{model_class.__name__}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.optimization_results[model_key] = result
        
        self.logger.info(f"âœ… ìµœì í™” ì™„ë£Œ: {optimization_time:.2f}ì´ˆ, ìµœê³  ì ìˆ˜: {optimization_result['best_score']:.4f}")
        
        return result
    
    def _evaluate_model(self, model_class, params: Dict, X: pd.DataFrame, y: pd.Series) -> float:
        """ëª¨ë¸ í‰ê°€"""
        try:
            # íŒŒë¼ë¯¸í„° ì „ì²˜ë¦¬
            processed_params = self._preprocess_params(params)
            
            # ëª¨ë¸ ìƒì„±
            model = model_class(**processed_params)
            
            # êµì°¨ ê²€ì¦
            tscv = TimeSeriesSplit(n_splits=self.config.cv_folds)
            scores = cross_val_score(
                model, X, y, 
                cv=tscv, 
                scoring=self.config.cv_scoring,
                n_jobs=1  # ê°œë³„ ëª¨ë¸ í‰ê°€ëŠ” ë‹¨ì¼ ìŠ¤ë ˆë“œ
            )
            
            # í‰ê·  ì ìˆ˜ ë°˜í™˜ (ìŒìˆ˜ ì ìˆ˜ëŠ” ì–‘ìˆ˜ë¡œ ë³€í™˜)
            mean_score = np.mean(scores)
            if self.config.cv_scoring.startswith('neg_'):
                mean_score = -mean_score
            
            return mean_score
            
        except Exception as e:
            self.logger.warning(f"ëª¨ë¸ í‰ê°€ ì‹¤íŒ¨: {e}")
            return float('-inf')
    
    def _preprocess_params(self, params: Dict) -> Dict:
        """íŒŒë¼ë¯¸í„° ì „ì²˜ë¦¬ (íƒ€ì… ë³€í™˜ ë“±)"""
        processed = {}
        
        for key, value in params.items():
            # ì •ìˆ˜í˜• íŒŒë¼ë¯¸í„° ë³€í™˜
            if key in ['n_estimators', 'max_depth', 'min_samples_split', 
                      'min_samples_leaf', 'max_features_int']:
                processed[key] = int(value)
            # ë¶€ë™ì†Œìˆ˜ì  íŒŒë¼ë¯¸í„°
            elif key in ['learning_rate', 'subsample', 'colsample_bytree', 
                        'alpha', 'l1_ratio']:
                processed[key] = float(value)
            # ë¬¸ìì—´ íŒŒë¼ë¯¸í„°
            else:
                processed[key] = value
        
        return processed
    
    def _analyze_parameter_importance(self, optimization_history: List[Dict], 
                                    search_space: Dict) -> Dict:
        """íŒŒë¼ë¯¸í„° ì¤‘ìš”ë„ ë¶„ì„"""
        if len(optimization_history) < 10:
            return {'insufficient_data': True}
        
        # ë°ì´í„° ì¤€ë¹„
        params_df = pd.DataFrame([h['params'] for h in optimization_history])
        scores = np.array([h['score'] for h in optimization_history])
        
        # ìƒê´€ê´€ê³„ ë¶„ì„
        importance_scores = {}
        
        for param_name in params_df.columns:
            param_values = params_df[param_name]
            
            # ìˆ˜ì¹˜í˜• íŒŒë¼ë¯¸í„°ì˜ ê²½ìš° ìƒê´€ê³„ìˆ˜ ê³„ì‚°
            if pd.api.types.is_numeric_dtype(param_values):
                correlation = np.corrcoef(param_values, scores)[0, 1]
                importance_scores[param_name] = abs(correlation) if not np.isnan(correlation) else 0
            else:
                # ë²”ì£¼í˜• íŒŒë¼ë¯¸í„°ì˜ ê²½ìš° ë¶„ì‚° ë¶„ì„
                unique_values = param_values.unique()
                if len(unique_values) > 1:
                    group_means = []
                    for value in unique_values:
                        group_scores = scores[param_values == value]
                        if len(group_scores) > 0:
                            group_means.append(np.mean(group_scores))
                    
                    importance_scores[param_name] = np.std(group_means) if len(group_means) > 1 else 0
                else:
                    importance_scores[param_name] = 0
        
        # ì •ê·œí™”
        max_importance = max(importance_scores.values()) if importance_scores else 1
        if max_importance > 0:
            importance_scores = {k: v/max_importance for k, v in importance_scores.items()}
        
        return importance_scores
    
    def optimize_multiple_models(self, model_configs: List[Dict], 
                                X: pd.DataFrame, y: pd.Series) -> Dict:
        """ë‹¤ì¤‘ ëª¨ë¸ ìµœì í™”"""
        self.logger.info("ğŸš€ ë‹¤ì¤‘ ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘...")
        
        results = {}
        
        for config in model_configs:
            model_class = config['model_class']
            search_space = config['search_space']
            model_name = config.get('name', model_class.__name__)
            
            try:
                result = self.optimize_model(model_class, X, y, search_space)
                results[model_name] = result
                
                # ì¤‘ê°„ ê²°ê³¼ ì €ì¥
                if self.config.save_intermediate:
                    self._save_intermediate_result(model_name, result)
                    
            except Exception as e:
                self.logger.error(f"ëª¨ë¸ {model_name} ìµœì í™” ì‹¤íŒ¨: {e}")
                results[model_name] = {'error': str(e)}
        
        # ìµœê³  ëª¨ë¸ ì„ íƒ
        best_model = self._select_best_model(results)
        
        comprehensive_result = {
            'individual_results': results,
            'best_model': best_model,
            'optimization_summary': self._generate_optimization_summary(results),
            'timestamp': datetime.now().isoformat()
        }
        
        return comprehensive_result
    
    def _select_best_model(self, results: Dict) -> Dict:
        """ìµœê³  ëª¨ë¸ ì„ íƒ"""
        best_score = float('-inf')
        best_model = None
        best_name = None
        
        for model_name, result in results.items():
            if 'error' not in result and 'best_score' in result:
                if result['best_score'] > best_score:
                    best_score = result['best_score']
                    best_model = result
                    best_name = model_name
        
        if best_model:
            return {
                'model_name': best_name,
                'best_params': best_model['best_params'],
                'best_score': best_model['best_score'],
                'model_class': best_model['model_class']
            }
        else:
            return {'error': 'ìœ íš¨í•œ ìµœì í™” ê²°ê³¼ ì—†ìŒ'}
    
    def _generate_optimization_summary(self, results: Dict) -> Dict:
        """ìµœì í™” ìš”ì•½ ìƒì„±"""
        summary = {
            'total_models': len(results),
            'successful_optimizations': sum(1 for r in results.values() if 'error' not in r),
            'failed_optimizations': sum(1 for r in results.values() if 'error' in r),
            'total_evaluations': sum(r.get('n_evaluations', 0) for r in results.values() if 'error' not in r),
            'total_optimization_time': sum(r.get('optimization_time_seconds', 0) for r in results.values() if 'error' not in r)
        }
        
        # ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ
        model_scores = {}
        for name, result in results.items():
            if 'error' not in result and 'best_score' in result:
                model_scores[name] = result['best_score']
        
        if model_scores:
            summary['model_performance_ranking'] = sorted(
                model_scores.items(), 
                key=lambda x: x[1], 
                reverse=True
            )
        
        return summary
    
    def create_optimization_dashboard(self, results: Dict) -> str:
        """ìµœì í™” ëŒ€ì‹œë³´ë“œ ìƒì„±"""
        self.logger.info("ğŸ“Š ìµœì í™” ëŒ€ì‹œë³´ë“œ ìƒì„± ì¤‘...")
        
        fig = make_subplots(
            rows=3, cols=3,
            subplot_titles=(
                "ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ", "ìµœì í™” ìˆ˜ë ´ ê³¡ì„ ", "íŒŒë¼ë¯¸í„° ì¤‘ìš”ë„",
                "ìµœì í™” ì‹œê°„ ë¹„êµ", "í‰ê°€ íšŸìˆ˜ ë¹„êµ", "íŒŒë¼ë¯¸í„° ë¶„í¬",
                "êµì°¨ ê²€ì¦ ì ìˆ˜", "ìµœì  íŒŒë¼ë¯¸í„°", "ìµœì í™” íš¨ìœ¨ì„±"
            ),
            specs=[[{"type": "bar"}, {"type": "scatter"}, {"type": "bar"}],
                   [{"type": "bar"}, {"type": "bar"}, {"type": "histogram"}],
                   [{"type": "box"}, {"type": "table"}, {"type": "scatter"}]]
        )
        
        # ë°ì´í„° ì¶”ì¶œ
        if 'individual_results' in results:
            individual_results = results['individual_results']
            
            # 1. ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ
            model_names = []
            model_scores = []
            
            for name, result in individual_results.items():
                if 'error' not in result and 'best_score' in result:
                    model_names.append(name)
                    model_scores.append(result['best_score'])
            
            if model_names:
                fig.add_trace(
                    go.Bar(x=model_names, y=model_scores, name='Best Score'),
                    row=1, col=1
                )
            
            # 2. ìµœì í™” ìˆ˜ë ´ ê³¡ì„  (ì²« ë²ˆì§¸ ëª¨ë¸)
            first_model = next((r for r in individual_results.values() if 'error' not in r), None)
            if first_model and 'optimization_history' in first_model:
                history = first_model['optimization_history']
                iterations = list(range(len(history)))
                scores = [h['score'] for h in history]
                
                # ëˆ„ì  ìµœëŒ€ê°’ (ìˆ˜ë ´ ê³¡ì„ )
                cummax_scores = np.maximum.accumulate(scores)
                
                fig.add_trace(
                    go.Scatter(x=iterations, y=cummax_scores, mode='lines', name='Convergence'),
                    row=1, col=2
                )
            
            # 3. íŒŒë¼ë¯¸í„° ì¤‘ìš”ë„ (ì²« ë²ˆì§¸ ëª¨ë¸)
            if first_model and 'parameter_importance' in first_model:
                importance = first_model['parameter_importance']
                if 'insufficient_data' not in importance:
                    params = list(importance.keys())
                    importances = list(importance.values())
                    
                    fig.add_trace(
                        go.Bar(x=params, y=importances, name='Parameter Importance'),
                        row=1, col=3
                    )
            
            # 4. ìµœì í™” ì‹œê°„ ë¹„êµ
            opt_times = []
            for name, result in individual_results.items():
                if 'error' not in result and 'optimization_time_seconds' in result:
                    opt_times.append(result['optimization_time_seconds'])
            
            if opt_times and model_names:
                fig.add_trace(
                    go.Bar(x=model_names[:len(opt_times)], y=opt_times, name='Optimization Time'),
                    row=2, col=1
                )
            
            # 5. í‰ê°€ íšŸìˆ˜ ë¹„êµ
            n_evals = []
            for name, result in individual_results.items():
                if 'error' not in result and 'n_evaluations' in result:
                    n_evals.append(result['n_evaluations'])
            
            if n_evals and model_names:
                fig.add_trace(
                    go.Bar(x=model_names[:len(n_evals)], y=n_evals, name='N Evaluations'),
                    row=2, col=2
                )
        
        fig.update_layout(
            title="ğŸ¯ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ëŒ€ì‹œë³´ë“œ",
            height=1200,
            showlegend=True,
            template='plotly_dark'
        )
        
        # ì €ì¥
        dashboard_path = os.path.join(self.data_path, 'hyperparameter_optimization_dashboard.html')
        fig.write_html(dashboard_path, include_plotlyjs=True)
        
        return dashboard_path
    
    def _save_intermediate_result(self, model_name: str, result: Dict):
        """ì¤‘ê°„ ê²°ê³¼ ì €ì¥"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f'hyperopt_{model_name}_{timestamp}.json'
        filepath = os.path.join(self.data_path, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False, default=str)
    
    def save_optimization_results(self, results: Dict):
        """ìµœì í™” ê²°ê³¼ ì €ì¥"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ì „ì²´ ê²°ê³¼ ì €ì¥
        full_results_path = os.path.join(self.data_path, f'hyperparameter_optimization_results_{timestamp}.json')
        with open(full_results_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)
        
        # ìš”ì•½ ë³´ê³ ì„œ ì €ì¥
        summary_path = os.path.join(self.data_path, f'hyperparameter_optimization_summary_{timestamp}.txt')
        self._generate_summary_report(results, summary_path)
        
        self.logger.info(f"ìµœì í™” ê²°ê³¼ ì €ì¥: {full_results_path}")
        self.logger.info(f"ìš”ì•½ ë³´ê³ ì„œ ì €ì¥: {summary_path}")
    
    def _generate_summary_report(self, results: Dict, output_path: str):
        """ìš”ì•½ ë³´ê³ ì„œ ìƒì„±"""
        lines = [
            "ğŸ¯ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ê²°ê³¼ ìš”ì•½",
            "=" * 60,
            f"ìƒì„± ì¼ì‹œ: {datetime.now().strftime('%Yë…„ %mì›” %dì¼ %Hì‹œ %Më¶„')}",
            ""
        ]
        
        # ì „ì²´ ìš”ì•½
        if 'optimization_summary' in results:
            summary = results['optimization_summary']
            lines.extend([
                "ğŸ“Š ì „ì²´ ìš”ì•½",
                f"  â€¢ ìµœì í™”í•œ ëª¨ë¸ ìˆ˜: {summary.get('total_models', 0)}ê°œ",
                f"  â€¢ ì„±ê³µí•œ ìµœì í™”: {summary.get('successful_optimizations', 0)}ê°œ",
                f"  â€¢ ì‹¤íŒ¨í•œ ìµœì í™”: {summary.get('failed_optimizations', 0)}ê°œ",
                f"  â€¢ ì´ í‰ê°€ íšŸìˆ˜: {summary.get('total_evaluations', 0):,}íšŒ",
                f"  â€¢ ì´ ìµœì í™” ì‹œê°„: {summary.get('total_optimization_time', 0):.1f}ì´ˆ",
                ""
            ])
            
            # ëª¨ë¸ ìˆœìœ„
            if 'model_performance_ranking' in summary:
                lines.extend([
                    "ğŸ† ëª¨ë¸ ì„±ëŠ¥ ìˆœìœ„",
                ])
                for i, (model_name, score) in enumerate(summary['model_performance_ranking'], 1):
                    lines.append(f"  {i}. {model_name}: {score:.4f}")
                lines.append("")
        
        # ìµœê³  ëª¨ë¸ ì •ë³´
        if 'best_model' in results and 'error' not in results['best_model']:
            best = results['best_model']
            lines.extend([
                "ğŸ¥‡ ìµœê³  ì„±ëŠ¥ ëª¨ë¸",
                f"  â€¢ ëª¨ë¸: {best.get('model_name', 'Unknown')}",
                f"  â€¢ ì ìˆ˜: {best.get('best_score', 0):.4f}",
                f"  â€¢ ìµœì  íŒŒë¼ë¯¸í„°:",
            ])
            
            for param, value in best.get('best_params', {}).items():
                lines.append(f"    - {param}: {value}")
            lines.append("")
        
        # ê°œë³„ ëª¨ë¸ ì„¸ë¶€ì‚¬í•­
        if 'individual_results' in results:
            lines.append("ğŸ“‹ ê°œë³„ ëª¨ë¸ ì„¸ë¶€ì‚¬í•­")
            for model_name, result in results['individual_results'].items():
                if 'error' not in result:
                    lines.extend([
                        f"  â€¢ {model_name}",
                        f"    - ìµœê³  ì ìˆ˜: {result.get('best_score', 0):.4f}",
                        f"    - í‰ê°€ íšŸìˆ˜: {result.get('n_evaluations', 0)}íšŒ",
                        f"    - ìµœì í™” ì‹œê°„: {result.get('optimization_time_seconds', 0):.1f}ì´ˆ",
                    ])
                else:
                    lines.append(f"  â€¢ {model_name}: ìµœì í™” ì‹¤íŒ¨ ({result['error']})")
        
        # íŒŒì¼ ì €ì¥
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(lines))

def create_default_search_spaces() -> Dict:
    """ê¸°ë³¸ ê²€ìƒ‰ ê³µê°„ ì •ì˜"""
    return {
        'RandomForestRegressor': {
            'n_estimators': {'type': 'integer', 'low': 50, 'high': 500},
            'max_depth': {'type': 'integer', 'low': 3, 'high': 20},
            'min_samples_split': {'type': 'integer', 'low': 2, 'high': 20},
            'min_samples_leaf': {'type': 'integer', 'low': 1, 'high': 10},
            'max_features': {'type': 'categorical', 'choices': ['sqrt', 'log2', None]}
        },
        'XGBRegressor': {
            'n_estimators': {'type': 'integer', 'low': 50, 'high': 500},
            'max_depth': {'type': 'integer', 'low': 3, 'high': 10},
            'learning_rate': {'type': 'real', 'low': 0.01, 'high': 0.3},
            'subsample': {'type': 'real', 'low': 0.6, 'high': 1.0},
            'colsample_bytree': {'type': 'real', 'low': 0.6, 'high': 1.0}
        },
        'LGBMRegressor': {
            'n_estimators': {'type': 'integer', 'low': 50, 'high': 500},
            'max_depth': {'type': 'integer', 'low': 3, 'high': 10},
            'learning_rate': {'type': 'real', 'low': 0.01, 'high': 0.3},
            'feature_fraction': {'type': 'real', 'low': 0.6, 'high': 1.0},
            'bagging_fraction': {'type': 'real', 'low': 0.6, 'high': 1.0}
        }
    }

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ¯ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œìŠ¤í…œ")
    print("=" * 50)
    
    # ì„¤ì •
    config = OptimizationConfig(
        optimization_method='bayesian_gp' if SKOPT_AVAILABLE else 'random',
        n_calls=50,
        cv_folds=5,
        early_stopping=True
    )
    
    # ìµœì í™”ê¸° ì´ˆê¸°í™”
    optimizer = HyperparameterOptimizer(config)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    np.random.seed(42)
    n_samples, n_features = 1000, 20
    
    X = pd.DataFrame(
        np.random.randn(n_samples, n_features),
        columns=[f'feature_{i}' for i in range(n_features)]
    )
    y = pd.Series(
        np.sum(X.iloc[:, :5], axis=1) + np.random.randn(n_samples) * 0.1
    )
    
    # ë‹¤ì¤‘ ëª¨ë¸ ìµœì í™”
    search_spaces = create_default_search_spaces()
    model_configs = [
        {
            'name': 'RandomForest',
            'model_class': RandomForestRegressor,
            'search_space': search_spaces['RandomForestRegressor']
        }
    ]
    
    # XGBoost ì¶”ê°€ (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
    try:
        import xgboost
        model_configs.append({
            'name': 'XGBoost',
            'model_class': xgb.XGBRegressor,
            'search_space': search_spaces['XGBRegressor']
        })
    except ImportError:
        pass
    
    # ìµœì í™” ì‹¤í–‰
    results = optimizer.optimize_multiple_models(model_configs, X, y)
    
    print(f"\nğŸ¯ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì™„ë£Œ!")
    
    if 'best_model' in results and 'error' not in results['best_model']:
        best = results['best_model']
        print(f"ìµœê³  ëª¨ë¸: {best['model_name']}")
        print(f"ìµœê³  ì ìˆ˜: {best['best_score']:.4f}")
        print(f"ìµœì  íŒŒë¼ë¯¸í„°: {best['best_params']}")
    
    # ëŒ€ì‹œë³´ë“œ ìƒì„±
    dashboard_path = optimizer.create_optimization_dashboard(results)
    print(f"ëŒ€ì‹œë³´ë“œ: {dashboard_path}")
    
    # ê²°ê³¼ ì €ì¥
    optimizer.save_optimization_results(results)

if __name__ == "__main__":
    main()