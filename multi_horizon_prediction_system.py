#!/usr/bin/env python3
"""
ğŸ¯ Multi-Horizon Bitcoin Price Prediction System
ì •êµí•œ ë‹¤ì¤‘ ì‹œê°„ëŒ€ ì˜ˆì¸¡ ì‹œìŠ¤í…œìœ¼ë¡œ 90%+ ì •í™•ë„ ë‹¬ì„±

ì£¼ìš” ê¸°ëŠ¥:
1. Multi-Task Learning Architecture - ê³µìœ  íŠ¹ì„± ì¸ì½”ë”
2. Temporal Hierarchy Modeling - ì¥/ì¤‘/ë‹¨ê¸° íŠ¸ë Œë“œ ë¶„ì„
3. Uncertainty Quantification - ëª¬í…Œì¹´ë¥¼ë¡œ ë“œë¡­ì•„ì›ƒê³¼ ì•™ìƒë¸” ì‹ ë¢°ë„
4. Dynamic Horizon Weighting - ì‹œì¥ ë³€ë™ì„± ê¸°ë°˜ ì‹œê°„ëŒ€ ìµœì í™”
5. Integration Strategies - ê³„ì¸µì  ì˜ˆì¸¡ í†µí•©

ëª©í‘œ ì‹œê°„ëŒ€: 1h, 4h, 24h, 72h, 168h
ëª©í‘œ ì •í™•ë„: 90%+
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
import warnings
import json
import os
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Union
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import norm
import joblib
from dataclasses import dataclass
from collections import defaultdict
import logging

warnings.filterwarnings('ignore')

@dataclass
class HorizonConfig:
    """ì‹œê°„ëŒ€ë³„ ì„¤ì •"""
    horizon_hours: int
    weight: float
    lookback_periods: int
    feature_dims: int
    uncertainty_samples: int

class MultiHorizonDataset(Dataset):
    """ë‹¤ì¤‘ ì‹œê°„ëŒ€ ë°ì´í„°ì…‹"""
    
    def __init__(self, data: np.ndarray, horizons: List[int], lookback: int = 168):
        self.data = data
        self.horizons = horizons
        self.lookback = lookback
        self.samples = []
        
        # ë‹¤ì¤‘ ì‹œê°„ëŒ€ ìƒ˜í”Œ ìƒì„±
        for i in range(lookback, len(data) - max(horizons)):
            sample = {
                'features': data[i-lookback:i],
                'targets': {}
            }
            
            for horizon in horizons:
                if i + horizon < len(data):
                    sample['targets'][horizon] = data[i + horizon, 0]  # ê°€ê²©ë§Œ ì˜ˆì¸¡
            
            self.samples.append(sample)
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        sample = self.samples[idx]
        features = torch.FloatTensor(sample['features'])
        targets = {h: torch.FloatTensor([sample['targets'][h]]) for h in sample['targets']}
        return features, targets

class SharedFeatureEncoder(nn.Module):
    """ê³µìœ  íŠ¹ì„± ì¸ì½”ë”"""
    
    def __init__(self, input_dim: int, hidden_dims: List[int], dropout_rate: float = 0.2):
        super().__init__()
        
        layers = []
        prev_dim = input_dim
        
        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.ReLU(),
                nn.BatchNorm1d(hidden_dim),
                nn.Dropout(dropout_rate)
            ])
            prev_dim = hidden_dim
        
        self.encoder = nn.Sequential(*layers)
        self.output_dim = prev_dim
    
    def forward(self, x):
        # x shape: (batch, sequence, features)
        batch_size, seq_len, features = x.shape
        
        # Flatten for processing
        x = x.view(batch_size * seq_len, features)
        encoded = self.encoder(x)
        
        # Reshape back
        encoded = encoded.view(batch_size, seq_len, -1)
        
        # Global pooling for sequence
        encoded = torch.mean(encoded, dim=1)
        
        return encoded

class TemporalAttention(nn.Module):
    """ì‹œê°„ì  ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜"""
    
    def __init__(self, input_dim: int, num_heads: int = 8):
        super().__init__()
        self.attention = nn.MultiheadAttention(
            embed_dim=input_dim,
            num_heads=num_heads,
            batch_first=True,
            dropout=0.1
        )
        self.norm = nn.LayerNorm(input_dim)
    
    def forward(self, x):
        attended, weights = self.attention(x, x, x)
        return self.norm(attended + x), weights

class HorizonSpecificHead(nn.Module):
    """ì‹œê°„ëŒ€ë³„ ì˜ˆì¸¡ í—¤ë“œ"""
    
    def __init__(self, input_dim: int, hidden_dim: int, dropout_rate: float = 0.3):
        super().__init__()
        
        self.head = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_dim // 2, 1)
        )
    
    def forward(self, x):
        return self.head(x)

class MultiHorizonPredictor(nn.Module):
    """ë‹¤ì¤‘ ì‹œê°„ëŒ€ ì˜ˆì¸¡ ëª¨ë¸"""
    
    def __init__(self, 
                 input_dim: int, 
                 horizons: List[int],
                 encoder_dims: List[int] = [512, 256, 128],
                 head_dim: int = 64,
                 dropout_rate: float = 0.2):
        super().__init__()
        
        self.horizons = horizons
        self.encoder = SharedFeatureEncoder(input_dim, encoder_dims, dropout_rate)
        self.attention = TemporalAttention(self.encoder.output_dim)
        
        # ì‹œê°„ëŒ€ë³„ ì˜ˆì¸¡ í—¤ë“œ
        self.heads = nn.ModuleDict({
            str(h): HorizonSpecificHead(self.encoder.output_dim, head_dim, dropout_rate)
            for h in horizons
        })
        
        # í¬ë¡œìŠ¤ í˜¸ë¼ì´ì¦Œ ì¼ê´€ì„± ì œì•½
        self.consistency_weight = nn.Parameter(torch.tensor(0.1))
    
    def forward(self, x, return_attention=False):
        # ê³µìœ  íŠ¹ì„± ì¸ì½”ë”©
        encoded = self.encoder(x)
        
        # ì‹œê°„ì  ì–´í…ì…˜ ì ìš©
        if len(encoded.shape) == 2:
            encoded = encoded.unsqueeze(1)  # Add sequence dimension
        
        attended, attention_weights = self.attention(encoded)
        attended = attended.squeeze(1)  # Remove sequence dimension
        
        # ì‹œê°„ëŒ€ë³„ ì˜ˆì¸¡
        predictions = {}
        for horizon in self.horizons:
            predictions[horizon] = self.heads[str(horizon)](attended)
        
        if return_attention:
            return predictions, attention_weights
        return predictions
    
    def compute_consistency_loss(self, predictions):
        """í¬ë¡œìŠ¤ í˜¸ë¼ì´ì¦Œ ì¼ê´€ì„± ì†ì‹¤ ê³„ì‚°"""
        consistency_loss = 0.0
        sorted_horizons = sorted(self.horizons)
        
        for i in range(len(sorted_horizons) - 1):
            h1, h2 = sorted_horizons[i], sorted_horizons[i + 1]
            pred1, pred2 = predictions[h1], predictions[h2]
            
            # ë‹¨ê¸° ì˜ˆì¸¡ì´ ì¥ê¸° ì˜ˆì¸¡ê³¼ ì¼ê´€ì„±ì„ ê°€ì ¸ì•¼ í•¨
            consistency_loss += torch.mean(torch.abs(pred1 - pred2)) * (h2 - h1) / max(self.horizons)
        
        return self.consistency_weight * consistency_loss

class UncertaintyQuantifier:
    """ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™” ì‹œìŠ¤í…œ"""
    
    def __init__(self, model: MultiHorizonPredictor, num_samples: int = 100):
        self.model = model
        self.num_samples = num_samples
    
    def monte_carlo_predictions(self, x: torch.Tensor) -> Dict[int, Dict[str, float]]:
        """ëª¬í…Œì¹´ë¥¼ë¡œ ë“œë¡­ì•„ì›ƒì„ ì´ìš©í•œ ë¶ˆí™•ì‹¤ì„± ì¶”ì •"""
        self.model.train()  # ë“œë¡­ì•„ì›ƒ í™œì„±í™”
        
        predictions = {h: [] for h in self.model.horizons}
        
        with torch.no_grad():
            for _ in range(self.num_samples):
                preds = self.model(x)
                for horizon in self.model.horizons:
                    predictions[horizon].append(preds[horizon].cpu().numpy())
        
        # í†µê³„ ê³„ì‚°
        results = {}
        for horizon in self.model.horizons:
            pred_array = np.array(predictions[horizon])
            results[horizon] = {
                'mean': float(np.mean(pred_array)),
                'std': float(np.std(pred_array)),
                'lower_95': float(np.percentile(pred_array, 2.5)),
                'upper_95': float(np.percentile(pred_array, 97.5)),
                'confidence': float(1.0 - (np.std(pred_array) / np.abs(np.mean(pred_array))))
            }
        
        return results

class TemporalHierarchyAnalyzer:
    """ì‹œê°„ ê³„ì¸µ ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.trend_analyzers = {
            'long_term': {'window': 168, 'weight': 0.5},    # 1ì£¼ì¼ ì¥ê¸° íŠ¸ë Œë“œ
            'medium_term': {'window': 72, 'weight': 0.3},   # 3ì¼ ì¤‘ê¸° íŠ¸ë Œë“œ  
            'short_term': {'window': 24, 'weight': 0.2}     # 1ì¼ ë‹¨ê¸° íŠ¸ë Œë“œ
        }
    
    def analyze_trends(self, data: np.ndarray) -> Dict[str, float]:
        """ë‹¤ì¸µ íŠ¸ë Œë“œ ë¶„ì„"""
        trends = {}
        
        for trend_type, config in self.trend_analyzers.items():
            window = config['window']
            if len(data) >= window:
                recent_data = data[-window:]
                
                # íŠ¸ë Œë“œ ê°•ë„ ê³„ì‚°
                x = np.arange(len(recent_data))
                slope = np.polyfit(x, recent_data[:, 0], 1)[0]
                
                # RÂ² ê³„ì‚°ìœ¼ë¡œ íŠ¸ë Œë“œ ì¼ê´€ì„± ì¸¡ì •
                y_pred = slope * x + np.polyfit(x, recent_data[:, 0], 1)[1]
                r2 = 1 - np.sum((recent_data[:, 0] - y_pred) ** 2) / np.sum((recent_data[:, 0] - np.mean(recent_data[:, 0])) ** 2)
                
                trends[trend_type] = {
                    'slope': float(slope),
                    'consistency': float(max(0, r2)),
                    'volatility': float(np.std(recent_data[:, 0])),
                    'weight': config['weight']
                }
        
        return trends

class DynamicHorizonWeighter:
    """ë™ì  ì‹œê°„ëŒ€ ê°€ì¤‘ì¹˜ ì¡°ì •ê¸°"""
    
    def __init__(self):
        self.volatility_threshold = 0.05
        self.performance_history = defaultdict(list)
    
    def update_performance(self, horizon: int, accuracy: float, volatility: float):
        """ì„±ëŠ¥ ê¸°ë¡ ì—…ë°ì´íŠ¸"""
        self.performance_history[horizon].append({
            'accuracy': accuracy,
            'volatility': volatility,
            'timestamp': datetime.now()
        })
        
        # ìµœê·¼ 100ê°œ ê¸°ë¡ë§Œ ìœ ì§€
        if len(self.performance_history[horizon]) > 100:
            self.performance_history[horizon].pop(0)
    
    def compute_dynamic_weights(self, current_volatility: float) -> Dict[int, float]:
        """í˜„ì¬ ì‹œì¥ ìƒí™©ì— ë”°ë¥¸ ë™ì  ê°€ì¤‘ì¹˜ ê³„ì‚°"""
        base_weights = {1: 0.1, 4: 0.2, 24: 0.3, 72: 0.25, 168: 0.15}
        
        if current_volatility > self.volatility_threshold:
            # ê³ ë³€ë™ì„± ì‹œì¥: ë‹¨ê¸° ì˜ˆì¸¡ì— ë” ë§ì€ ê°€ì¤‘ì¹˜
            adjusted_weights = {1: 0.25, 4: 0.3, 24: 0.25, 72: 0.15, 168: 0.05}
        else:
            # ì €ë³€ë™ì„± ì‹œì¥: ì¥ê¸° ì˜ˆì¸¡ì— ë” ë§ì€ ê°€ì¤‘ì¹˜
            adjusted_weights = {1: 0.05, 4: 0.1, 24: 0.25, 72: 0.3, 168: 0.3}
        
        # ê³¼ê±° ì„±ëŠ¥ ê¸°ë°˜ ì¡°ì •
        for horizon in base_weights:
            if horizon in self.performance_history and self.performance_history[horizon]:
                recent_performance = np.mean([p['accuracy'] for p in self.performance_history[horizon][-10:]])
                performance_multiplier = max(0.5, min(2.0, recent_performance / 0.8))  # 80% ê¸°ì¤€
                adjusted_weights[horizon] *= performance_multiplier
        
        # ì •ê·œí™”
        total_weight = sum(adjusted_weights.values())
        return {h: w / total_weight for h, w in adjusted_weights.items()}

class MultiHorizonPredictionSystem:
    """ì™„ì „í•œ ë‹¤ì¤‘ ì‹œê°„ëŒ€ ì˜ˆì¸¡ ì‹œìŠ¤í…œ"""
    
    def __init__(self, data_path: str):
        self.data_path = data_path
        self.horizons = [1, 4, 24, 72, 168]  # ì‹œê°„ ë‹¨ìœ„
        self.lookback = 168  # 1ì£¼ì¼ ë£©ë°±
        
        # êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™”
        self.model = None
        self.scaler = StandardScaler()
        self.uncertainty_quantifier = None
        self.temporal_analyzer = TemporalHierarchyAnalyzer()
        self.horizon_weighter = DynamicHorizonWeighter()
        
        # ì„±ëŠ¥ ì¶”ì 
        self.performance_history = defaultdict(list)
        self.setup_logging()
    
    def setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('multi_horizon_prediction.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def load_and_prepare_data(self) -> Tuple[np.ndarray, np.ndarray]:
        """ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""
        self.logger.info("ğŸ” ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ ì‹œì‘")
        
        # AI ìµœì í™”ëœ ë°ì´í„° ë¡œë“œ
        csv_path = os.path.join(self.data_path, "ai_optimized_3month_data", "ai_matrix_complete.csv")
        
        if not os.path.exists(csv_path):
            # ëŒ€ì²´ ë°ì´í„° ê²½ë¡œ ì‹œë„
            csv_path = os.path.join(self.data_path, "historical_6month_data", "btc_price_hourly.csv")
        
        if not os.path.exists(csv_path):
            raise FileNotFoundError(f"ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}")
        
        df = pd.read_csv(csv_path)
        self.logger.info(f"ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {df.shape}")
        
        # ì‹œê°„ ì»¬ëŸ¼ ì²˜ë¦¬
        if 'datetime' in df.columns:
            df['datetime'] = pd.to_datetime(df['datetime'])
            df = df.sort_values('datetime')
        elif 'timestamp' in df.columns:
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            df = df.sort_values('timestamp')
        
        # ìˆ«ìí˜• ì»¬ëŸ¼ë§Œ ì„ íƒ
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        df_numeric = df[numeric_columns].copy()
        
        # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
        df_numeric = df_numeric.ffill().bfill()
        
        # ì´ìƒì¹˜ ì œê±° (IQR ë°©ë²•)
        Q1 = df_numeric.quantile(0.25)
        Q3 = df_numeric.quantile(0.75)
        IQR = Q3 - Q1
        df_clean = df_numeric[~((df_numeric < (Q1 - 1.5 * IQR)) | (df_numeric > (Q3 + 1.5 * IQR))).any(axis=1)]
        
        # íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
        if 'btc_price' in df_clean.columns:
            price_col = 'btc_price'
        elif 'close' in df_clean.columns:
            price_col = 'close'
        else:
            price_col = df_clean.columns[0]  # ì²« ë²ˆì§¸ ì»¬ëŸ¼ì„ ê°€ê²©ìœ¼ë¡œ ê°€ì •
        
        # íƒ€ê²Ÿì„ ì²« ë²ˆì§¸ ì»¬ëŸ¼ìœ¼ë¡œ ì´ë™
        cols = [price_col] + [col for col in df_clean.columns if col != price_col]
        df_clean = df_clean[cols]
        
        # ì •ê·œí™”
        data_scaled = self.scaler.fit_transform(df_clean.values)
        
        # í›ˆë ¨/ê²€ì¦ ë¶„í•  (ì‹œê°„ìˆœ ë¶„í• )
        train_size = int(0.8 * len(data_scaled))
        train_data = data_scaled[:train_size]
        test_data = data_scaled[train_size:]
        
        self.logger.info(f"í›ˆë ¨ ë°ì´í„°: {train_data.shape}, í…ŒìŠ¤íŠ¸ ë°ì´í„°: {test_data.shape}")
        
        return train_data, test_data
    
    def create_model(self, input_dim: int) -> MultiHorizonPredictor:
        """ëª¨ë¸ ìƒì„±"""
        model = MultiHorizonPredictor(
            input_dim=input_dim,
            horizons=self.horizons,
            encoder_dims=[512, 256, 128],
            head_dim=64,
            dropout_rate=0.2
        )
        
        self.uncertainty_quantifier = UncertaintyQuantifier(model, num_samples=100)
        return model
    
    def train_model(self, train_data: np.ndarray, val_data: np.ndarray = None) -> Dict:
        """ëª¨ë¸ í›ˆë ¨"""
        self.logger.info("ğŸš€ Multi-Horizon ëª¨ë¸ í›ˆë ¨ ì‹œì‘")
        
        # ë°ì´í„°ì…‹ ìƒì„±
        train_dataset = MultiHorizonDataset(train_data, self.horizons, self.lookback)
        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
        
        if val_data is not None:
            val_dataset = MultiHorizonDataset(val_data, self.horizons, self.lookback)
            val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
        
        # ëª¨ë¸ ìƒì„±
        input_dim = train_data.shape[1]
        self.model = self.create_model(input_dim)
        
        # ì˜µí‹°ë§ˆì´ì € ë° ì†ì‹¤ í•¨ìˆ˜
        optimizer = torch.optim.AdamW(self.model.parameters(), lr=0.001, weight_decay=0.01)
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=10, factor=0.5)
        
        # í›ˆë ¨ ë£¨í”„
        best_loss = float('inf')
        patience_counter = 0
        training_history = {'train_loss': [], 'val_loss': []}
        
        epochs = 100
        for epoch in range(epochs):
            # í›ˆë ¨
            self.model.train()
            train_losses = []
            
            for features, targets in train_loader:
                optimizer.zero_grad()
                
                predictions = self.model(features)
                
                # ë‹¤ì¤‘ ì‹œê°„ëŒ€ ì†ì‹¤ ê³„ì‚°
                total_loss = 0.0
                for horizon in self.horizons:
                    if horizon in targets:
                        horizon_loss = F.mse_loss(predictions[horizon], targets[horizon])
                        total_loss += horizon_loss
                
                # ì¼ê´€ì„± ì†ì‹¤ ì¶”ê°€
                consistency_loss = self.model.compute_consistency_loss(predictions)
                total_loss += consistency_loss
                
                total_loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                optimizer.step()
                
                train_losses.append(total_loss.item())
            
            avg_train_loss = np.mean(train_losses)
            training_history['train_loss'].append(avg_train_loss)
            
            # ê²€ì¦
            if val_data is not None:
                self.model.eval()
                val_losses = []
                
                with torch.no_grad():
                    for features, targets in val_loader:
                        predictions = self.model(features)
                        
                        total_loss = 0.0
                        for horizon in self.horizons:
                            if horizon in targets:
                                horizon_loss = F.mse_loss(predictions[horizon], targets[horizon])
                                total_loss += horizon_loss
                        
                        consistency_loss = self.model.compute_consistency_loss(predictions)
                        total_loss += consistency_loss
                        
                        val_losses.append(total_loss.item())
                
                avg_val_loss = np.mean(val_losses)
                training_history['val_loss'].append(avg_val_loss)
                
                # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
                scheduler.step(avg_val_loss)
                
                # ì¡°ê¸° ì¢…ë£Œ ê²€ì‚¬
                if avg_val_loss < best_loss:
                    best_loss = avg_val_loss
                    patience_counter = 0
                    # ëª¨ë¸ ì €ì¥
                    torch.save(self.model.state_dict(), 'best_multi_horizon_model.pth')
                else:
                    patience_counter += 1
                
                if patience_counter >= 20:
                    self.logger.info(f"ì¡°ê¸° ì¢…ë£Œ: {epoch+1} ì—í¬í¬")
                    break
                
                if (epoch + 1) % 10 == 0:
                    self.logger.info(f"ì—í¬í¬ {epoch+1}/{epochs}: í›ˆë ¨ ì†ì‹¤={avg_train_loss:.6f}, ê²€ì¦ ì†ì‹¤={avg_val_loss:.6f}")
        
        # ìµœì  ëª¨ë¸ ë¡œë“œ
        if val_data is not None:
            self.model.load_state_dict(torch.load('best_multi_horizon_model.pth'))
        
        self.logger.info("ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ")
        return training_history
    
    def predict_with_uncertainty(self, data: np.ndarray) -> Dict:
        """ë¶ˆí™•ì‹¤ì„±ì„ í¬í•¨í•œ ì˜ˆì¸¡"""
        if self.model is None:
            raise ValueError("ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        self.model.eval()
        
        # ìµœê·¼ ë°ì´í„°ë¡œ ì˜ˆì¸¡
        recent_data = torch.FloatTensor(data[-self.lookback:]).unsqueeze(0)
        
        # í™•ì •ì  ì˜ˆì¸¡
        with torch.no_grad():
            predictions = self.model(recent_data)
            deterministic_preds = {h: float(pred.item()) for h, pred in predictions.items()}
        
        # ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™”
        uncertainty_results = self.uncertainty_quantifier.monte_carlo_predictions(recent_data)
        
        # ì‹œê°„ ê³„ì¸µ ë¶„ì„
        trend_analysis = self.temporal_analyzer.analyze_trends(data)
        
        # í˜„ì¬ ë³€ë™ì„± ê³„ì‚°
        current_volatility = float(np.std(data[-24:, 0]))  # ìµœê·¼ 24ì‹œê°„ ë³€ë™ì„±
        
        # ë™ì  ê°€ì¤‘ì¹˜ ê³„ì‚°
        dynamic_weights = self.horizon_weighter.compute_dynamic_weights(current_volatility)
        
        return {
            'deterministic_predictions': deterministic_preds,
            'uncertainty_analysis': uncertainty_results,
            'trend_analysis': trend_analysis,
            'market_volatility': current_volatility,
            'dynamic_weights': dynamic_weights,
            'prediction_timestamp': datetime.now().isoformat()
        }
    
    def evaluate_performance(self, test_data: np.ndarray) -> Dict:
        """ì„±ëŠ¥ í‰ê°€"""
        self.logger.info("ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì‹œì‘")
        
        if self.model is None:
            raise ValueError("ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        test_dataset = MultiHorizonDataset(test_data, self.horizons, self.lookback)
        test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)
        
        horizon_results = {h: {'predictions': [], 'actuals': []} for h in self.horizons}
        
        self.model.eval()
        with torch.no_grad():
            for features, targets in test_loader:
                predictions = self.model(features)
                
                for horizon in self.horizons:
                    if horizon in targets:
                        pred_value = float(predictions[horizon].item())
                        actual_value = float(targets[horizon].item())
                        
                        horizon_results[horizon]['predictions'].append(pred_value)
                        horizon_results[horizon]['actuals'].append(actual_value)
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
        performance_metrics = {}
        for horizon in self.horizons:
            if horizon_results[horizon]['predictions']:
                preds = np.array(horizon_results[horizon]['predictions'])
                actuals = np.array(horizon_results[horizon]['actuals'])
                
                # ì—­ì •ê·œí™”
                preds_denorm = self.scaler.inverse_transform(
                    np.column_stack([preds, np.zeros((len(preds), self.scaler.scale_.shape[0] - 1))])
                )[:, 0]
                actuals_denorm = self.scaler.inverse_transform(
                    np.column_stack([actuals, np.zeros((len(actuals), self.scaler.scale_.shape[0] - 1))])
                )[:, 0]
                
                # ë©”íŠ¸ë¦­ ê³„ì‚°
                mae = mean_absolute_error(actuals_denorm, preds_denorm)
                rmse = np.sqrt(mean_squared_error(actuals_denorm, preds_denorm))
                
                # ë°©í–¥ ì •í™•ë„ (ìƒìŠ¹/í•˜ë½ ì˜ˆì¸¡ ì •í™•ë„)
                actual_changes = np.sign(np.diff(actuals_denorm))
                pred_changes = np.sign(np.diff(preds_denorm))
                direction_accuracy = np.mean(actual_changes == pred_changes) * 100
                
                # MAPE ê³„ì‚°
                mape = np.mean(np.abs((actuals_denorm - preds_denorm) / actuals_denorm)) * 100
                
                performance_metrics[f'{horizon}h'] = {
                    'mae': float(mae),
                    'rmse': float(rmse),
                    'direction_accuracy': float(direction_accuracy),
                    'mape': float(mape),
                    'samples': len(preds)
                }
                
                # ì„±ëŠ¥ ê¸°ë¡ ì—…ë°ì´íŠ¸
                current_volatility = float(np.std(actuals_denorm))
                self.horizon_weighter.update_performance(horizon, direction_accuracy, current_volatility)
        
        # ì „ì²´ ì„±ëŠ¥ ìš”ì•½
        overall_metrics = {
            'overall_direction_accuracy': np.mean([m['direction_accuracy'] for m in performance_metrics.values()]),
            'overall_mape': np.mean([m['mape'] for m in performance_metrics.values()]),
            'target_achieved': np.mean([m['direction_accuracy'] for m in performance_metrics.values()]) >= 90.0,
            'evaluation_timestamp': datetime.now().isoformat()
        }
        
        self.logger.info(f"ì „ì²´ ë°©í–¥ ì •í™•ë„: {overall_metrics['overall_direction_accuracy']:.2f}%")
        self.logger.info(f"90% ëª©í‘œ ë‹¬ì„±: {'âœ… YES' if overall_metrics['target_achieved'] else 'âŒ NO'}")
        
        return {
            'horizon_metrics': performance_metrics,
            'overall_metrics': overall_metrics,
            'detailed_results': horizon_results
        }
    
    def save_system(self, filepath: str):
        """ì‹œìŠ¤í…œ ì €ì¥"""
        save_data = {
            'model_state': self.model.state_dict() if self.model else None,
            'scaler': self.scaler,
            'horizons': self.horizons,
            'lookback': self.lookback,
            'performance_history': dict(self.performance_history)
        }
        
        joblib.dump(save_data, filepath)
        self.logger.info(f"ì‹œìŠ¤í…œ ì €ì¥ ì™„ë£Œ: {filepath}")
    
    def load_system(self, filepath: str):
        """ì‹œìŠ¤í…œ ë¡œë“œ"""
        save_data = joblib.load(filepath)
        
        self.scaler = save_data['scaler']
        self.horizons = save_data['horizons']
        self.lookback = save_data['lookback']
        self.performance_history = defaultdict(list, save_data.get('performance_history', {}))
        
        if save_data['model_state']:
            # ëª¨ë¸ êµ¬ì¡° ì¬ìƒì„± í•„ìš”
            input_dim = save_data['scaler'].scale_.shape[0]
            self.model = self.create_model(input_dim)
            self.model.load_state_dict(save_data['model_state'])
        
        self.logger.info(f"ì‹œìŠ¤í…œ ë¡œë“œ ì™„ë£Œ: {filepath}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    system = MultiHorizonPredictionSystem("/Users/parkyoungjun/Desktop/BTC_Analysis_System")
    
    try:
        # ë°ì´í„° ë¡œë“œ
        train_data, test_data = system.load_and_prepare_data()
        
        # ëª¨ë¸ í›ˆë ¨
        training_history = system.train_model(train_data, test_data)
        
        # ì„±ëŠ¥ í‰ê°€
        performance_results = system.evaluate_performance(test_data)
        
        # ì‹¤ì‹œê°„ ì˜ˆì¸¡ ì˜ˆì‹œ
        prediction_results = system.predict_with_uncertainty(test_data)
        
        # ê²°ê³¼ ì €ì¥
        results = {
            'training_history': training_history,
            'performance_results': performance_results,
            'prediction_example': prediction_results,
            'system_info': {
                'horizons': system.horizons,
                'target_accuracy': 90.0,
                'achieved_accuracy': performance_results['overall_metrics']['overall_direction_accuracy'],
                'target_met': performance_results['overall_metrics']['target_achieved']
            }
        }
        
        # JSONìœ¼ë¡œ ê²°ê³¼ ì €ì¥
        with open('multi_horizon_results.json', 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        # ì‹œìŠ¤í…œ ì €ì¥
        system.save_system('multi_horizon_system.pkl')
        
        print("ğŸ¯ Multi-Horizon Bitcoin Prediction System")
        print("="*60)
        print(f"ğŸ“Š ì „ì²´ ë°©í–¥ ì •í™•ë„: {performance_results['overall_metrics']['overall_direction_accuracy']:.2f}%")
        print(f"ğŸ¯ 90% ëª©í‘œ ë‹¬ì„±: {'âœ… YES' if performance_results['overall_metrics']['target_achieved'] else 'âŒ NO'}")
        print(f"ğŸ“ˆ ì‹œê°„ëŒ€ë³„ ì„±ëŠ¥:")
        
        for horizon, metrics in performance_results['horizon_metrics'].items():
            print(f"  {horizon}: {metrics['direction_accuracy']:.2f}% (MAPE: {metrics['mape']:.2f}%)")
        
        return results
        
    except Exception as e:
        system.logger.error(f"ì‹œìŠ¤í…œ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        raise

if __name__ == "__main__":
    main()