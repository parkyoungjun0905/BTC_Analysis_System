#!/usr/bin/env python3
"""
ğŸ¯ Uncertainty Quantification System
ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™” ì‹œìŠ¤í…œ - ì˜ˆì¸¡ ì‹ ë¢°ë„ ì¸¡ì • ë° ë¦¬ìŠ¤í¬ ê´€ë¦¬

ì£¼ìš” ê¸°ëŠ¥:
1. Monte Carlo Dropout - ë“œë¡­ì•„ì›ƒ ê¸°ë°˜ ë¶ˆí™•ì‹¤ì„± ì¶”ì •
2. Ensemble Methods - ì•™ìƒë¸” ê¸°ë°˜ ì‹ ë¢°ë„ ì¸¡ì •
3. Bayesian Neural Networks - ë² ì´ì§€ì•ˆ ë‰´ëŸ´ ë„¤íŠ¸ì›Œí¬
4. Prediction Intervals - ì˜ˆì¸¡ êµ¬ê°„ ì¶”ì •
5. Risk Assessment - ìœ„í—˜ë„ í‰ê°€ ë° ê´€ë¦¬
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.distributions import Normal, MultivariateNormal
from scipy import stats
from scipy.stats import norm, t
import warnings
import json
import os
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Union
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.model_selection import cross_val_score
import matplotlib.pyplot as plt
import seaborn as sns
from dataclasses import dataclass
from collections import defaultdict
import logging
import joblib

warnings.filterwarnings('ignore')

@dataclass
class UncertaintyMetrics:
    """ë¶ˆí™•ì‹¤ì„± ë©”íŠ¸ë¦­"""
    mean: float
    std: float
    lower_ci: float
    upper_ci: float
    confidence: float
    epistemic_uncertainty: float  # ëª¨ë¸ ë¶ˆí™•ì‹¤ì„±
    aleatoric_uncertainty: float  # ë°ì´í„° ë¶ˆí™•ì‹¤ì„±
    total_uncertainty: float

class BayesianLinearLayer(nn.Module):
    """ë² ì´ì§€ì•ˆ ì„ í˜• ë ˆì´ì–´"""
    
    def __init__(self, in_features: int, out_features: int, prior_std: float = 1.0):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        
        # ê°€ì¤‘ì¹˜ í‰ê· ê³¼ ë¶„ì‚° íŒŒë¼ë¯¸í„°
        self.weight_mu = nn.Parameter(torch.Tensor(out_features, in_features))
        self.weight_rho = nn.Parameter(torch.Tensor(out_features, in_features))
        
        # í¸í–¥ í‰ê· ê³¼ ë¶„ì‚° íŒŒë¼ë¯¸í„°
        self.bias_mu = nn.Parameter(torch.Tensor(out_features))
        self.bias_rho = nn.Parameter(torch.Tensor(out_features))
        
        # ì‚¬ì „ ë¶„í¬ ì„¤ì •
        self.prior_std = prior_std
        
        self.reset_parameters()
    
    def reset_parameters(self):
        """íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”"""
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        nn.init.normal_(self.weight_mu, 0, 0.1)
        nn.init.constant_(self.weight_rho, -3)  # ì‘ì€ ë¶„ì‚°ìœ¼ë¡œ ì‹œì‘
        
        # í¸í–¥ ì´ˆê¸°í™”
        nn.init.normal_(self.bias_mu, 0, 0.1)
        nn.init.constant_(self.bias_rho, -3)
    
    def forward(self, input: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        # ê°€ì¤‘ì¹˜ í‘œì¤€í¸ì°¨ ê³„ì‚° (softplusë¡œ ì–‘ìˆ˜ ë³´ì¥)
        weight_std = torch.log1p(torch.exp(self.weight_rho))
        bias_std = torch.log1p(torch.exp(self.bias_rho))
        
        # ê°€ì¤‘ì¹˜ì™€ í¸í–¥ ìƒ˜í”Œë§
        if self.training:
            weight = self.weight_mu + weight_std * torch.randn_like(weight_std)
            bias = self.bias_mu + bias_std * torch.randn_like(bias_std)
        else:
            weight = self.weight_mu
            bias = self.bias_mu
        
        # ì¶œë ¥ ê³„ì‚°
        output = F.linear(input, weight, bias)
        
        # KL ë°œì‚° ê³„ì‚° (ì •ê·œí™” í•­)
        weight_kl = self._kl_divergence(self.weight_mu, weight_std)
        bias_kl = self._kl_divergence(self.bias_mu, bias_std)
        kl_loss = weight_kl + bias_kl
        
        return output, kl_loss
    
    def _kl_divergence(self, mu: torch.Tensor, std: torch.Tensor) -> torch.Tensor:
        """ê°€ìš°ì‹œì•ˆ KL ë°œì‚° ê³„ì‚°"""
        prior_std = self.prior_std
        var = std ** 2
        
        kl = torch.log(prior_std / std) + (var + mu ** 2) / (2 * prior_std ** 2) - 0.5
        return kl.sum()

class BayesianNeuralNetwork(nn.Module):
    """ë² ì´ì§€ì•ˆ ë‰´ëŸ´ ë„¤íŠ¸ì›Œí¬"""
    
    def __init__(self, input_dim: int, hidden_dims: List[int], output_dim: int, 
                 prior_std: float = 1.0, dropout_rate: float = 0.1):
        super().__init__()
        
        self.layers = nn.ModuleList()
        self.dropout_layers = nn.ModuleList()
        
        # ì…ë ¥ ë ˆì´ì–´
        prev_dim = input_dim
        
        # ìˆ¨ê²¨ì§„ ë ˆì´ì–´ë“¤
        for hidden_dim in hidden_dims:
            self.layers.append(BayesianLinearLayer(prev_dim, hidden_dim, prior_std))
            self.dropout_layers.append(nn.Dropout(dropout_rate))
            prev_dim = hidden_dim
        
        # ì¶œë ¥ ë ˆì´ì–´
        self.output_layer = BayesianLinearLayer(prev_dim, output_dim, prior_std)
        
        self.kl_weight = 1.0
    
    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        total_kl = 0.0
        
        # ìˆœì „íŒŒ
        for layer, dropout in zip(self.layers, self.dropout_layers):
            x, kl = layer(x)
            x = F.relu(x)
            x = dropout(x)
            total_kl += kl
        
        # ì¶œë ¥ ë ˆì´ì–´
        output, kl = self.output_layer(x)
        total_kl += kl
        
        return output, total_kl * self.kl_weight

class MonteCarloDropout:
    """ëª¬í…Œì¹´ë¥¼ë¡œ ë“œë¡­ì•„ì›ƒ ë¶ˆí™•ì‹¤ì„± ì¶”ì •"""
    
    def __init__(self, model: nn.Module, num_samples: int = 100):
        self.model = model
        self.num_samples = num_samples
    
    def predict_with_uncertainty(self, X: torch.Tensor) -> UncertaintyMetrics:
        """ë¶ˆí™•ì‹¤ì„±ê³¼ í•¨ê»˜ ì˜ˆì¸¡"""
        self.model.train()  # ë“œë¡­ì•„ì›ƒ í™œì„±í™”
        
        predictions = []
        
        with torch.no_grad():
            for _ in range(self.num_samples):
                pred = self.model(X)
                if isinstance(pred, tuple):
                    pred = pred[0]  # ë² ì´ì§€ì•ˆ ëª¨ë¸ì¸ ê²½ìš°
                predictions.append(pred.cpu().numpy())
        
        predictions = np.array(predictions)
        
        # í†µê³„ ê³„ì‚°
        mean_pred = np.mean(predictions, axis=0)
        std_pred = np.std(predictions, axis=0)
        
        # ì‹ ë¢° êµ¬ê°„ (95%)
        lower_ci = np.percentile(predictions, 2.5, axis=0)
        upper_ci = np.percentile(predictions, 97.5, axis=0)
        
        # ì‹ ë¢°ë„ ê³„ì‚° (ë³€ì´ ê³„ìˆ˜ì˜ ì—­ìˆ˜)
        confidence = 1.0 / (1.0 + np.mean(std_pred) / (np.abs(np.mean(mean_pred)) + 1e-8))
        
        # ë¶ˆí™•ì‹¤ì„± ë¶„í•´
        epistemic_uncertainty = np.mean(std_pred)  # ëª¨ë¸ ë¶ˆí™•ì‹¤ì„±
        aleatoric_uncertainty = self._estimate_aleatoric_uncertainty(predictions)
        total_uncertainty = np.sqrt(epistemic_uncertainty**2 + aleatoric_uncertainty**2)
        
        return UncertaintyMetrics(
            mean=float(np.mean(mean_pred)),
            std=float(np.mean(std_pred)),
            lower_ci=float(np.mean(lower_ci)),
            upper_ci=float(np.mean(upper_ci)),
            confidence=float(confidence),
            epistemic_uncertainty=float(epistemic_uncertainty),
            aleatoric_uncertainty=float(aleatoric_uncertainty),
            total_uncertainty=float(total_uncertainty)
        )
    
    def _estimate_aleatoric_uncertainty(self, predictions: np.ndarray) -> float:
        """ë°ì´í„° ë¶ˆí™•ì‹¤ì„± ì¶”ì •"""
        # ì˜ˆì¸¡ì˜ ë¶„ì‚°ì„ ì´ìš©í•œ ê°„ë‹¨í•œ ì¶”ì •
        sample_vars = np.var(predictions, axis=1)
        return float(np.sqrt(np.mean(sample_vars)))

class EnsembleUncertainty:
    """ì•™ìƒë¸” ê¸°ë°˜ ë¶ˆí™•ì‹¤ì„± ì¶”ì •"""
    
    def __init__(self, models: List, model_types: List[str] = None):
        self.models = models
        self.model_types = model_types or ['model'] * len(models)
        self.weights = None
    
    def fit_ensemble_weights(self, X_val: np.ndarray, y_val: np.ndarray):
        """ê²€ì¦ ë°ì´í„°ë¡œ ì•™ìƒë¸” ê°€ì¤‘ì¹˜ í•™ìŠµ"""
        predictions = []
        
        for model in self.models:
            if hasattr(model, 'predict'):
                pred = model.predict(X_val)
            else:
                pred = model(torch.FloatTensor(X_val)).detach().cpu().numpy()
            predictions.append(pred.flatten())
        
        predictions = np.array(predictions).T  # (samples, models)
        
        # ê° ëª¨ë¸ì˜ ì„±ëŠ¥ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ê³„ì‚°
        weights = []
        for i, pred in enumerate(predictions.T):
            mae = mean_absolute_error(y_val, pred)
            weight = 1.0 / (mae + 1e-8)  # MAEì˜ ì—­ìˆ˜
            weights.append(weight)
        
        # ì •ê·œí™”
        weights = np.array(weights)
        self.weights = weights / np.sum(weights)
        
        return self.weights
    
    def predict_with_uncertainty(self, X: Union[np.ndarray, torch.Tensor]) -> UncertaintyMetrics:
        """ì•™ìƒë¸” ë¶ˆí™•ì‹¤ì„± ì˜ˆì¸¡"""
        predictions = []
        
        for i, model in enumerate(self.models):
            if hasattr(model, 'predict'):
                pred = model.predict(X)
            else:
                if isinstance(X, np.ndarray):
                    X_tensor = torch.FloatTensor(X)
                else:
                    X_tensor = X
                pred = model(X_tensor)
                if isinstance(pred, tuple):
                    pred = pred[0]
                pred = pred.detach().cpu().numpy()
            
            predictions.append(pred.flatten())
        
        predictions = np.array(predictions)  # (models, samples)
        
        # ê°€ì¤‘ í‰ê·  (ê°€ì¤‘ì¹˜ê°€ ìˆëŠ” ê²½ìš°)
        if self.weights is not None:
            mean_pred = np.average(predictions, axis=0, weights=self.weights)
        else:
            mean_pred = np.mean(predictions, axis=0)
        
        # ì•™ìƒë¸” ë¶ˆí™•ì‹¤ì„± ê³„ì‚°
        ensemble_std = np.std(predictions, axis=0)
        
        # ì‹ ë¢° êµ¬ê°„
        lower_ci = np.percentile(predictions, 2.5, axis=0)
        upper_ci = np.percentile(predictions, 97.5, axis=0)
        
        # ì‹ ë¢°ë„ (ì¼ì¹˜ë„ ê¸°ë°˜)
        agreement = 1.0 - np.mean(ensemble_std) / (np.abs(np.mean(mean_pred)) + 1e-8)
        confidence = max(0.0, min(1.0, agreement))
        
        return UncertaintyMetrics(
            mean=float(np.mean(mean_pred)),
            std=float(np.mean(ensemble_std)),
            lower_ci=float(np.mean(lower_ci)),
            upper_ci=float(np.mean(upper_ci)),
            confidence=float(confidence),
            epistemic_uncertainty=float(np.mean(ensemble_std)),  # ëª¨ë¸ ê°„ ë¶ˆì¼ì¹˜
            aleatoric_uncertainty=0.0,  # ì•™ìƒë¸”ì—ì„œëŠ” ì§ì ‘ ì¸¡ì • ì–´ë ¤ì›€
            total_uncertainty=float(np.mean(ensemble_std))
        )

class GaussianProcessUncertainty:
    """ê°€ìš°ì‹œì•ˆ í”„ë¡œì„¸ìŠ¤ ë¶ˆí™•ì‹¤ì„± ì¶”ì •"""
    
    def __init__(self, kernel=None, alpha: float = 1e-10):
        if kernel is None:
            kernel = ConstantKernel(1.0) * RBF(length_scale=1.0)
        
        self.gp = GaussianProcessRegressor(
            kernel=kernel,
            alpha=alpha,
            n_restarts_optimizer=10,
            normalize_y=True
        )
    
    def fit(self, X: np.ndarray, y: np.ndarray):
        """ê°€ìš°ì‹œì•ˆ í”„ë¡œì„¸ìŠ¤ í›ˆë ¨"""
        self.gp.fit(X, y)
        return self
    
    def predict_with_uncertainty(self, X: np.ndarray) -> UncertaintyMetrics:
        """ê°€ìš°ì‹œì•ˆ í”„ë¡œì„¸ìŠ¤ ë¶ˆí™•ì‹¤ì„± ì˜ˆì¸¡"""
        mean_pred, std_pred = self.gp.predict(X, return_std=True)
        
        # ì‹ ë¢° êµ¬ê°„ (95%)
        lower_ci = mean_pred - 1.96 * std_pred
        upper_ci = mean_pred + 1.96 * std_pred
        
        # ì‹ ë¢°ë„ (ë¶ˆí™•ì‹¤ì„±ì˜ ì—­ìˆ˜)
        confidence = 1.0 / (1.0 + np.mean(std_pred))
        
        return UncertaintyMetrics(
            mean=float(np.mean(mean_pred)),
            std=float(np.mean(std_pred)),
            lower_ci=float(np.mean(lower_ci)),
            upper_ci=float(np.mean(upper_ci)),
            confidence=float(confidence),
            epistemic_uncertainty=float(np.mean(std_pred)),  # GP ë¶ˆí™•ì‹¤ì„±
            aleatoric_uncertainty=0.0,  # GPì—ì„œëŠ” ë…¸ì´ì¦ˆ ëª¨ë¸ë§ í•„ìš”
            total_uncertainty=float(np.mean(std_pred))
        )

class QuantileRegression:
    """ë¶„ìœ„ìˆ˜ íšŒê·€ ê¸°ë°˜ ì˜ˆì¸¡ êµ¬ê°„"""
    
    def __init__(self, quantiles: List[float] = [0.025, 0.5, 0.975]):
        self.quantiles = quantiles
        self.models = {}
    
    def fit(self, X: np.ndarray, y: np.ndarray):
        """ë¶„ìœ„ìˆ˜ íšŒê·€ ëª¨ë¸ í›ˆë ¨"""
        from sklearn.ensemble import GradientBoostingRegressor
        
        for q in self.quantiles:
            model = GradientBoostingRegressor(
                loss='quantile',
                alpha=q,
                n_estimators=100,
                random_state=42
            )
            model.fit(X, y)
            self.models[q] = model
        
        return self
    
    def predict_with_uncertainty(self, X: np.ndarray) -> UncertaintyMetrics:
        """ë¶„ìœ„ìˆ˜ ê¸°ë°˜ ë¶ˆí™•ì‹¤ì„± ì˜ˆì¸¡"""
        predictions = {}
        
        for q, model in self.models.items():
            predictions[q] = model.predict(X)
        
        # ì¤‘ì•™ê°’ì„ í‰ê· ìœ¼ë¡œ ì‚¬ìš©
        mean_pred = predictions[0.5]
        
        # ì‹ ë¢° êµ¬ê°„
        lower_ci = predictions[0.025]
        upper_ci = predictions[0.975]
        
        # í‘œì¤€í¸ì°¨ ì¶”ì • (IQR ê¸°ë°˜)
        std_pred = (upper_ci - lower_ci) / 3.92  # 95% CIë¥¼ í‘œì¤€í¸ì°¨ë¡œ ë³€í™˜
        
        # ì‹ ë¢°ë„
        interval_width = upper_ci - lower_ci
        confidence = 1.0 / (1.0 + np.mean(interval_width) / (np.abs(np.mean(mean_pred)) + 1e-8))
        
        return UncertaintyMetrics(
            mean=float(np.mean(mean_pred)),
            std=float(np.mean(std_pred)),
            lower_ci=float(np.mean(lower_ci)),
            upper_ci=float(np.mean(upper_ci)),
            confidence=float(confidence),
            epistemic_uncertainty=float(np.mean(std_pred)),
            aleatoric_uncertainty=0.0,
            total_uncertainty=float(np.mean(std_pred))
        )

class RiskAssessment:
    """ìœ„í—˜ë„ í‰ê°€ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.risk_thresholds = {
            'low': 0.05,      # 5% ë³€ë™
            'medium': 0.10,   # 10% ë³€ë™
            'high': 0.20,     # 20% ë³€ë™
            'extreme': 0.30   # 30% ë³€ë™
        }
    
    def assess_prediction_risk(self, uncertainty_metrics: UncertaintyMetrics, 
                              current_price: float) -> Dict:
        """ì˜ˆì¸¡ ìœ„í—˜ë„ í‰ê°€"""
        # ìƒëŒ€ì  ë¶ˆí™•ì‹¤ì„± ê³„ì‚°
        relative_uncertainty = uncertainty_metrics.std / current_price if current_price > 0 else 0
        
        # ìœ„í—˜ ë“±ê¸‰ ê²°ì •
        risk_level = 'low'
        for level, threshold in sorted(self.risk_thresholds.items(), 
                                     key=lambda x: x[1], reverse=True):
            if relative_uncertainty >= threshold:
                risk_level = level
                break
        
        # VaR (Value at Risk) ê³„ì‚° (95% ì‹ ë¢°ë„)
        var_95 = current_price - uncertainty_metrics.lower_ci
        
        # ì†ì‹¤ í™•ë¥  ì¶”ì •
        if uncertainty_metrics.std > 0:
            loss_prob = 1 - stats.norm.cdf(0, 
                                          uncertainty_metrics.mean - current_price, 
                                          uncertainty_metrics.std)
        else:
            loss_prob = 0.0
        
        # ê·¹ë‹¨ ìœ„í—˜ ì§€í‘œ
        extreme_risk_indicators = {
            'high_uncertainty': relative_uncertainty > self.risk_thresholds['high'],
            'low_confidence': uncertainty_metrics.confidence < 0.5,
            'wide_prediction_interval': (uncertainty_metrics.upper_ci - uncertainty_metrics.lower_ci) / current_price > 0.2,
            'high_epistemic_uncertainty': uncertainty_metrics.epistemic_uncertainty / current_price > 0.1
        }
        
        return {
            'risk_level': risk_level,
            'relative_uncertainty': float(relative_uncertainty),
            'var_95': float(var_95),
            'loss_probability': float(loss_prob),
            'confidence_score': uncertainty_metrics.confidence,
            'extreme_risk_indicators': extreme_risk_indicators,
            'risk_score': self._calculate_composite_risk_score(uncertainty_metrics, current_price)
        }
    
    def _calculate_composite_risk_score(self, uncertainty_metrics: UncertaintyMetrics, 
                                      current_price: float) -> float:
        """ë³µí•© ìœ„í—˜ ì ìˆ˜ ê³„ì‚° (0-1, ë†’ì„ìˆ˜ë¡ ìœ„í—˜)"""
        factors = [
            uncertainty_metrics.std / current_price if current_price > 0 else 0,  # ìƒëŒ€ ë¶ˆí™•ì‹¤ì„±
            1 - uncertainty_metrics.confidence,  # ì‹ ë¢°ë„ì˜ ì—­ìˆ˜
            uncertainty_metrics.epistemic_uncertainty / current_price if current_price > 0 else 0,  # ëª¨ë¸ ë¶ˆí™•ì‹¤ì„±
            (uncertainty_metrics.upper_ci - uncertainty_metrics.lower_ci) / current_price if current_price > 0 else 0  # ì˜ˆì¸¡ êµ¬ê°„ í­
        ]
        
        # ê°€ì¤‘ í‰ê·  (ë™ì¼ ê°€ì¤‘ì¹˜)
        risk_score = np.mean(factors)
        return float(min(1.0, risk_score))

class UncertaintyQuantificationSystem:
    """ì™„ì „í•œ ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™” ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.methods = {}
        self.risk_assessor = RiskAssessment()
        self.calibration_data = []
        self.setup_logging()
    
    def setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
    
    def add_uncertainty_method(self, name: str, method):
        """ë¶ˆí™•ì‹¤ì„± ì¶”ì • ë°©ë²• ì¶”ê°€"""
        self.methods[name] = method
        self.logger.info(f"ë¶ˆí™•ì‹¤ì„± ë°©ë²• ì¶”ê°€: {name}")
    
    def create_bayesian_ensemble(self, input_dim: int, horizons: List[int]) -> Dict:
        """ë² ì´ì§€ì•ˆ ì•™ìƒë¸” ìƒì„±"""
        ensemble_models = {}
        
        for horizon in horizons:
            models = []
            
            # ë² ì´ì§€ì•ˆ ë‰´ëŸ´ ë„¤íŠ¸ì›Œí¬
            bnn = BayesianNeuralNetwork(
                input_dim=input_dim,
                hidden_dims=[128, 64, 32],
                output_dim=1,
                prior_std=1.0,
                dropout_rate=0.2
            )
            models.append(bnn)
            
            # ë‹¤ì–‘í•œ ëœë¤ í¬ë ˆìŠ¤íŠ¸
            rf_models = [
                RandomForestRegressor(n_estimators=100, max_depth=10, random_state=i)
                for i in range(3)
            ]
            models.extend(rf_models)
            
            # ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ…
            gb_models = [
                GradientBoostingRegressor(n_estimators=100, max_depth=6, random_state=i)
                for i in range(2)
            ]
            models.extend(gb_models)
            
            # ì•™ìƒë¸” ë¶ˆí™•ì‹¤ì„± ì¶”ì •ê¸°
            ensemble_uncertainty = EnsembleUncertainty(
                models=models,
                model_types=['bayesian'] + ['rf']*3 + ['gb']*2
            )
            
            ensemble_models[horizon] = ensemble_uncertainty
        
        return ensemble_models
    
    def fit_uncertainty_methods(self, X_train: np.ndarray, y_train: np.ndarray,
                               X_val: np.ndarray, y_val: np.ndarray):
        """ë¶ˆí™•ì‹¤ì„± ì¶”ì • ë°©ë²•ë“¤ í›ˆë ¨"""
        self.logger.info("ğŸ¯ ë¶ˆí™•ì‹¤ì„± ì¶”ì • ë°©ë²• í›ˆë ¨ ì‹œì‘")
        
        # ê°€ìš°ì‹œì•ˆ í”„ë¡œì„¸ìŠ¤
        gp_uncertainty = GaussianProcessUncertainty()
        gp_uncertainty.fit(X_train, y_train)
        self.add_uncertainty_method('gaussian_process', gp_uncertainty)
        
        # ë¶„ìœ„ìˆ˜ íšŒê·€
        quantile_regression = QuantileRegression()
        quantile_regression.fit(X_train, y_train)
        self.add_uncertainty_method('quantile_regression', quantile_regression)
        
        # ë² ì´ì§€ì•ˆ ì•™ìƒë¸”
        bayesian_ensemble = self.create_bayesian_ensemble(X_train.shape[1], [1, 24, 168])
        
        # ì•™ìƒë¸” í›ˆë ¨ (ê°„ë‹¨í•œ ê²½ìš°ë§Œ)
        for horizon, ensemble in bayesian_ensemble.items():
            # ê²€ì¦ ë°ì´í„°ë¡œ ê°€ì¤‘ì¹˜ í•™ìŠµ
            sklearn_models = [m for m in ensemble.models if hasattr(m, 'fit')]
            for model in sklearn_models:
                model.fit(X_train, y_train)
            
            ensemble.fit_ensemble_weights(X_val, y_val)
            self.add_uncertainty_method(f'bayesian_ensemble_{horizon}h', ensemble)
        
        self.logger.info("âœ… ë¶ˆí™•ì‹¤ì„± ì¶”ì • ë°©ë²• í›ˆë ¨ ì™„ë£Œ")
    
    def predict_with_full_uncertainty(self, X: np.ndarray, current_price: float) -> Dict:
        """ì™„ì „í•œ ë¶ˆí™•ì‹¤ì„± ë¶„ì„ìœ¼ë¡œ ì˜ˆì¸¡"""
        self.logger.info(f"ğŸ”® ë¶ˆí™•ì‹¤ì„± ë¶„ì„ ì˜ˆì¸¡ ì‹œì‘ - ìƒ˜í”Œ: {X.shape[0]}")
        
        method_results = {}
        
        # ê° ë°©ë²•ìœ¼ë¡œ ë¶ˆí™•ì‹¤ì„± ì¶”ì •
        for method_name, method in self.methods.items():
            try:
                uncertainty_metrics = method.predict_with_uncertainty(X)
                risk_assessment = self.risk_assessor.assess_prediction_risk(
                    uncertainty_metrics, current_price
                )
                
                method_results[method_name] = {
                    'uncertainty_metrics': uncertainty_metrics,
                    'risk_assessment': risk_assessment
                }
                
            except Exception as e:
                self.logger.warning(f"ë°©ë²• {method_name} ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                continue
        
        # ë©”íƒ€ ë¶ˆí™•ì‹¤ì„± ë¶„ì„
        meta_analysis = self._meta_uncertainty_analysis(method_results)
        
        # ìµœì¢… í†µí•© ê²°ê³¼
        final_result = self._integrate_uncertainty_results(method_results, meta_analysis, current_price)
        
        self.logger.info("âœ… ë¶ˆí™•ì‹¤ì„± ë¶„ì„ ì˜ˆì¸¡ ì™„ë£Œ")
        
        return final_result
    
    def _meta_uncertainty_analysis(self, method_results: Dict) -> Dict:
        """ë©”íƒ€ ë¶ˆí™•ì‹¤ì„± ë¶„ì„"""
        if not method_results:
            return {'consensus': 'low', 'agreement_score': 0.0}
        
        # ëª¨ë“  ë°©ë²•ì˜ ì˜ˆì¸¡ ìˆ˜ì§‘
        predictions = []
        confidences = []
        risk_levels = []
        
        for method_name, result in method_results.items():
            metrics = result['uncertainty_metrics']
            risk = result['risk_assessment']
            
            predictions.append(metrics.mean)
            confidences.append(metrics.confidence)
            risk_levels.append(risk['risk_score'])
        
        # ë°©ë²• ê°„ ì¼ì¹˜ë„
        pred_std = np.std(predictions) if len(predictions) > 1 else 0.0
        conf_mean = np.mean(confidences)
        risk_mean = np.mean(risk_levels)
        
        # í•©ì˜ ìˆ˜ì¤€
        if pred_std / (np.abs(np.mean(predictions)) + 1e-8) < 0.05:
            consensus = 'high'
        elif pred_std / (np.abs(np.mean(predictions)) + 1e-8) < 0.15:
            consensus = 'medium'
        else:
            consensus = 'low'
        
        # ì¼ì¹˜ë„ ì ìˆ˜
        agreement_score = 1.0 / (1.0 + pred_std)
        
        return {
            'consensus': consensus,
            'agreement_score': float(agreement_score),
            'prediction_variance': float(pred_std),
            'average_confidence': float(conf_mean),
            'average_risk': float(risk_mean),
            'method_count': len(method_results)
        }
    
    def _integrate_uncertainty_results(self, method_results: Dict, meta_analysis: Dict, 
                                     current_price: float) -> Dict:
        """ë¶ˆí™•ì‹¤ì„± ê²°ê³¼ í†µí•©"""
        if not method_results:
            return self._create_empty_uncertainty_result()
        
        # ê°€ì¤‘ í‰ê·  ê³„ì‚°
        total_weight = 0.0
        weighted_mean = 0.0
        weighted_std = 0.0
        weighted_confidence = 0.0
        
        method_weights = {
            'gaussian_process': 0.3,
            'quantile_regression': 0.2,
            'bayesian_ensemble_1h': 0.2,
            'bayesian_ensemble_24h': 0.2,
            'bayesian_ensemble_168h': 0.1
        }
        
        individual_results = {}
        
        for method_name, result in method_results.items():
            metrics = result['uncertainty_metrics']
            risk = result['risk_assessment']
            
            # ë°©ë²•ë³„ ê°€ì¤‘ì¹˜ (ì‹ ë¢°ë„ì™€ ì‚¬ì „ ì •ì˜ ê°€ì¤‘ì¹˜ ê²°í•©)
            base_weight = method_weights.get(method_name, 0.1)
            confidence_weight = metrics.confidence
            final_weight = base_weight * confidence_weight
            
            weighted_mean += metrics.mean * final_weight
            weighted_std += metrics.std * final_weight
            weighted_confidence += metrics.confidence * final_weight
            total_weight += final_weight
            
            # ê°œë³„ ê²°ê³¼ ì €ì¥
            individual_results[method_name] = {
                'prediction': metrics.mean,
                'uncertainty': metrics.std,
                'confidence': metrics.confidence,
                'risk_level': risk['risk_level'],
                'risk_score': risk['risk_score']
            }
        
        # ê°€ì¤‘ í‰ê·  ì •ê·œí™”
        if total_weight > 0:
            final_mean = weighted_mean / total_weight
            final_std = weighted_std / total_weight
            final_confidence = weighted_confidence / total_weight
        else:
            final_mean = final_std = final_confidence = 0.0
        
        # ìµœì¢… ìœ„í—˜ í‰ê°€
        final_uncertainty_metrics = UncertaintyMetrics(
            mean=final_mean,
            std=final_std,
            lower_ci=final_mean - 1.96 * final_std,
            upper_ci=final_mean + 1.96 * final_std,
            confidence=final_confidence,
            epistemic_uncertainty=final_std * 0.7,  # ì¶”ì •
            aleatoric_uncertainty=final_std * 0.3,   # ì¶”ì •
            total_uncertainty=final_std
        )
        
        final_risk = self.risk_assessor.assess_prediction_risk(
            final_uncertainty_metrics, current_price
        )
        
        return {
            'integrated_prediction': {
                'mean': final_mean,
                'std': final_std,
                'lower_ci': final_mean - 1.96 * final_std,
                'upper_ci': final_mean + 1.96 * final_std,
                'confidence': final_confidence
            },
            'risk_assessment': final_risk,
            'meta_analysis': meta_analysis,
            'individual_methods': individual_results,
            'analysis_timestamp': datetime.now().isoformat(),
            'summary': {
                'prediction_quality': 'high' if final_confidence > 0.8 else 'medium' if final_confidence > 0.5 else 'low',
                'uncertainty_level': final_risk['risk_level'],
                'recommendation': self._generate_recommendation(final_uncertainty_metrics, final_risk)
            }
        }
    
    def _generate_recommendation(self, uncertainty_metrics: UncertaintyMetrics, 
                               risk_assessment: Dict) -> str:
        """íˆ¬ì ê¶Œê³  ìƒì„±"""
        confidence = uncertainty_metrics.confidence
        risk_level = risk_assessment['risk_level']
        
        if confidence > 0.8 and risk_level == 'low':
            return "ë†’ì€ ì‹ ë¢°ë„, ë‚®ì€ ìœ„í—˜ - íˆ¬ì ì ê·¹ ê³ ë ¤"
        elif confidence > 0.6 and risk_level in ['low', 'medium']:
            return "ì¤‘ê°„ ì‹ ë¢°ë„, ì ì • ìœ„í—˜ - ì‹ ì¤‘í•œ íˆ¬ì ê³ ë ¤"
        elif risk_level in ['high', 'extreme']:
            return "ë†’ì€ ìœ„í—˜ - íˆ¬ì ì£¼ì˜ ë˜ëŠ” íšŒí”¼"
        else:
            return "ë‚®ì€ ì‹ ë¢°ë„ - ì¶”ê°€ ë¶„ì„ í•„ìš”"
    
    def _create_empty_uncertainty_result(self) -> Dict:
        """ë¹ˆ ë¶ˆí™•ì‹¤ì„± ê²°ê³¼ ìƒì„±"""
        return {
            'integrated_prediction': {
                'mean': 0.0,
                'std': 0.0,
                'lower_ci': 0.0,
                'upper_ci': 0.0,
                'confidence': 0.0
            },
            'risk_assessment': {
                'risk_level': 'unknown',
                'relative_uncertainty': 0.0,
                'confidence_score': 0.0
            },
            'meta_analysis': {
                'consensus': 'none',
                'agreement_score': 0.0
            },
            'individual_methods': {},
            'analysis_timestamp': datetime.now().isoformat(),
            'summary': {
                'prediction_quality': 'unknown',
                'uncertainty_level': 'unknown',
                'recommendation': 'ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ ë¶„ì„ ë¶ˆê°€'
            }
        }
    
    def calibrate_uncertainty(self, X_test: np.ndarray, y_test: np.ndarray):
        """ë¶ˆí™•ì‹¤ì„± ë³´ì •"""
        self.logger.info("ğŸ¯ ë¶ˆí™•ì‹¤ì„± ë³´ì • ì‹œì‘")
        
        calibration_results = {}
        
        for method_name, method in self.methods.items():
            try:
                uncertainty_metrics = method.predict_with_uncertainty(X_test)
                
                # ë³´ì • ë©”íŠ¸ë¦­ ê³„ì‚°
                predictions = np.full(len(X_test), uncertainty_metrics.mean)
                errors = np.abs(y_test - predictions)
                
                # ì˜ˆì¸¡ êµ¬ê°„ í¬í•¨ ë¹„ìœ¨
                lower_bound = np.full(len(X_test), uncertainty_metrics.lower_ci)
                upper_bound = np.full(len(X_test), uncertainty_metrics.upper_ci)
                
                coverage = np.mean((y_test >= lower_bound) & (y_test <= upper_bound))
                
                calibration_results[method_name] = {
                    'coverage': float(coverage),
                    'mean_error': float(np.mean(errors)),
                    'calibration_score': float(abs(coverage - 0.95))  # 95% êµ¬ê°„ ëŒ€ë¹„
                }
                
            except Exception as e:
                self.logger.warning(f"ë³´ì • ì¤‘ ì˜¤ë¥˜ {method_name}: {str(e)}")
                continue
        
        self.calibration_data.append(calibration_results)
        self.logger.info("âœ… ë¶ˆí™•ì‹¤ì„± ë³´ì • ì™„ë£Œ")
        
        return calibration_results
    
    def save_system(self, filepath: str):
        """ì‹œìŠ¤í…œ ì €ì¥"""
        save_data = {
            'calibration_data': self.calibration_data,
            'risk_thresholds': self.risk_assessor.risk_thresholds,
            'system_timestamp': datetime.now().isoformat()
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, indent=2, ensure_ascii=False)
        
        # ëª¨ë¸ë“¤ì€ ë³„ë„ ì €ì¥
        model_data = {}
        for name, method in self.methods.items():
            if hasattr(method, 'gp'):  # ê°€ìš°ì‹œì•ˆ í”„ë¡œì„¸ìŠ¤
                joblib.dump(method.gp, f'{filepath}_{name}_gp.pkl')
            elif hasattr(method, 'models') and hasattr(method.models, 'items'):  # ë¶„ìœ„ìˆ˜ íšŒê·€
                joblib.dump(method.models, f'{filepath}_{name}_models.pkl')
        
        self.logger.info(f"ë¶ˆí™•ì‹¤ì„± ì‹œìŠ¤í…œ ì €ì¥ ì™„ë£Œ: {filepath}")

def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    np.random.seed(42)
    n_samples = 1000
    n_features = 20
    
    # í•©ì„± íŠ¹ì„± ë°ì´í„°
    X = np.random.randn(n_samples, n_features)
    
    # ë¹„ì„ í˜• íƒ€ê²Ÿ (ë…¸ì´ì¦ˆ í¬í•¨)
    true_function = lambda x: np.sum(x[:, :5] ** 2, axis=1) + 0.1 * np.sum(x[:, 5:10], axis=1)
    noise = np.random.normal(0, 0.1, n_samples)
    y = true_function(X) + noise
    
    # í›ˆë ¨/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë¶„í• 
    train_size = int(0.6 * n_samples)
    val_size = int(0.2 * n_samples)
    
    X_train = X[:train_size]
    y_train = y[:train_size]
    X_val = X[train_size:train_size+val_size]
    y_val = y[train_size:train_size+val_size]
    X_test = X[train_size+val_size:]
    y_test = y[train_size+val_size:]
    
    # ë¶ˆí™•ì‹¤ì„± ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    uncertainty_system = UncertaintyQuantificationSystem()
    
    print("ğŸ¯ Uncertainty Quantification System Test")
    print("="*60)
    
    # ì‹œìŠ¤í…œ í›ˆë ¨
    uncertainty_system.fit_uncertainty_methods(X_train, y_train, X_val, y_val)
    
    # ì˜ˆì¸¡ ë° ë¶ˆí™•ì‹¤ì„± ë¶„ì„
    current_price = 55000.0  # í˜„ì¬ BTC ê°€ê²© ê°€ì •
    test_sample = X_test[:5]  # 5ê°œ ìƒ˜í”Œ í…ŒìŠ¤íŠ¸
    
    uncertainty_results = uncertainty_system.predict_with_full_uncertainty(test_sample, current_price)
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"\nğŸ“Š í†µí•© ë¶ˆí™•ì‹¤ì„± ë¶„ì„ ê²°ê³¼:")
    pred = uncertainty_results['integrated_prediction']
    print(f"  ì˜ˆì¸¡ê°’: {pred['mean']:.4f} Â± {pred['std']:.4f}")
    print(f"  ì‹ ë¢°êµ¬ê°„: [{pred['lower_ci']:.4f}, {pred['upper_ci']:.4f}]")
    print(f"  ì‹ ë¢°ë„: {pred['confidence']:.3f}")
    
    risk = uncertainty_results['risk_assessment']
    print(f"\nâš ï¸ ìœ„í—˜ í‰ê°€:")
    print(f"  ìœ„í—˜ ìˆ˜ì¤€: {risk['risk_level']}")
    print(f"  ìœ„í—˜ ì ìˆ˜: {risk['risk_score']:.3f}")
    print(f"  ì†ì‹¤ í™•ë¥ : {risk['loss_probability']:.3f}")
    
    meta = uncertainty_results['meta_analysis']
    print(f"\nğŸ¤ ë©”íƒ€ ë¶„ì„:")
    print(f"  í•©ì˜ ìˆ˜ì¤€: {meta['consensus']}")
    print(f"  ì¼ì¹˜ë„: {meta['agreement_score']:.3f}")
    print(f"  ë°©ë²• ìˆ˜: {meta['method_count']}")
    
    summary = uncertainty_results['summary']
    print(f"\nğŸ’¡ ìš”ì•½:")
    print(f"  ì˜ˆì¸¡ í’ˆì§ˆ: {summary['prediction_quality']}")
    print(f"  ë¶ˆí™•ì‹¤ì„±: {summary['uncertainty_level']}")
    print(f"  ê¶Œê³ ì‚¬í•­: {summary['recommendation']}")
    
    # ê°œë³„ ë°©ë²• ê²°ê³¼
    print(f"\nğŸ” ê°œë³„ ë°©ë²•ë³„ ê²°ê³¼:")
    for method_name, result in uncertainty_results['individual_methods'].items():
        print(f"  {method_name}:")
        print(f"    ì˜ˆì¸¡: {result['prediction']:.4f}")
        print(f"    ë¶ˆí™•ì‹¤ì„±: {result['uncertainty']:.4f}")
        print(f"    ì‹ ë¢°ë„: {result['confidence']:.3f}")
        print(f"    ìœ„í—˜: {result['risk_level']}")
    
    # ë³´ì • í…ŒìŠ¤íŠ¸
    calibration_results = uncertainty_system.calibrate_uncertainty(X_test, y_test)
    
    print(f"\nğŸ“ˆ ë¶ˆí™•ì‹¤ì„± ë³´ì • ê²°ê³¼:")
    for method_name, calibration in calibration_results.items():
        print(f"  {method_name}:")
        print(f"    ì»¤ë²„ë¦¬ì§€: {calibration['coverage']:.3f}")
        print(f"    í‰ê·  ì˜¤ì°¨: {calibration['mean_error']:.4f}")
        print(f"    ë³´ì • ì ìˆ˜: {calibration['calibration_score']:.4f}")
    
    # ê²°ê³¼ ì €ì¥
    with open('uncertainty_quantification_results.json', 'w', encoding='utf-8') as f:
        json.dump(uncertainty_results, f, indent=2, ensure_ascii=False, default=str)
    
    uncertainty_system.save_system('uncertainty_system.json')
    
    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ")
    print(f"  - uncertainty_quantification_results.json")
    print(f"  - uncertainty_system.json")
    
    return uncertainty_results

if __name__ == "__main__":
    main()