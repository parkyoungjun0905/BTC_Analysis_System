#!/usr/bin/env python3
"""
ğŸ¯ ì •êµí•œ ë¹„íŠ¸ì½”ì¸ ì˜ˆì¸¡ ëª¨ë¸ ë°±í…ŒìŠ¤íŒ… í”„ë ˆì„ì›Œí¬ v3.0
- í˜„ì‹¤ì ì¸ ì‹œì¥ ì‹œë®¬ë ˆì´ì…˜ (ê±°ë˜ë¹„ìš©, ìŠ¬ë¦¬í”¼ì§€, ìœ ë™ì„± ì œì•½)
- ì›Œí¬í¬ì›Œë“œ ìµœì í™” (ë¡¤ë§ ìœˆë„ìš°, ì•„ì›ƒì˜¤ë¸Œìƒ˜í”Œ ê²€ì¦)
- ë¦¬ìŠ¤í¬ ì¡°ì • ì„±ê³¼ ì§€í‘œ (ìƒ¤í”„/ì†Œë¥´í‹°ë…¸ ë¹„ìœ¨, ìµœëŒ€ë‚™í­, VaR)
- ì‹œì¥ ìƒí™©ë³„ ë¶„ì„ (ê°•ì„¸/ì•½ì„¸/íš¡ë³´ì¥ ì„±ëŠ¥ ë¶„ì„)
- í†µê³„ì  ìœ ì˜ì„± ê²€ì • (ë¶€íŠ¸ìŠ¤íŠ¸ë©, ê°€ì„¤ê²€ì •)
- ë² ì´ì§€ì•ˆ ìµœì í™” í¬í•¨ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
- ëŒ€í™”í˜• ì‹œê°í™” ë° ìƒì„¸ ë¦¬í¬íŒ…
"""

import numpy as np
import pandas as pd
import warnings
import logging
from datetime import datetime, timedelta
import json
import os
from typing import Dict, List, Tuple, Optional, Union
import pickle
import asyncio
from dataclasses import dataclass
from abc import ABC, abstractmethod
import itertools

# í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
from sklearn.model_selection import TimeSeriesSplit, ParameterGrid
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.feature_selection import SelectFromModel, RFE
import xgboost as xgb
import lightgbm as lgb

# ì‹œê°í™” ë¼ì´ë¸ŒëŸ¬ë¦¬
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.express as px

# í†µê³„ ë° ìµœì í™”
from scipy import stats
from scipy.optimize import minimize
try:
    from skopt import gp_minimize
    from skopt.space import Real, Integer
    from skopt.utils import use_named_args
    BAYESIAN_OPT_AVAILABLE = True
except ImportError:
    BAYESIAN_OPT_AVAILABLE = False
    print("âš ï¸ scikit-optimize ë¯¸ì„¤ì¹˜: ê·¸ë¦¬ë“œ ì„œì¹˜ë§Œ ì‚¬ìš©")

warnings.filterwarnings('ignore')

@dataclass
class MarketCondition:
    """ì‹œì¥ ìƒí™© ë°ì´í„° í´ë˜ìŠ¤"""
    name: str
    start_date: datetime
    end_date: datetime
    volatility: float
    trend: str  # 'bull', 'bear', 'sideways'
    description: str

@dataclass
class BacktestConfig:
    """ë°±í…ŒìŠ¤íŠ¸ ì„¤ì • í´ë˜ìŠ¤"""
    # ê±°ë˜ ë¹„ìš©
    transaction_cost: float = 0.001  # 0.1% ê±°ë˜ ìˆ˜ìˆ˜ë£Œ
    slippage: float = 0.0005  # 0.05% ìŠ¬ë¦¬í”¼ì§€
    min_order_size: float = 100.0  # ìµœì†Œ ì£¼ë¬¸ í¬ê¸° ($)
    
    # ë°±í…ŒìŠ¤íŠ¸ ì„¤ì •
    initial_capital: float = 10000.0  # ì´ˆê¸° ìë³¸
    max_position_size: float = 0.95  # ìµœëŒ€ í¬ì§€ì…˜ í¬ê¸°
    lookback_window: int = 720  # í•™ìŠµ ë°ì´í„° ìœˆë„ìš° (ì‹œê°„)
    rebalance_frequency: int = 24  # ë¦¬ë°¸ëŸ°ì‹± ì£¼ê¸° (ì‹œê°„)
    
    # ë¦¬ìŠ¤í¬ ê´€ë¦¬
    max_drawdown_limit: float = 0.20  # 20% ìµœëŒ€ ë‚™í­ ì œí•œ
    stop_loss: float = 0.05  # 5% ìŠ¤íƒ‘ë¡œìŠ¤
    take_profit: float = 0.15  # 15% ì´ìµì‹¤í˜„
    
    # ê²€ì¦ ì„¤ì •
    bootstrap_samples: int = 1000  # ë¶€íŠ¸ìŠ¤íŠ¸ë© ìƒ˜í”Œ ìˆ˜
    confidence_level: float = 0.95  # ì‹ ë¢°êµ¬ê°„
    min_samples: int = 100  # ìµœì†Œ ìƒ˜í”Œ ìˆ˜

class MarketSimulator:
    """í˜„ì‹¤ì ì¸ ì‹œì¥ ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“ˆ"""
    
    def __init__(self, config: BacktestConfig):
        self.config = config
        self.order_book = {'bid': [], 'ask': []}
        self.market_impact_cache = {}
        
    def calculate_transaction_costs(self, trade_size: float, price: float) -> float:
        """ê±°ë˜ ë¹„ìš© ê³„ì‚°"""
        # ê¸°ë³¸ ê±°ë˜ ìˆ˜ìˆ˜ë£Œ
        base_cost = trade_size * price * self.config.transaction_cost
        
        # ì‹œì¥ ì˜í–¥ë¹„ìš© (ê±°ë˜ëŸ‰ì— ë”°ë¼ ì¦ê°€)
        market_impact = self.calculate_market_impact(trade_size, price)
        
        # ìŠ¬ë¦¬í”¼ì§€
        slippage_cost = trade_size * price * self.config.slippage
        
        return base_cost + market_impact + slippage_cost
    
    def calculate_market_impact(self, trade_size: float, price: float) -> float:
        """ì‹œì¥ ì˜í–¥ë¹„ìš© ê³„ì‚° (Square-root law)"""
        # ê±°ë˜ í¬ê¸°ì— ë”°ë¥¸ ë¹„ì„ í˜• ì˜í–¥
        notional = trade_size * price
        
        # ì„ì‹œ ì˜í–¥ (ì¼ì‹œì )
        temporary_impact = 0.01 * np.sqrt(notional / 100000)  # ìŠ¤ì¼€ì¼ë§ íŒ©í„°
        
        # ì˜êµ¬ ì˜í–¥ (ê°€ê²©ì— ì˜êµ¬ì ìœ¼ë¡œ ë°˜ì˜)
        permanent_impact = 0.005 * (notional / 100000)
        
        return (temporary_impact + permanent_impact) * price
    
    def simulate_order_execution(self, order_size: float, current_price: float, 
                                volume_24h: float) -> Dict[str, float]:
        """ì£¼ë¬¸ ì‹¤í–‰ ì‹œë®¬ë ˆì´ì…˜"""
        # ìœ ë™ì„± ì œì•½ í™•ì¸
        max_executable = volume_24h * 0.001  # 24ì‹œê°„ ê±°ë˜ëŸ‰ì˜ 0.1%ê¹Œì§€ë§Œ
        executable_size = min(order_size, max_executable)
        
        if executable_size < self.config.min_order_size:
            return {
                'executed_size': 0,
                'execution_price': current_price,
                'total_cost': 0,
                'slippage': 0,
                'rejected': True
            }
        
        # ì‹¤í–‰ ê°€ê²© ê³„ì‚° (ìŠ¬ë¦¬í”¼ì§€ í¬í•¨)
        slippage_factor = (executable_size / volume_24h) * 100  # ê±°ë˜ëŸ‰ ëŒ€ë¹„ ìŠ¬ë¦¬í”¼ì§€
        execution_price = current_price * (1 + slippage_factor * self.config.slippage)
        
        # ì´ ê±°ë˜ ë¹„ìš©
        total_cost = self.calculate_transaction_costs(executable_size, execution_price)
        
        return {
            'executed_size': executable_size,
            'execution_price': execution_price,
            'total_cost': total_cost,
            'slippage': abs(execution_price - current_price) / current_price,
            'rejected': False
        }

class WalkForwardOptimizer:
    """ì›Œí¬í¬ì›Œë“œ ìµœì í™” ì‹œìŠ¤í…œ"""
    
    def __init__(self, config: BacktestConfig):
        self.config = config
        self.optimization_history = []
        self.parameter_stability = {}
        
    def walk_forward_analysis(self, data: pd.DataFrame, model_class, 
                             param_grid: Dict) -> Dict:
        """ì›Œí¬í¬ì›Œë“œ ë¶„ì„ ì‹¤í–‰"""
        print("ğŸš€ ì›Œí¬í¬ì›Œë“œ ìµœì í™” ì‹œì‘...")
        
        results = []
        optimal_params_history = []
        
        # ë¡¤ë§ ìœˆë„ìš° ì„¤ì •
        n_samples = len(data)
        train_size = self.config.lookback_window
        test_size = self.config.rebalance_frequency
        
        # ì›Œí¬í¬ì›Œë“œ ë£¨í”„
        for i in range(train_size, n_samples - test_size, test_size):
            # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í• 
            train_data = data.iloc[i-train_size:i]
            test_data = data.iloc[i:i+test_size]
            
            print(f"   ğŸ“Š Period {len(results)+1}: Train {len(train_data)} â†’ Test {len(test_data)}")
            
            # ì¸ìƒ˜í”Œ ìµœì í™”
            best_params = self.optimize_parameters(train_data, model_class, param_grid)
            optimal_params_history.append(best_params)
            
            # ì•„ì›ƒì˜¤ë¸Œìƒ˜í”Œ í…ŒìŠ¤íŠ¸
            model = model_class(**best_params)
            oos_performance = self.evaluate_out_of_sample(
                model, train_data, test_data
            )
            
            results.append({
                'period': len(results) + 1,
                'train_start': train_data.index[0],
                'train_end': train_data.index[-1], 
                'test_start': test_data.index[0],
                'test_end': test_data.index[-1],
                'optimal_params': best_params,
                'oos_performance': oos_performance
            })
        
        # íŒŒë¼ë¯¸í„° ì•ˆì •ì„± ë¶„ì„
        self.analyze_parameter_stability(optimal_params_history)
        
        return {
            'results': results,
            'parameter_history': optimal_params_history,
            'stability_analysis': self.parameter_stability
        }
    
    def optimize_parameters(self, train_data: pd.DataFrame, model_class, 
                          param_grid: Dict) -> Dict:
        """íŒŒë¼ë¯¸í„° ìµœì í™”"""
        if BAYESIAN_OPT_AVAILABLE and len(param_grid) > 3:
            return self.bayesian_optimization(train_data, model_class, param_grid)
        else:
            return self.grid_search_optimization(train_data, model_class, param_grid)
    
    def bayesian_optimization(self, train_data: pd.DataFrame, model_class, 
                            param_grid: Dict) -> Dict:
        """ë² ì´ì§€ì•ˆ ìµœì í™”"""
        # íŒŒë¼ë¯¸í„° ê³µê°„ ì •ì˜
        dimensions = []
        param_names = list(param_grid.keys())
        
        for param, values in param_grid.items():
            if isinstance(values[0], int):
                dimensions.append(Integer(min(values), max(values), name=param))
            else:
                dimensions.append(Real(min(values), max(values), name=param))
        
        @use_named_args(dimensions)
        def objective(**params):
            try:
                model = model_class(**params)
                score = self.cross_validate_model(model, train_data)
                return -score  # ìµœì†Œí™”ë¥¼ ìœ„í•´ ìŒìˆ˜í™”
            except:
                return 1000  # ì‹¤íŒ¨ì‹œ í° ê°’ ë°˜í™˜
        
        # ë² ì´ì§€ì•ˆ ìµœì í™” ì‹¤í–‰
        result = gp_minimize(objective, dimensions, n_calls=50, random_state=42)
        
        # ìµœì  íŒŒë¼ë¯¸í„° ë°˜í™˜
        optimal_params = {}
        for i, param_name in enumerate(param_names):
            optimal_params[param_name] = result.x[i]
        
        return optimal_params
    
    def grid_search_optimization(self, train_data: pd.DataFrame, model_class,
                               param_grid: Dict) -> Dict:
        """ê·¸ë¦¬ë“œ ì„œì¹˜ ìµœì í™”"""
        best_score = -np.inf
        best_params = {}
        
        # ëª¨ë“  íŒŒë¼ë¯¸í„° ì¡°í•© íƒìƒ‰
        param_combinations = list(ParameterGrid(param_grid))
        
        for params in param_combinations:
            try:
                model = model_class(**params)
                score = self.cross_validate_model(model, train_data)
                
                if score > best_score:
                    best_score = score
                    best_params = params
            except:
                continue
        
        return best_params
    
    def cross_validate_model(self, model, data: pd.DataFrame) -> float:
        """ì‹œê³„ì—´ êµì°¨ ê²€ì¦"""
        tscv = TimeSeriesSplit(n_splits=5)
        scores = []
        
        X = data.drop(columns=['target'])
        y = data['target']
        
        for train_idx, val_idx in tscv.split(X):
            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
            
            model.fit(X_train, y_train)
            pred = model.predict(X_val)
            score = r2_score(y_val, pred)
            scores.append(score)
        
        return np.mean(scores)
    
    def evaluate_out_of_sample(self, model, train_data: pd.DataFrame, 
                              test_data: pd.DataFrame) -> Dict:
        """ì•„ì›ƒì˜¤ë¸Œìƒ˜í”Œ ì„±ëŠ¥ í‰ê°€"""
        X_train = train_data.drop(columns=['target'])
        y_train = train_data['target']
        X_test = test_data.drop(columns=['target'])
        y_test = test_data['target']
        
        # ëª¨ë¸ í›ˆë ¨
        model.fit(X_train, y_train)
        
        # ì˜ˆì¸¡
        predictions = model.predict(X_test)
        
        # ì„±ê³¼ ì§€í‘œ ê³„ì‚°
        mae = mean_absolute_error(y_test, predictions)
        rmse = np.sqrt(mean_squared_error(y_test, predictions))
        r2 = r2_score(y_test, predictions)
        
        return {
            'mae': mae,
            'rmse': rmse,
            'r2': r2,
            'predictions': predictions,
            'actuals': y_test.values
        }
    
    def analyze_parameter_stability(self, param_history: List[Dict]):
        """íŒŒë¼ë¯¸í„° ì•ˆì •ì„± ë¶„ì„"""
        if not param_history:
            return
            
        # ê° íŒŒë¼ë¯¸í„°ë³„ í†µê³„
        all_params = list(param_history[0].keys())
        
        for param in all_params:
            values = [p[param] for p in param_history]
            
            self.parameter_stability[param] = {
                'mean': np.mean(values),
                'std': np.std(values),
                'cv': np.std(values) / np.mean(values) if np.mean(values) != 0 else 0,
                'stability_score': 1 / (1 + np.std(values))  # ì•ˆì •ì„± ì ìˆ˜ (ë‚®ì€ ë³€ë™ì„±ì¼ìˆ˜ë¡ ë†’ìŒ)
            }

class RiskAdjustedMetrics:
    """ë¦¬ìŠ¤í¬ ì¡°ì • ì„±ê³¼ ì§€í‘œ ì‹œìŠ¤í…œ"""
    
    def __init__(self, risk_free_rate: float = 0.02):
        self.risk_free_rate = risk_free_rate
        
    def calculate_all_metrics(self, returns: np.ndarray, 
                            portfolio_values: np.ndarray) -> Dict:
        """ëª¨ë“  ë¦¬ìŠ¤í¬ ì¡°ì • ì§€í‘œ ê³„ì‚°"""
        return {
            'sharpe_ratio': self.sharpe_ratio(returns),
            'sortino_ratio': self.sortino_ratio(returns),
            'calmar_ratio': self.calmar_ratio(returns, portfolio_values),
            'max_drawdown': self.max_drawdown(portfolio_values),
            'var_95': self.value_at_risk(returns, 0.95),
            'cvar_95': self.conditional_var(returns, 0.95),
            'omega_ratio': self.omega_ratio(returns),
            'tail_ratio': self.tail_ratio(returns),
            'skewness': stats.skew(returns),
            'kurtosis': stats.kurtosis(returns),
            'downside_deviation': self.downside_deviation(returns)
        }
    
    def sharpe_ratio(self, returns: np.ndarray) -> float:
        """ìƒ¤í”„ ë¹„ìœ¨"""
        excess_returns = returns - self.risk_free_rate / 365  # ì¼ê°„ ë¬´ìœ„í—˜ ìˆ˜ìµë¥ 
        return np.mean(excess_returns) / np.std(returns) * np.sqrt(365) if np.std(returns) > 0 else 0
    
    def sortino_ratio(self, returns: np.ndarray) -> float:
        """ì†Œë¥´í‹°ë…¸ ë¹„ìœ¨"""
        excess_returns = returns - self.risk_free_rate / 365
        downside_returns = returns[returns < 0]
        downside_std = np.std(downside_returns) if len(downside_returns) > 0 else np.std(returns)
        return np.mean(excess_returns) / downside_std * np.sqrt(365) if downside_std > 0 else 0
    
    def calmar_ratio(self, returns: np.ndarray, portfolio_values: np.ndarray) -> float:
        """ì¹¼ë§ˆ ë¹„ìœ¨"""
        annual_return = (portfolio_values[-1] / portfolio_values[0]) ** (365 / len(returns)) - 1
        max_dd = self.max_drawdown(portfolio_values)
        return annual_return / abs(max_dd) if max_dd != 0 else 0
    
    def max_drawdown(self, portfolio_values: np.ndarray) -> float:
        """ìµœëŒ€ ë‚™í­"""
        peak = np.maximum.accumulate(portfolio_values)
        drawdown = (portfolio_values - peak) / peak
        return np.min(drawdown)
    
    def value_at_risk(self, returns: np.ndarray, confidence_level: float) -> float:
        """VaR (Value at Risk)"""
        return np.percentile(returns, (1 - confidence_level) * 100)
    
    def conditional_var(self, returns: np.ndarray, confidence_level: float) -> float:
        """CVaR (Conditional Value at Risk)"""
        var = self.value_at_risk(returns, confidence_level)
        return np.mean(returns[returns <= var])
    
    def omega_ratio(self, returns: np.ndarray, threshold: float = 0) -> float:
        """ì˜¤ë©”ê°€ ë¹„ìœ¨"""
        gains = returns[returns > threshold] - threshold
        losses = threshold - returns[returns <= threshold]
        return np.sum(gains) / np.sum(losses) if np.sum(losses) > 0 else np.inf
    
    def tail_ratio(self, returns: np.ndarray) -> float:
        """í…Œì¼ ë¹„ìœ¨ (ìƒìœ„ 5% ìˆ˜ìµë¥  / í•˜ìœ„ 5% ì†ì‹¤ë¥ )"""
        upper_tail = np.percentile(returns, 95)
        lower_tail = np.percentile(returns, 5)
        return abs(upper_tail) / abs(lower_tail) if lower_tail != 0 else np.inf
    
    def downside_deviation(self, returns: np.ndarray, target_return: float = 0) -> float:
        """í•˜ë°© í¸ì°¨"""
        downside_returns = np.minimum(returns - target_return, 0)
        return np.sqrt(np.mean(downside_returns ** 2))

class MarketRegimeAnalyzer:
    """ì‹œì¥ ìƒí™©ë³„ ë¶„ì„ ëª¨ë“ˆ"""
    
    def __init__(self):
        self.regimes = []
        self.regime_performance = {}
        
    def identify_market_regimes(self, data: pd.DataFrame, 
                              price_col: str = 'price') -> List[MarketCondition]:
        """ì‹œì¥ ìƒí™© ì‹ë³„"""
        prices = data[price_col]
        returns = prices.pct_change().dropna()
        
        # 60ì¼ ë¡¤ë§ ìœˆë„ìš°ë¡œ ë³€ë™ì„±ê³¼ ì¶”ì„¸ ê³„ì‚°
        window = 60
        volatility = returns.rolling(window).std() * np.sqrt(365)  # ì—°ìœ¨í™” ë³€ë™ì„±
        trend = prices.rolling(window).apply(lambda x: self.calculate_trend(x))
        
        regimes = []
        current_regime = None
        regime_start = None
        
        for i, (date, vol, tr) in enumerate(zip(data.index, volatility, trend)):
            if pd.isna(vol) or pd.isna(tr):
                continue
                
            # ë³€ë™ì„± ì„ê³„ê°’ (ì¤‘ê°„ê°’ ê¸°ì¤€)
            vol_median = volatility.median()
            
            # ì‹œì¥ ìƒí™© ë¶„ë¥˜
            if tr > 0.1 and vol < vol_median * 1.2:
                regime_type = 'bull_low_vol'
                description = 'ê°•ì„¸ì¥ (ë‚®ì€ ë³€ë™ì„±)'
            elif tr > 0.1 and vol >= vol_median * 1.2:
                regime_type = 'bull_high_vol'
                description = 'ê°•ì„¸ì¥ (ë†’ì€ ë³€ë™ì„±)'
            elif tr < -0.1 and vol < vol_median * 1.2:
                regime_type = 'bear_low_vol'
                description = 'ì•½ì„¸ì¥ (ë‚®ì€ ë³€ë™ì„±)'
            elif tr < -0.1 and vol >= vol_median * 1.2:
                regime_type = 'bear_high_vol'
                description = 'ì•½ì„¸ì¥ (ë†’ì€ ë³€ë™ì„±)'
            elif abs(tr) <= 0.1 and vol < vol_median:
                regime_type = 'sideways_low_vol'
                description = 'íš¡ë³´ì¥ (ë‚®ì€ ë³€ë™ì„±)'
            else:
                regime_type = 'sideways_high_vol'
                description = 'íš¡ë³´ì¥ (ë†’ì€ ë³€ë™ì„±)'
            
            # ìƒí™© ë³€ê²½ ê°ì§€
            if current_regime != regime_type:
                # ì´ì „ êµ¬ê°„ ì¢…ë£Œ
                if current_regime and regime_start:
                    regimes.append(MarketCondition(
                        name=current_regime,
                        start_date=regime_start,
                        end_date=date,
                        volatility=vol,
                        trend='bull' if 'bull' in current_regime else 'bear' if 'bear' in current_regime else 'sideways',
                        description=description
                    ))
                
                # ìƒˆë¡œìš´ êµ¬ê°„ ì‹œì‘
                current_regime = regime_type
                regime_start = date
        
        # ë§ˆì§€ë§‰ êµ¬ê°„ ì²˜ë¦¬
        if current_regime and regime_start:
            regimes.append(MarketCondition(
                name=current_regime,
                start_date=regime_start,
                end_date=data.index[-1],
                volatility=volatility.iloc[-1],
                trend='bull' if 'bull' in current_regime else 'bear' if 'bear' in current_regime else 'sideways',
                description=description
            ))
        
        self.regimes = regimes
        return regimes
    
    def calculate_trend(self, prices: pd.Series) -> float:
        """ì¶”ì„¸ ê³„ì‚° (ì„ í˜• íšŒê·€ ê¸°ìš¸ê¸°)"""
        if len(prices) < 2:
            return 0
        x = np.arange(len(prices))
        slope, _, _, _, _ = stats.linregress(x, prices)
        return slope / prices.mean()  # ì •ê·œí™”ëœ ê¸°ìš¸ê¸°
    
    def analyze_regime_performance(self, backtest_results: Dict, 
                                 regimes: List[MarketCondition]) -> Dict:
        """ì‹œì¥ ìƒí™©ë³„ ì„±ëŠ¥ ë¶„ì„"""
        regime_performance = {}
        
        for regime in regimes:
            # í•´ë‹¹ ê¸°ê°„ ê²°ê³¼ í•„í„°ë§
            regime_results = self.filter_results_by_period(
                backtest_results, regime.start_date, regime.end_date
            )
            
            if regime_results:
                regime_performance[regime.name] = {
                    'period': f"{regime.start_date.strftime('%Y-%m-%d')} ~ {regime.end_date.strftime('%Y-%m-%d')}",
                    'trend': regime.trend,
                    'description': regime.description,
                    'performance': regime_results,
                    'duration_days': (regime.end_date - regime.start_date).days
                }
        
        self.regime_performance = regime_performance
        return regime_performance
    
    def filter_results_by_period(self, results: Dict, start_date: datetime, 
                               end_date: datetime) -> Dict:
        """ê¸°ê°„ë³„ ê²°ê³¼ í•„í„°ë§"""
        # êµ¬í˜„ ì„¸ë¶€ì‚¬í•­ì€ ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ êµ¬ì¡°ì— ë”°ë¼ ì¡°ì •
        return {}

class StatisticalSignificanceTester:
    """í†µê³„ì  ìœ ì˜ì„± ê²€ì • ì‹œìŠ¤í…œ"""
    
    def __init__(self, config: BacktestConfig):
        self.config = config
        
    def bootstrap_analysis(self, returns: np.ndarray, 
                         metric_func, n_bootstrap: int = 1000) -> Dict:
        """ë¶€íŠ¸ìŠ¤íŠ¸ë© ë¶„ì„"""
        n_samples = len(returns)
        bootstrap_metrics = []
        
        for _ in range(n_bootstrap):
            # ë³µì› ì¶”ì¶œ
            bootstrap_sample = np.random.choice(returns, size=n_samples, replace=True)
            metric = metric_func(bootstrap_sample)
            bootstrap_metrics.append(metric)
        
        bootstrap_metrics = np.array(bootstrap_metrics)
        
        # ì‹ ë¢°êµ¬ê°„ ê³„ì‚°
        alpha = 1 - self.config.confidence_level
        lower_percentile = (alpha / 2) * 100
        upper_percentile = (1 - alpha / 2) * 100
        
        return {
            'mean': np.mean(bootstrap_metrics),
            'std': np.std(bootstrap_metrics),
            'confidence_interval': [
                np.percentile(bootstrap_metrics, lower_percentile),
                np.percentile(bootstrap_metrics, upper_percentile)
            ],
            'bootstrap_distribution': bootstrap_metrics
        }
    
    def significance_test(self, strategy_returns: np.ndarray, 
                         benchmark_returns: np.ndarray) -> Dict:
        """ì „ëµ vs ë²¤ì¹˜ë§ˆí¬ ìœ ì˜ì„± ê²€ì •"""
        # t-ê²€ì •
        t_stat, t_pvalue = stats.ttest_ind(strategy_returns, benchmark_returns)
        
        # Wilcoxon ìˆœìœ„í•© ê²€ì • (ë¹„ëª¨ìˆ˜)
        wilcoxon_stat, wilcoxon_pvalue = stats.ranksums(strategy_returns, benchmark_returns)
        
        # Kolmogorov-Smirnov ê²€ì •
        ks_stat, ks_pvalue = stats.ks_2samp(strategy_returns, benchmark_returns)
        
        return {
            't_test': {'statistic': t_stat, 'p_value': t_pvalue},
            'wilcoxon_test': {'statistic': wilcoxon_stat, 'p_value': wilcoxon_pvalue},
            'ks_test': {'statistic': ks_stat, 'p_value': ks_pvalue},
            'significant': t_pvalue < 0.05 and wilcoxon_pvalue < 0.05
        }
    
    def multiple_comparisons_correction(self, p_values: List[float], 
                                      method: str = 'bonferroni') -> Dict:
        """ë‹¤ì¤‘ë¹„êµ ë³´ì •"""
        p_values = np.array(p_values)
        n_comparisons = len(p_values)
        
        if method == 'bonferroni':
            corrected_alpha = 0.05 / n_comparisons
            significant = p_values < corrected_alpha
        elif method == 'holm':
            sorted_indices = np.argsort(p_values)
            corrected_alpha = 0.05 / (n_comparisons - np.arange(n_comparisons))
            significant = np.zeros_like(p_values, dtype=bool)
            for i, idx in enumerate(sorted_indices):
                if p_values[idx] < corrected_alpha[i]:
                    significant[idx] = True
                else:
                    break
        else:
            significant = p_values < 0.05
        
        return {
            'method': method,
            'corrected_alpha': corrected_alpha if method == 'bonferroni' else None,
            'significant': significant,
            'adjusted_p_values': p_values * n_comparisons if method == 'bonferroni' else p_values
        }

class InteractiveVisualizationSystem:
    """ëŒ€í™”í˜• ì‹œê°í™” ì‹œìŠ¤í…œ"""
    
    def __init__(self, output_path: str):
        self.output_path = output_path
        
    def create_comprehensive_dashboard(self, results: Dict) -> str:
        """ì¢…í•© ëŒ€ì‹œë³´ë“œ ìƒì„±"""
        fig = make_subplots(
            rows=4, cols=3,
            subplot_titles=(
                "í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜ ë³€í™”", "ì›”ë³„ ìˆ˜ìµë¥ ", "ë“œë¡œë‹¤ìš´ ë¶„ì„",
                "ë¡¤ë§ ìƒ¤í”„ ë¹„ìœ¨", "VaR ë¶„ì„", "ìˆ˜ìµë¥  ë¶„í¬",
                "ì‹œì¥ ìƒí™©ë³„ ì„±ëŠ¥", "íŒŒë¼ë¯¸í„° ì•ˆì •ì„±", "ì˜ˆì¸¡ ì •í™•ë„",
                "ê±°ë˜ í†µê³„", "ë¦¬ìŠ¤í¬ ì§€í‘œ", "ì„±ëŠ¥ ë¹„êµ"
            ),
            specs=[[{"secondary_y": True}, {"type": "bar"}, {"type": "scatter"}],
                   [{"type": "scatter"}, {"type": "box"}, {"type": "histogram"}],
                   [{"type": "bar"}, {"type": "heatmap"}, {"type": "scatter"}],
                   [{"type": "table"}, {"type": "bar"}, {"type": "bar"}]],
            vertical_spacing=0.08,
            horizontal_spacing=0.08
        )
        
        # 1. í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜ ë³€í™”
        if 'portfolio_values' in results:
            fig.add_trace(
                go.Scatter(
                    y=results['portfolio_values'],
                    mode='lines',
                    name='í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜',
                    line=dict(color='blue', width=2)
                ),
                row=1, col=1
            )
        
        # 2. ì›”ë³„ ìˆ˜ìµë¥ 
        if 'monthly_returns' in results:
            fig.add_trace(
                go.Bar(
                    x=list(results['monthly_returns'].keys()),
                    y=list(results['monthly_returns'].values()),
                    name='ì›”ë³„ ìˆ˜ìµë¥ ',
                    marker_color=['green' if x > 0 else 'red' for x in results['monthly_returns'].values()]
                ),
                row=1, col=2
            )
        
        # 3. ë“œë¡œë‹¤ìš´ ë¶„ì„
        if 'drawdown_series' in results:
            fig.add_trace(
                go.Scatter(
                    y=results['drawdown_series'],
                    mode='lines',
                    fill='tozeroy',
                    name='ë“œë¡œë‹¤ìš´',
                    line=dict(color='red'),
                    fillcolor='rgba(255,0,0,0.3)'
                ),
                row=1, col=3
            )
        
        # ì¶”ê°€ ì°¨íŠ¸ë“¤...
        
        fig.update_layout(
            title="ğŸ¯ ë¹„íŠ¸ì½”ì¸ ì˜ˆì¸¡ ëª¨ë¸ ë°±í…ŒìŠ¤íŒ… ì¢…í•© ëŒ€ì‹œë³´ë“œ",
            height=1600,
            showlegend=True,
            template='plotly_dark'
        )
        
        output_file = os.path.join(self.output_path, "comprehensive_backtesting_dashboard.html")
        fig.write_html(output_file, include_plotlyjs=True)
        
        return output_file
    
    def create_regime_analysis_chart(self, regime_performance: Dict) -> str:
        """ì‹œì¥ ìƒí™©ë³„ ë¶„ì„ ì°¨íŠ¸"""
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=("ìƒí™©ë³„ ìˆ˜ìµë¥ ", "ìƒí™©ë³„ ìƒ¤í”„ë¹„ìœ¨", "ìƒí™©ë³„ ìµœëŒ€ë‚™í­", "ìƒí™© ì§€ì†ê¸°ê°„")
        )
        
        regimes = list(regime_performance.keys())
        
        # ì°¨íŠ¸ ë°ì´í„° ì¶”ì¶œ ë° ì‹œê°í™”
        # ...
        
        output_file = os.path.join(self.output_path, "regime_analysis_chart.html")
        fig.write_html(output_file, include_plotlyjs=True)
        
        return output_file

class SophisticatedBacktestingFramework:
    """ì •êµí•œ ë°±í…ŒìŠ¤íŒ… í”„ë ˆì„ì›Œí¬ ë©”ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self, config: BacktestConfig = None):
        self.config = config or BacktestConfig()
        self.data_path = "/Users/parkyoungjun/Desktop/BTC_Analysis_System"
        
        # ì„œë¸Œì‹œìŠ¤í…œ ì´ˆê¸°í™”
        self.market_simulator = MarketSimulator(self.config)
        self.walk_forward_optimizer = WalkForwardOptimizer(self.config)
        self.risk_metrics = RiskAdjustedMetrics()
        self.regime_analyzer = MarketRegimeAnalyzer()
        self.significance_tester = StatisticalSignificanceTester(self.config)
        self.visualizer = InteractiveVisualizationSystem(self.data_path)
        
        # ë¡œê¹… ì„¤ì •
        self.setup_logging()
        
    def setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(os.path.join(self.data_path, 'sophisticated_backtest.log'), encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def load_data(self) -> pd.DataFrame:
        """ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""
        try:
            # ë°ì´í„° ë¡œë“œ (ê¸°ì¡´ ì‹œìŠ¤í…œê³¼ í˜¸í™˜)
            data_files = [
                "ai_matrix_complete.csv",
                "complete_indicators_data.csv",
                "btc_1h_data.csv"
            ]
            
            for filename in data_files:
                filepath = os.path.join(self.data_path, "historical_data", filename)
                if os.path.exists(filepath):
                    df = pd.read_csv(filepath)
                    self.logger.info(f"ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {filename} ({df.shape})")
                    return self.preprocess_data(df)
            
            raise FileNotFoundError("ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            self.logger.error(f"ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    def preprocess_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """ë°ì´í„° ì „ì²˜ë¦¬"""
        # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë§Œ ì„ íƒ
        numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()
        df_clean = df[numeric_columns].copy()
        
        # ê²°ì¸¡ê°’ ì²˜ë¦¬
        df_clean = df_clean.ffill().bfill().fillna(0)
        
        # ë¬´í•œëŒ€ê°’ ì²˜ë¦¬
        df_clean = df_clean.replace([np.inf, -np.inf], 0)
        
        # íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„± (1ì‹œê°„ í›„ ê°€ê²© ë³€í™”ìœ¨)
        if 'price' in df_clean.columns:
            df_clean['target'] = df_clean['price'].pct_change().shift(-1)
        else:
            # ì²« ë²ˆì§¸ ì»¬ëŸ¼ì„ ê°€ê²©ìœ¼ë¡œ ê°€ì •
            price_col = df_clean.columns[0]
            df_clean['target'] = df_clean[price_col].pct_change().shift(-1)
        
        # ë§ˆì§€ë§‰ í–‰ ì œê±° (íƒ€ê²Ÿì´ NaN)
        df_clean = df_clean[:-1]
        
        # ì‹œê°„ ì¸ë±ìŠ¤ ìƒì„±
        df_clean.index = pd.date_range(start='2024-01-01', periods=len(df_clean), freq='H')
        
        return df_clean
    
    def run_comprehensive_backtest(self, model_class=RandomForestRegressor, 
                                 param_grid: Dict = None) -> Dict:
        """ì¢…í•©ì ì¸ ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        self.logger.info("ğŸš€ ì •êµí•œ ë°±í…ŒìŠ¤íŒ… í”„ë ˆì„ì›Œí¬ ì‹œì‘...")
        
        try:
            # 1. ë°ì´í„° ë¡œë“œ
            data = self.load_data()
            self.logger.info(f"ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {data.shape}")
            
            # 2. ì‹œì¥ ìƒí™© ì‹ë³„
            regimes = self.regime_analyzer.identify_market_regimes(data)
            self.logger.info(f"ì‹œì¥ ìƒí™© ì‹ë³„ ì™„ë£Œ: {len(regimes)}ê°œ êµ¬ê°„")
            
            # 3. íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì„¤ì •
            if param_grid is None:
                param_grid = {
                    'n_estimators': [100, 200, 300],
                    'max_depth': [5, 10, 15],
                    'min_samples_split': [2, 5, 10]
                }
            
            # 4. ì›Œí¬í¬ì›Œë“œ ìµœì í™”
            wf_results = self.walk_forward_optimizer.walk_forward_analysis(
                data, model_class, param_grid
            )
            
            # 5. ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰
            backtest_results = self.execute_backtesting_simulation(data, wf_results)
            
            # 6. ë¦¬ìŠ¤í¬ ì¡°ì • ì§€í‘œ ê³„ì‚°
            risk_metrics = self.calculate_comprehensive_risk_metrics(backtest_results)
            
            # 7. ì‹œì¥ ìƒí™©ë³„ ì„±ëŠ¥ ë¶„ì„
            regime_performance = self.regime_analyzer.analyze_regime_performance(
                backtest_results, regimes
            )
            
            # 8. í†µê³„ì  ìœ ì˜ì„± ê²€ì •
            significance_results = self.perform_significance_testing(backtest_results)
            
            # 9. ê²°ê³¼ í†µí•©
            comprehensive_results = {
                'backtest_config': self.config.__dict__,
                'data_info': {
                    'shape': data.shape,
                    'period': f"{data.index[0]} ~ {data.index[-1]}",
                    'features': len(data.columns) - 1  # target ì œì™¸
                },
                'walk_forward_results': wf_results,
                'backtest_performance': backtest_results,
                'risk_metrics': risk_metrics,
                'regime_analysis': regime_performance,
                'significance_tests': significance_results,
                'timestamp': datetime.now().isoformat()
            }
            
            # 10. ê²°ê³¼ ì €ì¥ ë° ì‹œê°í™”
            self.save_comprehensive_results(comprehensive_results)
            self.create_comprehensive_visualizations(comprehensive_results)
            
            self.logger.info("âœ… ì •êµí•œ ë°±í…ŒìŠ¤íŒ… í”„ë ˆì„ì›Œí¬ ì™„ë£Œ!")
            return comprehensive_results
            
        except Exception as e:
            self.logger.error(f"ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            raise
    
    def execute_backtesting_simulation(self, data: pd.DataFrame, 
                                     wf_results: Dict) -> Dict:
        """ë°±í…ŒìŠ¤íŠ¸ ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰"""
        self.logger.info("ğŸ“Š ë°±í…ŒìŠ¤íŠ¸ ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘...")
        
        portfolio_value = [self.config.initial_capital]
        positions = [0]  # BTC í¬ì§€ì…˜
        cash = [self.config.initial_capital]
        trades = []
        
        # ì›Œí¬í¬ì›Œë“œ ê²°ê³¼ì—ì„œ ìµœì  ëª¨ë¸ ì‚¬ìš©
        for period_result in wf_results['results']:
            # ê° ê¸°ê°„ë³„ ê±°ë˜ ì‹œë®¬ë ˆì´ì…˜
            period_trades, period_portfolio = self.simulate_trading_period(
                data, period_result
            )
            trades.extend(period_trades)
            portfolio_value.extend(period_portfolio)
        
        # ìˆ˜ìµë¥  ê³„ì‚°
        returns = pd.Series(portfolio_value).pct_change().dropna()
        
        return {
            'portfolio_values': portfolio_value,
            'returns': returns.tolist(),
            'positions': positions,
            'cash': cash,
            'trades': trades,
            'total_return': (portfolio_value[-1] / portfolio_value[0]) - 1,
            'annualized_return': ((portfolio_value[-1] / portfolio_value[0]) ** (365 / len(portfolio_value))) - 1,
            'volatility': returns.std() * np.sqrt(365)
        }
    
    def simulate_trading_period(self, data: pd.DataFrame, period_result: Dict) -> Tuple[List, List]:
        """ê±°ë˜ ê¸°ê°„ ì‹œë®¬ë ˆì´ì…˜"""
        trades = []
        portfolio_values = []
        
        # ì‹¤ì œ ê±°ë˜ ë¡œì§ êµ¬í˜„
        # (ê°„ë‹¨í™”ëœ ë²„ì „ - ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ì‹ í˜¸ ìƒì„± ë° í¬ì§€ì…˜ ê´€ë¦¬ í•„ìš”)
        
        return trades, portfolio_values
    
    def calculate_comprehensive_risk_metrics(self, backtest_results: Dict) -> Dict:
        """ì¢…í•© ë¦¬ìŠ¤í¬ ì§€í‘œ ê³„ì‚°"""
        returns = np.array(backtest_results['returns'])
        portfolio_values = np.array(backtest_results['portfolio_values'])
        
        return self.risk_metrics.calculate_all_metrics(returns, portfolio_values)
    
    def perform_significance_testing(self, backtest_results: Dict) -> Dict:
        """í†µê³„ì  ìœ ì˜ì„± ê²€ì •"""
        returns = np.array(backtest_results['returns'])
        
        # ë²¤ì¹˜ë§ˆí¬ ìˆ˜ìµë¥  (ë‹¨ìˆœ ë³´ìœ  ì „ëµ)
        benchmark_returns = np.random.normal(0.0001, 0.02, len(returns))  # ì„ì‹œ ë²¤ì¹˜ë§ˆí¬
        
        # ë¶€íŠ¸ìŠ¤íŠ¸ë© ë¶„ì„
        bootstrap_sharpe = self.significance_tester.bootstrap_analysis(
            returns, lambda x: self.risk_metrics.sharpe_ratio(x)
        )
        
        # ìœ ì˜ì„± ê²€ì •
        significance = self.significance_tester.significance_test(returns, benchmark_returns)
        
        return {
            'bootstrap_sharpe': bootstrap_sharpe,
            'strategy_vs_benchmark': significance
        }
    
    def save_comprehensive_results(self, results: Dict):
        """ì¢…í•© ê²°ê³¼ ì €ì¥"""
        # JSON í˜•íƒœë¡œ ì €ì¥
        output_file = os.path.join(
            self.data_path, 
            f"sophisticated_backtest_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        )
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)
        
        self.logger.info(f"ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_file}")
    
    def create_comprehensive_visualizations(self, results: Dict):
        """ì¢…í•© ì‹œê°í™” ìƒì„±"""
        # ëŒ€ì‹œë³´ë“œ ìƒì„±
        dashboard_file = self.visualizer.create_comprehensive_dashboard(results)
        self.logger.info(f"ëŒ€ì‹œë³´ë“œ ìƒì„± ì™„ë£Œ: {dashboard_file}")
        
        # ì‹œì¥ ìƒí™©ë³„ ë¶„ì„ ì°¨íŠ¸
        if 'regime_analysis' in results:
            regime_chart = self.visualizer.create_regime_analysis_chart(
                results['regime_analysis']
            )
            self.logger.info(f"ì‹œì¥ ë¶„ì„ ì°¨íŠ¸ ìƒì„± ì™„ë£Œ: {regime_chart}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ¯ ì •êµí•œ ë¹„íŠ¸ì½”ì¸ ì˜ˆì¸¡ ëª¨ë¸ ë°±í…ŒìŠ¤íŒ… í”„ë ˆì„ì›Œí¬ v3.0")
    print("="*80)
    
    # ì„¤ì •
    config = BacktestConfig(
        transaction_cost=0.001,  # 0.1% ê±°ë˜ ìˆ˜ìˆ˜ë£Œ
        slippage=0.0005,        # 0.05% ìŠ¬ë¦¬í”¼ì§€
        initial_capital=10000,   # $10,000 ì´ˆê¸° ìë³¸
        max_drawdown_limit=0.20, # 20% ìµœëŒ€ ë‚™í­ ì œí•œ
        bootstrap_samples=1000   # 1000íšŒ ë¶€íŠ¸ìŠ¤íŠ¸ë©
    )
    
    # í”„ë ˆì„ì›Œí¬ ì´ˆê¸°í™”
    framework = SophisticatedBacktestingFramework(config)
    
    # ì¢…í•© ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    results = framework.run_comprehensive_backtest()
    
    print("\nğŸ† ë°±í…ŒìŠ¤íŒ… ì™„ë£Œ!")
    print(f"ì´ ìˆ˜ìµë¥ : {results['backtest_performance']['total_return']:.2%}")
    print(f"ì—°ìœ¨í™” ìˆ˜ìµë¥ : {results['backtest_performance']['annualized_return']:.2%}")
    print(f"ìƒ¤í”„ ë¹„ìœ¨: {results['risk_metrics']['sharpe_ratio']:.3f}")
    print(f"ìµœëŒ€ ë‚™í­: {results['risk_metrics']['max_drawdown']:.2%}")

if __name__ == "__main__":
    main()