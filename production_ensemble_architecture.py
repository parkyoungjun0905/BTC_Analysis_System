#!/usr/bin/env python3
"""
ğŸ­ í”„ë¡œë•ì…˜ ì•™ìƒë¸” ì•„í‚¤í…ì²˜ ì‹œìŠ¤í…œ
ë³‘ë ¬ ëª¨ë¸ í›ˆë ¨, ì‹¤ì‹œê°„ ì¶”ë¡ , ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬, ì„±ëŠ¥ ì¶”ì 

í•µì‹¬ ê¸°ëŠ¥:
- ë³‘ë ¬/ë¶„ì‚° ëª¨ë¸ í›ˆë ¨ ì‹œìŠ¤í…œ
- ì‹¤ì‹œê°„ ì•™ìƒë¸” ì¶”ë¡  ì—”ì§„
- ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ë° ë²„ì „ ê´€ë¦¬
- ìë™í™”ëœ ì„±ëŠ¥ ì¶”ì  ë° ë¡œê¹…
- CI/CD íŒŒì´í”„ë¼ì¸ í†µí•©
- ìŠ¤ì¼€ì¼ë§ ë° ë¡œë“œ ë°¸ëŸ°ì‹±
"""

import asyncio
import threading
import multiprocessing
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import queue
import time
import psutil
import json
import pickle
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Any, Optional, Union, Callable
from pathlib import Path
import sqlite3
import logging
from dataclasses import dataclass, asdict
import hashlib
import shutil

# ì›¹ ë° API í”„ë ˆì„ì›Œí¬
try:
    from fastapi import FastAPI, HTTPException, BackgroundTasks
    from fastapi.middleware.cors import CORSMiddleware
    import uvicorn
    from pydantic import BaseModel
    FASTAPI_AVAILABLE = True
except ImportError:
    FASTAPI_AVAILABLE = False
    print("âš ï¸ FastAPI ë¯¸ì„¤ì¹˜ - ì›¹ API ê¸°ëŠ¥ ë¹„í™œì„±í™”")

# Redis (ìºì‹±)
try:
    import redis
    REDIS_AVAILABLE = True
except ImportError:
    REDIS_AVAILABLE = False
    print("âš ï¸ Redis ë¯¸ì„¤ì¹˜ - ìºì‹± ê¸°ëŠ¥ ë¹„í™œì„±í™”")

# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
import numpy as np
import pandas as pd
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import TimeSeriesSplit

# ì•ì„œ êµ¬í˜„í•œ ì‹œìŠ¤í…œë“¤ import
from advanced_ensemble_learning_system import AdvancedEnsembleLearningSystem
from advanced_model_selection_optimizer import AdvancedModelSelectionSystem
from robust_reliability_system import ReliabilitySystemManager

@dataclass
class ModelMetadata:
    """ëª¨ë¸ ë©”íƒ€ë°ì´í„°"""
    model_id: str
    model_name: str
    version: str
    created_at: datetime
    accuracy: float
    model_type: str
    hyperparameters: Dict[str, Any]
    file_path: str
    file_size: int
    checksum: str
    status: str  # 'training', 'active', 'deprecated', 'failed'

@dataclass
class TrainingJob:
    """í›ˆë ¨ ì‘ì—…"""
    job_id: str
    model_name: str
    config: Dict[str, Any]
    status: str  # 'queued', 'running', 'completed', 'failed'
    created_at: datetime
    started_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None
    progress: float = 0.0
    error_message: Optional[str] = None
    result_path: Optional[str] = None

class ModelRegistry:
    """
    ğŸ“š ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ë° ë²„ì „ ê´€ë¦¬ ì‹œìŠ¤í…œ
    """
    
    def __init__(self, registry_path: str = "/Users/parkyoungjun/Desktop/BTC_Analysis_System/model_registry"):
        self.registry_path = Path(registry_path)
        self.registry_path.mkdir(exist_ok=True)
        
        # ë©”íƒ€ë°ì´í„° ë°ì´í„°ë² ì´ìŠ¤
        self.db_path = self.registry_path / "registry.db"
        self.init_database()
        
        self.logger = logging.getLogger(__name__)

    def init_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS models (
                    model_id TEXT PRIMARY KEY,
                    model_name TEXT NOT NULL,
                    version TEXT NOT NULL,
                    created_at TEXT NOT NULL,
                    accuracy REAL,
                    model_type TEXT,
                    hyperparameters TEXT,
                    file_path TEXT,
                    file_size INTEGER,
                    checksum TEXT,
                    status TEXT,
                    UNIQUE(model_name, version)
                )
            """)
            
            conn.execute("""
                CREATE TABLE IF NOT EXISTS model_performance (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    model_id TEXT,
                    timestamp TEXT,
                    accuracy REAL,
                    mse REAL,
                    r2 REAL,
                    prediction_time REAL,
                    FOREIGN KEY (model_id) REFERENCES models (model_id)
                )
            """)

    def register_model(self, model: Any, metadata: ModelMetadata) -> str:
        """
        ëª¨ë¸ ë“±ë¡
        
        Args:
            model: í›ˆë ¨ëœ ëª¨ë¸ ê°ì²´
            metadata: ëª¨ë¸ ë©”íƒ€ë°ì´í„°
            
        Returns:
            str: ë“±ë¡ëœ ëª¨ë¸ ID
        """
        # ëª¨ë¸ ì €ì¥
        model_dir = self.registry_path / metadata.model_name / metadata.version
        model_dir.mkdir(parents=True, exist_ok=True)
        
        model_file = model_dir / f"{metadata.model_name}_{metadata.version}.pkl"
        
        with open(model_file, 'wb') as f:
            pickle.dump(model, f)
        
        # íŒŒì¼ ì •ë³´ ì—…ë°ì´íŠ¸
        metadata.file_path = str(model_file)
        metadata.file_size = model_file.stat().st_size
        metadata.checksum = self._calculate_checksum(model_file)
        
        # ë°ì´í„°ë² ì´ìŠ¤ì— ë“±ë¡
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                INSERT OR REPLACE INTO models 
                (model_id, model_name, version, created_at, accuracy, model_type, 
                 hyperparameters, file_path, file_size, checksum, status)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                metadata.model_id, metadata.model_name, metadata.version,
                metadata.created_at.isoformat(), metadata.accuracy, metadata.model_type,
                json.dumps(metadata.hyperparameters), metadata.file_path,
                metadata.file_size, metadata.checksum, metadata.status
            ))
        
        self.logger.info(f"ğŸ“š ëª¨ë¸ ë“±ë¡ ì™„ë£Œ: {metadata.model_id}")
        return metadata.model_id

    def load_model(self, model_id: str) -> Tuple[Any, ModelMetadata]:
        """ëª¨ë¸ ë¡œë“œ"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute(
                "SELECT * FROM models WHERE model_id = ?", (model_id,)
            )
            row = cursor.fetchone()
        
        if not row:
            raise ValueError(f"ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_id}")
        
        # ë©”íƒ€ë°ì´í„° ë³µì›
        metadata = ModelMetadata(
            model_id=row[0],
            model_name=row[1],
            version=row[2],
            created_at=datetime.fromisoformat(row[3]),
            accuracy=row[4],
            model_type=row[5],
            hyperparameters=json.loads(row[6] or "{}"),
            file_path=row[7],
            file_size=row[8],
            checksum=row[9],
            status=row[10]
        )
        
        # ëª¨ë¸ íŒŒì¼ ë¡œë“œ
        with open(metadata.file_path, 'rb') as f:
            model = pickle.load(f)
        
        return model, metadata

    def list_models(self, model_name: str = None, status: str = None) -> List[ModelMetadata]:
        """ëª¨ë¸ ëª©ë¡ ì¡°íšŒ"""
        query = "SELECT * FROM models WHERE 1=1"
        params = []
        
        if model_name:
            query += " AND model_name = ?"
            params.append(model_name)
        
        if status:
            query += " AND status = ?"
            params.append(status)
        
        query += " ORDER BY created_at DESC"
        
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute(query, params)
            rows = cursor.fetchall()
        
        models = []
        for row in rows:
            models.append(ModelMetadata(
                model_id=row[0],
                model_name=row[1],
                version=row[2],
                created_at=datetime.fromisoformat(row[3]),
                accuracy=row[4],
                model_type=row[5],
                hyperparameters=json.loads(row[6] or "{}"),
                file_path=row[7],
                file_size=row[8],
                checksum=row[9],
                status=row[10]
            ))
        
        return models

    def _calculate_checksum(self, file_path: Path) -> str:
        """íŒŒì¼ ì²´í¬ì„¬ ê³„ì‚°"""
        sha256_hash = hashlib.sha256()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                sha256_hash.update(chunk)
        return sha256_hash.hexdigest()

class ParallelTrainingManager:
    """
    âš¡ ë³‘ë ¬ ëª¨ë¸ í›ˆë ¨ ê´€ë¦¬ì
    """
    
    def __init__(self, max_workers: int = None):
        self.max_workers = max_workers or multiprocessing.cpu_count() // 2
        self.training_queue = queue.Queue()
        self.active_jobs = {}
        self.completed_jobs = {}
        
        # ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬
        self.model_registry = ModelRegistry()
        
        # ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§
        self.resource_monitor = ResourceMonitor()
        
        self.logger = logging.getLogger(__name__)

    def submit_training_job(self, job: TrainingJob) -> str:
        """í›ˆë ¨ ì‘ì—… ì œì¶œ"""
        job.status = 'queued'
        self.training_queue.put(job)
        self.logger.info(f"âš¡ í›ˆë ¨ ì‘ì—… ì œì¶œ: {job.job_id}")
        return job.job_id

    def start_training_workers(self):
        """í›ˆë ¨ ì›Œì»¤ ì‹œì‘"""
        with ProcessPoolExecutor(max_workers=self.max_workers) as executor:
            futures = []
            
            while True:
                try:
                    # ëŒ€ê¸° ì¤‘ì¸ ì‘ì—… í™•ì¸
                    job = self.training_queue.get(timeout=1)
                    
                    # ë¦¬ì†ŒìŠ¤ ì²´í¬
                    if not self.resource_monitor.can_start_training():
                        self.training_queue.put(job)  # ë‹¤ì‹œ íì— ë„£ê¸°
                        time.sleep(5)
                        continue
                    
                    # ë³‘ë ¬ í›ˆë ¨ ì‹œì‘
                    future = executor.submit(self._train_model_worker, job)
                    futures.append((future, job))
                    
                    job.status = 'running'
                    job.started_at = datetime.now()
                    self.active_jobs[job.job_id] = job
                    
                    self.logger.info(f"ğŸ”¥ í›ˆë ¨ ì‹œì‘: {job.job_id}")
                    
                except queue.Empty:
                    # ì™„ë£Œëœ ì‘ì—… í™•ì¸
                    completed_futures = []
                    for future, job in futures:
                        if future.done():
                            try:
                                result = future.result()
                                self._handle_training_completion(job, result)
                                completed_futures.append((future, job))
                            except Exception as e:
                                self._handle_training_error(job, e)
                                completed_futures.append((future, job))
                    
                    # ì™„ë£Œëœ ì‘ì—… ì œê±°
                    for completed in completed_futures:
                        futures.remove(completed)
                    
                    time.sleep(1)
                
                except KeyboardInterrupt:
                    self.logger.info("â¹ï¸ í›ˆë ¨ ê´€ë¦¬ì ì¢…ë£Œ")
                    break

    def _train_model_worker(self, job: TrainingJob) -> Dict[str, Any]:
        """ëª¨ë¸ í›ˆë ¨ ì›Œì»¤ (ë³„ë„ í”„ë¡œì„¸ìŠ¤ì—ì„œ ì‹¤í–‰)"""
        try:
            # ì•™ìƒë¸” ì‹œìŠ¤í…œ ì´ˆê¸°í™”
            ensemble_system = AdvancedEnsembleLearningSystem()
            
            # í›ˆë ¨ ì‹¤í–‰
            result = ensemble_system.train_ensemble_system()
            
            if result['success']:
                # ëª¨ë¸ ë“±ë¡
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                model_id = f"{job.model_name}_{timestamp}"
                
                metadata = ModelMetadata(
                    model_id=model_id,
                    model_name=job.model_name,
                    version=timestamp,
                    created_at=datetime.now(),
                    accuracy=result['ensemble_performance']['direction_accuracy'],
                    model_type='ensemble',
                    hyperparameters=job.config,
                    file_path="",  # ë“±ë¡ ì‹œ ì„¤ì •ë¨
                    file_size=0,   # ë“±ë¡ ì‹œ ì„¤ì •ë¨
                    checksum="",   # ë“±ë¡ ì‹œ ì„¤ì •ë¨
                    status='active'
                )
                
                return {
                    'success': True,
                    'model_id': model_id,
                    'metadata': metadata,
                    'result': result
                }
            else:
                return {
                    'success': False,
                    'error': result.get('error', 'Unknown error')
                }
                
        except Exception as e:
            return {
                'success': False,
                'error': str(e)
            }

    def _handle_training_completion(self, job: TrainingJob, result: Dict[str, Any]):
        """í›ˆë ¨ ì™„ë£Œ ì²˜ë¦¬"""
        job.completed_at = datetime.now()
        job.progress = 1.0
        
        if result['success']:
            job.status = 'completed'
            job.result_path = result.get('model_id', '')
            self.logger.info(f"âœ… í›ˆë ¨ ì™„ë£Œ: {job.job_id} -> {job.result_path}")
        else:
            job.status = 'failed'
            job.error_message = result['error']
            self.logger.error(f"âŒ í›ˆë ¨ ì‹¤íŒ¨: {job.job_id} - {job.error_message}")
        
        # í™œì„± ì‘ì—…ì—ì„œ ì™„ë£Œ ì‘ì—…ìœ¼ë¡œ ì´ë™
        if job.job_id in self.active_jobs:
            del self.active_jobs[job.job_id]
        
        self.completed_jobs[job.job_id] = job

    def _handle_training_error(self, job: TrainingJob, error: Exception):
        """í›ˆë ¨ ì˜¤ë¥˜ ì²˜ë¦¬"""
        job.completed_at = datetime.now()
        job.status = 'failed'
        job.error_message = str(error)
        
        self.logger.error(f"âŒ í›ˆë ¨ ì˜¤ë¥˜: {job.job_id} - {error}")
        
        if job.job_id in self.active_jobs:
            del self.active_jobs[job.job_id]
        
        self.completed_jobs[job.job_id] = job

class ResourceMonitor:
    """
    ğŸ“Š ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ
    """
    
    def __init__(self):
        self.thresholds = {
            'cpu_percent': 80.0,      # 80% CPU ì‚¬ìš©ë¥ 
            'memory_percent': 85.0,   # 85% ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ 
            'disk_percent': 90.0      # 90% ë””ìŠ¤í¬ ì‚¬ìš©ë¥ 
        }

    def get_system_resources(self) -> Dict[str, float]:
        """í˜„ì¬ ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ìƒíƒœ"""
        return {
            'cpu_percent': psutil.cpu_percent(interval=1),
            'memory_percent': psutil.virtual_memory().percent,
            'disk_percent': psutil.disk_usage('/').percent,
            'available_memory_gb': psutil.virtual_memory().available / (1024**3)
        }

    def can_start_training(self) -> bool:
        """í›ˆë ¨ ì‹œì‘ ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        resources = self.get_system_resources()
        
        # ì„ê³„ê°’ í™•ì¸
        if resources['cpu_percent'] > self.thresholds['cpu_percent']:
            return False
        
        if resources['memory_percent'] > self.thresholds['memory_percent']:
            return False
        
        if resources['disk_percent'] > self.thresholds['disk_percent']:
            return False
        
        # ìµœì†Œ ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­ (2GB)
        if resources['available_memory_gb'] < 2.0:
            return False
        
        return True

class RealtimeInferenceEngine:
    """
    âš¡ ì‹¤ì‹œê°„ ì•™ìƒë¸” ì¶”ë¡  ì—”ì§„
    """
    
    def __init__(self, model_registry: ModelRegistry):
        self.model_registry = model_registry
        self.loaded_models = {}  # ëª¨ë¸ ìºì‹œ
        self.prediction_cache = {}  # ì˜ˆì¸¡ ìºì‹œ
        
        # Redis ìºì‹± (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
        if REDIS_AVAILABLE:
            try:
                self.redis_client = redis.Redis(host='localhost', port=6379, db=0)
                self.redis_client.ping()
                self.redis_available = True
            except:
                self.redis_available = False
        else:
            self.redis_available = False
        
        self.logger = logging.getLogger(__name__)

    def load_active_models(self) -> int:
        """í™œì„± ëª¨ë¸ë“¤ ë¡œë“œ"""
        active_models = self.model_registry.list_models(status='active')
        loaded_count = 0
        
        for metadata in active_models:
            try:
                model, _ = self.model_registry.load_model(metadata.model_id)
                self.loaded_models[metadata.model_id] = {
                    'model': model,
                    'metadata': metadata,
                    'loaded_at': datetime.now()
                }
                loaded_count += 1
                
            except Exception as e:
                self.logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {metadata.model_id} - {e}")
        
        self.logger.info(f"ğŸ“š í™œì„± ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {loaded_count}ê°œ")
        return loaded_count

    async def predict_ensemble(self, features: Dict[str, float], 
                             use_cache: bool = True) -> Dict[str, Any]:
        """
        ì•™ìƒë¸” ì˜ˆì¸¡ ìˆ˜í–‰
        
        Args:
            features: ì…ë ¥ íŠ¹ì„±ë“¤
            use_cache: ìºì‹œ ì‚¬ìš© ì—¬ë¶€
            
        Returns:
            Dict[str, Any]: ì˜ˆì¸¡ ê²°ê³¼
        """
        start_time = time.time()
        
        # ìºì‹œ í‚¤ ìƒì„±
        cache_key = hashlib.md5(json.dumps(features, sort_keys=True).encode()).hexdigest()
        
        # ìºì‹œ í™•ì¸
        if use_cache:
            cached_result = await self._get_cached_prediction(cache_key)
            if cached_result:
                return cached_result
        
        # ê°œë³„ ëª¨ë¸ ì˜ˆì¸¡ (ë¹„ë™ê¸° ë³‘ë ¬ ì²˜ë¦¬)
        model_predictions = await self._parallel_model_predictions(features)
        
        if not model_predictions:
            raise HTTPException(status_code=500, detail="ì˜ˆì¸¡ ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")
        
        # ì•™ìƒë¸” ì§‘ê³„ (ê°€ì¤‘ í‰ê· )
        total_weight = sum(pred['weight'] for pred in model_predictions.values())
        
        if total_weight == 0:
            ensemble_prediction = np.mean([pred['value'] for pred in model_predictions.values()])
        else:
            ensemble_prediction = sum(
                pred['value'] * pred['weight'] for pred in model_predictions.values()
            ) / total_weight
        
        # ë¶ˆí™•ì‹¤ì„± ê³„ì‚°
        individual_preds = [pred['value'] for pred in model_predictions.values()]
        prediction_std = np.std(individual_preds) if len(individual_preds) > 1 else 0.0
        
        confidence = max(0.0, 1.0 - prediction_std / (abs(ensemble_prediction) + 1e-6))
        
        # ê²°ê³¼ êµ¬ì„±
        result = {
            'prediction': float(ensemble_prediction),
            'confidence': float(confidence),
            'prediction_std': float(prediction_std),
            'models_used': list(model_predictions.keys()),
            'model_count': len(model_predictions),
            'individual_predictions': model_predictions,
            'prediction_time_ms': (time.time() - start_time) * 1000,
            'timestamp': datetime.now().isoformat(),
            'cache_hit': False
        }
        
        # ìºì‹œ ì €ì¥
        if use_cache:
            await self._cache_prediction(cache_key, result)
        
        return result

    async def _parallel_model_predictions(self, features: Dict[str, float]) -> Dict[str, Dict]:
        """ë³‘ë ¬ ëª¨ë¸ ì˜ˆì¸¡"""
        predictions = {}
        
        # ThreadPoolExecutorë¡œ ë³‘ë ¬ ì²˜ë¦¬
        with ThreadPoolExecutor(max_workers=len(self.loaded_models)) as executor:
            futures = {}
            
            for model_id, model_info in self.loaded_models.items():
                future = executor.submit(
                    self._single_model_prediction, 
                    model_info, features
                )
                futures[model_id] = future
            
            # ê²°ê³¼ ìˆ˜ì§‘
            for model_id, future in futures.items():
                try:
                    prediction = future.result(timeout=10)  # 10ì´ˆ íƒ€ì„ì•„ì›ƒ
                    if prediction is not None:
                        predictions[model_id] = prediction
                        
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ëª¨ë¸ {model_id} ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
                    continue
        
        return predictions

    def _single_model_prediction(self, model_info: Dict, 
                               features: Dict[str, float]) -> Optional[Dict]:
        """ë‹¨ì¼ ëª¨ë¸ ì˜ˆì¸¡"""
        try:
            model = model_info['model']
            metadata = model_info['metadata']
            
            # íŠ¹ì„±ì„ ë°°ì—´ë¡œ ë³€í™˜ (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ëª¨ë¸ì— ë§ê²Œ ì¡°ì • í•„ìš”)
            # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ ì˜ˆì‹œ
            feature_array = np.array(list(features.values())).reshape(1, -1)
            
            # ì˜ˆì¸¡ ìˆ˜í–‰
            prediction = model.predict(feature_array)[0] if hasattr(model, 'predict') else 0.0
            
            # ê°€ì¤‘ì¹˜ (ì •í™•ë„ ê¸°ë°˜)
            weight = metadata.accuracy
            
            return {
                'value': float(prediction),
                'weight': float(weight),
                'model_type': metadata.model_type,
                'accuracy': metadata.accuracy
            }
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë‹¨ì¼ ëª¨ë¸ ì˜ˆì¸¡ ì˜¤ë¥˜: {e}")
            return None

    async def _get_cached_prediction(self, cache_key: str) -> Optional[Dict]:
        """ìºì‹œëœ ì˜ˆì¸¡ ì¡°íšŒ"""
        try:
            if self.redis_available:
                cached = self.redis_client.get(cache_key)
                if cached:
                    result = json.loads(cached.decode())
                    result['cache_hit'] = True
                    return result
            
            # ë¡œì»¬ ìºì‹œ í™•ì¸
            if cache_key in self.prediction_cache:
                cache_entry = self.prediction_cache[cache_key]
                # 5ë¶„ ìºì‹œ ìœ íš¨ì‹œê°„
                if datetime.now() - cache_entry['timestamp'] < timedelta(minutes=5):
                    result = cache_entry['data'].copy()
                    result['cache_hit'] = True
                    return result
                else:
                    del self.prediction_cache[cache_key]
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ìºì‹œ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        
        return None

    async def _cache_prediction(self, cache_key: str, result: Dict):
        """ì˜ˆì¸¡ ê²°ê³¼ ìºì‹œ"""
        try:
            # Redis ìºì‹œ
            if self.redis_available:
                self.redis_client.setex(
                    cache_key, 300, json.dumps(result, default=str)
                )  # 5ë¶„ TTL
            
            # ë¡œì»¬ ìºì‹œ
            self.prediction_cache[cache_key] = {
                'data': result.copy(),
                'timestamp': datetime.now()
            }
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

# FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
if FASTAPI_AVAILABLE:
    app = FastAPI(title="ì•™ìƒë¸” í•™ìŠµ ì‹œìŠ¤í…œ API", version="1.0.0")
    
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )
    
    # ì „ì—­ ê°ì²´ë“¤
    model_registry = ModelRegistry()
    inference_engine = RealtimeInferenceEngine(model_registry)
    training_manager = ParallelTrainingManager()
    
    class PredictionRequest(BaseModel):
        features: Dict[str, float]
        use_cache: bool = True
    
    class TrainingRequest(BaseModel):
        model_name: str
        config: Dict[str, Any] = {}
    
    @app.on_event("startup")
    async def startup_event():
        """ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ì‹œ ì´ˆê¸°í™”"""
        inference_engine.load_active_models()
    
    @app.post("/predict")
    async def predict(request: PredictionRequest):
        """ì˜ˆì¸¡ API"""
        try:
            result = await inference_engine.predict_ensemble(
                request.features, request.use_cache
            )
            return result
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))
    
    @app.post("/train")
    async def train_model(request: TrainingRequest, background_tasks: BackgroundTasks):
        """ëª¨ë¸ í›ˆë ¨ API"""
        job_id = f"train_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        job = TrainingJob(
            job_id=job_id,
            model_name=request.model_name,
            config=request.config,
            status='queued',
            created_at=datetime.now()
        )
        
        training_manager.submit_training_job(job)
        
        return {"job_id": job_id, "status": "queued"}
    
    @app.get("/models")
    async def list_models(model_name: str = None, status: str = None):
        """ëª¨ë¸ ëª©ë¡ API"""
        models = model_registry.list_models(model_name, status)
        return [asdict(model) for model in models]
    
    @app.get("/health")
    async def health_check():
        """í—¬ìŠ¤ ì²´í¬ API"""
        resource_monitor = ResourceMonitor()
        resources = resource_monitor.get_system_resources()
        
        return {
            "status": "healthy",
            "timestamp": datetime.now().isoformat(),
            "resources": resources,
            "loaded_models": len(inference_engine.loaded_models)
        }

class ProductionEnsembleSystem:
    """
    ğŸ­ í”„ë¡œë•ì…˜ ì•™ìƒë¸” ì‹œìŠ¤í…œ í†µí•© ê´€ë¦¬ì
    """
    
    def __init__(self):
        self.model_registry = ModelRegistry()
        self.training_manager = ParallelTrainingManager()
        self.inference_engine = RealtimeInferenceEngine(self.model_registry)
        self.resource_monitor = ResourceMonitor()
        
        # ì„±ëŠ¥ ì¶”ì 
        self.performance_tracker = PerformanceTracker()
        
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('production_ensemble.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)

    def deploy_system(self, host: str = "0.0.0.0", port: int = 8000):
        """ì‹œìŠ¤í…œ ë°°í¬"""
        if not FASTAPI_AVAILABLE:
            print("âŒ FastAPIê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ì›¹ APIë¥¼ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return
        
        print("ğŸ­ í”„ë¡œë•ì…˜ ì•™ìƒë¸” ì‹œìŠ¤í…œ ë°°í¬ ì‹œì‘...")
        
        # ì´ˆê¸° ëª¨ë¸ ë¡œë“œ
        self.inference_engine.load_active_models()
        
        # í›ˆë ¨ ì›Œì»¤ ë°±ê·¸ë¼ìš´ë“œ ì‹œì‘
        training_thread = threading.Thread(
            target=self.training_manager.start_training_workers,
            daemon=True
        )
        training_thread.start()
        
        # FastAPI ì„œë²„ ì‹œì‘
        print(f"ğŸŒ API ì„œë²„ ì‹œì‘: http://{host}:{port}")
        print("ğŸ“š ì‚¬ìš© ê°€ëŠ¥í•œ ì—”ë“œí¬ì¸íŠ¸:")
        print("  POST /predict - ì•™ìƒë¸” ì˜ˆì¸¡")
        print("  POST /train - ëª¨ë¸ í›ˆë ¨ ì‹œì‘")
        print("  GET /models - ëª¨ë¸ ëª©ë¡")
        print("  GET /health - ì‹œìŠ¤í…œ ìƒíƒœ")
        
        uvicorn.run(app, host=host, port=port, log_level="info")

    def run_comprehensive_test(self) -> Dict[str, Any]:
        """ì¢…í•© ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
        print("ğŸ§ª ì¢…í•© ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        test_results = {
            'timestamp': datetime.now(),
            'test_components': [],
            'overall_status': 'unknown',
            'performance_metrics': {},
            'recommendations': []
        }
        
        # 1. ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ í…ŒìŠ¤íŠ¸
        print("ğŸ“Š ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ í…ŒìŠ¤íŠ¸...")
        resources = self.resource_monitor.get_system_resources()
        test_results['test_components'].append({
            'component': 'system_resources',
            'status': 'pass' if resources['available_memory_gb'] > 1.0 else 'fail',
            'details': resources
        })
        
        # 2. ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ í…ŒìŠ¤íŠ¸
        print("ğŸ“š ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ í…ŒìŠ¤íŠ¸...")
        try:
            models = self.model_registry.list_models()
            registry_status = 'pass' if len(models) >= 0 else 'fail'
            test_results['test_components'].append({
                'component': 'model_registry',
                'status': registry_status,
                'details': {'model_count': len(models)}
            })
        except Exception as e:
            test_results['test_components'].append({
                'component': 'model_registry',
                'status': 'fail',
                'details': {'error': str(e)}
            })
        
        # 3. ì¶”ë¡  ì—”ì§„ í…ŒìŠ¤íŠ¸
        print("âš¡ ì¶”ë¡  ì—”ì§„ í…ŒìŠ¤íŠ¸...")
        try:
            loaded_count = self.inference_engine.load_active_models()
            inference_status = 'pass' if loaded_count >= 0 else 'fail'
            test_results['test_components'].append({
                'component': 'inference_engine',
                'status': inference_status,
                'details': {'loaded_models': loaded_count}
            })
        except Exception as e:
            test_results['test_components'].append({
                'component': 'inference_engine',
                'status': 'fail',
                'details': {'error': str(e)}
            })
        
        # ì „ì²´ ìƒíƒœ í‰ê°€
        failed_components = [c for c in test_results['test_components'] if c['status'] == 'fail']
        
        if not failed_components:
            test_results['overall_status'] = 'pass'
        elif len(failed_components) <= len(test_results['test_components']) // 2:
            test_results['overall_status'] = 'partial'
        else:
            test_results['overall_status'] = 'fail'
        
        # ê¶Œì¥ì‚¬í•­
        if failed_components:
            test_results['recommendations'].append(
                f"{len(failed_components)}ê°œ ì»´í¬ë„ŒíŠ¸ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”."
            )
        
        if resources['available_memory_gb'] < 2.0:
            test_results['recommendations'].append(
                "ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ìµœì†Œ 2GB ì´ìƒ í™•ë³´í•˜ì„¸ìš”."
            )
        
        # ê²°ê³¼ ì¶œë ¥
        print("\n" + "="*50)
        print("ğŸ§ª ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print("="*50)
        print(f"ğŸ† ì „ì²´ ìƒíƒœ: {test_results['overall_status'].upper()}")
        print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ì»´í¬ë„ŒíŠ¸: {len(test_results['test_components'])}ê°œ")
        print(f"âŒ ì‹¤íŒ¨í•œ ì»´í¬ë„ŒíŠ¸: {len(failed_components)}ê°œ")
        
        if test_results['recommendations']:
            print("\nğŸ“‹ ê¶Œì¥ì‚¬í•­:")
            for i, rec in enumerate(test_results['recommendations'], 1):
                print(f"  {i}. {rec}")
        
        return test_results

class PerformanceTracker:
    """ì„±ëŠ¥ ì¶”ì  ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.db_path = "/Users/parkyoungjun/Desktop/BTC_Analysis_System/performance_tracking.db"
        self.init_database()
    
    def init_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS predictions (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT,
                    prediction REAL,
                    actual REAL,
                    accuracy REAL,
                    model_count INTEGER,
                    response_time_ms REAL
                )
            """)

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ­ í”„ë¡œë•ì…˜ ì•™ìƒë¸” ì•„í‚¤í…ì²˜ ì‹œìŠ¤í…œ")
    
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    production_system = ProductionEnsembleSystem()
    
    # ì¢…í•© í…ŒìŠ¤íŠ¸
    test_results = production_system.run_comprehensive_test()
    
    # í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥
    test_result_path = "/Users/parkyoungjun/Desktop/BTC_Analysis_System/production_system_test_results.json"
    with open(test_result_path, 'w', encoding='utf-8') as f:
        json.dump(test_results, f, indent=2, ensure_ascii=False, default=str)
    
    print(f"\nğŸ“„ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥: {test_result_path}")
    
    return production_system

if __name__ == "__main__":
    system = main()