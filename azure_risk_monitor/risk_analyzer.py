#!/usr/bin/env python3
"""
ì‹œê³„ì—´ ê¸°ë°˜ ìœ„í—˜ ë¶„ì„ ì—”ì§„
ê³¼ê±° íŒ¨í„´ ë§¤ì¹­ê³¼ ì‹¤ì‹œê°„ ì´ìƒ ê°ì§€ë¥¼ í†µí•œ ê³ ì •í™•ë„ ìœ„í—˜ ì˜ˆì¸¡
"""

import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
import logging
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

class TimeSeriesRiskAnalyzer:
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.scaler = StandardScaler()
        self.isolation_forest = IsolationForest(contamination=0.1, random_state=42)
        
        # ê³¼ê±° ìœ„í—˜ íŒ¨í„´ë“¤ (ì‹¤ì œë¡œëŠ” ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë¡œë“œ)
        self.historical_patterns = self.load_historical_patterns()
        
    def load_historical_patterns(self) -> Dict:
        """ê³¼ê±° ìœ„í—˜ íŒ¨í„´ë“¤ ë¡œë“œ (í•˜ë“œì½”ë”©ìœ¼ë¡œ ì‹œì‘, ë‚˜ì¤‘ì— DB ì—°ë™)"""
        return {
            "flash_crash_2022": {
                "price_drop_5min": -0.15,
                "volume_spike": 8.5,
                "funding_rate": 0.008,
                "fear_greed": 12,
                "vix_level": 32
            },
            "luna_collapse_2022": {
                "price_drop_1hour": -0.35,
                "correlation_break": 0.6,
                "volume_anomaly": 12.0,
                "social_sentiment": -0.8
            },
            "covid_crash_2020": {
                "price_drop_1day": -0.50,
                "macro_correlation": 0.9,
                "vix_spike": 45,
                "liquidation_cascade": 500000000
            },
            "china_ban_2021": {
                "price_decline_7day": -0.40,
                "hash_rate_drop": -0.35,
                "regulatory_news": 1.0,
                "asian_premium": -0.05
            }
        }

    def analyze_timeseries_risk(self, current_data: Dict, historical_data: List[Dict]) -> Dict:
        """ì‹œê³„ì—´ ë°ì´í„° ê¸°ë°˜ ì¢…í•© ìœ„í—˜ ë¶„ì„"""
        try:
            # 1. ê¸‰ë³€ ê°ì§€ (Sudden Change Detection)
            sudden_change_risk = self.detect_sudden_changes(current_data, historical_data)
            
            # 2. íŒ¨í„´ ë§¤ì¹­ (Historical Pattern Matching)
            pattern_match_risk = self.match_historical_patterns(current_data, historical_data)
            
            # 3. ì´ìƒ ê°ì§€ (Anomaly Detection)
            anomaly_risk = self.detect_anomalies(current_data, historical_data)
            
            # 4. ì¶”ì„¸ ë³€í™” ê°ì§€ (Trend Change Detection)
            trend_change_risk = self.detect_trend_changes(historical_data)
            
            # 5. ìƒê´€ê´€ê³„ íŒŒê´´ ê°ì§€ (Correlation Breakdown)
            correlation_risk = self.detect_correlation_breakdown(current_data, historical_data)
            
            # 6. ì¢…í•© ìœ„í—˜ë„ ê³„ì‚°
            composite_risk = self.calculate_composite_risk({
                "sudden_change": sudden_change_risk,
                "pattern_match": pattern_match_risk,
                "anomaly": anomaly_risk,
                "trend_change": trend_change_risk,
                "correlation": correlation_risk
            })
            
            return {
                "timestamp": datetime.utcnow().isoformat(),
                "composite_risk_score": composite_risk["total_score"],
                "risk_level": composite_risk["risk_level"],
                "confidence": composite_risk["confidence"],
                "components": {
                    "sudden_change": sudden_change_risk,
                    "pattern_match": pattern_match_risk,
                    "anomaly": anomaly_risk,
                    "trend_change": trend_change_risk,
                    "correlation": correlation_risk
                },
                "recommendations": self.generate_recommendations(composite_risk),
                "next_check_in": self.calculate_next_check_time(composite_risk["risk_level"])
            }
            
        except Exception as e:
            self.logger.error(f"ìœ„í—˜ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                "error": str(e),
                "timestamp": datetime.utcnow().isoformat(),
                "composite_risk_score": 0.5,  # ê¸°ë³¸ê°’
                "risk_level": "WARNING"
            }

    def detect_sudden_changes(self, current_data: Dict, historical_data: List[Dict]) -> Dict:
        """ê¸‰ë³€ ê°ì§€ ì•Œê³ ë¦¬ì¦˜"""
        try:
            sudden_change_indicators = {
                "price_velocity": 0,
                "volume_spike": 0,
                "funding_rate_jump": 0,
                "macro_shock": 0,
                "composite_score": 0
            }
            
            if not historical_data or len(historical_data) < 10:
                return sudden_change_indicators
                
            # ê°€ê²© ê¸‰ë³€ ê°ì§€
            if "price_data" in current_data:
                current_price = current_data["price_data"].get("current_price", 0)
                
                # ìµœê·¼ 5ë¶„, 1ì‹œê°„, 24ì‹œê°„ ë³€í™”ìœ¨ ê³„ì‚°
                recent_prices = []
                for i, hist_data in enumerate(historical_data[-10:]):
                    if "price_data" in hist_data:
                        recent_prices.append(hist_data["price_data"].get("current_price", current_price))
                        
                if len(recent_prices) >= 5:
                    # 5ë¶„ê°„ ë³€í™”ìœ¨ (ìµœê·¼ 5ê°œ ë°ì´í„° í¬ì¸íŠ¸)
                    price_5min_change = (current_price - recent_prices[-5]) / recent_prices[-5]
                    sudden_change_indicators["price_velocity"] = min(abs(price_5min_change) / 0.05, 1.0)
                    
            # ê±°ë˜ëŸ‰ ê¸‰ì¦ ê°ì§€
            if "price_data" in current_data and "volume_24h" in current_data["price_data"]:
                current_volume = current_data["price_data"]["volume_24h"]
                
                # ê³¼ê±° í‰ê·  ê±°ë˜ëŸ‰ ê³„ì‚°
                historical_volumes = []
                for hist_data in historical_data[-30:]:  # ìµœê·¼ 30ê°œ ë°ì´í„°
                    if "price_data" in hist_data and "volume_24h" in hist_data["price_data"]:
                        historical_volumes.append(hist_data["price_data"]["volume_24h"])
                        
                if historical_volumes:
                    avg_volume = np.mean(historical_volumes)
                    volume_ratio = current_volume / avg_volume if avg_volume > 0 else 1
                    sudden_change_indicators["volume_spike"] = min((volume_ratio - 1) / 4, 1.0)
                    
            # VIX ê¸‰ë“± ê°ì§€
            if "macro_data" in current_data and "vix" in current_data["macro_data"]:
                current_vix = current_data["macro_data"]["vix"]["current"]
                vix_change = current_data["macro_data"]["vix"]["change"]
                
                if abs(vix_change) > 3:  # VIX 3í¬ì¸íŠ¸ ì´ìƒ ë³€í™”
                    sudden_change_indicators["macro_shock"] = min(abs(vix_change) / 10, 1.0)
                    
            # ì¢…í•© ê¸‰ë³€ ì ìˆ˜
            weights = {
                "price_velocity": 0.4,
                "volume_spike": 0.25,
                "funding_rate_jump": 0.2,
                "macro_shock": 0.15
            }
            
            sudden_change_indicators["composite_score"] = sum(
                sudden_change_indicators[key] * weight 
                for key, weight in weights.items()
            )
            
            return sudden_change_indicators
            
        except Exception as e:
            self.logger.error(f"ê¸‰ë³€ ê°ì§€ ì‹¤íŒ¨: {e}")
            return {"composite_score": 0, "error": str(e)}

    def match_historical_patterns(self, current_data: Dict, historical_data: List[Dict]) -> Dict:
        """ê³¼ê±° ìœ„í—˜ íŒ¨í„´ê³¼ì˜ ìœ ì‚¬ë„ ë¶„ì„"""
        try:
            pattern_matches = {}
            
            # í˜„ì¬ ìƒí™©ì˜ íŠ¹ì§• ë²¡í„° ìƒì„±
            current_features = self.extract_features(current_data, historical_data)
            
            # ê° ê³¼ê±° íŒ¨í„´ê³¼ ë¹„êµ
            for pattern_name, pattern_features in self.historical_patterns.items():
                similarity = self.calculate_pattern_similarity(current_features, pattern_features)
                pattern_matches[pattern_name] = {
                    "similarity": similarity,
                    "risk_level": self.get_pattern_risk_level(pattern_name),
                    "trigger_probability": similarity * self.get_pattern_severity(pattern_name)
                }
                
            # ê°€ì¥ ìœ ì‚¬í•œ íŒ¨í„´ ì°¾ê¸°
            best_match = max(pattern_matches.items(), key=lambda x: x[1]["similarity"])
            
            return {
                "best_match": {
                    "pattern": best_match[0],
                    "similarity": best_match[1]["similarity"],
                    "risk_level": best_match[1]["risk_level"]
                },
                "all_matches": pattern_matches,
                "composite_score": best_match[1]["trigger_probability"]
            }
            
        except Exception as e:
            self.logger.error(f"íŒ¨í„´ ë§¤ì¹­ ì‹¤íŒ¨: {e}")
            return {"composite_score": 0, "error": str(e)}

    def detect_anomalies(self, current_data: Dict, historical_data: List[Dict]) -> Dict:
        """ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ì´ìƒ ê°ì§€"""
        try:
            if len(historical_data) < 50:  # ìµœì†Œ ë°ì´í„° ìš”êµ¬ëŸ‰
                return {"composite_score": 0, "note": "insufficient_data"}
                
            # íŠ¹ì§• ë²¡í„° ìƒì„±
            feature_vectors = []
            for hist_data in historical_data:
                features = self.extract_numerical_features(hist_data)
                if features:
                    feature_vectors.append(features)
                    
            if len(feature_vectors) < 20:
                return {"composite_score": 0, "note": "insufficient_features"}
                
            # ì •ê·œí™”
            feature_matrix = np.array(feature_vectors)
            feature_matrix_scaled = self.scaler.fit_transform(feature_matrix)
            
            # Isolation Forestë¡œ ì´ìƒ ê°ì§€ ëª¨ë¸ í›ˆë ¨
            self.isolation_forest.fit(feature_matrix_scaled)
            
            # í˜„ì¬ ë°ì´í„°ì˜ ì´ìƒë„ ê³„ì‚°
            current_features = self.extract_numerical_features(current_data)
            if current_features:
                current_scaled = self.scaler.transform([current_features])
                anomaly_score = self.isolation_forest.decision_function(current_scaled)[0]
                is_anomaly = self.isolation_forest.predict(current_scaled)[0] == -1
                
                # ì ìˆ˜ ì •ê·œí™” (0-1 ë²”ìœ„)
                normalized_score = max(0, min(1, (0.5 - anomaly_score) / 1.0))
                
                return {
                    "is_anomaly": is_anomaly,
                    "anomaly_score": float(anomaly_score),
                    "composite_score": normalized_score,
                    "confidence": 0.7 if len(feature_vectors) > 100 else 0.5
                }
            else:
                return {"composite_score": 0, "note": "no_current_features"}
                
        except Exception as e:
            self.logger.error(f"ì´ìƒ ê°ì§€ ì‹¤íŒ¨: {e}")
            return {"composite_score": 0, "error": str(e)}

    def detect_trend_changes(self, historical_data: List[Dict]) -> Dict:
        """ì¶”ì„¸ ë³€í™”ì  ê°ì§€"""
        try:
            if len(historical_data) < 20:
                return {"composite_score": 0, "note": "insufficient_data"}
                
            # ê°€ê²© ì‹œê³„ì—´ ì¶”ì¶œ
            prices = []
            timestamps = []
            
            for hist_data in historical_data:
                if "price_data" in hist_data and "current_price" in hist_data["price_data"]:
                    prices.append(hist_data["price_data"]["current_price"])
                    timestamps.append(hist_data.get("timestamp", datetime.utcnow().isoformat()))
                    
            if len(prices) < 20:
                return {"composite_score": 0, "note": "insufficient_price_data"}
                
            prices = np.array(prices)
            
            # ì´ë™í‰ê·  ê¸°ë°˜ ì¶”ì„¸ ë¶„ì„
            short_ma = np.mean(prices[-5:])   # ë‹¨ê¸° (5ê°œ)
            medium_ma = np.mean(prices[-10:]) # ì¤‘ê¸° (10ê°œ)
            long_ma = np.mean(prices[-20:])   # ì¥ê¸° (20ê°œ)
            
            # ì¶”ì„¸ ê°•ë„ ê³„ì‚°
            trend_strength = 0
            if long_ma > 0:
                short_vs_long = (short_ma - long_ma) / long_ma
                medium_vs_long = (medium_ma - long_ma) / long_ma
                
                # ì¶”ì„¸ ë³€í™” ê°ì§€ (ë‹¨ê¸°ê°€ ì¥ê¸°ì™€ í¬ê²Œ ë²—ì–´ë‚˜ëŠ” ê²½ìš°)
                trend_strength = abs(short_vs_long) + abs(medium_vs_long)
                
            # ë³€í™”ìœ¨ ê°€ì†ë„ (2ì°¨ ë¯¸ë¶„)
            if len(prices) >= 10:
                returns = np.diff(np.log(prices))
                acceleration = np.diff(returns)
                recent_acceleration = np.mean(np.abs(acceleration[-5:]))
                trend_strength += recent_acceleration * 100
                
            return {
                "trend_strength": float(trend_strength),
                "short_ma": float(short_ma),
                "medium_ma": float(medium_ma), 
                "long_ma": float(long_ma),
                "composite_score": min(trend_strength / 0.1, 1.0)  # 0.1 = 10% ë³€í™”ë¥¼ 1.0 ì ìˆ˜ë¡œ
            }
            
        except Exception as e:
            self.logger.error(f"ì¶”ì„¸ ë³€í™” ê°ì§€ ì‹¤íŒ¨: {e}")
            return {"composite_score": 0, "error": str(e)}

    def detect_correlation_breakdown(self, current_data: Dict, historical_data: List[Dict]) -> Dict:
        """ìƒê´€ê´€ê³„ íŒŒê´´ ê°ì§€"""
        try:
            if len(historical_data) < 30:
                return {"composite_score": 0, "note": "insufficient_data"}
                
            # BTC vs VIX, BTC vs DXY ìƒê´€ê´€ê³„ ë¶„ì„
            btc_prices = []
            vix_values = []
            dxy_values = []
            
            for hist_data in historical_data:
                if "price_data" in hist_data and "current_price" in hist_data["price_data"]:
                    btc_prices.append(hist_data["price_data"]["current_price"])
                    
                if "macro_data" in hist_data:
                    if "vix" in hist_data["macro_data"]:
                        vix_values.append(hist_data["macro_data"]["vix"]["current"])
                    if "dxy" in hist_data["macro_data"]:
                        dxy_values.append(hist_data["macro_data"]["dxy"]["current"])
                        
            correlation_breakdown = 0
            
            # BTC-VIX ìƒê´€ê´€ê³„ (ë³´í†µì€ ìŒì˜ ìƒê´€ê´€ê³„)
            if len(btc_prices) == len(vix_values) and len(btc_prices) >= 20:
                btc_returns = np.diff(np.log(btc_prices))
                vix_returns = np.diff(vix_values)
                
                # ìµœê·¼ ìƒê´€ê´€ê³„ vs ê³¼ê±° ìƒê´€ê´€ê³„
                recent_corr = np.corrcoef(btc_returns[-10:], vix_returns[-10:])[0,1]
                historical_corr = np.corrcoef(btc_returns[:-10], vix_returns[:-10])[0,1]
                
                if not (np.isnan(recent_corr) or np.isnan(historical_corr)):
                    corr_change = abs(recent_corr - historical_corr)
                    correlation_breakdown += corr_change
                    
            return {
                "correlation_breakdown_score": float(correlation_breakdown),
                "composite_score": min(correlation_breakdown / 0.5, 1.0)  # 0.5 ìƒê´€ê´€ê³„ ë³€í™”ë¥¼ 1.0ìœ¼ë¡œ
            }
            
        except Exception as e:
            self.logger.error(f"ìƒê´€ê´€ê³„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"composite_score": 0, "error": str(e)}

    def calculate_composite_risk(self, risk_components: Dict) -> Dict:
        """ì¢…í•© ìœ„í—˜ë„ ê³„ì‚°"""
        try:
            # ê°€ì¤‘ì¹˜ ì„¤ì • (í•©ê³„ = 1.0)
            weights = {
                "sudden_change": 0.30,    # ê¸‰ë³€ì´ ê°€ì¥ ì¤‘ìš”
                "pattern_match": 0.25,    # ê³¼ê±° íŒ¨í„´ ë§¤ì¹­
                "anomaly": 0.20,         # ì´ìƒ ê°ì§€
                "trend_change": 0.15,     # ì¶”ì„¸ ë³€í™”
                "correlation": 0.10       # ìƒê´€ê´€ê³„ íŒŒê´´
            }
            
            # ê° ì»´í¬ë„ŒíŠ¸ ì ìˆ˜ ì¶”ì¶œ
            scores = {}
            for component, weight in weights.items():
                component_data = risk_components.get(component, {})
                scores[component] = component_data.get("composite_score", 0) * weight
                
            # ì¢…í•© ì ìˆ˜ ê³„ì‚°
            total_score = sum(scores.values())
            
            # ìœ„í—˜ ë ˆë²¨ ê²°ì •
            if total_score >= 0.8:
                risk_level = "CRITICAL"
                confidence = 0.9
            elif total_score >= 0.6:
                risk_level = "WARNING"  
                confidence = 0.8
            elif total_score >= 0.4:
                risk_level = "INFO"
                confidence = 0.7
            else:
                risk_level = "LOW"
                confidence = 0.6
                
            return {
                "total_score": float(total_score),
                "risk_level": risk_level,
                "confidence": confidence,
                "component_scores": scores,
                "timestamp": datetime.utcnow().isoformat()
            }
            
        except Exception as e:
            self.logger.error(f"ì¢…í•© ìœ„í—˜ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {
                "total_score": 0.5,
                "risk_level": "WARNING",
                "confidence": 0.5,
                "error": str(e)
            }

    def extract_features(self, data: Dict, historical_data: List[Dict]) -> Dict:
        """ë°ì´í„°ì—ì„œ íŠ¹ì§• ì¶”ì¶œ"""
        features = {}
        
        try:
            # ê°€ê²© ê´€ë ¨ íŠ¹ì§•
            if "price_data" in data:
                features["current_price"] = data["price_data"].get("current_price", 0)
                features["volume_24h"] = data["price_data"].get("volume_24h", 0)
                features["change_24h"] = data["price_data"].get("change_24h", 0)
                
            # ê±°ì‹œê²½ì œ íŠ¹ì§•
            if "macro_data" in data:
                if "vix" in data["macro_data"]:
                    features["vix_level"] = data["macro_data"]["vix"]["current"]
                    features["vix_change"] = data["macro_data"]["vix"]["change"]
                if "dxy" in data["macro_data"]:
                    features["dxy_level"] = data["macro_data"]["dxy"]["current"]
                    features["dxy_change"] = data["macro_data"]["dxy"]["change"]
                    
            # ì„¼í‹°ë¨¼íŠ¸ íŠ¹ì§•
            if "sentiment_data" in data and "fear_greed" in data["sentiment_data"]:
                features["fear_greed_index"] = data["sentiment_data"]["fear_greed"]["current_index"]
                
            return features
            
        except Exception as e:
            self.logger.error(f"íŠ¹ì§• ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {}

    def extract_numerical_features(self, data: Dict) -> Optional[List[float]]:
        """ìˆ˜ì¹˜í˜• íŠ¹ì§•ë§Œ ì¶”ì¶œ (MLìš©)"""
        try:
            features = []
            
            # ê°€ê²© ë°ì´í„°
            if "price_data" in data:
                price_data = data["price_data"]
                features.extend([
                    price_data.get("current_price", 0),
                    price_data.get("volume_24h", 0),
                    price_data.get("change_24h", 0),
                    price_data.get("market_cap", 0)
                ])
                
            # ê±°ì‹œê²½ì œ ë°ì´í„°
            if "macro_data" in data:
                macro_data = data["macro_data"]
                if "vix" in macro_data:
                    features.extend([
                        macro_data["vix"].get("current", 20),
                        macro_data["vix"].get("change", 0)
                    ])
                else:
                    features.extend([20, 0])  # ê¸°ë³¸ê°’
                    
                if "dxy" in macro_data:
                    features.extend([
                        macro_data["dxy"].get("current", 100),
                        macro_data["dxy"].get("change", 0)
                    ])
                else:
                    features.extend([100, 0])  # ê¸°ë³¸ê°’
            else:
                features.extend([20, 0, 100, 0])  # ê¸°ë³¸ê°’ë“¤
                
            # ì„¼í‹°ë¨¼íŠ¸ ë°ì´í„°
            if "sentiment_data" in data and "fear_greed" in data["sentiment_data"]:
                features.append(data["sentiment_data"]["fear_greed"]["current_index"])
            else:
                features.append(50)  # ì¤‘ë¦½ê°’
                
            return features if len(features) > 0 else None
            
        except Exception as e:
            self.logger.error(f"ìˆ˜ì¹˜í˜• íŠ¹ì§• ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None

    def calculate_pattern_similarity(self, current_features: Dict, pattern_features: Dict) -> float:
        """íŒ¨í„´ ìœ ì‚¬ë„ ê³„ì‚°"""
        try:
            if not current_features or not pattern_features:
                return 0.0
                
            # ê³µí†µ íŠ¹ì§•ë§Œ ë¹„êµ
            common_features = set(current_features.keys()) & set(pattern_features.keys())
            
            if not common_features:
                return 0.0
                
            similarities = []
            for feature in common_features:
                current_val = current_features[feature]
                pattern_val = pattern_features[feature]
                
                # ì •ê·œí™”ëœ ì°¨ì´ ê³„ì‚°
                if pattern_val != 0:
                    diff = abs(current_val - pattern_val) / abs(pattern_val)
                    similarity = max(0, 1 - diff)
                    similarities.append(similarity)
                    
            return np.mean(similarities) if similarities else 0.0
            
        except Exception as e:
            self.logger.error(f"ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0

    def get_pattern_risk_level(self, pattern_name: str) -> str:
        """íŒ¨í„´ë³„ ìœ„í—˜ ë ˆë²¨ ë°˜í™˜"""
        risk_levels = {
            "flash_crash_2022": "CRITICAL",
            "luna_collapse_2022": "CRITICAL",
            "covid_crash_2020": "CRITICAL",
            "china_ban_2021": "WARNING"
        }
        return risk_levels.get(pattern_name, "INFO")

    def get_pattern_severity(self, pattern_name: str) -> float:
        """íŒ¨í„´ë³„ ì‹¬ê°ë„ ë°˜í™˜"""
        severities = {
            "flash_crash_2022": 0.9,
            "luna_collapse_2022": 0.95,
            "covid_crash_2020": 1.0,
            "china_ban_2021": 0.8
        }
        return severities.get(pattern_name, 0.5)

    def generate_recommendations(self, composite_risk: Dict) -> List[str]:
        """ìœ„í—˜ë„ì— ë”°ë¥¸ ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        risk_level = composite_risk["risk_level"]
        total_score = composite_risk["total_score"]
        
        if risk_level == "CRITICAL":
            recommendations.extend([
                "ì¦‰ì‹œ ë ˆë²„ë¦¬ì§€ í¬ì§€ì…˜ ì ê²€ í•„ìš”",
                "ì†ì ˆê°€ ìƒí–¥ ì¡°ì • ê¶Œì¥",
                "í¬ì§€ì…˜ í¬ê¸° ì¶•ì†Œ ê³ ë ¤",
                "15ë¶„ ë‚´ ì¬ë¶„ì„ ì˜ˆì •"
            ])
        elif risk_level == "WARNING":
            recommendations.extend([
                "í¬ì§€ì…˜ ê´€ë¦¬ ì ê²€ ê¶Œì¥",
                "ì‹œì¥ ë³€í™” ì£¼ì˜ ê¹Šê²Œ ëª¨ë‹ˆí„°ë§",
                "1ì‹œê°„ í›„ ì¬í‰ê°€"
            ])
        elif risk_level == "INFO":
            recommendations.extend([
                "ì¼ë°˜ì ì¸ ì‹œì¥ ëª¨ë‹ˆí„°ë§ ì§€ì†",
                "4ì‹œê°„ í›„ ì •ê¸° ì ê²€"
            ])
            
        return recommendations

    def calculate_next_check_time(self, risk_level: str) -> str:
        """ë‹¤ìŒ ì²´í¬ ì‹œì  ê³„ì‚°"""
        intervals = {
            "CRITICAL": 5,   # 5ë¶„
            "WARNING": 30,   # 30ë¶„
            "INFO": 120,     # 2ì‹œê°„
            "LOW": 240       # 4ì‹œê°„
        }
        
        minutes = intervals.get(risk_level, 60)
        next_time = datetime.utcnow() + timedelta(minutes=minutes)
        return next_time.isoformat()

# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def test_risk_analyzer():
    """ìœ„í—˜ ë¶„ì„ê¸° í…ŒìŠ¤íŠ¸"""
    print("ğŸ§  ìœ„í—˜ ë¶„ì„ê¸° í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    analyzer = TimeSeriesRiskAnalyzer()
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    current_data = {
        "price_data": {
            "current_price": 60000,
            "volume_24h": 30000000000,
            "change_24h": -8.5
        },
        "macro_data": {
            "vix": {"current": 28, "change": 5.2},
            "dxy": {"current": 103, "change": 0.8}
        },
        "sentiment_data": {
            "fear_greed": {"current_index": 25}
        }
    }
    
    # ê°€ì§œ íˆìŠ¤í† ë¦¬ì»¬ ë°ì´í„° ìƒì„±
    historical_data = []
    base_price = 65000
    for i in range(100):
        hist_point = {
            "timestamp": (datetime.utcnow() - timedelta(minutes=i)).isoformat(),
            "price_data": {
                "current_price": base_price + np.random.normal(0, 1000),
                "volume_24h": 25000000000 + np.random.normal(0, 5000000000),
                "change_24h": np.random.normal(0, 3)
            },
            "macro_data": {
                "vix": {"current": 22 + np.random.normal(0, 2), "change": np.random.normal(0, 1)},
                "dxy": {"current": 102 + np.random.normal(0, 1), "change": np.random.normal(0, 0.5)}
            }
        }
        historical_data.append(hist_point)
    
    # ë¶„ì„ ì‹¤í–‰
    risk_analysis = analyzer.analyze_timeseries_risk(current_data, historical_data)
    
    print("âœ… ìœ„í—˜ ë¶„ì„ ê²°ê³¼:")
    print(f"  ì¢…í•© ìœ„í—˜ë„: {risk_analysis['composite_risk_score']:.3f}")
    print(f"  ìœ„í—˜ ë ˆë²¨: {risk_analysis['risk_level']}")
    print(f"  ì‹ ë¢°ë„: {risk_analysis['confidence']:.3f}")
    print(f"  ë‹¤ìŒ ì²´í¬: {risk_analysis['next_check_in']}")
    
    print("\n  ì»´í¬ë„ŒíŠ¸ë³„ ì ìˆ˜:")
    for component, data in risk_analysis['components'].items():
        score = data.get('composite_score', 0)
        print(f"    {component}: {score:.3f}")
        
    print("\n  ê¶Œì¥ì‚¬í•­:")
    for i, rec in enumerate(risk_analysis['recommendations'], 1):
        print(f"    {i}. {rec}")
    
    return risk_analysis

if __name__ == "__main__":
    test_risk_analyzer()