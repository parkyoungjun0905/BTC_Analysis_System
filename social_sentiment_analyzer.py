#!/usr/bin/env python3
"""
ì†Œì…œ ë¯¸ë””ì–´ ê°ì • ë¶„ì„ ì‹œìŠ¤í…œ
Twitter/X, Reddit, ë‰´ìŠ¤ ë“±ì˜ ì‹¤ì‹œê°„ ê°ì • ë¶„ì„ìœ¼ë¡œ 90% ì˜ˆì¸¡ ì •í™•ë„ ê¸°ì—¬
"""

import asyncio
import aiohttp
import json
import re
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
import logging
from dataclasses import dataclass
from collections import defaultdict
import sqlite3

@dataclass
class SentimentData:
    source: str
    content: str
    timestamp: datetime
    sentiment_score: float  # -1.0 to 1.0
    influence_score: float  # 0.0 to 1.0 (ì˜í–¥ë ¥)
    engagement_metrics: Dict
    keywords: List[str]

class SocialSentimentAnalyzer:
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.db_path = "sentiment_data.db"
        self._init_database()
        
        # ê°ì • ë¶„ì„ì„ ìœ„í•œ í‚¤ì›Œë“œ ì‚¬ì „
        self.bullish_keywords = {
            'strong': ['moon', 'bullish', 'buy', 'hodl', 'pump', 'ìƒìŠ¹', 'ë§¤ìˆ˜', 'ê°•ì„¸', 'breakout', 'rally', 'surge'],
            'moderate': ['positive', 'good', 'up', 'green', 'gain', 'ì¢‹ë‹¤', 'ì˜¤ë¥¸ë‹¤', 'support', 'bounce'],
            'weak': ['interesting', 'potential', 'maybe', 'could', 'ê´€ì‹¬', 'ê°€ëŠ¥ì„±', 'watch']
        }
        
        self.bearish_keywords = {
            'strong': ['crash', 'dump', 'sell', 'bearish', 'drop', 'í•˜ë½', 'ë§¤ë„', 'ì•½ì„¸', 'breakdown', 'collapse'],
            'moderate': ['negative', 'bad', 'down', 'red', 'loss', 'ë‚˜ì˜ë‹¤', 'ë‚´ë ¤ê°„ë‹¤', 'resistance', 'decline'],
            'weak': ['concern', 'worried', 'uncertain', 'ê±±ì •', 'ìš°ë ¤', 'ë¶ˆí™•ì‹¤', 'caution']
        }
        
        # ì˜í–¥ë ¥ ìˆëŠ” ê³„ì •ë“¤ì˜ ê°€ì¤‘ì¹˜
        self.influencer_weights = {
            'elon_musk': 10.0,
            'michael_saylor': 8.0,
            'plan_b': 7.0,
            'willy_woo': 6.0,
            'crypto_whale': 5.0,
            'default': 1.0
        }
    
    def _init_database(self):
        """ê°ì • ë¶„ì„ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS sentiment_data (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    source TEXT NOT NULL,
                    content TEXT NOT NULL,
                    timestamp DATETIME NOT NULL,
                    sentiment_score REAL NOT NULL,
                    influence_score REAL NOT NULL,
                    engagement_metrics TEXT,
                    keywords TEXT,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS sentiment_aggregates (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp DATETIME NOT NULL,
                    twitter_sentiment REAL,
                    reddit_sentiment REAL,
                    news_sentiment REAL,
                    overall_sentiment REAL,
                    confidence_score REAL,
                    volume_indicator REAL,
                    trend_direction TEXT,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # ì¸ë±ìŠ¤ ìƒì„±
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_timestamp ON sentiment_data(timestamp)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_source ON sentiment_data(source)')
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            self.logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    async def collect_twitter_sentiment(self) -> List[SentimentData]:
        """íŠ¸ìœ„í„°/X ê°ì • ë¶„ì„ ë°ì´í„° ìˆ˜ì§‘"""
        try:
            # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” Twitter API v2 ì‚¬ìš©
            # í˜„ì¬ëŠ” ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°
            
            twitter_data = []
            
            # ì‹œë®¬ë ˆì´ì…˜: ë‹¤ì–‘í•œ íŠ¸ìœ— ë°ì´í„°
            simulated_tweets = [
                {
                    'content': 'Bitcoin breaking above resistance levels! Very bullish momentum building ğŸš€',
                    'user': 'crypto_analyst',
                    'followers': 50000,
                    'retweets': 120,
                    'likes': 450,
                    'timestamp': datetime.utcnow() - timedelta(minutes=15)
                },
                {
                    'content': 'Seeing massive whale accumulation on-chain. Big moves incoming? ğŸ‹',
                    'user': 'whale_tracker',
                    'followers': 80000,
                    'retweets': 200,
                    'likes': 800,
                    'timestamp': datetime.utcnow() - timedelta(minutes=30)
                },
                {
                    'content': 'Market looking weak, might see some correction soon',
                    'user': 'trader_joe',
                    'followers': 20000,
                    'retweets': 45,
                    'likes': 120,
                    'timestamp': datetime.utcnow() - timedelta(minutes=45)
                }
            ]
            
            for tweet in simulated_tweets:
                sentiment_score = self._calculate_text_sentiment(tweet['content'])
                influence_score = self._calculate_influence_score(
                    tweet['user'], tweet['followers'], tweet['retweets'], tweet['likes']
                )
                keywords = self._extract_keywords(tweet['content'])
                
                sentiment_data = SentimentData(
                    source='twitter',
                    content=tweet['content'],
                    timestamp=tweet['timestamp'],
                    sentiment_score=sentiment_score,
                    influence_score=influence_score,
                    engagement_metrics={
                        'followers': tweet['followers'],
                        'retweets': tweet['retweets'],
                        'likes': tweet['likes']
                    },
                    keywords=keywords
                )
                
                twitter_data.append(sentiment_data)
            
            # ì‹¤ì œ êµ¬í˜„ ì˜ˆì‹œ (API ì‚¬ìš©ì‹œ)
            """
            async with aiohttp.ClientSession() as session:
                headers = {'Authorization': f'Bearer {TWITTER_BEARER_TOKEN}'}
                url = 'https://api.twitter.com/2/tweets/search/recent'
                params = {
                    'query': 'bitcoin OR BTC lang:en -is:retweet',
                    'max_results': 100,
                    'tweet.fields': 'public_metrics,created_at,author_id',
                    'user.fields': 'public_metrics,verified'
                }
                
                async with session.get(url, headers=headers, params=params) as response:
                    data = await response.json()
                    # íŠ¸ìœ— ë°ì´í„° ì²˜ë¦¬
            """
            
            return twitter_data
            
        except Exception as e:
            self.logger.error(f"íŠ¸ìœ„í„° ê°ì • ë¶„ì„ ì‹¤íŒ¨: {e}")
            return []
    
    async def collect_reddit_sentiment(self) -> List[SentimentData]:
        """Reddit ê°ì • ë¶„ì„ ë°ì´í„° ìˆ˜ì§‘"""
        try:
            reddit_data = []
            
            # ì‹œë®¬ë ˆì´ì…˜: Reddit í¬ìŠ¤íŠ¸/ëŒ“ê¸€
            simulated_posts = [
                {
                    'title': 'Why Bitcoin is heading to 100k - Technical Analysis',
                    'content': 'Looking at the charts, we have strong support at 58k and resistance is breaking...',
                    'subreddit': 'Bitcoin',
                    'upvotes': 2500,
                    'downvotes': 150,
                    'comments': 450,
                    'timestamp': datetime.utcnow() - timedelta(hours=2)
                },
                {
                    'title': 'Am I the only one worried about macro conditions?',
                    'content': 'Fed policy changes could really impact crypto markets negatively...',
                    'subreddit': 'CryptoCurrency', 
                    'upvotes': 800,
                    'downvotes': 300,
                    'comments': 200,
                    'timestamp': datetime.utcnow() - timedelta(hours=4)
                }
            ]
            
            for post in simulated_posts:
                full_text = f"{post['title']} {post['content']}"
                sentiment_score = self._calculate_text_sentiment(full_text)
                influence_score = self._calculate_reddit_influence(
                    post['upvotes'], post['downvotes'], post['comments']
                )
                keywords = self._extract_keywords(full_text)
                
                sentiment_data = SentimentData(
                    source='reddit',
                    content=full_text,
                    timestamp=post['timestamp'],
                    sentiment_score=sentiment_score,
                    influence_score=influence_score,
                    engagement_metrics={
                        'upvotes': post['upvotes'],
                        'downvotes': post['downvotes'],
                        'comments': post['comments'],
                        'subreddit': post['subreddit']
                    },
                    keywords=keywords
                )
                
                reddit_data.append(sentiment_data)
            
            # ì‹¤ì œ Reddit API ì‚¬ìš© ì˜ˆì‹œ
            """
            async with aiohttp.ClientSession() as session:
                # Reddit API í˜¸ì¶œ
                subreddits = ['Bitcoin', 'CryptoCurrency', 'BitcoinMarkets']
                for subreddit in subreddits:
                    url = f'https://www.reddit.com/r/{subreddit}/hot.json'
                    async with session.get(url, headers={'User-Agent': 'BTC-Analyzer'}) as response:
                        data = await response.json()
                        # Reddit ë°ì´í„° ì²˜ë¦¬
            """
            
            return reddit_data
            
        except Exception as e:
            self.logger.error(f"Reddit ê°ì • ë¶„ì„ ì‹¤íŒ¨: {e}")
            return []
    
    async def collect_news_sentiment(self) -> List[SentimentData]:
        """ë‰´ìŠ¤ ê°ì • ë¶„ì„ ë°ì´í„° ìˆ˜ì§‘"""
        try:
            news_data = []
            
            # ì‹œë®¬ë ˆì´ì…˜: ë‰´ìŠ¤ ê¸°ì‚¬
            simulated_news = [
                {
                    'title': 'Major Bank Announces Bitcoin Treasury Allocation',
                    'content': 'A leading financial institution revealed plans to allocate 5% of reserves to Bitcoin...',
                    'source': 'CoinDesk',
                    'author_credibility': 0.9,
                    'timestamp': datetime.utcnow() - timedelta(hours=1)
                },
                {
                    'title': 'SEC Increases Scrutiny on Crypto Exchanges',
                    'content': 'Regulatory pressure mounts as SEC announces enhanced oversight measures...',
                    'source': 'Reuters',
                    'author_credibility': 0.95,
                    'timestamp': datetime.utcnow() - timedelta(hours=3)
                }
            ]
            
            for article in simulated_news:
                full_text = f"{article['title']} {article['content']}"
                sentiment_score = self._calculate_text_sentiment(full_text)
                influence_score = article['author_credibility']  # ë‰´ìŠ¤ëŠ” ì¶œì²˜ ì‹ ë¢°ë„ê°€ ì˜í–¥ë ¥
                keywords = self._extract_keywords(full_text)
                
                sentiment_data = SentimentData(
                    source='news',
                    content=full_text,
                    timestamp=article['timestamp'],
                    sentiment_score=sentiment_score,
                    influence_score=influence_score,
                    engagement_metrics={
                        'source': article['source'],
                        'credibility': article['author_credibility']
                    },
                    keywords=keywords
                )
                
                news_data.append(sentiment_data)
            
            return news_data
            
        except Exception as e:
            self.logger.error(f"ë‰´ìŠ¤ ê°ì • ë¶„ì„ ì‹¤íŒ¨: {e}")
            return []
    
    def _calculate_text_sentiment(self, text: str) -> float:
        """í…ìŠ¤íŠ¸ ê°ì • ì ìˆ˜ ê³„ì‚° (-1.0 ~ 1.0)"""
        try:
            text_lower = text.lower()
            bullish_score = 0
            bearish_score = 0
            
            # ê°•ì„¸ í‚¤ì›Œë“œ ì ìˆ˜
            for strength, keywords in self.bullish_keywords.items():
                multiplier = {'strong': 3, 'moderate': 2, 'weak': 1}[strength]
                for keyword in keywords:
                    count = len(re.findall(r'\b' + re.escape(keyword) + r'\b', text_lower))
                    bullish_score += count * multiplier
            
            # ì•½ì„¸ í‚¤ì›Œë“œ ì ìˆ˜  
            for strength, keywords in self.bearish_keywords.items():
                multiplier = {'strong': 3, 'moderate': 2, 'weak': 1}[strength]
                for keyword in keywords:
                    count = len(re.findall(r'\b' + re.escape(keyword) + r'\b', text_lower))
                    bearish_score += count * multiplier
            
            # ê°ì • ì´ëª¨ì§€ ë¶„ì„
            bullish_emojis = ['ğŸš€', 'ğŸŒ™', 'ğŸ’', 'ğŸ”¥', 'ğŸ’ª', 'ğŸ“ˆ', 'ğŸŸ¢']
            bearish_emojis = ['ğŸ“‰', 'ğŸ’€', 'ğŸ”»', 'ğŸ©¸', 'ğŸ˜°', 'ğŸ”´', 'ğŸ’¥']
            
            for emoji in bullish_emojis:
                bullish_score += text.count(emoji) * 2
            
            for emoji in bearish_emojis:
                bearish_score += text.count(emoji) * 2
            
            # ì •ê·œí™”ëœ ê°ì • ì ìˆ˜ ê³„ì‚°
            total_score = bullish_score + bearish_score
            if total_score == 0:
                return 0.0
            
            sentiment = (bullish_score - bearish_score) / max(total_score, 1)
            return max(-1.0, min(1.0, sentiment))
            
        except Exception as e:
            self.logger.error(f"ê°ì • ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
    
    def _calculate_influence_score(self, username: str, followers: int, retweets: int, likes: int) -> float:
        """ì˜í–¥ë ¥ ì ìˆ˜ ê³„ì‚° (0.0 ~ 1.0)"""
        try:
            # ê¸°ë³¸ ì‚¬ìš©ì ê°€ì¤‘ì¹˜
            user_weight = self.influencer_weights.get(username.lower(), self.influencer_weights['default'])
            
            # íŒ”ë¡œì›Œ ê¸°ë°˜ ì ìˆ˜ (ë¡œê·¸ ìŠ¤ì¼€ì¼)
            follower_score = min(1.0, (followers / 100000) ** 0.5)
            
            # ì°¸ì—¬ë„ ê¸°ë°˜ ì ìˆ˜
            engagement_rate = (retweets + likes) / max(followers, 1)
            engagement_score = min(1.0, engagement_rate * 1000)  # 0.1% = 1.0 ì 
            
            # ì¢…í•© ì˜í–¥ë ¥ ì ìˆ˜
            influence = (user_weight * 0.4 + follower_score * 0.3 + engagement_score * 0.3) / 10
            return min(1.0, influence)
            
        except Exception as e:
            self.logger.error(f"ì˜í–¥ë ¥ ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.1
    
    def _calculate_reddit_influence(self, upvotes: int, downvotes: int, comments: int) -> float:
        """Reddit ì˜í–¥ë ¥ ì ìˆ˜ ê³„ì‚°"""
        try:
            # Reddit ì ìˆ˜ ê³µì‹
            net_score = upvotes - downvotes
            ratio = upvotes / max(upvotes + downvotes, 1)
            
            # ì ìˆ˜ ì •ê·œí™”
            score_influence = min(1.0, net_score / 5000)
            ratio_influence = ratio
            comment_influence = min(1.0, comments / 1000)
            
            return (score_influence * 0.4 + ratio_influence * 0.3 + comment_influence * 0.3)
            
        except Exception as e:
            self.logger.error(f"Reddit ì˜í–¥ë ¥ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.1
    
    def _extract_keywords(self, text: str) -> List[str]:
        """ì¤‘ìš” í‚¤ì›Œë“œ ì¶”ì¶œ"""
        try:
            keywords = []
            text_lower = text.lower()
            
            # ì¤‘ìš” í‚¤ì›Œë“œ ëª©ë¡
            important_keywords = [
                'bitcoin', 'btc', 'ethereum', 'eth', 'fed', 'inflation', 'rate',
                'bull', 'bear', 'breakout', 'support', 'resistance', 'whale',
                'institutional', 'etf', 'regulation', 'sec', 'mining'
            ]
            
            for keyword in important_keywords:
                if keyword in text_lower:
                    keywords.append(keyword)
            
            return keywords[:10]  # ìƒìœ„ 10ê°œë§Œ ë°˜í™˜
            
        except Exception as e:
            self.logger.error(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    async def analyze_sentiment_trends(self) -> Dict:
        """ê°ì • íŠ¸ë Œë“œ ë¶„ì„"""
        try:
            # ê° ì†ŒìŠ¤ë³„ ê°ì • ë°ì´í„° ìˆ˜ì§‘
            twitter_data = await self.collect_twitter_sentiment()
            reddit_data = await self.collect_reddit_sentiment()
            news_data = await self.collect_news_sentiment()
            
            all_data = twitter_data + reddit_data + news_data
            
            # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
            await self._save_sentiment_data(all_data)
            
            # ì†ŒìŠ¤ë³„ ê°€ì¤‘ í‰ê·  ê³„ì‚°
            twitter_sentiment = self._calculate_weighted_sentiment(twitter_data)
            reddit_sentiment = self._calculate_weighted_sentiment(reddit_data)
            news_sentiment = self._calculate_weighted_sentiment(news_data)
            
            # ì „ì²´ ê°ì • ì ìˆ˜ (ë‰´ìŠ¤ê°€ ê°€ì¥ ë†’ì€ ê°€ì¤‘ì¹˜)
            overall_sentiment = (
                twitter_sentiment * 0.3 +
                reddit_sentiment * 0.2 + 
                news_sentiment * 0.5
            )
            
            # ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
            confidence = self._calculate_confidence_score(all_data)
            
            # ë³¼ë¥¨ ì§€í‘œ (í™œë™ ìˆ˜ì¤€)
            volume_indicator = len(all_data) / 100.0  # ì •ê·œí™”
            
            # íŠ¸ë Œë“œ ë°©í–¥
            trend_direction = self._determine_trend_direction(overall_sentiment, confidence)
            
            result = {
                'timestamp': datetime.utcnow().isoformat(),
                'twitter_sentiment': twitter_sentiment,
                'reddit_sentiment': reddit_sentiment,
                'news_sentiment': news_sentiment,
                'overall_sentiment': overall_sentiment,
                'confidence_score': confidence,
                'volume_indicator': min(1.0, volume_indicator),
                'trend_direction': trend_direction,
                'data_points': len(all_data),
                'top_keywords': self._get_trending_keywords(all_data)
            }
            
            # ì§‘ê³„ ë°ì´í„° ì €ì¥
            await self._save_sentiment_aggregate(result)
            
            return result
            
        except Exception as e:
            self.logger.error(f"ê°ì • íŠ¸ë Œë“œ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def _calculate_weighted_sentiment(self, data: List[SentimentData]) -> float:
        """ê°€ì¤‘ í‰ê·  ê°ì • ì ìˆ˜ ê³„ì‚°"""
        if not data:
            return 0.0
        
        total_weighted_sentiment = 0
        total_weights = 0
        
        for item in data:
            weight = item.influence_score
            total_weighted_sentiment += item.sentiment_score * weight
            total_weights += weight
        
        return total_weighted_sentiment / max(total_weights, 0.001)
    
    def _calculate_confidence_score(self, data: List[SentimentData]) -> float:
        """ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°"""
        if not data:
            return 0.0
        
        # ë°ì´í„° í¬ì¸íŠ¸ ìˆ˜
        data_count_score = min(1.0, len(data) / 50)
        
        # ì†ŒìŠ¤ ë‹¤ì–‘ì„±
        sources = set(item.source for item in data)
        diversity_score = len(sources) / 3  # 3ê°œ ì†ŒìŠ¤ ê¸°ì¤€
        
        # ì˜í–¥ë ¥ ë¶„í¬
        avg_influence = sum(item.influence_score for item in data) / len(data)
        influence_score = min(1.0, avg_influence * 2)
        
        return (data_count_score * 0.4 + diversity_score * 0.3 + influence_score * 0.3)
    
    def _determine_trend_direction(self, sentiment: float, confidence: float) -> str:
        """íŠ¸ë Œë“œ ë°©í–¥ ê²°ì •"""
        if confidence < 0.3:
            return 'UNCERTAIN'
        
        if sentiment > 0.3:
            return 'VERY_BULLISH' if sentiment > 0.6 else 'BULLISH'
        elif sentiment < -0.3:
            return 'VERY_BEARISH' if sentiment < -0.6 else 'BEARISH'
        else:
            return 'NEUTRAL'
    
    def _get_trending_keywords(self, data: List[SentimentData]) -> List[str]:
        """íŠ¸ë Œë”© í‚¤ì›Œë“œ ì¶”ì¶œ"""
        keyword_count = defaultdict(int)
        
        for item in data:
            for keyword in item.keywords:
                keyword_count[keyword] += 1
        
        # ë¹ˆë„ìˆœ ì •ë ¬
        sorted_keywords = sorted(keyword_count.items(), key=lambda x: x[1], reverse=True)
        return [keyword for keyword, count in sorted_keywords[:10]]
    
    async def _save_sentiment_data(self, data: List[SentimentData]):
        """ê°ì • ë°ì´í„° ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            for item in data:
                cursor.execute('''
                    INSERT INTO sentiment_data 
                    (source, content, timestamp, sentiment_score, influence_score, engagement_metrics, keywords)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                ''', (
                    item.source,
                    item.content,
                    item.timestamp.isoformat(),
                    item.sentiment_score,
                    item.influence_score,
                    json.dumps(item.engagement_metrics),
                    json.dumps(item.keywords)
                ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            self.logger.error(f"ê°ì • ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
    
    async def _save_sentiment_aggregate(self, result: Dict):
        """ê°ì • ì§‘ê³„ ë°ì´í„° ì €ì¥"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT INTO sentiment_aggregates 
                (timestamp, twitter_sentiment, reddit_sentiment, news_sentiment, 
                 overall_sentiment, confidence_score, volume_indicator, trend_direction)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                result['timestamp'],
                result['twitter_sentiment'],
                result['reddit_sentiment'], 
                result['news_sentiment'],
                result['overall_sentiment'],
                result['confidence_score'],
                result['volume_indicator'],
                result['trend_direction']
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            self.logger.error(f"ì§‘ê³„ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")

# í…ŒìŠ¤íŠ¸ ë° ì‹¤í–‰ í•¨ìˆ˜
async def test_social_sentiment_analyzer():
    """ì†Œì…œ ê°ì • ë¶„ì„ê¸° í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª ì†Œì…œ ë¯¸ë””ì–´ ê°ì • ë¶„ì„ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸...")
    
    analyzer = SocialSentimentAnalyzer()
    result = await analyzer.analyze_sentiment_trends()
    
    if 'error' in result:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {result['error']}")
        return False
    
    print("âœ… ê°ì • ë¶„ì„ ê²°ê³¼:")
    print(f"  ğŸ“± íŠ¸ìœ„í„° ê°ì •: {result['twitter_sentiment']:.3f}")
    print(f"  ğŸ’¬ Reddit ê°ì •: {result['reddit_sentiment']:.3f}")  
    print(f"  ğŸ“° ë‰´ìŠ¤ ê°ì •: {result['news_sentiment']:.3f}")
    print(f"  ğŸ¯ ì¢…í•© ê°ì •: {result['overall_sentiment']:.3f}")
    print(f"  ğŸ“Š ì‹ ë¢°ë„: {result['confidence_score']:.3f}")
    print(f"  ğŸ“ˆ íŠ¸ë Œë“œ: {result['trend_direction']}")
    print(f"  ğŸ”¥ ì¸ê¸° í‚¤ì›Œë“œ: {', '.join(result['top_keywords'])}")
    
    return True

if __name__ == "__main__":
    asyncio.run(test_social_sentiment_analyzer())