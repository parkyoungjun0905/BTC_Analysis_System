#!/usr/bin/env python3
"""
ÏÜåÏÖú ÎØ∏ÎîîÏñ¥ Í∞êÏ†ï Î∂ÑÏÑù ÏãúÏä§ÌÖú
Twitter/X, Reddit, Îâ¥Ïä§ Îì±Ïùò Ïã§ÏãúÍ∞Ñ Í∞êÏ†ï Î∂ÑÏÑùÏúºÎ°ú 90% ÏòàÏ∏° Ï†ïÌôïÎèÑ Í∏∞Ïó¨
"""

import asyncio
import aiohttp
import json
import re
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
import logging
from dataclasses import dataclass
from collections import defaultdict
import sqlite3

@dataclass
class SentimentData:
    source: str
    content: str
    timestamp: datetime
    sentiment_score: float  # -1.0 to 1.0
    influence_score: float  # 0.0 to 1.0 (ÏòÅÌñ•Î†•)
    engagement_metrics: Dict
    keywords: List[str]

class SocialSentimentAnalyzer:
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.db_path = "sentiment_data.db"
        self._init_database()
        
        # Í∞êÏ†ï Î∂ÑÏÑùÏùÑ ÏúÑÌïú ÌÇ§ÏõåÎìú ÏÇ¨Ï†Ñ
        self.bullish_keywords = {
            'strong': ['moon', 'bullish', 'buy', 'hodl', 'pump', 'ÏÉÅÏäπ', 'Îß§Ïàò', 'Í∞ïÏÑ∏', 'breakout', 'rally', 'surge'],
            'moderate': ['positive', 'good', 'up', 'green', 'gain', 'Ï¢ãÎã§', 'Ïò§Î•∏Îã§', 'support', 'bounce'],
            'weak': ['interesting', 'potential', 'maybe', 'could', 'Í¥ÄÏã¨', 'Í∞ÄÎä•ÏÑ±', 'watch']
        }
        
        self.bearish_keywords = {
            'strong': ['crash', 'dump', 'sell', 'bearish', 'drop', 'ÌïòÎùΩ', 'Îß§ÎèÑ', 'ÏïΩÏÑ∏', 'breakdown', 'collapse'],
            'moderate': ['negative', 'bad', 'down', 'red', 'loss', 'ÎÇòÏÅòÎã§', 'ÎÇ¥Î†§Í∞ÑÎã§', 'resistance', 'decline'],
            'weak': ['concern', 'worried', 'uncertain', 'Í±±Ï†ï', 'Ïö∞Î†§', 'Î∂àÌôïÏã§', 'caution']
        }
        
        # ÏòÅÌñ•Î†• ÏûàÎäî Í≥ÑÏ†ïÎì§Ïùò Í∞ÄÏ§ëÏπò
        self.influencer_weights = {
            'elon_musk': 10.0,
            'michael_saylor': 8.0,
            'plan_b': 7.0,
            'willy_woo': 6.0,
            'crypto_whale': 5.0,
            'default': 1.0
        }
    
    def _init_database(self):
        """Í∞êÏ†ï Î∂ÑÏÑù Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ï¥àÍ∏∞Ìôî"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS sentiment_data (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    source TEXT NOT NULL,
                    content TEXT NOT NULL,
                    timestamp DATETIME NOT NULL,
                    sentiment_score REAL NOT NULL,
                    influence_score REAL NOT NULL,
                    engagement_metrics TEXT,
                    keywords TEXT,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS sentiment_aggregates (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp DATETIME NOT NULL,
                    twitter_sentiment REAL,
                    reddit_sentiment REAL,
                    news_sentiment REAL,
                    overall_sentiment REAL,
                    confidence_score REAL,
                    volume_indicator REAL,
                    trend_direction TEXT,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # Ïù∏Îç±Ïä§ ÏÉùÏÑ±
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_timestamp ON sentiment_data(timestamp)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_source ON sentiment_data(source)')
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            self.logger.error(f"Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
    
    async def collect_twitter_sentiment(self) -> List[SentimentData]:
        """Ìä∏ÏúÑÌÑ∞/X Í∞êÏ†ï Î∂ÑÏÑù Îç∞Ïù¥ÌÑ∞ ÏàòÏßë"""
        try:
            # Ïã§Ï†ú Íµ¨ÌòÑÏóêÏÑúÎäî Twitter API v2 ÏÇ¨Ïö©
            # ÌòÑÏû¨Îäî ÏãúÎÆ¨Î†àÏù¥ÏÖò Îç∞Ïù¥ÌÑ∞
            
            twitter_data = []
            
            # ÏãúÎÆ¨Î†àÏù¥ÏÖò: Îã§ÏñëÌïú Ìä∏Ïúó Îç∞Ïù¥ÌÑ∞
            simulated_tweets = [
                {
                    'content': 'Bitcoin breaking above resistance levels! Very bullish momentum building üöÄ',
                    'user': 'crypto_analyst',
                    'followers': 50000,
                    'retweets': 120,
                    'likes': 450,
                    'timestamp': datetime.utcnow() - timedelta(minutes=15)
                },
                {
                    'content': 'Seeing massive whale accumulation on-chain. Big moves incoming? üêã',
                    'user': 'whale_tracker',
                    'followers': 80000,
                    'retweets': 200,
                    'likes': 800,
                    'timestamp': datetime.utcnow() - timedelta(minutes=30)
                },
                {
                    'content': 'Market looking weak, might see some correction soon',
                    'user': 'trader_joe',
                    'followers': 20000,
                    'retweets': 45,
                    'likes': 120,
                    'timestamp': datetime.utcnow() - timedelta(minutes=45)
                }
            ]
            
            for tweet in simulated_tweets:
                sentiment_score = self._calculate_text_sentiment(tweet['content'])
                influence_score = self._calculate_influence_score(
                    tweet['user'], tweet['followers'], tweet['retweets'], tweet['likes']
                )
                keywords = self._extract_keywords(tweet['content'])
                
                sentiment_data = SentimentData(
                    source='twitter',
                    content=tweet['content'],
                    timestamp=tweet['timestamp'],
                    sentiment_score=sentiment_score,
                    influence_score=influence_score,
                    engagement_metrics={
                        'followers': tweet['followers'],
                        'retweets': tweet['retweets'],
                        'likes': tweet['likes']
                    },
                    keywords=keywords
                )
                
                twitter_data.append(sentiment_data)
            
            # Ïã§Ï†ú Íµ¨ÌòÑ ÏòàÏãú (API ÏÇ¨Ïö©Ïãú)
            """
            async with aiohttp.ClientSession() as session:
                headers = {'Authorization': f'Bearer {TWITTER_BEARER_TOKEN}'}
                url = 'https://api.twitter.com/2/tweets/search/recent'
                params = {
                    'query': 'bitcoin OR BTC lang:en -is:retweet',
                    'max_results': 100,
                    'tweet.fields': 'public_metrics,created_at,author_id',
                    'user.fields': 'public_metrics,verified'
                }
                
                async with session.get(url, headers=headers, params=params) as response:
                    data = await response.json()
                    # Ìä∏Ïúó Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨
            """
            
            return twitter_data
            
        except Exception as e:
            self.logger.error(f"Ìä∏ÏúÑÌÑ∞ Í∞êÏ†ï Î∂ÑÏÑù Ïã§Ìå®: {e}")
            return []
    
    async def collect_reddit_sentiment(self) -> List[SentimentData]:
        """Reddit Í∞êÏ†ï Î∂ÑÏÑù Îç∞Ïù¥ÌÑ∞ ÏàòÏßë"""
        try:
            reddit_data = []
            
            # ÏãúÎÆ¨Î†àÏù¥ÏÖò: Reddit Ìè¨Ïä§Ìä∏/ÎåìÍ∏Ä
            simulated_posts = [
                {
                    'title': 'Why Bitcoin is heading to 100k - Technical Analysis',
                    'content': 'Looking at the charts, we have strong support at 58k and resistance is breaking...',
                    'subreddit': 'Bitcoin',
                    'upvotes': 2500,
                    'downvotes': 150,
                    'comments': 450,
                    'timestamp': datetime.utcnow() - timedelta(hours=2)
                },
                {
                    'title': 'Am I the only one worried about macro conditions?',
                    'content': 'Fed policy changes could really impact crypto markets negatively...',
                    'subreddit': 'CryptoCurrency', 
                    'upvotes': 800,
                    'downvotes': 300,
                    'comments': 200,
                    'timestamp': datetime.utcnow() - timedelta(hours=4)
                }
            ]
            
            for post in simulated_posts:
                full_text = f"{post['title']} {post['content']}"
                sentiment_score = self._calculate_text_sentiment(full_text)
                influence_score = self._calculate_reddit_influence(
                    post['upvotes'], post['downvotes'], post['comments']
                )
                keywords = self._extract_keywords(full_text)
                
                sentiment_data = SentimentData(
                    source='reddit',
                    content=full_text,
                    timestamp=post['timestamp'],
                    sentiment_score=sentiment_score,
                    influence_score=influence_score,
                    engagement_metrics={
                        'upvotes': post['upvotes'],
                        'downvotes': post['downvotes'],
                        'comments': post['comments'],
                        'subreddit': post['subreddit']
                    },
                    keywords=keywords
                )
                
                reddit_data.append(sentiment_data)
            
            # Ïã§Ï†ú Reddit API ÏÇ¨Ïö© ÏòàÏãú
            """
            async with aiohttp.ClientSession() as session:
                # Reddit API Ìò∏Ï∂ú
                subreddits = ['Bitcoin', 'CryptoCurrency', 'BitcoinMarkets']
                for subreddit in subreddits:
                    url = f'https://www.reddit.com/r/{subreddit}/hot.json'
                    async with session.get(url, headers={'User-Agent': 'BTC-Analyzer'}) as response:
                        data = await response.json()
                        # Reddit Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨
            """
            
            return reddit_data
            
        except Exception as e:
            self.logger.error(f"Reddit Í∞êÏ†ï Î∂ÑÏÑù Ïã§Ìå®: {e}")
            return []
    
    async def collect_news_sentiment(self) -> List[SentimentData]:
        """Îâ¥Ïä§ Í∞êÏ†ï Î∂ÑÏÑù Îç∞Ïù¥ÌÑ∞ ÏàòÏßë"""
        try:
            news_data = []
            
            # ÏãúÎÆ¨Î†àÏù¥ÏÖò: Îâ¥Ïä§ Í∏∞ÏÇ¨
            simulated_news = [
                {
                    'title': 'Major Bank Announces Bitcoin Treasury Allocation',
                    'content': 'A leading financial institution revealed plans to allocate 5% of reserves to Bitcoin...',
                    'source': 'CoinDesk',
                    'author_credibility': 0.9,
                    'timestamp': datetime.utcnow() - timedelta(hours=1)
                },
                {
                    'title': 'SEC Increases Scrutiny on Crypto Exchanges',
                    'content': 'Regulatory pressure mounts as SEC announces enhanced oversight measures...',
                    'source': 'Reuters',
                    'author_credibility': 0.95,
                    'timestamp': datetime.utcnow() - timedelta(hours=3)
                }
            ]
            
            for article in simulated_news:
                full_text = f"{article['title']} {article['content']}"
                sentiment_score = self._calculate_text_sentiment(full_text)
                influence_score = article['author_credibility']  # Îâ¥Ïä§Îäî Ï∂úÏ≤ò Ïã†Î¢∞ÎèÑÍ∞Ä ÏòÅÌñ•Î†•
                keywords = self._extract_keywords(full_text)
                
                sentiment_data = SentimentData(
                    source='news',
                    content=full_text,
                    timestamp=article['timestamp'],
                    sentiment_score=sentiment_score,
                    influence_score=influence_score,
                    engagement_metrics={
                        'source': article['source'],
                        'credibility': article['author_credibility']
                    },
                    keywords=keywords
                )
                
                news_data.append(sentiment_data)
            
            return news_data
            
        except Exception as e:
            self.logger.error(f"Îâ¥Ïä§ Í∞êÏ†ï Î∂ÑÏÑù Ïã§Ìå®: {e}")
            return []
    
    def _calculate_text_sentiment(self, text: str) -> float:
        """ÌÖçÏä§Ìä∏ Í∞êÏ†ï Ï†êÏàò Í≥ÑÏÇ∞ (-1.0 ~ 1.0)"""
        try:
            text_lower = text.lower()
            bullish_score = 0
            bearish_score = 0
            
            # Í∞ïÏÑ∏ ÌÇ§ÏõåÎìú Ï†êÏàò
            for strength, keywords in self.bullish_keywords.items():
                multiplier = {'strong': 3, 'moderate': 2, 'weak': 1}[strength]
                for keyword in keywords:
                    count = len(re.findall(r'\b' + re.escape(keyword) + r'\b', text_lower))
                    bullish_score += count * multiplier
            
            # ÏïΩÏÑ∏ ÌÇ§ÏõåÎìú Ï†êÏàò  
            for strength, keywords in self.bearish_keywords.items():
                multiplier = {'strong': 3, 'moderate': 2, 'weak': 1}[strength]
                for keyword in keywords:
                    count = len(re.findall(r'\b' + re.escape(keyword) + r'\b', text_lower))
                    bearish_score += count * multiplier
            
            # Í∞êÏ†ï Ïù¥Î™®ÏßÄ Î∂ÑÏÑù
            bullish_emojis = ['üöÄ', 'üåô', 'üíé', 'üî•', 'üí™', 'üìà', 'üü¢']
            bearish_emojis = ['üìâ', 'üíÄ', 'üîª', 'ü©∏', 'üò∞', 'üî¥', 'üí•']
            
            for emoji in bullish_emojis:
                bullish_score += text.count(emoji) * 2
            
            for emoji in bearish_emojis:
                bearish_score += text.count(emoji) * 2
            
            # Ï†ïÍ∑úÌôîÎêú Í∞êÏ†ï Ï†êÏàò Í≥ÑÏÇ∞
            total_score = bullish_score + bearish_score
            if total_score == 0:
                return 0.0
            
            sentiment = (bullish_score - bearish_score) / max(total_score, 1)
            return max(-1.0, min(1.0, sentiment))
            
        except Exception as e:
            self.logger.error(f"Í∞êÏ†ï Ï†êÏàò Í≥ÑÏÇ∞ Ïã§Ìå®: {e}")
            return 0.0
    
    def _calculate_influence_score(self, username: str, followers: int, retweets: int, likes: int) -> float:
        """ÏòÅÌñ•Î†• Ï†êÏàò Í≥ÑÏÇ∞ (0.0 ~ 1.0)"""
        try:
            # Í∏∞Î≥∏ ÏÇ¨Ïö©Ïûê Í∞ÄÏ§ëÏπò
            user_weight = self.influencer_weights.get(username.lower(), self.influencer_weights['default'])
            
            # ÌåîÎ°úÏõå Í∏∞Î∞ò Ï†êÏàò (Î°úÍ∑∏ Ïä§ÏºÄÏùº)
            follower_score = min(1.0, (followers / 100000) ** 0.5)
            
            # Ï∞∏Ïó¨ÎèÑ Í∏∞Î∞ò Ï†êÏàò
            engagement_rate = (retweets + likes) / max(followers, 1)
            engagement_score = min(1.0, engagement_rate * 1000)  # 0.1% = 1.0 Ï†ê
            
            # Ï¢ÖÌï© ÏòÅÌñ•Î†• Ï†êÏàò
            influence = (user_weight * 0.4 + follower_score * 0.3 + engagement_score * 0.3) / 10
            return min(1.0, influence)
            
        except Exception as e:
            self.logger.error(f"ÏòÅÌñ•Î†• Ï†êÏàò Í≥ÑÏÇ∞ Ïã§Ìå®: {e}")
            return 0.1
    
    def _calculate_reddit_influence(self, upvotes: int, downvotes: int, comments: int) -> float:
        """Reddit ÏòÅÌñ•Î†• Ï†êÏàò Í≥ÑÏÇ∞"""
        try:
            # Reddit Ï†êÏàò Í≥µÏãù
            net_score = upvotes - downvotes
            ratio = upvotes / max(upvotes + downvotes, 1)
            
            # Ï†êÏàò Ï†ïÍ∑úÌôî
            score_influence = min(1.0, net_score / 5000)
            ratio_influence = ratio
            comment_influence = min(1.0, comments / 1000)
            
            return (score_influence * 0.4 + ratio_influence * 0.3 + comment_influence * 0.3)
            
        except Exception as e:
            self.logger.error(f"Reddit ÏòÅÌñ•Î†• Í≥ÑÏÇ∞ Ïã§Ìå®: {e}")
            return 0.1
    
    def _extract_keywords(self, text: str) -> List[str]:
        """Ï§ëÏöî ÌÇ§ÏõåÎìú Ï∂îÏ∂ú"""
        try:
            keywords = []
            text_lower = text.lower()
            
            # Ï§ëÏöî ÌÇ§ÏõåÎìú Î™©Î°ù
            important_keywords = [
                'bitcoin', 'btc', 'ethereum', 'eth', 'fed', 'inflation', 'rate',
                'bull', 'bear', 'breakout', 'support', 'resistance', 'whale',
                'institutional', 'etf', 'regulation', 'sec', 'mining'
            ]
            
            for keyword in important_keywords:
                if keyword in text_lower:
                    keywords.append(keyword)
            
            return keywords[:10]  # ÏÉÅÏúÑ 10Í∞úÎßå Î∞òÌôò
            
        except Exception as e:
            self.logger.error(f"ÌÇ§ÏõåÎìú Ï∂îÏ∂ú Ïã§Ìå®: {e}")
            return []
    
    async def analyze_sentiment_trends(self) -> Dict:
        """Í∞êÏ†ï Ìä∏Î†åÎìú Î∂ÑÏÑù"""
        try:
            # Í∞Å ÏÜåÏä§Î≥Ñ Í∞êÏ†ï Îç∞Ïù¥ÌÑ∞ ÏàòÏßë
            twitter_data = await self.collect_twitter_sentiment()
            reddit_data = await self.collect_reddit_sentiment()
            news_data = await self.collect_news_sentiment()
            
            all_data = twitter_data + reddit_data + news_data
            
            # Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§Ïóê Ï†ÄÏû•
            await self._save_sentiment_data(all_data)
            
            # ÏÜåÏä§Î≥Ñ Í∞ÄÏ§ë ÌèâÍ∑† Í≥ÑÏÇ∞
            twitter_sentiment = self._calculate_weighted_sentiment(twitter_data)
            reddit_sentiment = self._calculate_weighted_sentiment(reddit_data)
            news_sentiment = self._calculate_weighted_sentiment(news_data)
            
            # Ï†ÑÏ≤¥ Í∞êÏ†ï Ï†êÏàò (Îâ¥Ïä§Í∞Ä Í∞ÄÏû• ÎÜíÏùÄ Í∞ÄÏ§ëÏπò)
            overall_sentiment = (
                twitter_sentiment * 0.3 +
                reddit_sentiment * 0.2 + 
                news_sentiment * 0.5
            )
            
            # Ïã†Î¢∞ÎèÑ Ï†êÏàò Í≥ÑÏÇ∞
            confidence = self._calculate_confidence_score(all_data)
            
            # Î≥ºÎ•® ÏßÄÌëú (ÌôúÎèô ÏàòÏ§Ä)
            volume_indicator = len(all_data) / 100.0  # Ï†ïÍ∑úÌôî
            
            # Ìä∏Î†åÎìú Î∞©Ìñ•
            trend_direction = self._determine_trend_direction(overall_sentiment, confidence)
            
            result = {
                'timestamp': datetime.utcnow().isoformat(),
                'twitter_sentiment': twitter_sentiment,
                'reddit_sentiment': reddit_sentiment,
                'news_sentiment': news_sentiment,
                'overall_sentiment': overall_sentiment,
                'confidence_score': confidence,
                'volume_indicator': min(1.0, volume_indicator),
                'trend_direction': trend_direction,
                'data_points': len(all_data),
                'top_keywords': self._get_trending_keywords(all_data)
            }
            
            # ÏßëÍ≥Ñ Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû•
            await self._save_sentiment_aggregate(result)
            
            return result
            
        except Exception as e:
            self.logger.error(f"Í∞êÏ†ï Ìä∏Î†åÎìú Î∂ÑÏÑù Ïã§Ìå®: {e}")
            return {"error": str(e)}
    
    def _calculate_weighted_sentiment(self, data: List[SentimentData]) -> float:
        """Í∞ÄÏ§ë ÌèâÍ∑† Í∞êÏ†ï Ï†êÏàò Í≥ÑÏÇ∞"""
        if not data:
            return 0.0
        
        total_weighted_sentiment = 0
        total_weights = 0
        
        for item in data:
            weight = item.influence_score
            total_weighted_sentiment += item.sentiment_score * weight
            total_weights += weight
        
        return total_weighted_sentiment / max(total_weights, 0.001)
    
    def _calculate_confidence_score(self, data: List[SentimentData]) -> float:
        """Ïã†Î¢∞ÎèÑ Ï†êÏàò Í≥ÑÏÇ∞"""
        if not data:
            return 0.0
        
        # Îç∞Ïù¥ÌÑ∞ Ìè¨Ïù∏Ìä∏ Ïàò
        data_count_score = min(1.0, len(data) / 50)
        
        # ÏÜåÏä§ Îã§ÏñëÏÑ±
        sources = set(item.source for item in data)
        diversity_score = len(sources) / 3  # 3Í∞ú ÏÜåÏä§ Í∏∞Ï§Ä
        
        # ÏòÅÌñ•Î†• Î∂ÑÌè¨
        avg_influence = sum(item.influence_score for item in data) / len(data)
        influence_score = min(1.0, avg_influence * 2)
        
        return (data_count_score * 0.4 + diversity_score * 0.3 + influence_score * 0.3)
    
    def _determine_trend_direction(self, sentiment: float, confidence: float) -> str:
        """Ìä∏Î†åÎìú Î∞©Ìñ• Í≤∞Ï†ï"""
        if confidence < 0.3:
            return 'UNCERTAIN'
        
        if sentiment > 0.3:
            return 'VERY_BULLISH' if sentiment > 0.6 else 'BULLISH'
        elif sentiment < -0.3:
            return 'VERY_BEARISH' if sentiment < -0.6 else 'BEARISH'
        else:
            return 'NEUTRAL'
    
    def _get_trending_keywords(self, data: List[SentimentData]) -> List[str]:
        """Ìä∏Î†åÎî© ÌÇ§ÏõåÎìú Ï∂îÏ∂ú"""
        keyword_count = defaultdict(int)
        
        for item in data:
            for keyword in item.keywords:
                keyword_count[keyword] += 1
        
        # ÎπàÎèÑÏàú Ï†ïÎ†¨
        sorted_keywords = sorted(keyword_count.items(), key=lambda x: x[1], reverse=True)
        return [keyword for keyword, count in sorted_keywords[:10]]
    
    async def _save_sentiment_data(self, data: List[SentimentData]):
        """Í∞êÏ†ï Îç∞Ïù¥ÌÑ∞ Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ï†ÄÏû•"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            for item in data:
                cursor.execute('''
                    INSERT INTO sentiment_data 
                    (source, content, timestamp, sentiment_score, influence_score, engagement_metrics, keywords)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                ''', (
                    item.source,
                    item.content,
                    item.timestamp.isoformat(),
                    item.sentiment_score,
                    item.influence_score,
                    json.dumps(item.engagement_metrics),
                    json.dumps(item.keywords)
                ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            self.logger.error(f"Í∞êÏ†ï Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû• Ïã§Ìå®: {e}")
    
    async def _save_sentiment_aggregate(self, result: Dict):
        """Í∞êÏ†ï ÏßëÍ≥Ñ Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû•"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT INTO sentiment_aggregates 
                (timestamp, twitter_sentiment, reddit_sentiment, news_sentiment, 
                 overall_sentiment, confidence_score, volume_indicator, trend_direction)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                result['timestamp'],
                result['twitter_sentiment'],
                result['reddit_sentiment'], 
                result['news_sentiment'],
                result['overall_sentiment'],
                result['confidence_score'],
                result['volume_indicator'],
                result['trend_direction']
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            self.logger.error(f"ÏßëÍ≥Ñ Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû• Ïã§Ìå®: {e}")

# ÌÖåÏä§Ìä∏ Î∞è Ïã§Ìñâ Ìï®Ïàò
async def test_social_sentiment_analyzer():
    """ÏÜåÏÖú Í∞êÏ†ï Î∂ÑÏÑùÍ∏∞ ÌÖåÏä§Ìä∏"""
    print("üß™ ÏÜåÏÖú ÎØ∏ÎîîÏñ¥ Í∞êÏ†ï Î∂ÑÏÑù ÏãúÏä§ÌÖú ÌÖåÏä§Ìä∏...")
    
    analyzer = SocialSentimentAnalyzer()
    result = await analyzer.analyze_sentiment_trends()
    
    if 'error' in result:
        print(f"‚ùå ÌÖåÏä§Ìä∏ Ïã§Ìå®: {result['error']}")
        return False
    
    print("‚úÖ Í∞êÏ†ï Î∂ÑÏÑù Í≤∞Í≥º:")
    print(f"  üì± Ìä∏ÏúÑÌÑ∞ Í∞êÏ†ï: {result['twitter_sentiment']:.3f}")
    print(f"  üí¨ Reddit Í∞êÏ†ï: {result['reddit_sentiment']:.3f}")  
    print(f"  üì∞ Îâ¥Ïä§ Í∞êÏ†ï: {result['news_sentiment']:.3f}")
    print(f"  üéØ Ï¢ÖÌï© Í∞êÏ†ï: {result['overall_sentiment']:.3f}")
    print(f"  üìä Ïã†Î¢∞ÎèÑ: {result['confidence_score']:.3f}")
    print(f"  üìà Ìä∏Î†åÎìú: {result['trend_direction']}")
    print(f"  üî• Ïù∏Í∏∞ ÌÇ§ÏõåÎìú: {', '.join(result['top_keywords'])}")
    
    return True

if __name__ == "__main__":
    asyncio.run(test_social_sentiment_analyzer())