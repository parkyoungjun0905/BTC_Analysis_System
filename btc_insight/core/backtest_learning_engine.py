#!/usr/bin/env python3
"""
ğŸ•°ï¸ ë°±í…ŒìŠ¤íŠ¸ í•™ìŠµ ì—”ì§„
- ì‹œê°„ì—¬í–‰ ë°±í…ŒìŠ¤íŠ¸: ê³¼ê±° ì‹œì ìœ¼ë¡œ ëŒì•„ê°€ì„œ ë¯¸ë˜ ì˜ˆì¸¡ â†’ ì‹¤ì œê°’ ë¹„êµ â†’ í•™ìŠµ
- ì‚¬ìš©ì ì˜ˆì‹œ: 25ë…„ 7ì›” 23ì¼ â†’ 7ì›” 26ì¼ 17ì‹œ ì˜ˆì¸¡ â†’ ê²€ì¦ â†’ í•™ìŠµ
- ëª©í‘œ: 95% ì •í™•ë„ ë‹¬ì„±ì„ ìœ„í•œ ì§€ì†ì  í•™ìŠµ
"""

import pandas as pd
import numpy as np
import json
import os
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
import warnings
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import Ridge
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import joblib

warnings.filterwarnings('ignore')

class BacktestLearningEngine:
    """ë°±í…ŒìŠ¤íŠ¸ í•™ìŠµ ì—”ì§„ - ì‹œê°„ì—¬í–‰ì„ í†µí•œ ì§€ì†ì  í•™ìŠµ"""
    
    def __init__(self, data_path: str):
        """
        Args:
            data_path: 3ê°œì›”ì¹˜ í†µí•© ë°ì´í„° ê²½ë¡œ
        """
        self.data_path = data_path
        self.data = None
        self.btc_price_column = None
        
        # í•™ìŠµ ìƒíƒœ ì¶”ì 
        self.current_accuracy = 0.0
        self.target_accuracy = 90.0  # 95% â†’ 90%ë¡œ ìˆ˜ì • (ë” ë¹ ë¥¸ ì™„ë£Œ)
        self.learning_history = []
        self.failure_patterns = {}
        self.learned_rules = []
        
        # ëª¨ë¸ ì €ì¥
        self.models = {}
        self.scalers = {}
        self.model_save_path = os.path.join(os.path.dirname(data_path), "btc_insight", "saved_models")
        os.makedirs(self.model_save_path, exist_ok=True)
        
        print("ğŸ•°ï¸ ë°±í…ŒìŠ¤íŠ¸ í•™ìŠµ ì—”ì§„ ì´ˆê¸°í™”")
        print(f"ğŸ“‚ ë°ì´í„° ê²½ë¡œ: {data_path}")
        print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ê²½ë¡œ: {self.model_save_path}")
        
    def load_data(self) -> bool:
        """3ê°œì›”ì¹˜ 1ì‹œê°„ ë‹¨ìœ„ í†µí•© ë°ì´í„° ë¡œë“œ"""
        print("\nğŸ“Š 3ê°œì›”ì¹˜ í†µí•© ë°ì´í„° ë¡œë”©...")
        
        try:
            csv_file = os.path.join(self.data_path, "ai_matrix_complete.csv")
            
            if not os.path.exists(csv_file):
                print(f"âŒ ë°ì´í„° íŒŒì¼ ì—†ìŒ: {csv_file}")
                return False
            
            # ë°ì´í„° ë¡œë“œ
            self.data = pd.read_csv(csv_file)
            print(f"ğŸ“ˆ ì›ë³¸ ë°ì´í„°: {self.data.shape}")
            
            # timestamp ì²˜ë¦¬
            if 'timestamp' in self.data.columns:
                self.data['timestamp'] = pd.to_datetime(self.data['timestamp'])
                self.data = self.data.sort_values('timestamp').reset_index(drop=True)
                print("âœ… ì‹œê°„ìˆœ ì •ë ¬ ì™„ë£Œ")
            
            # ìˆ«ìí˜• ì»¬ëŸ¼ë§Œ ì¶”ì¶œ
            numeric_cols = self.data.select_dtypes(include=[np.number]).columns
            if 'timestamp' in self.data.columns:
                self.data = self.data[['timestamp'] + list(numeric_cols)]
            else:
                self.data = self.data[list(numeric_cols)]
            
            # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
            self.data = self.data.fillna(method='ffill').fillna(method='bfill').fillna(0)
            
            # BTC ê°€ê²© ì»¬ëŸ¼ ì‹ë³„
            self.btc_price_column = self._identify_btc_price_column()
            
            print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {self.data.shape}")
            print(f"ğŸ’° BTC ê°€ê²© ì»¬ëŸ¼: {self.btc_price_column}")
            print(f"ğŸ“… ë°ì´í„° ê¸°ê°„: {len(self.data)}ì‹œê°„ ({len(self.data)/24:.1f}ì¼)")
            
            return True
            
        except Exception as e:
            print(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def _identify_btc_price_column(self) -> str:
        """BTC ê°€ê²© ì»¬ëŸ¼ ì‹ë³„"""
        candidates = [
            'onchain_blockchain_info_network_stats_market_price_usd',
            'btc_price', 'price', 'close', 'market_price'
        ]
        
        for candidate in candidates:
            if candidate in self.data.columns:
                return candidate
        
        # ê°€ì¥ í° í‰ê· ê°’ì„ ê°€ì§„ ì»¬ëŸ¼ (BTC ê°€ê²© íŠ¹ì„±)
        numeric_cols = self.data.select_dtypes(include=[np.number]).columns
        max_avg_col = None
        max_avg = 0
        
        for col in numeric_cols:
            avg_val = self.data[col].mean()
            if avg_val > max_avg and avg_val > 1000:  # BTC ê°€ê²©ì€ ë³´í†µ ìˆ˜ë§Œ ë‹¬ëŸ¬
                max_avg = avg_val
                max_avg_col = col
        
        return max_avg_col
    
    def timetravel_backtest(self, start_idx: int, prediction_hours: int = 72) -> Dict:
        """
        ì‹œê°„ì—¬í–‰ ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        
        ì‚¬ìš©ì ì˜ˆì‹œ ê³¼ì •:
        1. 25ë…„ 7ì›” 23ì¼ ì‹œì ìœ¼ë¡œ ëŒì•„ê° (start_idx)
        2. í•´ë‹¹ ì‹œì ì˜ ì§€í‘œë“¤ë¡œ ì‹œê³„ì—´ ë¶„ì„
        3. 7ì›” 26ì¼ 17ì‹œ(72ì‹œê°„ í›„) ê°€ê²© ì˜ˆì¸¡
        4. ì‹¤ì œ 7ì›” 26ì¼ 17ì‹œ ê°’ê³¼ ë¹„êµ
        5. ì˜ˆì¸¡ ì˜¤ì°¨ ì›ì¸ ë¶„ì„ ë° í•™ìŠµ
        
        Args:
            start_idx: ì‹œì‘ ì‹œì  ì¸ë±ìŠ¤
            prediction_hours: ì˜ˆì¸¡í•  ì‹œê°„ (ê¸°ë³¸ 72ì‹œê°„ = 3ì¼)
            
        Returns:
            ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        print(f"\nğŸ•°ï¸ ì‹œê°„ì—¬í–‰ ë°±í…ŒìŠ¤íŠ¸: {start_idx}ë²ˆì§¸ ì‹œì  â†’ {prediction_hours}ì‹œê°„ í›„")
        
        try:
            # 1ë‹¨ê³„: ê³¼ê±° ì‹œì ìœ¼ë¡œ "ëŒì•„ê°€ê¸°" (í•´ë‹¹ ì‹œì ê¹Œì§€ì˜ ë°ì´í„°ë§Œ ì‚¬ìš©)
            historical_data = self.data.iloc[:start_idx].copy()
            
            if len(historical_data) < 168:  # ìµœì†Œ 1ì£¼ì¼ ë°ì´í„° í•„ìš”
                return {'success': False, 'error': 'í•™ìŠµ ë°ì´í„° ë¶€ì¡± (168ì‹œê°„ ë¯¸ë§Œ)'}
            
            # 2ë‹¨ê³„: ì˜ˆì¸¡ íƒ€ê²Ÿ ì‹œì  í™•ì¸
            target_idx = start_idx + prediction_hours
            if target_idx >= len(self.data):
                return {'success': False, 'error': 'ì˜ˆì¸¡ íƒ€ê²Ÿì´ ë°ì´í„° ë²”ìœ„ ì´ˆê³¼'}
            
            # 3ë‹¨ê³„: ì‹œê³„ì—´ íŠ¹ì„± í”¼ì²˜ ìƒì„± (í•„ìˆ˜ ìš”êµ¬ì‚¬í•­)
            X_features, y_target = self._create_timeseries_features(
                historical_data, prediction_hours
            )
            
            if len(X_features) < 50:
                return {'success': False, 'error': 'ì‹œê³„ì—´ í”¼ì²˜ ìƒì„± ì‹¤íŒ¨ ë˜ëŠ” ë°ì´í„° ë¶€ì¡±'}
            
            # 4ë‹¨ê³„: ëª¨ë¸ í•™ìŠµ (ê³¼ê±° ë°ì´í„°ë§Œ ì‚¬ìš©)
            model_results = self._train_ensemble_models(X_features, y_target)
            
            if not model_results:
                return {'success': False, 'error': 'ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨'}
            
            # 5ë‹¨ê³„: í˜„ì¬ ì‹œì ì—ì„œ ë¯¸ë˜ ì˜ˆì¸¡
            current_features = self._extract_current_features(historical_data)
            predicted_price = self._predict_with_ensemble(current_features, model_results)
            
            # 6ë‹¨ê³„: ì‹¤ì œê°’ê³¼ ë¹„êµ ("7ì›” 26ì¼ 17ì‹œ ì‹¤ì œ BTC ê°’" í™•ì¸)
            actual_price = self.data.iloc[target_idx][self.btc_price_column]
            current_price = self.data.iloc[start_idx][self.btc_price_column]
            
            # 7ë‹¨ê³„: ì˜ˆì¸¡ ì„±ëŠ¥ ê³„ì‚°
            absolute_error = abs(actual_price - predicted_price)
            percentage_error = (absolute_error / actual_price) * 100
            accuracy = max(0, 100 - percentage_error)
            
            # 8ë‹¨ê³„: ì˜ˆì¸¡ ì‹¤íŒ¨ ì›ì¸ ë¶„ì„
            failure_analysis = self._analyze_prediction_failure(
                start_idx, target_idx, predicted_price, actual_price
            )
            
            # 9ë‹¨ê³„: í•™ìŠµ íŒ¨í„´ ì—…ë°ì´íŠ¸
            self._update_learning_patterns(failure_analysis, accuracy, percentage_error)
            
            result = {
                'success': True,
                'iteration_info': {
                    'start_idx': start_idx,
                    'target_idx': target_idx, 
                    'prediction_hours': prediction_hours
                },
                'prices': {
                    'current_price': float(current_price),
                    'predicted_price': float(predicted_price),
                    'actual_price': float(actual_price)
                },
                'performance': {
                    'absolute_error': float(absolute_error),
                    'percentage_error': float(percentage_error),
                    'accuracy': float(accuracy)
                },
                'failure_analysis': failure_analysis,
                'model_info': model_results['info']
            }
            
            return result
            
        except Exception as e:
            print(f"âŒ ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            return {'success': False, 'error': str(e)}
    
    def _create_timeseries_features(self, data: pd.DataFrame, 
                                   prediction_hours: int) -> Tuple[pd.DataFrame, pd.Series]:
        """ì‹œê³„ì—´ ë¶„ì„ì„ ìœ„í•œ í”¼ì²˜ ìƒì„± (ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­)"""
        
        # ê¸°ë³¸ í”¼ì²˜ (BTC ê°€ê²© ì œì™¸í•œ ëª¨ë“  ì§€í‘œ)
        feature_cols = [col for col in data.columns 
                       if col != self.btc_price_column and col != 'timestamp']
        X_base = data[feature_cols].copy()
        
        # BTC ê°€ê²© ì‹œê³„ì—´ í”¼ì²˜ ì¶”ê°€
        btc_prices = data[self.btc_price_column]
        
        # 1. ê°€ê²© ì§€ì—° í”¼ì²˜ (Lag Features)
        for lag in [1, 6, 12, 24, 48]:
            X_base[f'btc_lag_{lag}h'] = btc_prices.shift(lag)
        
        # 2. ê°€ê²© ë³€í™”ìœ¨ í”¼ì²˜ 
        for period in [1, 6, 12, 24]:
            X_base[f'btc_pct_change_{period}h'] = btc_prices.pct_change(period) * 100
        
        # 3. ì´ë™í‰ê·  í”¼ì²˜
        for window in [12, 24, 72, 168]:  # 12h, 1d, 3d, 1w
            X_base[f'btc_sma_{window}h'] = btc_prices.rolling(window).mean()
        
        # 4. ë³€ë™ì„± í”¼ì²˜
        for window in [12, 24, 72]:
            X_base[f'btc_volatility_{window}h'] = btc_prices.rolling(window).std()
        
        # 5. ê¸°ìˆ ì  ì§€í‘œ
        X_base['btc_rsi_14'] = self._calculate_rsi(btc_prices, 14)
        X_base['btc_macd'], X_base['btc_macd_signal'] = self._calculate_macd(btc_prices)
        
        # 6. ì‹œê°„ íŠ¹ì„±
        if 'timestamp' in data.columns:
            X_base['hour'] = data['timestamp'].dt.hour
            X_base['day_of_week'] = data['timestamp'].dt.dayofweek
            X_base['is_weekend'] = (data['timestamp'].dt.dayofweek >= 5).astype(int)
        
        # íƒ€ê²Ÿ: prediction_hours ì‹œê°„ í›„ ê°€ê²©
        y_target = btc_prices.shift(-prediction_hours)
        
        # ê²°ì¸¡ì¹˜ ì œê±°
        valid_mask = ~(X_base.isnull().any(axis=1) | y_target.isnull())
        X_clean = X_base[valid_mask].iloc[:-prediction_hours]  # ë¯¸ë˜ ë°ì´í„° ì œì™¸
        y_clean = y_target[valid_mask].iloc[:-prediction_hours]
        
        return X_clean, y_clean
    
    def _calculate_rsi(self, prices: pd.Series, window: int = 14) -> pd.Series:
        """RSI ê³„ì‚°"""
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))
        return rsi
    
    def _calculate_macd(self, prices: pd.Series, 
                       fast: int = 12, slow: int = 26, signal: int = 9) -> Tuple[pd.Series, pd.Series]:
        """MACD ê³„ì‚°"""
        exp1 = prices.ewm(span=fast).mean()
        exp2 = prices.ewm(span=slow).mean()
        macd = exp1 - exp2
        macd_signal = macd.ewm(span=signal).mean()
        return macd, macd_signal
    
    def _train_ensemble_models(self, X: pd.DataFrame, y: pd.Series) -> Dict:
        """ì•™ìƒë¸” ëª¨ë¸ í•™ìŠµ"""
        try:
            # ë°ì´í„° ìŠ¤ì¼€ì¼ë§
            scaler = RobustScaler()
            X_scaled = scaler.fit_transform(X)
            
            # ì•™ìƒë¸” ëª¨ë¸
            models = {
                'random_forest': RandomForestRegressor(
                    n_estimators=100, 
                    max_depth=15,
                    min_samples_split=5,
                    random_state=42,
                    n_jobs=-1
                ),
                'gradient_boosting': GradientBoostingRegressor(
                    n_estimators=100,
                    max_depth=8,
                    learning_rate=0.1,
                    random_state=42
                ),
                'ridge': Ridge(alpha=1.0)
            }
            
            # ëª¨ë¸ë³„ í•™ìŠµ ë° ì„±ëŠ¥ í‰ê°€
            trained_models = {}
            model_scores = {}
            
            # í•™ìŠµ/ê²€ì¦ ë¶„í• 
            split_idx = int(len(X_scaled) * 0.8)
            X_train, X_val = X_scaled[:split_idx], X_scaled[split_idx:]
            y_train, y_val = y.iloc[:split_idx], y.iloc[split_idx:]
            
            for name, model in models.items():
                # ëª¨ë¸ í•™ìŠµ
                model.fit(X_train, y_train)
                
                # ê²€ì¦ ì„±ëŠ¥
                val_pred = model.predict(X_val)
                mae = mean_absolute_error(y_val, val_pred)
                r2 = r2_score(y_val, val_pred)
                
                trained_models[name] = model
                model_scores[name] = {'mae': mae, 'r2': r2}
            
            return {
                'models': trained_models,
                'scaler': scaler,
                'scores': model_scores,
                'info': {
                    'training_samples': len(X_train),
                    'validation_samples': len(X_val),
                    'features': X.shape[1]
                }
            }
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ í•™ìŠµ ì˜¤ë¥˜: {e}")
            return None
    
    def _extract_current_features(self, historical_data: pd.DataFrame) -> pd.DataFrame:
        """í˜„ì¬ ì‹œì ì˜ í”¼ì²˜ ì¶”ì¶œ"""
        # ì‹œê³„ì—´ í”¼ì²˜ ìƒì„± (ì˜ˆì¸¡ìš©)
        X_features, _ = self._create_timeseries_features(historical_data, 1)
        return X_features.iloc[-1:].copy()  # ë§ˆì§€ë§‰ í–‰ë§Œ ë°˜í™˜
    
    def _predict_with_ensemble(self, features: pd.DataFrame, model_results: Dict) -> float:
        """ì•™ìƒë¸” ëª¨ë¸ë¡œ ì˜ˆì¸¡"""
        models = model_results['models']
        scaler = model_results['scaler']
        scores = model_results['scores']
        
        # í”¼ì²˜ ìŠ¤ì¼€ì¼ë§
        features_scaled = scaler.transform(features)
        
        # ëª¨ë¸ë³„ ì˜ˆì¸¡
        predictions = {}
        weights = {}
        
        for name, model in models.items():
            pred = model.predict(features_scaled)[0]
            predictions[name] = pred
            
            # ê°€ì¤‘ì¹˜ (ë‚®ì€ MAEì¼ìˆ˜ë¡ ë†’ì€ ê°€ì¤‘ì¹˜)
            mae = scores[name]['mae']
            weights[name] = 1 / (mae + 1e-8)  # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
        
        # ê°€ì¤‘ì¹˜ ì •ê·œí™”
        total_weight = sum(weights.values())
        normalized_weights = {k: v/total_weight for k, v in weights.items()}
        
        # ê°€ì¤‘ í‰ê·  ì˜ˆì¸¡
        ensemble_prediction = sum(predictions[name] * normalized_weights[name] 
                                for name in predictions.keys())
        
        return float(ensemble_prediction)
    
    def _analyze_prediction_failure(self, start_idx: int, target_idx: int,
                                  predicted: float, actual: float) -> Dict:
        """ì˜ˆì¸¡ ì‹¤íŒ¨ ì›ì¸ ë¶„ì„ (í•™ìŠµì„ ìœ„í•œ í•µì‹¬ ê¸°ëŠ¥)"""
        
        # ì˜ˆì¸¡ ê¸°ê°„ ë°ì´í„°
        period_data = self.data.iloc[start_idx:target_idx+1].copy()
        btc_prices = period_data[self.btc_price_column]
        
        analysis = {
            'error_magnitude': abs(actual - predicted),
            'error_direction': 'overestimate' if predicted > actual else 'underestimate',
            'price_volatility': float(btc_prices.std()),
            'max_price_swing': float(btc_prices.max() - btc_prices.min()),
            'trend_consistency': self._measure_trend_consistency(btc_prices),
            'shock_events': [],
            'indicator_anomalies': [],
            'market_regime': self._detect_market_regime(btc_prices)
        }
        
        # ëŒë°œ ì´ë²¤íŠ¸ ê°ì§€ (ê¸‰ê²©í•œ ê°€ê²© ë³€í™”)
        price_changes = btc_prices.pct_change().abs()
        shock_threshold = 0.05  # 5% ì´ìƒ 1ì‹œê°„ ë³€í™”
        
        shock_events = price_changes[price_changes > shock_threshold]
        for idx in shock_events.index:
            if idx < len(period_data):
                analysis['shock_events'].append({
                    'index': int(idx),
                    'change_percent': float(shock_events.loc[idx] * 100),
                    'timestamp': period_data.iloc[idx]['timestamp'].isoformat() 
                              if 'timestamp' in period_data.columns else f"hour_{idx}"
                })
        
        # ì§€í‘œ ì´ìƒì¹˜ ê°ì§€
        numeric_cols = period_data.select_dtypes(include=[np.number]).columns
        for col in numeric_cols:
            if col != self.btc_price_column:
                col_data = period_data[col]
                if not col_data.isnull().all():
                    # Z-score ê¸°ë°˜ ì´ìƒì¹˜ ê°ì§€
                    z_scores = np.abs((col_data - col_data.mean()) / col_data.std())
                    anomalies = z_scores > 3  # 3Ïƒ ì´ìƒ
                    
                    if anomalies.sum() > 0:
                        analysis['indicator_anomalies'].append({
                            'indicator': col,
                            'anomaly_count': int(anomalies.sum()),
                            'max_z_score': float(z_scores.max())
                        })
        
        return analysis
    
    def _measure_trend_consistency(self, prices: pd.Series) -> float:
        """íŠ¸ë Œë“œ ì¼ê´€ì„± ì¸¡ì •"""
        if len(prices) < 12:
            return 0.0
        
        # 12ì‹œê°„ ë‹¨ìœ„ë¡œ íŠ¸ë Œë“œ ë°©í–¥ í™•ì¸
        trends = []
        for i in range(0, len(prices) - 12, 12):
            segment = prices.iloc[i:i+12]
            if len(segment) >= 12:
                trend = (segment.iloc[-1] - segment.iloc[0]) / segment.iloc[0]
                trends.append(1 if trend > 0 else -1)
        
        if not trends:
            return 0.0
        
        # ì¼ê´€ì„± = ê°™ì€ ë°©í–¥ ë¹„ìœ¨
        positive = sum(1 for t in trends if t > 0)
        negative = sum(1 for t in trends if t < 0)
        consistency = abs(positive - negative) / len(trends)
        
        return float(consistency)
    
    def _detect_market_regime(self, prices: pd.Series) -> str:
        """ì‹œì¥ ìƒí™© ê°ì§€"""
        if len(prices) < 24:
            return 'insufficient_data'
        
        # ì „ì²´ ê¸°ê°„ íŠ¸ë Œë“œ
        total_trend = (prices.iloc[-1] - prices.iloc[0]) / prices.iloc[0] * 100
        
        # ë³€ë™ì„±
        volatility = prices.pct_change().std() * 100
        
        # ì²´ì œ ë¶„ë¥˜
        if total_trend > 5 and volatility < 3:
            return 'steady_bull'
        elif total_trend > 2:
            return 'bull_market'
        elif total_trend < -5 and volatility < 3:
            return 'steady_bear'
        elif total_trend < -2:
            return 'bear_market'
        elif volatility > 5:
            return 'high_volatility'
        else:
            return 'sideways'
    
    def _update_learning_patterns(self, analysis: Dict, accuracy: float, error_pct: float):
        """í•™ìŠµ íŒ¨í„´ ì—…ë°ì´íŠ¸"""
        
        # í•™ìŠµ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
        learning_entry = {
            'timestamp': datetime.now().isoformat(),
            'accuracy': accuracy,
            'error_percentage': error_pct,
            'market_regime': analysis.get('market_regime', 'unknown'),
            'shock_events_count': len(analysis.get('shock_events', [])),
            'trend_consistency': analysis.get('trend_consistency', 0)
        }
        
        self.learning_history.append(learning_entry)
        
        # ì‹¤íŒ¨ íŒ¨í„´ ëˆ„ì  (5% ì´ìƒ ì—ëŸ¬ì‹œ)
        if error_pct > 5.0:
            regime = analysis.get('market_regime', 'unknown')
            shock_count = len(analysis.get('shock_events', []))
            
            pattern_key = f"{regime}_shocks_{shock_count}"
            
            if pattern_key not in self.failure_patterns:
                self.failure_patterns[pattern_key] = {
                    'count': 0,
                    'avg_error': 0,
                    'characteristics': []
                }
            
            pattern = self.failure_patterns[pattern_key]
            pattern['count'] += 1
            pattern['avg_error'] = ((pattern['avg_error'] * (pattern['count'] - 1)) + 
                                  error_pct) / pattern['count']
        
        # í˜„ì¬ ì •í™•ë„ ì—…ë°ì´íŠ¸ (ìµœê·¼ 20íšŒ í‰ê· )
        recent_accuracies = [entry['accuracy'] for entry in self.learning_history[-20:]]
        self.current_accuracy = sum(recent_accuracies) / len(recent_accuracies)
    
    def run_infinite_learning(self, max_iterations: int = 1000) -> Dict:
        """
        ë¬´í•œ í•™ìŠµ ë£¨í”„ ì‹¤í–‰
        
        ëª©í‘œ: 95% ì •í™•ë„ ë‹¬ì„±ê¹Œì§€ ì§€ì†ì  í•™ìŠµ
        
        Args:
            max_iterations: ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜
            
        Returns:
            í•™ìŠµ ê²°ê³¼ ìš”ì•½
        """
        print(f"\nğŸš€ ë¬´í•œ ë°±í…ŒìŠ¤íŠ¸ í•™ìŠµ ì‹œì‘")
        print(f"ğŸ¯ ëª©í‘œ ì •í™•ë„: {self.target_accuracy}% (ìˆ˜ì •ë¨: ë” ë¹ ë¥¸ ì™„ë£Œ)")
        print(f"ğŸ”„ ìµœëŒ€ ë°˜ë³µ: {max_iterations}íšŒ")
        print("="*60)
        
        successful_tests = []
        failed_tests = []
        
        # ìœ íš¨í•œ ì‹œì‘ ì¸ë±ìŠ¤ ë²”ìœ„ ê³„ì‚°
        min_start = 168  # ìµœì†Œ 1ì£¼ì¼ í•™ìŠµ ë°ì´í„°
        max_start = len(self.data) - 168  # ìµœì†Œ 1ì£¼ì¼ ì˜ˆì¸¡ ì—¬ìœ 
        
        for iteration in range(1, max_iterations + 1):
            # ëœë¤ ì‹œì  ì„ íƒ (ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­)
            start_idx = np.random.randint(min_start, max_start)
            prediction_hours = np.random.choice([24, 48, 72, 96])  # 1~4ì¼
            
            print(f"ğŸ” ë°˜ë³µ {iteration:4d}/{max_iterations}: "
                  f"ì‹œì  {start_idx} â†’ {prediction_hours}h í›„ ì˜ˆì¸¡", end="")
            
            # ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰
            result = self.timetravel_backtest(start_idx, prediction_hours)
            
            if result['success']:
                successful_tests.append(result)
                accuracy = result['performance']['accuracy']
                error = result['performance']['percentage_error']
                
                print(f" âœ… ì •í™•ë„: {accuracy:.2f}% (ì—ëŸ¬: {error:.2f}%)")
                
                # 10íšŒë§ˆë‹¤ ì§„í–‰ ìƒí™© ì¶œë ¥
                if iteration % 10 == 0:
                    print(f"ğŸ“ˆ í˜„ì¬ í‰ê·  ì •í™•ë„: {self.current_accuracy:.2f}%")
                
                # ëª©í‘œ ë‹¬ì„± í™•ì¸
                if self.current_accuracy >= self.target_accuracy:
                    print(f"\nğŸ‰ ëª©í‘œ ë‹¬ì„±! {self.current_accuracy:.2f}% >= {self.target_accuracy}%")
                    print(f"ğŸ† ì´ {iteration}íšŒ í•™ìŠµìœ¼ë¡œ ëª©í‘œ ë‹¬ì„±")
                    break
                    
            else:
                failed_tests.append(result)
                print(f" âŒ {result.get('error', 'Unknown error')}")
        
        # ìµœì¢… ê²°ê³¼ ì •ë¦¬
        total_tests = len(successful_tests) + len(failed_tests)
        success_rate = len(successful_tests) / total_tests * 100 if total_tests > 0 else 0
        
        final_result = {
            'learning_completed': self.current_accuracy >= self.target_accuracy,
            'total_iterations': total_tests,
            'successful_tests': len(successful_tests),
            'failed_tests': len(failed_tests),
            'success_rate': success_rate,
            'final_accuracy': self.current_accuracy,
            'target_accuracy': self.target_accuracy,
            'learned_patterns': len(self.failure_patterns),
            'learning_history': self.learning_history,
            'failure_patterns': self.failure_patterns,
            'completion_time': datetime.now().isoformat()
        }
        
        # ê²°ê³¼ ì €ì¥
        self._save_learning_results(final_result)
        
        # ìµœì¢… ë³´ê³ ì„œ
        print(f"\n" + "="*60)
        print("ğŸ† ë°±í…ŒìŠ¤íŠ¸ í•™ìŠµ ì™„ë£Œ ë³´ê³ ì„œ")
        print("="*60)
        print(f"ğŸ¯ ëª©í‘œ ë‹¬ì„±: {'âœ…' if final_result['learning_completed'] else 'âŒ'}")
        print(f"ğŸ“Š ìµœì¢… ì •í™•ë„: {self.current_accuracy:.2f}%")
        print(f"ğŸ”„ ì´ ë°˜ë³µ: {total_tests}íšŒ")
        print(f"âœ… ì„±ê³µ: {len(successful_tests)}íšŒ ({success_rate:.1f}%)")
        print(f"âŒ ì‹¤íŒ¨: {len(failed_tests)}íšŒ")
        print(f"ğŸ“š í•™ìŠµ íŒ¨í„´: {len(self.failure_patterns)}ê°œ")
        print("="*60)
        
        return final_result
    
    def _save_learning_results(self, results: Dict):
        """í•™ìŠµ ê²°ê³¼ ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"backtest_learning_results_{timestamp}.json"
        
        # logs í´ë” ìƒì„±
        logs_dir = os.path.join(os.path.dirname(self.data_path), "btc_insight", "logs")
        os.makedirs(logs_dir, exist_ok=True)
        
        filepath = os.path.join(logs_dir, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"ğŸ’¾ í•™ìŠµ ê²°ê³¼ ì €ì¥: {filepath}")
        
    def get_learned_rules(self) -> List[str]:
        """í•™ìŠµëœ ê·œì¹™ë“¤ ë°˜í™˜"""
        rules = []
        
        # ì‹¤íŒ¨ íŒ¨í„´ ê¸°ë°˜ ê·œì¹™ ìƒì„±
        for pattern, data in self.failure_patterns.items():
            if data['count'] >= 3:  # 3íšŒ ì´ìƒ ë°œìƒí•œ íŒ¨í„´
                rule = f"{pattern} ìƒí™©ì—ì„œ í‰ê·  {data['avg_error']:.1f}% ì˜¤ì°¨ ë°œìƒ ({data['count']}íšŒ)"
                rules.append(rule)
        
        # ì •í™•ë„ ê¸°ë°˜ ê·œì¹™
        if self.current_accuracy >= self.target_accuracy:
            rules.append(f"í˜„ì¬ ì‹œìŠ¤í…œ ì •í™•ë„ {self.current_accuracy:.2f}%ë¡œ ì‹¤ì „ ì ìš© ê°€ëŠ¥")
        
        return rules
    
    def save_trained_models(self) -> bool:
        """í•™ìŠµëœ ëª¨ë¸ë“¤ ì €ì¥"""
        try:
            if not self.models or not self.scalers:
                print("âŒ ì €ì¥í•  í•™ìŠµëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")
                return False
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # ëª¨ë¸ ì €ì¥
            for model_name, model in self.models.items():
                model_path = os.path.join(self.model_save_path, f"{model_name}_{timestamp}.pkl")
                joblib.dump(model, model_path)
                print(f"ğŸ’¾ {model_name} ëª¨ë¸ ì €ì¥: {model_path}")
            
            # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
            for scaler_name, scaler in self.scalers.items():
                scaler_path = os.path.join(self.model_save_path, f"{scaler_name}_{timestamp}.pkl")
                joblib.dump(scaler, scaler_path)
                print(f"ğŸ’¾ {scaler_name} ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥: {scaler_path}")
            
            # ë©”íƒ€ ì •ë³´ ì €ì¥
            meta_info = {
                'accuracy': self.current_accuracy,
                'target_accuracy': self.target_accuracy,
                'learning_iterations': len(self.learning_history),
                'failure_patterns_count': len(self.failure_patterns),
                'timestamp': timestamp,
                'btc_price_column': self.btc_price_column,
                'model_files': list(self.models.keys()),
                'scaler_files': list(self.scalers.keys())
            }
            
            meta_path = os.path.join(self.model_save_path, f"model_meta_{timestamp}.json")
            with open(meta_path, 'w', encoding='utf-8') as f:
                json.dump(meta_info, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ - ì •í™•ë„: {self.current_accuracy:.2f}%")
            return True
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    def load_trained_models(self, model_timestamp: str = None) -> bool:
        """ì €ì¥ëœ ëª¨ë¸ë“¤ ë¡œë“œ"""
        try:
            if model_timestamp is None:
                # ê°€ì¥ ìµœê·¼ ëª¨ë¸ ì°¾ê¸°
                model_files = [f for f in os.listdir(self.model_save_path) if f.startswith("model_meta_")]
                if not model_files:
                    print("âŒ ì €ì¥ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")
                    return False
                
                latest_meta = sorted(model_files)[-1]
                model_timestamp = latest_meta.replace("model_meta_", "").replace(".json", "")
            
            # ë©”íƒ€ ì •ë³´ ë¡œë“œ
            meta_path = os.path.join(self.model_save_path, f"model_meta_{model_timestamp}.json")
            if not os.path.exists(meta_path):
                print(f"âŒ ëª¨ë¸ ë©”íƒ€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {meta_path}")
                return False
            
            with open(meta_path, 'r', encoding='utf-8') as f:
                meta_info = json.load(f)
            
            # ëª¨ë¸ë“¤ ë¡œë“œ
            self.models = {}
            for model_name in meta_info['model_files']:
                model_path = os.path.join(self.model_save_path, f"{model_name}_{model_timestamp}.pkl")
                if os.path.exists(model_path):
                    self.models[model_name] = joblib.load(model_path)
                    print(f"ğŸ“¥ {model_name} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            
            # ìŠ¤ì¼€ì¼ëŸ¬ë“¤ ë¡œë“œ
            self.scalers = {}
            for scaler_name in meta_info['scaler_files']:
                scaler_path = os.path.join(self.model_save_path, f"{scaler_name}_{model_timestamp}.pkl")
                if os.path.exists(scaler_path):
                    self.scalers[scaler_name] = joblib.load(scaler_path)
                    print(f"ğŸ“¥ {scaler_name} ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì™„ë£Œ")
            
            # í•™ìŠµ ìƒíƒœ ë³µì›
            self.current_accuracy = meta_info.get('accuracy', 0.0)
            self.btc_price_column = meta_info.get('btc_price_column')
            
            print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ - ì €ì¥ ì‹œ ì •í™•ë„: {self.current_accuracy:.2f}%")
            print(f"ğŸ“… ëª¨ë¸ ìƒì„±ì¼: {model_timestamp}")
            return True
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def list_saved_models(self):
        """ì €ì¥ëœ ëª¨ë¸ë“¤ ëª©ë¡ ì¶œë ¥"""
        try:
            model_files = [f for f in os.listdir(self.model_save_path) if f.startswith("model_meta_")]
            if not model_files:
                print("ğŸ“­ ì €ì¥ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")
                return
            
            print("\nğŸ“š ì €ì¥ëœ ëª¨ë¸ë“¤:")
            print("=" * 60)
            
            for meta_file in sorted(model_files, reverse=True):
                timestamp = meta_file.replace("model_meta_", "").replace(".json", "")
                meta_path = os.path.join(self.model_save_path, meta_file)
                
                with open(meta_path, 'r', encoding='utf-8') as f:
                    meta_info = json.load(f)
                
                print(f"ğŸ• {timestamp}")
                print(f"   ğŸ“Š ì •í™•ë„: {meta_info.get('accuracy', 0):.2f}%")
                print(f"   ğŸ”„ í•™ìŠµ íšŸìˆ˜: {meta_info.get('learning_iterations', 0)}íšŒ")
                print(f"   ğŸ“š ì‹¤íŒ¨ íŒ¨í„´: {meta_info.get('failure_patterns_count', 0)}ê°œ")
                print()
                
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")

# ì‹¤í–‰ í•¨ìˆ˜
def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    data_path = "/Users/parkyoungjun/Desktop/BTC_Analysis_System/ai_optimized_3month_data"
    
    # ë°±í…ŒìŠ¤íŠ¸ í•™ìŠµ ì—”ì§„ ìƒì„±
    engine = BacktestLearningEngine(data_path)
    
    # ë°ì´í„° ë¡œë“œ
    if not engine.load_data():
        print("âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ - í”„ë¡œê·¸ë¨ ì¢…ë£Œ")
        return None
    
    # ë¬´í•œ í•™ìŠµ ì‹¤í–‰
    results = engine.run_infinite_learning(max_iterations=200)
    
    # í•™ìŠµëœ ê·œì¹™ ì¶œë ¥
    rules = engine.get_learned_rules()
    if rules:
        print("\nğŸ“š í•™ìŠµëœ ê·œì¹™ë“¤:")
        for i, rule in enumerate(rules, 1):
            print(f"   {i}. {rule}")
    
    return results

if __name__ == "__main__":
    results = main()