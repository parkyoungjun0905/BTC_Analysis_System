#!/usr/bin/env python3
"""
ğŸ¯ ë¦¬ìŠ¤í¬ ì¡°ì • ì„±ê³¼ ì§€í‘œ ì‹œìŠ¤í…œ
- ì¢…í•©ì ì¸ ë¦¬ìŠ¤í¬ ì¸¡ì • (ìƒ¤í”„, ì†Œë¥´í‹°ë…¸, ì¹¼ë§ˆ ë¹„ìœ¨)
- ê³ ê¸‰ VaR ë° CVaR ê³„ì‚° (ë‹¤ì–‘í•œ ë°©ë²•ë¡ )
- ë“œë¡œë‹¤ìš´ ë¶„ì„ (ìµœëŒ€, í‰ê· , ì§€ì†ê¸°ê°„)
- í…Œì¼ ë¦¬ìŠ¤í¬ ë¶„ì„ (ê·¹ê°’ ì´ë¡  ì ìš©)
- ë¦¬ìŠ¤í¬ ê¸°ì—¬ë„ ë¶„ì„ (í¬íŠ¸í´ë¦¬ì˜¤ ê´€ì )
- ì‹œì¥ ìƒí™©ë³„ ë¦¬ìŠ¤í¬ ì¡°ì • ì„±ê³¼ ì¸¡ì •
- ë™ì  ë¦¬ìŠ¤í¬ ì§€í‘œ (ì‹œê°„ ë³€í™” ì¶”ì )
"""

import numpy as np
import pandas as pd
import warnings
from datetime import datetime, timedelta
import json
import os
from typing import Dict, List, Tuple, Optional, Union
from dataclasses import dataclass
import logging

# í†µê³„ ë° ìˆ˜ì¹˜ ê³„ì‚°
from scipy import stats
from scipy.optimize import minimize
from sklearn.preprocessing import StandardScaler
from sklearn.covariance import EmpiricalCovariance, LedoitWolf

# ì‹œê°í™”
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.express as px

# ê·¹ê°’ ì´ë¡  (ì˜µì…”ë„)
try:
    from scipy.stats import genpareto, genextreme
    EXTREME_VALUE_AVAILABLE = True
except ImportError:
    EXTREME_VALUE_AVAILABLE = False

warnings.filterwarnings('ignore')

@dataclass
class RiskConfig:
    """ë¦¬ìŠ¤í¬ ì¸¡ì • ì„¤ì •"""
    # ê¸°ë³¸ ì„¤ì •
    risk_free_rate: float = 0.02        # ì—°ê°„ ë¬´ìœ„í—˜ ìˆ˜ìµë¥  (2%)
    confidence_levels: List[float] = None  # VaR ì‹ ë¢°ìˆ˜ì¤€ë“¤
    
    # ê³„ì‚° ê¸°ê°„
    lookback_periods: List[int] = None  # ë¡¤ë§ ê³„ì‚° ê¸°ê°„ë“¤
    min_periods: int = 30               # ìµœì†Œ ê³„ì‚° ê¸°ê°„
    
    # VaR ë°©ë²•ë¡ 
    var_methods: List[str] = None       # VaR ê³„ì‚° ë°©ë²•ë“¤
    
    # ê·¹ê°’ ë¶„ì„
    extreme_percentile: float = 0.05    # ê·¹ê°’ ë¶„ì„ ì„ê³„ê°’ (5%)
    tail_threshold: float = 0.95        # í…Œì¼ ë¦¬ìŠ¤í¬ ì„ê³„ê°’
    
    # ë“œë¡œë‹¤ìš´ ë¶„ì„
    dd_recovery_threshold: float = 0.9  # íšŒë³µ ì„ê³„ê°’ (90%)
    
    def __post_init__(self):
        if self.confidence_levels is None:
            self.confidence_levels = [0.90, 0.95, 0.99]
        if self.lookback_periods is None:
            self.lookback_periods = [30, 60, 120, 252]  # 1ê°œì›”, 2ê°œì›”, 4ê°œì›”, 1ë…„
        if self.var_methods is None:
            self.var_methods = ['historical', 'parametric', 'monte_carlo', 'extreme_value']

class RiskAdjustedMetricsSystem:
    """ë¦¬ìŠ¤í¬ ì¡°ì • ì„±ê³¼ ì§€í‘œ ì‹œìŠ¤í…œ"""
    
    def __init__(self, config: RiskConfig = None):
        self.config = config or RiskConfig()
        self.data_path = "/Users/parkyoungjun/Desktop/BTC_Analysis_System"
        
        # ê³„ì‚° ê²°ê³¼ ì €ì¥
        self.risk_metrics_history = {}
        self.drawdown_analysis = {}
        self.var_estimates = {}
        self.tail_risk_measures = {}
        
        # ë¡œê¹… ì„¤ì •
        self.setup_logging()
        
    def setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(os.path.join(self.data_path, 'risk_metrics.log'), encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def calculate_comprehensive_risk_metrics(self, returns: Union[pd.Series, np.ndarray], 
                                           portfolio_values: Union[pd.Series, np.ndarray] = None,
                                           prices: Union[pd.Series, np.ndarray] = None) -> Dict:
        """ì¢…í•©ì ì¸ ë¦¬ìŠ¤í¬ ì§€í‘œ ê³„ì‚°"""
        self.logger.info("ğŸ“Š ì¢…í•©ì ì¸ ë¦¬ìŠ¤í¬ ì§€í‘œ ê³„ì‚° ì‹œì‘...")
        
        # ë°ì´í„° ì „ì²˜ë¦¬
        if isinstance(returns, np.ndarray):
            returns = pd.Series(returns)
        
        if portfolio_values is not None and isinstance(portfolio_values, np.ndarray):
            portfolio_values = pd.Series(portfolio_values)
        
        # ê¸°ë³¸ í†µê³„ëŸ‰
        basic_stats = self._calculate_basic_statistics(returns)
        
        # ë¦¬ìŠ¤í¬ ì¡°ì • ë¹„ìœ¨ë“¤
        risk_ratios = self._calculate_risk_ratios(returns, portfolio_values)
        
        # VaR ë° CVaR (ë‹¤ì–‘í•œ ë°©ë²•ë¡ )
        var_metrics = self._calculate_var_metrics(returns)
        
        # ë“œë¡œë‹¤ìš´ ë¶„ì„
        drawdown_metrics = self._calculate_drawdown_metrics(portfolio_values or returns.cumsum())
        
        # í…Œì¼ ë¦¬ìŠ¤í¬ ë¶„ì„
        tail_risk = self._calculate_tail_risk_measures(returns)
        
        # ê³ ì°¨ ëª¨ë©˜íŠ¸ ë¶„ì„
        higher_moments = self._calculate_higher_moments(returns)
        
        # ë™ì  ë¦¬ìŠ¤í¬ ì§€í‘œ
        dynamic_risk = self._calculate_dynamic_risk_metrics(returns)
        
        comprehensive_metrics = {
            'calculation_date': datetime.now().isoformat(),
            'data_period': {
                'start': returns.index[0] if hasattr(returns, 'index') else 'N/A',
                'end': returns.index[-1] if hasattr(returns, 'index') else 'N/A',
                'observations': len(returns)
            },
            'basic_statistics': basic_stats,
            'risk_adjusted_ratios': risk_ratios,
            'var_metrics': var_metrics,
            'drawdown_analysis': drawdown_metrics,
            'tail_risk_measures': tail_risk,
            'higher_moments': higher_moments,
            'dynamic_risk_metrics': dynamic_risk
        }
        
        return comprehensive_metrics
    
    def _calculate_basic_statistics(self, returns: pd.Series) -> Dict:
        """ê¸°ë³¸ í†µê³„ëŸ‰ ê³„ì‚°"""
        annual_factor = 252  # 1ë…„ = 252 ê±°ë˜ì¼
        
        return {
            'mean_return': float(returns.mean()),
            'annual_return': float(returns.mean() * annual_factor),
            'volatility': float(returns.std()),
            'annual_volatility': float(returns.std() * np.sqrt(annual_factor)),
            'skewness': float(stats.skew(returns.dropna())),
            'kurtosis': float(stats.kurtosis(returns.dropna())),
            'min_return': float(returns.min()),
            'max_return': float(returns.max()),
            'positive_returns_ratio': float((returns > 0).mean()),
            'jarque_bera_test': self._jarque_bera_test(returns)
        }
    
    def _jarque_bera_test(self, returns: pd.Series) -> Dict:
        """ì •ê·œì„± ê²€ì • (Jarque-Bera)"""
        clean_returns = returns.dropna()
        if len(clean_returns) < 20:
            return {'statistic': None, 'p_value': None, 'is_normal': None}
        
        jb_stat, jb_pvalue = stats.jarque_bera(clean_returns)
        return {
            'statistic': float(jb_stat),
            'p_value': float(jb_pvalue),
            'is_normal': bool(jb_pvalue > 0.05)
        }
    
    def _calculate_risk_ratios(self, returns: pd.Series, portfolio_values: pd.Series = None) -> Dict:
        """ë¦¬ìŠ¤í¬ ì¡°ì • ë¹„ìœ¨ ê³„ì‚°"""
        annual_factor = 252
        risk_free_daily = self.config.risk_free_rate / annual_factor
        
        # ê¸°ë³¸ ì§€í‘œë“¤
        excess_returns = returns - risk_free_daily
        mean_excess = excess_returns.mean()
        volatility = returns.std()
        
        # ìƒ¤í”„ ë¹„ìœ¨
        sharpe_ratio = (mean_excess * annual_factor) / (volatility * np.sqrt(annual_factor)) if volatility > 0 else 0
        
        # ì†Œë¥´í‹°ë…¸ ë¹„ìœ¨
        downside_returns = returns[returns < risk_free_daily]
        downside_std = downside_returns.std() if len(downside_returns) > 0 else volatility
        sortino_ratio = (mean_excess * annual_factor) / (downside_std * np.sqrt(annual_factor)) if downside_std > 0 else 0
        
        # ì¹¼ë§ˆ ë¹„ìœ¨
        calmar_ratio = 0
        if portfolio_values is not None:
            max_dd = self._calculate_max_drawdown(portfolio_values)['max_drawdown_pct']
            annual_return = mean_excess * annual_factor
            calmar_ratio = annual_return / abs(max_dd) if max_dd != 0 else 0
        
        # ì˜¤ë©”ê°€ ë¹„ìœ¨
        omega_ratio = self._calculate_omega_ratio(returns, risk_free_daily)
        
        # ì •ë³´ ë¹„ìœ¨ (ë²¤ì¹˜ë§ˆí¬ ëŒ€ë¹„)
        information_ratio = self._calculate_information_ratio(returns)
        
        # Treynor ë¹„ìœ¨ (ì‹œì¥ ë² íƒ€ í•„ìš”ì‹œ)
        treynor_ratio = self._calculate_treynor_ratio(returns)
        
        # ìµœëŒ€ ì†ì‹¤ ëŒ€ë¹„ ìˆ˜ìµë¥ 
        max_loss = returns.min()
        gain_to_pain_ratio = mean_excess / abs(max_loss) if max_loss < 0 else float('inf')
        
        return {
            'sharpe_ratio': float(sharpe_ratio),
            'sortino_ratio': float(sortino_ratio),
            'calmar_ratio': float(calmar_ratio),
            'omega_ratio': float(omega_ratio),
            'information_ratio': float(information_ratio),
            'treynor_ratio': float(treynor_ratio),
            'gain_to_pain_ratio': float(gain_to_pain_ratio),
            'return_to_var_ratio': self._calculate_return_to_var_ratio(returns)
        }
    
    def _calculate_omega_ratio(self, returns: pd.Series, threshold: float = 0) -> float:
        """ì˜¤ë©”ê°€ ë¹„ìœ¨ ê³„ì‚°"""
        gains = returns[returns > threshold] - threshold
        losses = threshold - returns[returns <= threshold]
        
        total_gains = gains.sum() if len(gains) > 0 else 0
        total_losses = losses.sum() if len(losses) > 0 else 1e-8  # 0 ë°©ì§€
        
        return total_gains / total_losses
    
    def _calculate_information_ratio(self, returns: pd.Series) -> float:
        """ì •ë³´ ë¹„ìœ¨ ê³„ì‚° (ë‹¨ìˆœ ë²¤ì¹˜ë§ˆí¬ ëŒ€ë¹„)"""
        # ë²¤ì¹˜ë§ˆí¬ë¥¼ 0ìœ¼ë¡œ ê°€ì • (ì ˆëŒ€ ìˆ˜ìµë¥  ê¸°ì¤€)
        benchmark_return = 0
        active_return = returns.mean() - benchmark_return
        tracking_error = returns.std()
        
        return (active_return * 252) / (tracking_error * np.sqrt(252)) if tracking_error > 0 else 0
    
    def _calculate_treynor_ratio(self, returns: pd.Series) -> float:
        """íŠ¸ë ˆì´ë„ˆ ë¹„ìœ¨ ê³„ì‚° (ë² íƒ€=1 ê°€ì •)"""
        risk_free_daily = self.config.risk_free_rate / 252
        excess_return = (returns.mean() - risk_free_daily) * 252
        beta = 1.0  # ì‹œì¥ ë² íƒ€ë¥¼ 1ë¡œ ê°€ì •
        
        return excess_return / beta
    
    def _calculate_return_to_var_ratio(self, returns: pd.Series) -> Dict:
        """ìˆ˜ìµë¥  ëŒ€ VaR ë¹„ìœ¨"""
        annual_return = returns.mean() * 252
        var_95 = self._calculate_historical_var(returns, 0.95)
        var_99 = self._calculate_historical_var(returns, 0.99)
        
        return {
            'return_to_var_95': float(annual_return / abs(var_95)) if var_95 != 0 else 0,
            'return_to_var_99': float(annual_return / abs(var_99)) if var_99 != 0 else 0
        }
    
    def _calculate_var_metrics(self, returns: pd.Series) -> Dict:
        """VaR ë° CVaR ê³„ì‚° (ë‹¤ì–‘í•œ ë°©ë²•ë¡ )"""
        var_results = {}
        
        for confidence_level in self.config.confidence_levels:
            level_results = {}
            
            # 1. ì—­ì‚¬ì  VaR
            if 'historical' in self.config.var_methods:
                level_results['historical_var'] = self._calculate_historical_var(returns, confidence_level)
                level_results['historical_cvar'] = self._calculate_historical_cvar(returns, confidence_level)
            
            # 2. íŒŒë¼ë¯¸í„°ì  VaR (ì •ê·œë¶„í¬ ê°€ì •)
            if 'parametric' in self.config.var_methods:
                level_results['parametric_var'] = self._calculate_parametric_var(returns, confidence_level)
                level_results['parametric_cvar'] = self._calculate_parametric_cvar(returns, confidence_level)
            
            # 3. ëª¬í…Œì¹´ë¥¼ë¡œ VaR
            if 'monte_carlo' in self.config.var_methods:
                mc_results = self._calculate_monte_carlo_var(returns, confidence_level)
                level_results.update(mc_results)
            
            # 4. ê·¹ê°’ ì´ë¡  VaR
            if 'extreme_value' in self.config.var_methods and EXTREME_VALUE_AVAILABLE:
                ev_results = self._calculate_extreme_value_var(returns, confidence_level)
                level_results.update(ev_results)
            
            var_results[f'confidence_{int(confidence_level*100)}'] = level_results
        
        return var_results
    
    def _calculate_historical_var(self, returns: pd.Series, confidence_level: float) -> float:
        """ì—­ì‚¬ì  VaR ê³„ì‚°"""
        clean_returns = returns.dropna()
        if len(clean_returns) == 0:
            return 0.0
        return float(np.percentile(clean_returns, (1 - confidence_level) * 100))
    
    def _calculate_historical_cvar(self, returns: pd.Series, confidence_level: float) -> float:
        """ì—­ì‚¬ì  CVaR ê³„ì‚°"""
        var = self._calculate_historical_var(returns, confidence_level)
        clean_returns = returns.dropna()
        tail_returns = clean_returns[clean_returns <= var]
        
        return float(tail_returns.mean()) if len(tail_returns) > 0 else var
    
    def _calculate_parametric_var(self, returns: pd.Series, confidence_level: float) -> float:
        """íŒŒë¼ë¯¸í„°ì  VaR (ì •ê·œë¶„í¬ ê°€ì •)"""
        mean_return = returns.mean()
        std_return = returns.std()
        z_score = stats.norm.ppf(1 - confidence_level)
        
        return float(mean_return + z_score * std_return)
    
    def _calculate_parametric_cvar(self, returns: pd.Series, confidence_level: float) -> float:
        """íŒŒë¼ë¯¸í„°ì  CVaR (ì •ê·œë¶„í¬ ê°€ì •)"""
        mean_return = returns.mean()
        std_return = returns.std()
        z_score = stats.norm.ppf(1 - confidence_level)
        
        # ì¡°ê±´ë¶€ ê¸°ëŒ“ê°’ ê³„ì‚°
        phi_z = stats.norm.pdf(z_score)
        cvar = mean_return - std_return * phi_z / (1 - confidence_level)
        
        return float(cvar)
    
    def _calculate_monte_carlo_var(self, returns: pd.Series, confidence_level: float, 
                                 n_simulations: int = 10000) -> Dict:
        """ëª¬í…Œì¹´ë¥¼ë¡œ VaR ê³„ì‚°"""
        clean_returns = returns.dropna()
        
        if len(clean_returns) < 30:
            return {'monte_carlo_var': 0.0, 'monte_carlo_cvar': 0.0}
        
        # íŒŒë¼ë¯¸í„° ì¶”ì •
        mean_return = clean_returns.mean()
        std_return = clean_returns.std()
        
        # ëª¬í…Œì¹´ë¥¼ë¡œ ì‹œë®¬ë ˆì´ì…˜
        np.random.seed(42)  # ì¬í˜„ ê°€ëŠ¥ì„±
        simulated_returns = np.random.normal(mean_return, std_return, n_simulations)
        
        # VaR ë° CVaR ê³„ì‚°
        var_mc = np.percentile(simulated_returns, (1 - confidence_level) * 100)
        tail_returns_mc = simulated_returns[simulated_returns <= var_mc]
        cvar_mc = np.mean(tail_returns_mc) if len(tail_returns_mc) > 0 else var_mc
        
        return {
            'monte_carlo_var': float(var_mc),
            'monte_carlo_cvar': float(cvar_mc)
        }
    
    def _calculate_extreme_value_var(self, returns: pd.Series, confidence_level: float) -> Dict:
        """ê·¹ê°’ ì´ë¡  VaR ê³„ì‚°"""
        if not EXTREME_VALUE_AVAILABLE:
            return {'extreme_value_var': 0.0, 'extreme_value_cvar': 0.0}
        
        clean_returns = returns.dropna()
        
        if len(clean_returns) < 100:  # ê·¹ê°’ ì´ë¡ ì—ëŠ” ì¶©ë¶„í•œ ë°ì´í„° í•„ìš”
            return {'extreme_value_var': 0.0, 'extreme_value_cvar': 0.0}
        
        try:
            # ê·¹ê°’ (í•˜ìœ„ 5%)ë§Œ ì¶”ì¶œ
            threshold = np.percentile(clean_returns, self.config.extreme_percentile * 100)
            exceedances = clean_returns[clean_returns <= threshold] - threshold
            
            if len(exceedances) < 20:
                return {'extreme_value_var': 0.0, 'extreme_value_cvar': 0.0}
            
            # GPD (Generalized Pareto Distribution) í”¼íŒ…
            shape, loc, scale = genpareto.fit(-exceedances, floc=0)  # ìŒìˆ˜ ë³€í™˜ (ì†ì‹¤ ê¸°ì¤€)
            
            # VaR ê³„ì‚°
            prob = (1 - confidence_level) / self.config.extreme_percentile
            var_ev = threshold - scale * ((prob ** (-shape)) - 1) / shape
            
            # CVaR ê³„ì‚° (ê·¼ì‚¬)
            cvar_ev = var_ev - scale / (1 - shape) * (prob ** (-shape))
            
            return {
                'extreme_value_var': float(var_ev),
                'extreme_value_cvar': float(cvar_ev),
                'gpd_shape': float(shape),
                'gpd_scale': float(scale)
            }
        except Exception:
            return {'extreme_value_var': 0.0, 'extreme_value_cvar': 0.0}
    
    def _calculate_drawdown_metrics(self, cumulative_returns: pd.Series) -> Dict:
        """ë“œë¡œë‹¤ìš´ ë¶„ì„"""
        if isinstance(cumulative_returns, np.ndarray):
            cumulative_returns = pd.Series(cumulative_returns)
        
        # ëˆ„ì  ìµœëŒ€ê°’ (peak) ê³„ì‚°
        peaks = cumulative_returns.expanding().max()
        
        # ë“œë¡œë‹¤ìš´ ê³„ì‚°
        drawdowns = (cumulative_returns - peaks) / peaks
        
        # ìµœëŒ€ ë“œë¡œë‹¤ìš´
        max_drawdown = drawdowns.min()
        max_dd_start = drawdowns.idxmin()
        max_dd_peak = peaks.loc[:max_dd_start].idxmax()
        
        # íšŒë³µ ì§€ì  ì°¾ê¸°
        max_dd_recovery = None
        recovery_threshold = peaks.loc[max_dd_peak] * self.config.dd_recovery_threshold
        
        post_dd_data = cumulative_returns.loc[max_dd_start:]
        recovery_candidates = post_dd_data[post_dd_data >= recovery_threshold]
        
        if len(recovery_candidates) > 0:
            max_dd_recovery = recovery_candidates.index[0]
        
        # ë“œë¡œë‹¤ìš´ ê¸°ê°„ ê³„ì‚°
        dd_duration = None
        if max_dd_recovery is not None and hasattr(cumulative_returns.index, 'to_pydatetime'):
            dd_duration = (max_dd_recovery - max_dd_peak).days
        elif max_dd_recovery is not None:
            # ì¸ë±ìŠ¤ê°€ ì •ìˆ˜ì¸ ê²½ìš°
            dd_duration = int(max_dd_recovery - max_dd_peak)
        
        # ë“œë¡œë‹¤ìš´ í†µê³„
        drawdown_stats = {
            'max_drawdown_pct': float(max_drawdown * 100),
            'max_drawdown_value': float(max_drawdown),
            'max_dd_start_date': max_dd_peak,
            'max_dd_end_date': max_dd_start,
            'max_dd_recovery_date': max_dd_recovery,
            'max_dd_duration_days': dd_duration,
            'avg_drawdown_pct': float(drawdowns[drawdowns < 0].mean() * 100) if (drawdowns < 0).any() else 0,
            'drawdown_frequency': float((drawdowns < -0.05).sum() / len(drawdowns)),  # 5% ì´ìƒ í•˜ë½ ë¹ˆë„
            'time_underwater_pct': float((drawdowns < -0.01).sum() / len(drawdowns) * 100)  # 1% ì´ìƒ í•˜ë½ ìƒíƒœ ë¹„ìœ¨
        }
        
        # ëª¨ë“  ë“œë¡œë‹¤ìš´ êµ¬ê°„ ë¶„ì„
        dd_periods = self._analyze_all_drawdown_periods(drawdowns)
        drawdown_stats['all_drawdown_periods'] = dd_periods
        
        return drawdown_stats
    
    def _calculate_max_drawdown(self, values: pd.Series) -> Dict:
        """ìµœëŒ€ ë“œë¡œë‹¤ìš´ ê³„ì‚° (ê°„ë‹¨ ë²„ì „)"""
        peaks = values.expanding().max()
        drawdowns = (values - peaks) / peaks
        max_dd = drawdowns.min()
        
        return {'max_drawdown_pct': float(max_dd * 100)}
    
    def _analyze_all_drawdown_periods(self, drawdowns: pd.Series, threshold: float = -0.01) -> List[Dict]:
        """ëª¨ë“  ë“œë¡œë‹¤ìš´ ê¸°ê°„ ë¶„ì„"""
        dd_periods = []
        in_drawdown = False
        current_dd = {}
        
        for idx, dd_value in drawdowns.items():
            if dd_value <= threshold and not in_drawdown:
                # ë“œë¡œë‹¤ìš´ ì‹œì‘
                in_drawdown = True
                current_dd = {
                    'start': idx,
                    'min_value': dd_value,
                    'min_date': idx
                }
            elif dd_value <= threshold and in_drawdown:
                # ë“œë¡œë‹¤ìš´ ì§€ì†
                if dd_value < current_dd['min_value']:
                    current_dd['min_value'] = dd_value
                    current_dd['min_date'] = idx
            elif dd_value > threshold and in_drawdown:
                # ë“œë¡œë‹¤ìš´ ì¢…ë£Œ
                current_dd['end'] = idx
                current_dd['duration'] = idx - current_dd['start']
                current_dd['magnitude_pct'] = current_dd['min_value'] * 100
                
                dd_periods.append(current_dd)
                in_drawdown = False
        
        # í˜„ì¬ ì§„í–‰ì¤‘ì¸ ë“œë¡œë‹¤ìš´ ì²˜ë¦¬
        if in_drawdown:
            current_dd['end'] = drawdowns.index[-1]
            current_dd['duration'] = drawdowns.index[-1] - current_dd['start']
            current_dd['magnitude_pct'] = current_dd['min_value'] * 100
            dd_periods.append(current_dd)
        
        return dd_periods
    
    def _calculate_tail_risk_measures(self, returns: pd.Series) -> Dict:
        """í…Œì¼ ë¦¬ìŠ¤í¬ ì¸¡ì •"""
        clean_returns = returns.dropna()
        
        if len(clean_returns) < 50:
            return {'insufficient_data': True}
        
        # í…Œì¼ ë¹„ìœ¨ ê³„ì‚°
        tail_ratios = {}
        for percentile in [90, 95, 99]:
            upper_tail = np.percentile(clean_returns, percentile)
            lower_tail = np.percentile(clean_returns, 100 - percentile)
            tail_ratios[f'tail_ratio_{percentile}'] = abs(upper_tail) / abs(lower_tail) if lower_tail != 0 else float('inf')
        
        # ê·¹ê°’ ì§€ìˆ˜ ê³„ì‚° (Hill estimator)
        tail_index = self._calculate_tail_index(clean_returns)
        
        # ì¢Œì¸¡/ìš°ì¸¡ í…Œì¼ ë¶„ì„
        left_tail = clean_returns[clean_returns <= np.percentile(clean_returns, 5)]
        right_tail = clean_returns[clean_returns >= np.percentile(clean_returns, 95)]
        
        tail_measures = {
            'tail_ratios': tail_ratios,
            'tail_index': tail_index,
            'left_tail_mean': float(left_tail.mean()) if len(left_tail) > 0 else 0,
            'right_tail_mean': float(right_tail.mean()) if len(right_tail) > 0 else 0,
            'tail_dependency': self._calculate_tail_dependency(clean_returns),
            'expected_shortfall': {
                f'es_{int(conf*100)}': float(clean_returns[clean_returns <= np.percentile(clean_returns, (1-conf)*100)].mean())
                for conf in self.config.confidence_levels
            }
        }
        
        return tail_measures
    
    def _calculate_tail_index(self, returns: pd.Series) -> float:
        """í…Œì¼ ì§€ìˆ˜ ê³„ì‚° (Hill ì¶”ì •ëŸ‰)"""
        k = int(len(returns) * 0.05)  # ìƒìœ„ 5% ì‚¬ìš©
        sorted_returns = np.sort(returns)
        extreme_values = sorted_returns[-k:]
        
        if len(extreme_values) < 2:
            return 0.0
        
        # Hill ì¶”ì •ëŸ‰
        log_ratios = np.log(extreme_values[1:] / extreme_values[0])
        tail_index = np.mean(log_ratios)
        
        return float(tail_index)
    
    def _calculate_tail_dependency(self, returns: pd.Series) -> Dict:
        """í…Œì¼ ì˜ì¡´ì„± ê³„ì‚°"""
        # ë‹¨ìˆœí™”ëœ í…Œì¼ ì˜ì¡´ì„± (ìê¸° ìƒê´€)
        lagged_returns = returns.shift(1).dropna()
        aligned_returns = returns[lagged_returns.index]
        
        if len(aligned_returns) < 50:
            return {'upper': 0, 'lower': 0}
        
        # ìƒìœ„/í•˜ìœ„ 10% ë™ì‹œ ë°œìƒ í™•ë¥ 
        upper_10 = np.percentile(returns, 90)
        lower_10 = np.percentile(returns, 10)
        
        upper_dependency = np.mean((aligned_returns > upper_10) & (lagged_returns > upper_10))
        lower_dependency = np.mean((aligned_returns < lower_10) & (lagged_returns < lower_10))
        
        return {
            'upper': float(upper_dependency),
            'lower': float(lower_dependency)
        }
    
    def _calculate_higher_moments(self, returns: pd.Series) -> Dict:
        """ê³ ì°¨ ëª¨ë©˜íŠ¸ ë¶„ì„"""
        clean_returns = returns.dropna()
        
        if len(clean_returns) < 30:
            return {'insufficient_data': True}
        
        return {
            'skewness': float(stats.skew(clean_returns)),
            'kurtosis': float(stats.kurtosis(clean_returns)),
            'excess_kurtosis': float(stats.kurtosis(clean_returns, fisher=False) - 3),
            'jarque_bera_stat': float(stats.jarque_bera(clean_returns)[0]),
            'jarque_bera_pvalue': float(stats.jarque_bera(clean_returns)[1]),
            'coskewness': self._calculate_coskewness(clean_returns),
            'cokurtosis': self._calculate_cokurtosis(clean_returns)
        }
    
    def _calculate_coskewness(self, returns: pd.Series) -> float:
        """ê³µì™œë„ ê³„ì‚° (ìê¸° ì§€ì—°ê³¼ì˜)"""
        lagged_returns = returns.shift(1).dropna()
        aligned_returns = returns[lagged_returns.index]
        
        if len(aligned_returns) < 30:
            return 0.0
        
        # í‘œì¤€í™”
        std_current = aligned_returns.std()
        std_lagged = lagged_returns.std()
        
        if std_current == 0 or std_lagged == 0:
            return 0.0
        
        normalized_current = (aligned_returns - aligned_returns.mean()) / std_current
        normalized_lagged = (lagged_returns - lagged_returns.mean()) / std_lagged
        
        coskewness = np.mean(normalized_current**2 * normalized_lagged)
        return float(coskewness)
    
    def _calculate_cokurtosis(self, returns: pd.Series) -> float:
        """ê³µì²¨ë„ ê³„ì‚°"""
        lagged_returns = returns.shift(1).dropna()
        aligned_returns = returns[lagged_returns.index]
        
        if len(aligned_returns) < 30:
            return 0.0
        
        # í‘œì¤€í™”
        std_current = aligned_returns.std()
        std_lagged = lagged_returns.std()
        
        if std_current == 0 or std_lagged == 0:
            return 0.0
        
        normalized_current = (aligned_returns - aligned_returns.mean()) / std_current
        normalized_lagged = (lagged_returns - lagged_returns.mean()) / std_lagged
        
        cokurtosis = np.mean(normalized_current**3 * normalized_lagged)
        return float(cokurtosis)
    
    def _calculate_dynamic_risk_metrics(self, returns: pd.Series) -> Dict:
        """ë™ì  ë¦¬ìŠ¤í¬ ì§€í‘œ ê³„ì‚°"""
        dynamic_metrics = {}
        
        for lookback in self.config.lookback_periods:
            if len(returns) < lookback + self.config.min_periods:
                continue
            
            rolling_sharpe = []
            rolling_var = []
            rolling_volatility = []
            
            for i in range(lookback, len(returns)):
                window_returns = returns.iloc[i-lookback:i]
                
                # ë¡¤ë§ ìƒ¤í”„ ë¹„ìœ¨
                excess_returns = window_returns - self.config.risk_free_rate / 252
                sharpe = (excess_returns.mean() * 252) / (window_returns.std() * np.sqrt(252)) if window_returns.std() > 0 else 0
                rolling_sharpe.append(sharpe)
                
                # ë¡¤ë§ VaR
                var_95 = self._calculate_historical_var(window_returns, 0.95)
                rolling_var.append(var_95)
                
                # ë¡¤ë§ ë³€ë™ì„±
                rolling_volatility.append(window_returns.std() * np.sqrt(252))
            
            dynamic_metrics[f'lookback_{lookback}'] = {
                'rolling_sharpe': rolling_sharpe,
                'rolling_var_95': rolling_var,
                'rolling_volatility': rolling_volatility,
                'sharpe_stability': np.std(rolling_sharpe) if len(rolling_sharpe) > 1 else 0,
                'var_stability': np.std(rolling_var) if len(rolling_var) > 1 else 0
            }
        
        return dynamic_metrics
    
    def create_risk_dashboard(self, risk_metrics: Dict) -> str:
        """ë¦¬ìŠ¤í¬ ëŒ€ì‹œë³´ë“œ ìƒì„±"""
        self.logger.info("ğŸ“Š ë¦¬ìŠ¤í¬ ë¶„ì„ ëŒ€ì‹œë³´ë“œ ìƒì„± ì¤‘...")
        
        fig = make_subplots(
            rows=4, cols=3,
            subplot_titles=(
                "ë¦¬ìŠ¤í¬-ìˆ˜ìµ ìŠ¤ìºí„°", "VaR ë¹„êµ (ë‹¤ì–‘í•œ ë°©ë²•ë¡ )", "ë“œë¡œë‹¤ìš´ ë¶„ì„",
                "í…Œì¼ ë¦¬ìŠ¤í¬ ë¶„í¬", "ë¡¤ë§ ìƒ¤í”„ ë¹„ìœ¨", "ê³ ì°¨ ëª¨ë©˜íŠ¸ ë¶„ì„",
                "ë¦¬ìŠ¤í¬ ì¡°ì • ë¹„ìœ¨", "CVaR vs VaR", "ë³€ë™ì„± í´ëŸ¬ìŠ¤í„°ë§",
                "ê·¹ê°’ ë¶„ì„", "í…Œì¼ ì˜ì¡´ì„±", "ë¦¬ìŠ¤í¬ ê¸°ì—¬ë„"
            ),
            specs=[[{"type": "scatter"}, {"type": "bar"}, {"type": "scatter"}],
                   [{"type": "histogram"}, {"type": "scatter"}, {"type": "radar"}],
                   [{"type": "bar"}, {"type": "scatter"}, {"type": "heatmap"}],
                   [{"type": "scatter"}, {"type": "bar"}, {"type": "pie"}]],
            vertical_spacing=0.08,
            horizontal_spacing=0.08
        )
        
        # 1. ë¦¬ìŠ¤í¬-ìˆ˜ìµ ìŠ¤ìºí„° í”Œë¡¯
        if 'basic_statistics' in risk_metrics:
            stats = risk_metrics['basic_statistics']
            fig.add_trace(
                go.Scatter(
                    x=[stats['annual_volatility']],
                    y=[stats['annual_return']],
                    mode='markers',
                    marker=dict(size=15, color='blue'),
                    name='í¬íŠ¸í´ë¦¬ì˜¤',
                    text=[f"ìƒ¤í”„: {risk_metrics.get('risk_adjusted_ratios', {}).get('sharpe_ratio', 0):.3f}"],
                    textposition="top center"
                ),
                row=1, col=1
            )
        
        # 2. VaR ë¹„êµ
        if 'var_metrics' in risk_metrics:
            var_data = risk_metrics['var_metrics'].get('confidence_95', {})
            methods = []
            values = []
            
            for method, value in var_data.items():
                if 'var' in method and not 'cvar' in method:
                    methods.append(method.replace('_var', '').title())
                    values.append(abs(value) * 100)  # ë°±ë¶„ìœ¨ë¡œ ë³€í™˜
            
            if methods:
                fig.add_trace(
                    go.Bar(x=methods, y=values, name='VaR 95%', marker_color='red'),
                    row=1, col=2
                )
        
        # 3. ë“œë¡œë‹¤ìš´ ì‹œê³„ì—´ (ê°„ë‹¨ ë²„ì „)
        if 'drawdown_analysis' in risk_metrics:
            dd_stats = risk_metrics['drawdown_analysis']
            fig.add_trace(
                go.Scatter(
                    y=[dd_stats.get('max_drawdown_pct', 0)],
                    mode='markers',
                    marker=dict(size=20, color='red'),
                    name=f"ìµœëŒ€ ë“œë¡œë‹¤ìš´: {dd_stats.get('max_drawdown_pct', 0):.2f}%"
                ),
                row=1, col=3
            )
        
        # ì¶”ê°€ ì°¨íŠ¸ë“¤ì€ ë°ì´í„° ê°€ìš©ì„±ì— ë”°ë¼ êµ¬í˜„...
        
        fig.update_layout(
            title="ğŸ¯ ì¢…í•© ë¦¬ìŠ¤í¬ ë¶„ì„ ëŒ€ì‹œë³´ë“œ",
            height=1600,
            showlegend=True,
            template='plotly_dark'
        )
        
        # ì €ì¥
        dashboard_path = os.path.join(self.data_path, 'risk_analysis_dashboard.html')
        fig.write_html(dashboard_path, include_plotlyjs=True)
        
        return dashboard_path
    
    def save_risk_analysis_report(self, risk_metrics: Dict):
        """ë¦¬ìŠ¤í¬ ë¶„ì„ ë³´ê³ ì„œ ì €ì¥"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # JSON ì €ì¥
        json_path = os.path.join(self.data_path, f'risk_analysis_report_{timestamp}.json')
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(risk_metrics, f, indent=2, ensure_ascii=False, default=str)
        
        self.logger.info(f"ë¦¬ìŠ¤í¬ ë¶„ì„ ë³´ê³ ì„œ ì €ì¥: {json_path}")
        
        # ìš”ì•½ ë³´ê³ ì„œ ìƒì„±
        self._generate_summary_report(risk_metrics, timestamp)
    
    def _generate_summary_report(self, risk_metrics: Dict, timestamp: str):
        """ìš”ì•½ ë³´ê³ ì„œ ìƒì„±"""
        summary_lines = [
            "ğŸ¯ ë¹„íŠ¸ì½”ì¸ íˆ¬ì ë¦¬ìŠ¤í¬ ë¶„ì„ ìš”ì•½ ë³´ê³ ì„œ",
            "=" * 60,
            f"ë¶„ì„ ì¼ì‹œ: {datetime.now().strftime('%Yë…„ %mì›” %dì¼ %Hì‹œ %Më¶„')}",
            ""
        ]
        
        # ê¸°ë³¸ í†µê³„
        if 'basic_statistics' in risk_metrics:
            stats = risk_metrics['basic_statistics']
            summary_lines.extend([
                "ğŸ“Š ê¸°ë³¸ í†µê³„",
                f"  â€¢ ì—°ê°„ ìˆ˜ìµë¥ : {stats.get('annual_return', 0)*100:.2f}%",
                f"  â€¢ ì—°ê°„ ë³€ë™ì„±: {stats.get('annual_volatility', 0)*100:.2f}%",
                f"  â€¢ ì™œë„: {stats.get('skewness', 0):.3f}",
                f"  â€¢ ì²¨ë„: {stats.get('kurtosis', 0):.3f}",
                ""
            ])
        
        # ë¦¬ìŠ¤í¬ ì¡°ì • ë¹„ìœ¨
        if 'risk_adjusted_ratios' in risk_metrics:
            ratios = risk_metrics['risk_adjusted_ratios']
            summary_lines.extend([
                "âš–ï¸ ë¦¬ìŠ¤í¬ ì¡°ì • ì„±ê³¼",
                f"  â€¢ ìƒ¤í”„ ë¹„ìœ¨: {ratios.get('sharpe_ratio', 0):.3f}",
                f"  â€¢ ì†Œë¥´í‹°ë…¸ ë¹„ìœ¨: {ratios.get('sortino_ratio', 0):.3f}",
                f"  â€¢ ì¹¼ë§ˆ ë¹„ìœ¨: {ratios.get('calmar_ratio', 0):.3f}",
                f"  â€¢ ì˜¤ë©”ê°€ ë¹„ìœ¨: {ratios.get('omega_ratio', 0):.3f}",
                ""
            ])
        
        # VaR ì •ë³´
        if 'var_metrics' in risk_metrics:
            var_95 = risk_metrics['var_metrics'].get('confidence_95', {})
            summary_lines.extend([
                "ğŸš¨ Value at Risk (95% ì‹ ë¢°ìˆ˜ì¤€)",
                f"  â€¢ ì—­ì‚¬ì  VaR: {var_95.get('historical_var', 0)*100:.2f}%",
                f"  â€¢ íŒŒë¼ë¯¸í„°ì  VaR: {var_95.get('parametric_var', 0)*100:.2f}%",
                f"  â€¢ ëª¬í…Œì¹´ë¥¼ë¡œ VaR: {var_95.get('monte_carlo_var', 0)*100:.2f}%",
                ""
            ])
        
        # ë“œë¡œë‹¤ìš´ ë¶„ì„
        if 'drawdown_analysis' in risk_metrics:
            dd = risk_metrics['drawdown_analysis']
            summary_lines.extend([
                "ğŸ“‰ ë“œë¡œë‹¤ìš´ ë¶„ì„",
                f"  â€¢ ìµœëŒ€ ë“œë¡œë‹¤ìš´: {dd.get('max_drawdown_pct', 0):.2f}%",
                f"  â€¢ í‰ê·  ë“œë¡œë‹¤ìš´: {dd.get('avg_drawdown_pct', 0):.2f}%",
                f"  â€¢ ìˆ˜ì¤‘ ì‹œê°„ ë¹„ìœ¨: {dd.get('time_underwater_pct', 0):.1f}%",
                ""
            ])
        
        # íŒŒì¼ ì €ì¥
        summary_path = os.path.join(self.data_path, f'risk_summary_report_{timestamp}.txt')
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(summary_lines))
        
        self.logger.info(f"ìš”ì•½ ë³´ê³ ì„œ ì €ì¥: {summary_path}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ¯ ë¦¬ìŠ¤í¬ ì¡°ì • ì„±ê³¼ ì§€í‘œ ì‹œìŠ¤í…œ")
    print("=" * 50)
    
    # ì„¤ì •
    config = RiskConfig(
        risk_free_rate=0.02,
        confidence_levels=[0.90, 0.95, 0.99],
        var_methods=['historical', 'parametric', 'monte_carlo']
    )
    
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    risk_system = RiskAdjustedMetricsSystem(config)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    np.random.seed(42)
    n_days = 1000
    
    # ì‹¤ì œ ë¹„íŠ¸ì½”ì¸ê³¼ ìœ ì‚¬í•œ ìˆ˜ìµë¥  ì‹œë®¬ë ˆì´ì…˜
    returns = np.random.normal(0.001, 0.03, n_days)  # í‰ê·  0.1%, ë³€ë™ì„± 3%
    returns = pd.Series(returns, index=pd.date_range('2021-01-01', periods=n_days))
    
    # í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜ ê³„ì‚°
    portfolio_values = (1 + returns).cumprod() * 10000
    
    # ì¢…í•© ë¦¬ìŠ¤í¬ ë¶„ì„
    risk_metrics = risk_system.calculate_comprehensive_risk_metrics(
        returns, portfolio_values
    )
    
    print("\nğŸ“Š ë¦¬ìŠ¤í¬ ë¶„ì„ ì™„ë£Œ!")
    print(f"ìƒ¤í”„ ë¹„ìœ¨: {risk_metrics['risk_adjusted_ratios']['sharpe_ratio']:.3f}")
    print(f"ìµœëŒ€ ë“œë¡œë‹¤ìš´: {risk_metrics['drawdown_analysis']['max_drawdown_pct']:.2f}%")
    print(f"VaR (95%): {risk_metrics['var_metrics']['confidence_95']['historical_var']*100:.2f}%")
    
    # ëŒ€ì‹œë³´ë“œ ìƒì„±
    dashboard_path = risk_system.create_risk_dashboard(risk_metrics)
    print(f"ëŒ€ì‹œë³´ë“œ ìƒì„±: {dashboard_path}")
    
    # ë³´ê³ ì„œ ì €ì¥
    risk_system.save_risk_analysis_report(risk_metrics)

if __name__ == "__main__":
    main()