#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í•©ì„± ë¹„íŠ¸ì½”ì¸ ë°ì´í„° ìƒì„± ì‹œìŠ¤í…œ
- GAN ê¸°ë°˜ í•©ì„± ë°ì´í„° ìƒì„±
- VAE ì ì¬ê³µê°„ ë°ì´í„° ìƒì„±
- ëª¬í…Œì¹´ë¥¼ë¡œ ì‹œë®¬ë ˆì´ì…˜
- ë¶€íŠ¸ìŠ¤íŠ¸ë© ìƒ˜í”Œë§
"""

import numpy as np
import pandas as pd
import warnings
warnings.filterwarnings('ignore')

import os
import sys
import json
import sqlite3
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Union
import logging
import pickle

# ê³¼í•™ ê³„ì‚°
from scipy import stats
from scipy.signal import savgol_filter
from scipy.interpolate import interp1d
from scipy.ndimage import gaussian_filter1d
from scipy.optimize import minimize
from scipy.stats import multivariate_normal

# ë¨¸ì‹ ëŸ¬ë‹
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.decomposition import PCA, FastICA
from sklearn.cluster import KMeans
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.model_selection import TimeSeriesSplit
from sklearn.ensemble import IsolationForest

# ë”¥ëŸ¬ë‹ ëª¨ë¸ êµ¬í˜„
try:
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import layers
    from tensorflow.keras.models import Model, Sequential
    from tensorflow.keras.layers import Dense, LSTM, Dropout, BatchNormalization
    from tensorflow.keras.layers import Input, Reshape, Conv1D, Conv1DTranspose
    from tensorflow.keras.optimizers import Adam
    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
    TENSORFLOW_AVAILABLE = True
except ImportError:
    TENSORFLOW_AVAILABLE = False
    print("âš ï¸ TensorFlow ì—†ìŒ - GAN/VAE ê¸°ëŠ¥ ì œí•œ")

# í†µê³„ ë° ì‹œê°í™”
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.dates import DateFormatter
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class SyntheticBitcoinDataGenerator:
    """
    ğŸ”® í•©ì„± ë¹„íŠ¸ì½”ì¸ ë°ì´í„° ìƒì„± ì‹œìŠ¤í…œ
    
    ì£¼ìš” ê¸°ëŠ¥:
    1. GAN ê¸°ë°˜ ì‹œê³„ì—´ í•©ì„± ë°ì´í„°
    2. VAE ì ì¬ê³µê°„ ë°ì´í„° ìƒì„±
    3. ëª¬í…Œì¹´ë¥¼ë¡œ ê°€ê²© ê²½ë¡œ ì‹œë®¬ë ˆì´ì…˜
    4. ë¶€íŠ¸ìŠ¤íŠ¸ë© ë¦¬ìƒ˜í”Œë§
    5. ì‹œì¥ ì¶©ê²© ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±
    """
    
    def __init__(self, sequence_length: int = 168):
        """
        ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        
        Args:
            sequence_length: ì‹œê³„ì—´ ì‹œí€€ìŠ¤ ê¸¸ì´ (ê¸°ë³¸: 7ì¼ = 168ì‹œê°„)
        """
        self.sequence_length = sequence_length
        self.logger = logging.getLogger(__name__)
        
        # ëª¨ë¸ ì €ì¥ì†Œ
        self.gan_generator = None
        self.gan_discriminator = None
        self.vae_encoder = None
        self.vae_decoder = None
        self.vae_model = None
        
        # ë°ì´í„° ì €ì¥ì†Œ
        self.training_data = None
        self.scalers = {}
        
        # í•˜ì´í¼íŒŒë¼ë¯¸í„°
        self.hyperparams = {
            'gan': {
                'latent_dim': 100,
                'generator_lr': 0.0002,
                'discriminator_lr': 0.0002,
                'batch_size': 32,
                'epochs': 1000,
                'beta_1': 0.5
            },
            'vae': {
                'latent_dim': 50,
                'learning_rate': 0.001,
                'batch_size': 32,
                'epochs': 500,
                'beta': 1.0  # KL divergence weight
            },
            'monte_carlo': {
                'n_simulations': 10000,
                'time_horizon': 168,  # 7 days
                'confidence_levels': [0.05, 0.95]
            }
        }
        
        self.logger.info("ğŸ”® í•©ì„± ë°ì´í„° ìƒì„± ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def prepare_training_data(self, data: pd.DataFrame) -> np.ndarray:
        """
        GAN/VAE í›ˆë ¨ìš© ë°ì´í„° ì¤€ë¹„
        
        Args:
            data: ì›ë³¸ ì‹œê³„ì—´ ë°ì´í„°
            
        Returns:
            í›ˆë ¨ìš© ì‹œí€€ìŠ¤ ë°°ì—´
        """
        self.logger.info("ğŸ“Š í›ˆë ¨ ë°ì´í„° ì¤€ë¹„ ì¤‘...")
        
        # ìˆ«ìí˜• ì»¬ëŸ¼ë§Œ ì„ íƒ
        numeric_cols = data.select_dtypes(include=[np.number]).columns
        numeric_data = data[numeric_cols].fillna(method='ffill').fillna(method='bfill')
        
        # ìŠ¤ì¼€ì¼ë§
        scaler = RobustScaler()
        scaled_data = scaler.fit_transform(numeric_data)
        self.scalers['main'] = scaler
        
        # ì‹œí€€ìŠ¤ ìƒì„±
        sequences = []
        for i in range(len(scaled_data) - self.sequence_length + 1):
            seq = scaled_data[i:i + self.sequence_length]
            sequences.append(seq)
        
        sequences = np.array(sequences)
        self.training_data = sequences
        
        self.logger.info(f"âœ… {len(sequences)}ê°œ ì‹œí€€ìŠ¤ ì¤€ë¹„ ì™„ë£Œ, í˜•íƒœ: {sequences.shape}")
        return sequences
    
    def build_gan_generator(self, input_dim: int, output_dim: int) -> Model:
        """
        GAN ìƒì„±ì ëª¨ë¸ êµ¬ì¶•
        
        Args:
            input_dim: ì…ë ¥ ì°¨ì› (ì ì¬ ê³µê°„)
            output_dim: ì¶œë ¥ ì°¨ì› (ì‹œí€€ìŠ¤ * íŠ¹ì„± ìˆ˜)
            
        Returns:
            ìƒì„±ì ëª¨ë¸
        """
        if not TENSORFLOW_AVAILABLE:
            raise ImportError("TensorFlowê°€ í•„ìš”í•©ë‹ˆë‹¤")
        
        model = Sequential([
            Dense(256, activation='leaky_relu', input_dim=input_dim),
            BatchNormalization(),
            Dropout(0.3),
            
            Dense(512, activation='leaky_relu'),
            BatchNormalization(),
            Dropout(0.3),
            
            Dense(1024, activation='leaky_relu'),
            BatchNormalization(),
            Dropout(0.3),
            
            Dense(output_dim, activation='tanh'),
            Reshape((self.sequence_length, -1))
        ])
        
        return model
    
    def build_gan_discriminator(self, input_shape: Tuple[int, int]) -> Model:
        """
        GAN íŒë³„ì ëª¨ë¸ êµ¬ì¶•
        
        Args:
            input_shape: ì…ë ¥ í˜•íƒœ (ì‹œí€€ìŠ¤ ê¸¸ì´, íŠ¹ì„± ìˆ˜)
            
        Returns:
            íŒë³„ì ëª¨ë¸
        """
        if not TENSORFLOW_AVAILABLE:
            raise ImportError("TensorFlowê°€ í•„ìš”í•©ë‹ˆë‹¤")
        
        model = Sequential([
            Conv1D(64, 3, activation='leaky_relu', input_shape=input_shape),
            Dropout(0.3),
            
            Conv1D(128, 3, activation='leaky_relu'),
            Dropout(0.3),
            
            Conv1D(256, 3, activation='leaky_relu'),
            Dropout(0.3),
            
            layers.GlobalMaxPooling1D(),
            Dense(512, activation='leaky_relu'),
            Dropout(0.3),
            Dense(1, activation='sigmoid')
        ])
        
        return model
    
    def build_vae_encoder(self, input_shape: Tuple[int, int], latent_dim: int) -> Model:
        """
        VAE ì¸ì½”ë” ëª¨ë¸ êµ¬ì¶•
        
        Args:
            input_shape: ì…ë ¥ í˜•íƒœ
            latent_dim: ì ì¬ ê³µê°„ ì°¨ì›
            
        Returns:
            ì¸ì½”ë” ëª¨ë¸
        """
        if not TENSORFLOW_AVAILABLE:
            raise ImportError("TensorFlowê°€ í•„ìš”í•©ë‹ˆë‹¤")
        
        inputs = Input(shape=input_shape)
        
        # ì¸ì½”ë” ë„¤íŠ¸ì›Œí¬
        x = Conv1D(64, 3, activation='relu', padding='same')(inputs)
        x = Conv1D(128, 3, activation='relu', padding='same')(x)
        x = layers.GlobalAveragePooling1D()(x)
        x = Dense(256, activation='relu')(x)
        
        # ì ì¬ ë³€ìˆ˜ íŒŒë¼ë¯¸í„°
        z_mean = Dense(latent_dim, name='z_mean')(x)
        z_log_var = Dense(latent_dim, name='z_log_var')(x)
        
        # ìƒ˜í”Œë§ ë ˆì´ì–´
        def sampling(args):
            z_mean, z_log_var = args
            batch = tf.shape(z_mean)[0]
            dim = tf.shape(z_mean)[1]
            epsilon = tf.random.normal(shape=(batch, dim))
            return z_mean + tf.exp(0.5 * z_log_var) * epsilon
        
        z = layers.Lambda(sampling, name='z')([z_mean, z_log_var])
        
        encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')
        return encoder
    
    def build_vae_decoder(self, latent_dim: int, output_shape: Tuple[int, int]) -> Model:
        """
        VAE ë””ì½”ë” ëª¨ë¸ êµ¬ì¶•
        
        Args:
            latent_dim: ì ì¬ ê³µê°„ ì°¨ì›
            output_shape: ì¶œë ¥ í˜•íƒœ
            
        Returns:
            ë””ì½”ë” ëª¨ë¸
        """
        if not TENSORFLOW_AVAILABLE:
            raise ImportError("TensorFlowê°€ í•„ìš”í•©ë‹ˆë‹¤")
        
        latent_inputs = Input(shape=(latent_dim,))
        
        # ë””ì½”ë” ë„¤íŠ¸ì›Œí¬
        x = Dense(256, activation='relu')(latent_inputs)
        x = Dense(output_shape[0] * 64, activation='relu')(x)
        x = Reshape((output_shape[0], 64))(x)
        
        x = Conv1DTranspose(128, 3, activation='relu', padding='same')(x)
        x = Conv1DTranspose(64, 3, activation='relu', padding='same')(x)
        outputs = Conv1DTranspose(output_shape[1], 3, activation='linear', padding='same')(x)
        
        decoder = Model(latent_inputs, outputs, name='decoder')
        return decoder
    
    def train_gan(self, data: np.ndarray, epochs: int = None) -> Dict[str, List[float]]:
        """
        GAN ëª¨ë¸ í›ˆë ¨
        
        Args:
            data: í›ˆë ¨ ë°ì´í„°
            epochs: ì—í­ ìˆ˜
            
        Returns:
            í›ˆë ¨ ê¸°ë¡
        """
        if not TENSORFLOW_AVAILABLE:
            raise ImportError("TensorFlowê°€ í•„ìš”í•©ë‹ˆë‹¤")
        
        self.logger.info("ğŸ”¥ GAN ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")
        
        epochs = epochs or self.hyperparams['gan']['epochs']
        batch_size = self.hyperparams['gan']['batch_size']
        latent_dim = self.hyperparams['gan']['latent_dim']
        
        # ëª¨ë¸ êµ¬ì¶•
        input_shape = (data.shape[1], data.shape[2])
        output_dim = data.shape[1] * data.shape[2]
        
        self.gan_generator = self.build_gan_generator(latent_dim, output_dim)
        self.gan_discriminator = self.build_gan_discriminator(input_shape)
        
        # ì˜µí‹°ë§ˆì´ì €
        g_optimizer = Adam(learning_rate=self.hyperparams['gan']['generator_lr'],
                          beta_1=self.hyperparams['gan']['beta_1'])
        d_optimizer = Adam(learning_rate=self.hyperparams['gan']['discriminator_lr'],
                          beta_1=self.hyperparams['gan']['beta_1'])
        
        # ì†ì‹¤ í•¨ìˆ˜
        cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False)
        
        def discriminator_loss(real_output, fake_output):
            real_loss = cross_entropy(tf.ones_like(real_output), real_output)
            fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)
            return real_loss + fake_loss
        
        def generator_loss(fake_output):
            return cross_entropy(tf.ones_like(fake_output), fake_output)
        
        # í›ˆë ¨ ë£¨í”„
        history = {'d_loss': [], 'g_loss': []}
        
        for epoch in range(epochs):
            # ë°°ì¹˜ ìƒì„±
            idx = np.random.randint(0, data.shape[0], batch_size)
            real_batch = data[idx]
            
            # íŒë³„ì í›ˆë ¨
            with tf.GradientTape() as disc_tape:
                noise = tf.random.normal([batch_size, latent_dim])
                generated_data = self.gan_generator(noise, training=True)
                
                real_output = self.gan_discriminator(real_batch, training=True)
                fake_output = self.gan_discriminator(generated_data, training=True)
                
                disc_loss = discriminator_loss(real_output, fake_output)
            
            disc_gradients = disc_tape.gradient(disc_loss, self.gan_discriminator.trainable_variables)
            d_optimizer.apply_gradients(zip(disc_gradients, self.gan_discriminator.trainable_variables))
            
            # ìƒì„±ì í›ˆë ¨
            with tf.GradientTape() as gen_tape:
                noise = tf.random.normal([batch_size, latent_dim])
                generated_data = self.gan_generator(noise, training=True)
                fake_output = self.gan_discriminator(generated_data, training=True)
                gen_loss = generator_loss(fake_output)
            
            gen_gradients = gen_tape.gradient(gen_loss, self.gan_generator.trainable_variables)
            g_optimizer.apply_gradients(zip(gen_gradients, self.gan_generator.trainable_variables))
            
            # ê¸°ë¡
            history['d_loss'].append(float(disc_loss))
            history['g_loss'].append(float(gen_loss))
            
            # ì§„í–‰ìƒí™© ì¶œë ¥
            if epoch % 100 == 0:
                self.logger.info(f"ì—í­ {epoch}/{epochs}, D ì†ì‹¤: {disc_loss:.4f}, G ì†ì‹¤: {gen_loss:.4f}")
        
        self.logger.info("âœ… GAN í›ˆë ¨ ì™„ë£Œ")
        return history
    
    def train_vae(self, data: np.ndarray, epochs: int = None) -> Dict[str, List[float]]:
        """
        VAE ëª¨ë¸ í›ˆë ¨
        
        Args:
            data: í›ˆë ¨ ë°ì´í„°
            epochs: ì—í­ ìˆ˜
            
        Returns:
            í›ˆë ¨ ê¸°ë¡
        """
        if not TENSORFLOW_AVAILABLE:
            raise ImportError("TensorFlowê°€ í•„ìš”í•©ë‹ˆë‹¤")
        
        self.logger.info("ğŸ”¥ VAE ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")
        
        epochs = epochs or self.hyperparams['vae']['epochs']
        batch_size = self.hyperparams['vae']['batch_size']
        latent_dim = self.hyperparams['vae']['latent_dim']
        beta = self.hyperparams['vae']['beta']
        
        # ëª¨ë¸ êµ¬ì¶•
        input_shape = (data.shape[1], data.shape[2])
        
        self.vae_encoder = self.build_vae_encoder(input_shape, latent_dim)
        self.vae_decoder = self.build_vae_decoder(latent_dim, input_shape)
        
        # VAE ì „ì²´ ëª¨ë¸
        inputs = Input(shape=input_shape)
        z_mean, z_log_var, z = self.vae_encoder(inputs)
        outputs = self.vae_decoder(z)
        
        self.vae_model = Model(inputs, outputs, name='vae')
        
        # ì†ì‹¤ í•¨ìˆ˜
        def vae_loss(x, x_decoded_mean):
            # ì¬êµ¬ì„± ì†ì‹¤
            reconstruction_loss = tf.reduce_mean(tf.keras.losses.mse(x, x_decoded_mean))
            
            # KL ë°œì‚° ì†ì‹¤
            kl_loss = -0.5 * tf.reduce_mean(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))
            
            return reconstruction_loss + beta * kl_loss
        
        # ì»´íŒŒì¼ ë° í›ˆë ¨
        self.vae_model.compile(optimizer=Adam(learning_rate=self.hyperparams['vae']['learning_rate']),
                              loss=vae_loss)
        
        # ì½œë°±
        callbacks = [
            EarlyStopping(patience=50, restore_best_weights=True),
            ReduceLROnPlateau(patience=25, factor=0.5)
        ]
        
        # í›ˆë ¨
        history = self.vae_model.fit(
            data, data,
            epochs=epochs,
            batch_size=batch_size,
            validation_split=0.2,
            callbacks=callbacks,
            verbose=1
        )
        
        self.logger.info("âœ… VAE í›ˆë ¨ ì™„ë£Œ")
        return history.history
    
    def generate_gan_samples(self, n_samples: int = 1000) -> np.ndarray:
        """
        GANìœ¼ë¡œ í•©ì„± ìƒ˜í”Œ ìƒì„±
        
        Args:
            n_samples: ìƒì„±í•  ìƒ˜í”Œ ìˆ˜
            
        Returns:
            í•©ì„± ë°ì´í„° ë°°ì—´
        """
        if self.gan_generator is None:
            raise ValueError("GAN ìƒì„±ìê°€ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        self.logger.info(f"ğŸ² GANìœ¼ë¡œ {n_samples}ê°œ í•©ì„± ìƒ˜í”Œ ìƒì„±...")
        
        latent_dim = self.hyperparams['gan']['latent_dim']
        noise = tf.random.normal([n_samples, latent_dim])
        synthetic_data = self.gan_generator(noise, training=False)
        
        return synthetic_data.numpy()
    
    def generate_vae_samples(self, n_samples: int = 1000) -> np.ndarray:
        """
        VAEë¡œ í•©ì„± ìƒ˜í”Œ ìƒì„±
        
        Args:
            n_samples: ìƒì„±í•  ìƒ˜í”Œ ìˆ˜
            
        Returns:
            í•©ì„± ë°ì´í„° ë°°ì—´
        """
        if self.vae_decoder is None:
            raise ValueError("VAE ë””ì½”ë”ê°€ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        self.logger.info(f"ğŸ² VAEë¡œ {n_samples}ê°œ í•©ì„± ìƒ˜í”Œ ìƒì„±...")
        
        latent_dim = self.hyperparams['vae']['latent_dim']
        
        # ì ì¬ ê³µê°„ì—ì„œ ìƒ˜í”Œë§
        z_samples = tf.random.normal([n_samples, latent_dim])
        synthetic_data = self.vae_decoder(z_samples)
        
        return synthetic_data.numpy()
    
    def monte_carlo_price_simulation(self, 
                                   initial_price: float,
                                   returns_data: pd.Series,
                                   n_simulations: int = None,
                                   time_horizon: int = None) -> np.ndarray:
        """
        ëª¬í…Œì¹´ë¥¼ë¡œ ê°€ê²© ê²½ë¡œ ì‹œë®¬ë ˆì´ì…˜
        
        Args:
            initial_price: ì´ˆê¸° ê°€ê²©
            returns_data: ìˆ˜ìµë¥  ë°ì´í„°
            n_simulations: ì‹œë®¬ë ˆì´ì…˜ íšŸìˆ˜
            time_horizon: ì‹œê°„ ì§€í‰ì„ 
            
        Returns:
            ì‹œë®¬ë ˆì´ì…˜ëœ ê°€ê²© ê²½ë¡œë“¤
        """
        self.logger.info("ğŸ“ˆ ëª¬í…Œì¹´ë¥¼ë¡œ ê°€ê²© ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘...")
        
        n_simulations = n_simulations or self.hyperparams['monte_carlo']['n_simulations']
        time_horizon = time_horizon or self.hyperparams['monte_carlo']['time_horizon']
        
        # ìˆ˜ìµë¥  ë¶„í¬ íŒŒë¼ë¯¸í„° ì¶”ì •
        mu = returns_data.mean()
        sigma = returns_data.std()
        
        # ì‹œë®¬ë ˆì´ì…˜
        price_paths = np.zeros((n_simulations, time_horizon))
        price_paths[:, 0] = initial_price
        
        for t in range(1, time_horizon):
            # ì •ê·œë¶„í¬ì—ì„œ ìˆ˜ìµë¥  ìƒ˜í”Œë§
            random_returns = np.random.normal(mu, sigma, n_simulations)
            
            # ê°€ê²© ì—…ë°ì´íŠ¸ (ë¡œê·¸ ì •ê·œ ê°€ì •)
            price_paths[:, t] = price_paths[:, t-1] * (1 + random_returns)
        
        self.logger.info(f"âœ… {n_simulations}ê°œ ì‹œë®¬ë ˆì´ì…˜ ê²½ë¡œ ìƒì„± ì™„ë£Œ")
        return price_paths
    
    def bootstrap_resample(self, data: pd.DataFrame, n_samples: int = 1000) -> List[pd.DataFrame]:
        """
        ë¶€íŠ¸ìŠ¤íŠ¸ë© ë¦¬ìƒ˜í”Œë§
        
        Args:
            data: ì›ë³¸ ë°ì´í„°
            n_samples: ìƒ˜í”Œ ìˆ˜
            
        Returns:
            ë¶€íŠ¸ìŠ¤íŠ¸ë© ìƒ˜í”Œë“¤
        """
        self.logger.info(f"ğŸ”„ ë¶€íŠ¸ìŠ¤íŠ¸ë© {n_samples}ê°œ ìƒ˜í”Œ ìƒì„±...")
        
        bootstrap_samples = []
        n_observations = len(data)
        
        for i in range(n_samples):
            # ë³µì› ì¶”ì¶œ
            indices = np.random.choice(n_observations, n_observations, replace=True)
            bootstrap_sample = data.iloc[indices].reset_index(drop=True)
            bootstrap_samples.append(bootstrap_sample)
        
        self.logger.info("âœ… ë¶€íŠ¸ìŠ¤íŠ¸ë© ìƒ˜í”Œë§ ì™„ë£Œ")
        return bootstrap_samples
    
    def generate_market_shock_scenarios(self, 
                                      base_data: pd.DataFrame,
                                      shock_types: List[str] = None) -> Dict[str, pd.DataFrame]:
        """
        ì‹œì¥ ì¶©ê²© ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±
        
        Args:
            base_data: ê¸°ì¤€ ë°ì´í„°
            shock_types: ì¶©ê²© ìœ í˜• ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ì¶©ê²© ì‹œë‚˜ë¦¬ì˜¤ë“¤
        """
        self.logger.info("âš¡ ì‹œì¥ ì¶©ê²© ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±...")
        
        shock_types = shock_types or ['flash_crash', 'bubble_burst', 'regulatory_shock', 'whale_dump']
        scenarios = {}
        
        # ê°€ê²© ì»¬ëŸ¼ ì°¾ê¸°
        price_col = None
        for col in base_data.columns:
            if 'price' in col.lower() or 'close' in col.lower():
                price_col = col
                break
        
        if price_col is None:
            price_col = base_data.select_dtypes(include=[np.number]).columns[0]
        
        for shock_type in shock_types:
            shocked_data = base_data.copy()
            
            if shock_type == 'flash_crash':
                # ê°‘ì‘ìŠ¤ëŸ¬ìš´ ê¸‰ë½ (30-50% í•˜ë½ í›„ ë¶€ë¶„ íšŒë³µ)
                shock_point = len(shocked_data) // 2
                crash_magnitude = np.random.uniform(0.3, 0.5)  # 30-50% í•˜ë½
                recovery_factor = np.random.uniform(0.6, 0.8)  # 60-80% íšŒë³µ
                
                # ê¸‰ë½
                shocked_data.loc[shock_point:shock_point+5, price_col] *= (1 - crash_magnitude)
                # ë¶€ë¶„ íšŒë³µ
                recovery_points = min(20, len(shocked_data) - shock_point - 5)
                recovery_slope = crash_magnitude * recovery_factor / recovery_points
                
                for i in range(recovery_points):
                    idx = shock_point + 5 + i
                    if idx < len(shocked_data):
                        shocked_data.loc[idx, price_col] *= (1 + recovery_slope * (i + 1) / recovery_points)
            
            elif shock_type == 'bubble_burst':
                # ë²„ë¸” ë¶•ê´´ (ì ì§„ì  ëŒ€í­ë½)
                burst_start = int(len(shocked_data) * 0.6)
                burst_duration = min(50, len(shocked_data) - burst_start)
                total_decline = np.random.uniform(0.6, 0.8)  # 60-80% í•˜ë½
                
                for i in range(burst_duration):
                    idx = burst_start + i
                    if idx < len(shocked_data):
                        daily_decline = total_decline * (1 - np.exp(-i / 10)) / burst_duration
                        shocked_data.loc[idx, price_col] *= (1 - daily_decline)
            
            elif shock_type == 'regulatory_shock':
                # ê·œì œ ì¶©ê²© (ê³„ë‹¨ì‹ í•˜ë½)
                shock_points = np.random.choice(len(shocked_data), 3, replace=False)
                shock_points = np.sort(shock_points)
                
                for point in shock_points:
                    decline = np.random.uniform(0.15, 0.25)  # 15-25% í•˜ë½
                    shocked_data.loc[point:, price_col] *= (1 - decline)
            
            elif shock_type == 'whale_dump':
                # ê³ ë˜ ë§¤ë„ (ëŒ€ëŸ‰ ë§¤ë„ë¡œ ì¸í•œ ê¸‰ë½ê³¼ ë¹ ë¥¸ íšŒë³µ)
                dump_point = np.random.randint(10, len(shocked_data) - 10)
                dump_magnitude = np.random.uniform(0.2, 0.35)  # 20-35% í•˜ë½
                
                # ê¸‰ë½
                shocked_data.loc[dump_point:dump_point+2, price_col] *= (1 - dump_magnitude)
                # ë¹ ë¥¸ íšŒë³µ (24ì‹œê°„ ë‚´)
                recovery_points = min(24, len(shocked_data) - dump_point - 2)
                for i in range(recovery_points):
                    idx = dump_point + 2 + i
                    if idx < len(shocked_data):
                        recovery = dump_magnitude * np.exp(-i / 8) / recovery_points  # ì§€ìˆ˜ì  íšŒë³µ
                        shocked_data.loc[idx, price_col] *= (1 + recovery)
            
            scenarios[shock_type] = shocked_data
        
        self.logger.info(f"âœ… {len(scenarios)}ê°œ ì¶©ê²© ì‹œë‚˜ë¦¬ì˜¤ ìƒì„± ì™„ë£Œ")
        return scenarios
    
    def inverse_transform_synthetic_data(self, synthetic_data: np.ndarray) -> List[pd.DataFrame]:
        """
        í•©ì„± ë°ì´í„°ë¥¼ ì›ë³¸ ìŠ¤ì¼€ì¼ë¡œ ì—­ë³€í™˜
        
        Args:
            synthetic_data: í•©ì„± ë°ì´í„°
            
        Returns:
            ì—­ë³€í™˜ëœ ë°ì´í„°í”„ë ˆì„ ë¦¬ìŠ¤íŠ¸
        """
        if 'main' not in self.scalers:
            self.logger.warning("ìŠ¤ì¼€ì¼ëŸ¬ê°€ ì—†ìŠµë‹ˆë‹¤. ì—­ë³€í™˜ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return []
        
        scaler = self.scalers['main']
        transformed_dfs = []
        
        for i, sequence in enumerate(synthetic_data):
            # ì—­ë³€í™˜
            original_shape = sequence.shape
            flattened = sequence.reshape(-1, sequence.shape[-1])
            inverse_scaled = scaler.inverse_transform(flattened)
            restored_sequence = inverse_scaled.reshape(original_shape)
            
            # ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
            df = pd.DataFrame(restored_sequence)
            df.index = pd.date_range(start='2024-01-01', periods=len(df), freq='H')
            
            transformed_dfs.append(df)
        
        return transformed_dfs
    
    def evaluate_synthetic_data_quality(self, 
                                       original_data: np.ndarray,
                                       synthetic_data: np.ndarray) -> Dict[str, float]:
        """
        í•©ì„± ë°ì´í„° í’ˆì§ˆ í‰ê°€
        
        Args:
            original_data: ì›ë³¸ ë°ì´í„°
            synthetic_data: í•©ì„± ë°ì´í„°
            
        Returns:
            í’ˆì§ˆ ë©”íŠ¸ë¦­
        """
        metrics = {}
        
        try:
            # 1. ë¶„í¬ ìœ ì‚¬ì„± (Wasserstein ê±°ë¦¬)
            from scipy.stats import wasserstein_distance
            
            wasserstein_distances = []
            for feature_idx in range(original_data.shape[-1]):
                orig_feature = original_data[:, :, feature_idx].flatten()
                synth_feature = synthetic_data[:, :, feature_idx].flatten()
                
                distance = wasserstein_distance(orig_feature, synth_feature)
                wasserstein_distances.append(distance)
            
            metrics['wasserstein_distance'] = np.mean(wasserstein_distances)
            
            # 2. í†µê³„ì  íŠ¹ì„± ë³´ì¡´
            orig_mean = np.mean(original_data, axis=(0, 1))
            synth_mean = np.mean(synthetic_data, axis=(0, 1))
            orig_std = np.std(original_data, axis=(0, 1))
            synth_std = np.std(synthetic_data, axis=(0, 1))
            
            mean_similarity = 1 - np.mean(np.abs(orig_mean - synth_mean) / (np.abs(orig_mean) + 1e-8))
            std_similarity = 1 - np.mean(np.abs(orig_std - synth_std) / (orig_std + 1e-8))
            
            metrics['statistical_similarity'] = (mean_similarity + std_similarity) / 2
            
            # 3. ì‹œê³„ì—´ íŠ¹ì„± (ìê¸°ìƒê´€)
            orig_autocorr = []
            synth_autocorr = []
            
            for i in range(min(10, original_data.shape[0])):
                for j in range(original_data.shape[-1]):
                    orig_series = original_data[i, :, j]
                    synth_series = synthetic_data[i, :, j]
                    
                    if len(orig_series) > 1:
                        orig_ac = np.corrcoef(orig_series[:-1], orig_series[1:])[0, 1]
                        synth_ac = np.corrcoef(synth_series[:-1], synth_series[1:])[0, 1]
                        
                        if not (np.isnan(orig_ac) or np.isnan(synth_ac)):
                            orig_autocorr.append(orig_ac)
                            synth_autocorr.append(synth_ac)
            
            if orig_autocorr and synth_autocorr:
                autocorr_similarity = 1 - np.mean(np.abs(np.array(orig_autocorr) - np.array(synth_autocorr)))
                metrics['temporal_similarity'] = max(0, autocorr_similarity)
            else:
                metrics['temporal_similarity'] = 0.5
            
            # 4. ì¢…í•© í’ˆì§ˆ ì ìˆ˜
            metrics['overall_quality'] = np.mean([
                1 - metrics['wasserstein_distance'] / (1 + metrics['wasserstein_distance']),
                metrics['statistical_similarity'],
                metrics['temporal_similarity']
            ])
            
        except Exception as e:
            self.logger.warning(f"í’ˆì§ˆ í‰ê°€ ì˜¤ë¥˜: {e}")
            metrics = {'error': str(e), 'overall_quality': 0}
        
        return metrics
    
    def save_synthetic_data(self, 
                          synthetic_data: Dict[str, np.ndarray],
                          output_dir: str = "synthetic_btc_data") -> None:
        """
        í•©ì„± ë°ì´í„° ì €ì¥
        
        Args:
            synthetic_data: í•©ì„± ë°ì´í„° ë”•ì…”ë„ˆë¦¬
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        """
        self.logger.info(f"ğŸ’¾ í•©ì„± ë°ì´í„° ì €ì¥: {output_dir}")
        
        os.makedirs(output_dir, exist_ok=True)
        
        for method, data in synthetic_data.items():
            method_dir = os.path.join(output_dir, method)
            os.makedirs(method_dir, exist_ok=True)
            
            # NumPy ë°°ì—´ë¡œ ì €ì¥
            np.save(os.path.join(method_dir, "synthetic_sequences.npy"), data)
            
            # CSV í˜•íƒœë¡œë„ ì €ì¥ (ì¼ë¶€ë§Œ)
            if len(data) > 0:
                sample_size = min(100, len(data))
                sample_data = data[:sample_size]
                
                for i, sequence in enumerate(sample_data):
                    df = pd.DataFrame(sequence)
                    df.to_csv(os.path.join(method_dir, f"sample_{i:03d}.csv"), index=False)
        
        self.logger.info("âœ… í•©ì„± ë°ì´í„° ì €ì¥ ì™„ë£Œ")


def main():
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
    """
    print("ğŸ”® í•©ì„± ë¹„íŠ¸ì½”ì¸ ë°ì´í„° ìƒì„± ì‹œìŠ¤í…œ ì‹œì‘")
    
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    generator = SyntheticBitcoinDataGenerator(sequence_length=168)
    
    # ì˜ˆì œ ë°ì´í„° ìƒì„± (ì‹¤ì œë¡œëŠ” ì‹¤ì œ ë°ì´í„°ë¥¼ ë¡œë“œ)
    print("\nğŸ“Š ì˜ˆì œ ë°ì´í„° ìƒì„±...")
    np.random.seed(42)
    
    # ê°€ìƒì˜ ë¹„íŠ¸ì½”ì¸ ì‹œê³„ì—´ ë°ì´í„° ìƒì„±
    n_samples = 1000
    seq_length = 168
    n_features = 5
    
    example_data = np.random.randn(n_samples, seq_length, n_features)
    example_data = np.cumsum(example_data, axis=1)  # ì‹œê³„ì—´ íŠ¹ì„± ë¶€ì—¬
    
    print(f"ì˜ˆì œ ë°ì´í„° í˜•íƒœ: {example_data.shape}")
    
    if TENSORFLOW_AVAILABLE:
        try:
            # GAN í›ˆë ¨
            print("\nğŸ”¥ GAN ëª¨ë¸ í›ˆë ¨...")
            gan_history = generator.train_gan(example_data, epochs=100)  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì ì€ ì—í­
            
            # VAE í›ˆë ¨
            print("\nğŸ”¥ VAE ëª¨ë¸ í›ˆë ¨...")
            vae_history = generator.train_vae(example_data, epochs=50)  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì ì€ ì—í­
            
            # í•©ì„± ë°ì´í„° ìƒì„±
            print("\nğŸ² í•©ì„± ë°ì´í„° ìƒì„±...")
            gan_samples = generator.generate_gan_samples(100)
            vae_samples = generator.generate_vae_samples(100)
            
            # í’ˆì§ˆ í‰ê°€
            print("\nğŸ” í’ˆì§ˆ í‰ê°€...")
            gan_quality = generator.evaluate_synthetic_data_quality(example_data[:100], gan_samples)
            vae_quality = generator.evaluate_synthetic_data_quality(example_data[:100], vae_samples)
            
            print(f"GAN í’ˆì§ˆ ì ìˆ˜: {gan_quality.get('overall_quality', 0):.3f}")
            print(f"VAE í’ˆì§ˆ ì ìˆ˜: {vae_quality.get('overall_quality', 0):.3f}")
            
            # ì €ì¥
            synthetic_data = {
                'gan_samples': gan_samples,
                'vae_samples': vae_samples
            }
            generator.save_synthetic_data(synthetic_data)
            
        except Exception as e:
            print(f"âš ï¸ ë”¥ëŸ¬ë‹ ëª¨ë¸ í›ˆë ¨ ì˜¤ë¥˜: {e}")
    
    # ëª¬í…Œì¹´ë¥¼ë¡œ ì‹œë®¬ë ˆì´ì…˜ (TensorFlow ì—†ì´ë„ ì‹¤í–‰ ê°€ëŠ¥)
    print("\nğŸ“ˆ ëª¬í…Œì¹´ë¥¼ë¡œ ì‹œë®¬ë ˆì´ì…˜...")
    returns_data = pd.Series(np.random.normal(0.001, 0.02, 1000))  # ì˜ˆì œ ìˆ˜ìµë¥ 
    price_paths = generator.monte_carlo_price_simulation(
        initial_price=50000,
        returns_data=returns_data,
        n_simulations=1000,
        time_horizon=168
    )
    
    print(f"ëª¬í…Œì¹´ë¥¼ë¡œ ê²½ë¡œ í˜•íƒœ: {price_paths.shape}")
    
    # ì‹œì¥ ì¶©ê²© ì‹œë‚˜ë¦¬ì˜¤
    print("\nâš¡ ì‹œì¥ ì¶©ê²© ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±...")
    example_df = pd.DataFrame({'price': np.random.randn(100).cumsum() + 50000})
    shock_scenarios = generator.generate_market_shock_scenarios(example_df)
    
    print(f"ìƒì„±ëœ ì¶©ê²© ì‹œë‚˜ë¦¬ì˜¤: {list(shock_scenarios.keys())}")
    
    print("\nâœ… í•©ì„± ë°ì´í„° ìƒì„± ì‹œìŠ¤í…œ ì™„ë£Œ!")


if __name__ == "__main__":
    main()