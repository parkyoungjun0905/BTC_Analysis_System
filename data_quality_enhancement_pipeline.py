#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë°ì´í„° í’ˆì§ˆ í–¥ìƒ íŒŒì´í”„ë¼ì¸
- ì´ìƒì¹˜ íƒì§€ ë° ì²˜ë¦¬
- ê²°ì¸¡ì¹˜ ëŒ€ì²´ ì „ëµ
- ë…¸ì´ì¦ˆ ê°ì†Œ ê¸°ë²•
- ì‹ í˜¸ ì¶”ì¶œ ë° ê°•í™”
"""

import numpy as np
import pandas as pd
import warnings
warnings.filterwarnings('ignore')

import os
import sys
import json
import sqlite3
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Union, Callable
import logging
import pickle
from dataclasses import dataclass
from abc import ABC, abstractmethod

# ê³¼í•™ ê³„ì‚°
from scipy import stats
from scipy.signal import savgol_filter, butter, filtfilt, find_peaks
from scipy.interpolate import interp1d, UnivariateSpline
from scipy.ndimage import gaussian_filter1d, median_filter
from scipy.optimize import minimize_scalar
from scipy.fft import fft, fftfreq

# ë¨¸ì‹ ëŸ¬ë‹
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.decomposition import PCA, FastICA
from sklearn.cluster import DBSCAN, IsolationForest
from sklearn.ensemble import IsolationForest, LocalOutlierFactor
from sklearn.covariance import EllipticEnvelope
from sklearn.neighbors import LocalOutlierFactor
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import HuberRegressor, RANSACRegressor
from sklearn.impute import KNNImputer

# í†µê³„ ë° ì‹œê°í™”
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.dates import DateFormatter
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class QualityMetrics:
    """ë°ì´í„° í’ˆì§ˆ ë©”íŠ¸ë¦­"""
    completeness: float  # ì™„ì „ì„± (ê²°ì¸¡ì¹˜ ë¹„ìœ¨)
    consistency: float   # ì¼ê´€ì„± 
    accuracy: float      # ì •í™•ì„±
    validity: float      # ìœ íš¨ì„±
    uniqueness: float    # ìœ ì¼ì„±
    timeliness: float    # ì ì‹œì„±
    overall_score: float # ì¢…í•© ì ìˆ˜

class OutlierDetector(ABC):
    """ì´ìƒì¹˜ íƒì§€ ê¸°ë³¸ í´ë˜ìŠ¤"""
    
    @abstractmethod
    def detect(self, data: np.ndarray) -> np.ndarray:
        """ì´ìƒì¹˜ íƒì§€ (True: ì´ìƒì¹˜, False: ì •ìƒ)"""
        pass

class StatisticalOutlierDetector(OutlierDetector):
    """í†µê³„ì  ì´ìƒì¹˜ íƒì§€"""
    
    def __init__(self, method: str = 'zscore', threshold: float = 3.0):
        """
        Args:
            method: íƒì§€ ë°©ë²• ('zscore', 'iqr', 'modified_zscore')
            threshold: ì„ê³„ê°’
        """
        self.method = method
        self.threshold = threshold
    
    def detect(self, data: np.ndarray) -> np.ndarray:
        """í†µê³„ì  ë°©ë²•ìœ¼ë¡œ ì´ìƒì¹˜ íƒì§€"""
        data = np.asarray(data).flatten()
        
        if self.method == 'zscore':
            z_scores = np.abs(stats.zscore(data, nan_policy='omit'))
            return z_scores > self.threshold
            
        elif self.method == 'iqr':
            q1, q3 = np.nanpercentile(data, [25, 75])
            iqr = q3 - q1
            lower_bound = q1 - self.threshold * iqr
            upper_bound = q3 + self.threshold * iqr
            return (data < lower_bound) | (data > upper_bound)
            
        elif self.method == 'modified_zscore':
            median = np.nanmedian(data)
            mad = np.nanmedian(np.abs(data - median))
            modified_z_scores = 0.6745 * (data - median) / mad
            return np.abs(modified_z_scores) > self.threshold
            
        else:
            raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ë°©ë²•: {self.method}")

class IsolationForestDetector(OutlierDetector):
    """Isolation Forest ì´ìƒì¹˜ íƒì§€"""
    
    def __init__(self, contamination: float = 0.1, random_state: int = 42):
        """
        Args:
            contamination: ì´ìƒì¹˜ ë¹„ìœ¨
            random_state: ëœë¤ ì‹œë“œ
        """
        self.contamination = contamination
        self.random_state = random_state
        self.model = IsolationForest(
            contamination=contamination, 
            random_state=random_state
        )
    
    def detect(self, data: np.ndarray) -> np.ndarray:
        """Isolation Forestë¡œ ì´ìƒì¹˜ íƒì§€"""
        if data.ndim == 1:
            data = data.reshape(-1, 1)
        
        # ê²°ì¸¡ì¹˜ ì œê±°
        mask = ~np.isnan(data).any(axis=1)
        clean_data = data[mask]
        
        if len(clean_data) == 0:
            return np.zeros(len(data), dtype=bool)
        
        # ì´ìƒì¹˜ íƒì§€
        outlier_labels = self.model.fit_predict(clean_data)
        
        # ê²°ê³¼ ë§¤í•‘
        result = np.zeros(len(data), dtype=bool)
        result[mask] = outlier_labels == -1
        
        return result

class TimeSeriesOutlierDetector(OutlierDetector):
    """ì‹œê³„ì—´ íŠ¹í™” ì´ìƒì¹˜ íƒì§€"""
    
    def __init__(self, window_size: int = 24, threshold: float = 3.0):
        """
        Args:
            window_size: ìœˆë„ìš° í¬ê¸°
            threshold: ì„ê³„ê°’
        """
        self.window_size = window_size
        self.threshold = threshold
    
    def detect(self, data: np.ndarray) -> np.ndarray:
        """ì‹œê³„ì—´ íŒ¨í„´ ê¸°ë°˜ ì´ìƒì¹˜ íƒì§€"""
        data = np.asarray(data).flatten()
        outliers = np.zeros(len(data), dtype=bool)
        
        # ë¡¤ë§ í†µê³„
        rolling_mean = pd.Series(data).rolling(
            window=self.window_size, center=True
        ).mean()
        rolling_std = pd.Series(data).rolling(
            window=self.window_size, center=True
        ).std()
        
        # ì´ìƒì¹˜ íƒì§€
        for i in range(len(data)):
            if pd.isna(rolling_mean.iloc[i]) or pd.isna(rolling_std.iloc[i]):
                continue
            
            deviation = abs(data[i] - rolling_mean.iloc[i])
            if deviation > self.threshold * rolling_std.iloc[i]:
                outliers[i] = True
        
        return outliers

class MissingValueImputer(ABC):
    """ê²°ì¸¡ì¹˜ ëŒ€ì²´ ê¸°ë³¸ í´ë˜ìŠ¤"""
    
    @abstractmethod
    def impute(self, data: np.ndarray) -> np.ndarray:
        """ê²°ì¸¡ì¹˜ ëŒ€ì²´"""
        pass

class TimeSeriesImputer(MissingValueImputer):
    """ì‹œê³„ì—´ íŠ¹í™” ê²°ì¸¡ì¹˜ ëŒ€ì²´"""
    
    def __init__(self, method: str = 'interpolation', **kwargs):
        """
        Args:
            method: ëŒ€ì²´ ë°©ë²• ('interpolation', 'forward_fill', 'backward_fill', 'seasonal')
            **kwargs: ì¶”ê°€ íŒŒë¼ë¯¸í„°
        """
        self.method = method
        self.kwargs = kwargs
    
    def impute(self, data: np.ndarray) -> np.ndarray:
        """ì‹œê³„ì—´ íŠ¹í™” ê²°ì¸¡ì¹˜ ëŒ€ì²´"""
        data = np.asarray(data).copy()
        
        if self.method == 'interpolation':
            # ì„ í˜• ë³´ê°„
            mask = ~np.isnan(data)
            if np.sum(mask) < 2:
                return data
            
            indices = np.arange(len(data))
            f = interp1d(indices[mask], data[mask], 
                        kind=self.kwargs.get('kind', 'linear'),
                        bounds_error=False, fill_value='extrapolate')
            data[~mask] = f(indices[~mask])
            
        elif self.method == 'forward_fill':
            # ì „ì§„ ì±„ìš°ê¸°
            for i in range(1, len(data)):
                if np.isnan(data[i]) and not np.isnan(data[i-1]):
                    data[i] = data[i-1]
                    
        elif self.method == 'backward_fill':
            # í›„ì§„ ì±„ìš°ê¸°
            for i in range(len(data)-2, -1, -1):
                if np.isnan(data[i]) and not np.isnan(data[i+1]):
                    data[i] = data[i+1]
                    
        elif self.method == 'seasonal':
            # ê³„ì ˆì„± ê¸°ë°˜ ëŒ€ì²´
            period = self.kwargs.get('period', 24)  # ê¸°ë³¸ 24ì‹œê°„ ì£¼ê¸°
            
            for i in range(len(data)):
                if np.isnan(data[i]):
                    # ê°™ì€ ì‹œê°„ëŒ€ì˜ ê³¼ê±° ê°’ë“¤ ì°¾ê¸°
                    seasonal_indices = np.arange(i % period, len(data), period)
                    seasonal_values = data[seasonal_indices]
                    seasonal_values = seasonal_values[~np.isnan(seasonal_values)]
                    
                    if len(seasonal_values) > 0:
                        data[i] = np.mean(seasonal_values)
        
        return data

class KNNTimeSeriesImputer(MissingValueImputer):
    """KNN ê¸°ë°˜ ì‹œê³„ì—´ ê²°ì¸¡ì¹˜ ëŒ€ì²´"""
    
    def __init__(self, n_neighbors: int = 5, window_size: int = 48):
        """
        Args:
            n_neighbors: ì´ì›ƒ ìˆ˜
            window_size: ìœˆë„ìš° í¬ê¸°
        """
        self.n_neighbors = n_neighbors
        self.window_size = window_size
    
    def impute(self, data: np.ndarray) -> np.ndarray:
        """KNNìœ¼ë¡œ ì‹œê³„ì—´ ê²°ì¸¡ì¹˜ ëŒ€ì²´"""
        if data.ndim == 1:
            data = data.reshape(-1, 1)
        
        # ìœˆë„ìš° ê¸°ë°˜ íŠ¹ì„± ìƒì„±
        windowed_data = []
        for i in range(self.window_size, len(data)):
            window = data[i-self.window_size:i].flatten()
            windowed_data.append(window)
        
        windowed_data = np.array(windowed_data)
        
        # KNN ëŒ€ì²´
        imputer = KNNImputer(n_neighbors=self.n_neighbors)
        imputed_windowed = imputer.fit_transform(windowed_data)
        
        # ê²°ê³¼ ì¬êµ¬ì„±
        result = data.copy()
        for i, window in enumerate(imputed_windowed):
            window = window.reshape(-1, data.shape[1])
            result[i+self.window_size-1] = window[-1]
        
        return result.flatten() if result.shape[1] == 1 else result

class NoiseReducer(ABC):
    """ë…¸ì´ì¦ˆ ê°ì†Œ ê¸°ë³¸ í´ë˜ìŠ¤"""
    
    @abstractmethod
    def reduce(self, data: np.ndarray) -> np.ndarray:
        """ë…¸ì´ì¦ˆ ê°ì†Œ"""
        pass

class SavitzkyGolayDenoiser(NoiseReducer):
    """Savitzky-Golay í•„í„° ë…¸ì´ì¦ˆ ê°ì†Œ"""
    
    def __init__(self, window_length: int = 11, polyorder: int = 3):
        """
        Args:
            window_length: ìœˆë„ìš° ê¸¸ì´ (í™€ìˆ˜)
            polyorder: ë‹¤í•­ì‹ ì°¨ìˆ˜
        """
        self.window_length = window_length if window_length % 2 == 1 else window_length + 1
        self.polyorder = polyorder
    
    def reduce(self, data: np.ndarray) -> np.ndarray:
        """Savitzky-Golay í•„í„° ì ìš©"""
        if len(data) < self.window_length:
            return data
        
        return savgol_filter(data, self.window_length, self.polyorder)

class WaveletDenoiser(NoiseReducer):
    """ì›¨ì´ë¸”ë¦¿ ê¸°ë°˜ ë…¸ì´ì¦ˆ ê°ì†Œ"""
    
    def __init__(self, wavelet: str = 'db4', threshold_mode: str = 'soft'):
        """
        Args:
            wavelet: ì›¨ì´ë¸”ë¦¿ íƒ€ì…
            threshold_mode: ì„ê³„ê°’ ëª¨ë“œ
        """
        self.wavelet = wavelet
        self.threshold_mode = threshold_mode
        
        try:
            import pywt
            self.pywt = pywt
            self.available = True
        except ImportError:
            self.available = False
            logger.warning("PyWavelets ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤. ì›¨ì´ë¸”ë¦¿ ë…¸ì´ì¦ˆ ì œê±°ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
    
    def reduce(self, data: np.ndarray) -> np.ndarray:
        """ì›¨ì´ë¸”ë¦¿ ë…¸ì´ì¦ˆ ê°ì†Œ"""
        if not self.available:
            return data
        
        # ì›¨ì´ë¸”ë¦¿ ë¶„í•´
        coeffs = self.pywt.wavedec(data, self.wavelet)
        
        # ì„ê³„ê°’ ê³„ì‚° (MAD ê¸°ë°˜)
        sigma = np.median(np.abs(coeffs[-1])) / 0.6745
        threshold = sigma * np.sqrt(2 * np.log(len(data)))
        
        # ì„ê³„ê°’ ì ìš©
        coeffs_thresh = list(coeffs)
        coeffs_thresh[1:] = [self.pywt.threshold(c, threshold, self.threshold_mode) 
                            for c in coeffs[1:]]
        
        # ì›¨ì´ë¸”ë¦¿ ì¬êµ¬ì„±
        return self.pywt.waverec(coeffs_thresh, self.wavelet)

class AdaptiveNoiseReducer(NoiseReducer):
    """ì ì‘í˜• ë…¸ì´ì¦ˆ ê°ì†Œ"""
    
    def __init__(self, methods: List[str] = None):
        """
        Args:
            methods: ì‚¬ìš©í•  ë°©ë²•ë“¤ ë¦¬ìŠ¤íŠ¸
        """
        self.methods = methods or ['savgol', 'median', 'gaussian']
        self.reducers = {
            'savgol': SavitzkyGolayDenoiser(),
            'median': lambda data: median_filter(data, size=5),
            'gaussian': lambda data: gaussian_filter1d(data, sigma=1.0)
        }
    
    def reduce(self, data: np.ndarray) -> np.ndarray:
        """ì ì‘í˜• ë…¸ì´ì¦ˆ ê°ì†Œ"""
        results = []
        
        for method in self.methods:
            if method in self.reducers:
                if hasattr(self.reducers[method], 'reduce'):
                    denoised = self.reducers[method].reduce(data)
                else:
                    denoised = self.reducers[method](data)
                results.append(denoised)
        
        if not results:
            return data
        
        # ê²°ê³¼ë“¤ì˜ ê°€ì¤‘ í‰ê· 
        weights = np.ones(len(results)) / len(results)
        return np.average(results, axis=0, weights=weights)

class SignalExtractor:
    """ì‹ í˜¸ ì¶”ì¶œ ë° ê°•í™”"""
    
    def __init__(self):
        """ì‹ í˜¸ ì¶”ì¶œê¸° ì´ˆê¸°í™”"""
        pass
    
    def extract_trend(self, data: np.ndarray, method: str = 'hp_filter', **kwargs) -> np.ndarray:
        """íŠ¸ë Œë“œ ì¶”ì¶œ"""
        if method == 'hp_filter':
            return self._hp_filter(data, **kwargs)
        elif method == 'moving_average':
            window = kwargs.get('window', 24)
            return pd.Series(data).rolling(window, center=True).mean().values
        elif method == 'lowess':
            return self._lowess_trend(data, **kwargs)
        else:
            raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ë°©ë²•: {method}")
    
    def _hp_filter(self, data: np.ndarray, lambda_param: float = 1600) -> np.ndarray:
        """Hodrick-Prescott í•„í„° (ë‹¨ìˆœí™”ëœ ë²„ì „)"""
        from scipy.sparse import diags
        from scipy.sparse.linalg import spsolve
        
        n = len(data)
        if n < 4:
            return data
        
        # 2ì°¨ ì°¨ë¶„ í–‰ë ¬
        d2 = diags([1, -2, 1], [0, 1, 2], shape=(n-2, n)).tocsc()
        
        # HP í•„í„° í–‰ë ¬
        I = diags([1], [0], shape=(n, n)).tocsc()
        trend = spsolve(I + lambda_param * d2.T @ d2, data)
        
        return trend
    
    def _lowess_trend(self, data: np.ndarray, frac: float = 0.3) -> np.ndarray:
        """LOWESS íŠ¸ë Œë“œ (ë‹¨ìˆœí™”ëœ ë²„ì „)"""
        n = len(data)
        trend = np.zeros(n)
        
        h = int(np.ceil(frac * n))
        
        for i in range(n):
            # ì´ì›ƒ ì¸ë±ìŠ¤
            start = max(0, i - h // 2)
            end = min(n, i + h // 2 + 1)
            
            # ê°€ì¤‘ì¹˜ (ê±°ë¦¬ ê¸°ë°˜)
            indices = np.arange(start, end)
            weights = np.maximum(0, 1 - np.abs(indices - i) / h)
            
            # ê°€ì¤‘ íšŒê·€
            if np.sum(weights) > 0:
                X = indices.reshape(-1, 1)
                y = data[start:end]
                w = weights
                
                # ê°€ì¤‘ ìµœì†Œì œê³±ë²•
                X_w = X * w.reshape(-1, 1)
                y_w = y * w
                
                try:
                    beta = np.linalg.solve(X_w.T @ X, X_w.T @ y_w)
                    trend[i] = beta @ np.array([i])
                except:
                    trend[i] = np.average(y, weights=w)
            else:
                trend[i] = data[i]
        
        return trend
    
    def extract_cycles(self, data: np.ndarray, min_period: int = 6, max_period: int = 72) -> Dict[str, np.ndarray]:
        """ì£¼ê¸°ì  ì‹ í˜¸ ì¶”ì¶œ (FFT ê¸°ë°˜)"""
        # FFT
        fft_values = fft(data)
        freqs = fftfreq(len(data))
        
        # ì£¼ê¸° ë²”ìœ„ ê³„ì‚°
        min_freq = 1 / max_period
        max_freq = 1 / min_period
        
        # ì£¼íŒŒìˆ˜ í•„í„°ë§
        filtered_fft = fft_values.copy()
        mask = (np.abs(freqs) < min_freq) | (np.abs(freqs) > max_freq)
        filtered_fft[mask] = 0
        
        # ì—­ë³€í™˜
        cycles = np.real(np.fft.ifft(filtered_fft))
        
        # ì£¼ìš” ì£¼ê¸° ì°¾ê¸°
        power_spectrum = np.abs(fft_values) ** 2
        periods = 1 / freqs[freqs > 0]
        
        # ìœ íš¨í•œ ì£¼ê¸° ë²”ìœ„ ë‚´ì—ì„œ ìµœëŒ€ íŒŒì›Œ ì£¼ê¸°ë“¤
        valid_mask = (periods >= min_period) & (periods <= max_period)
        valid_periods = periods[valid_mask]
        valid_powers = power_spectrum[freqs > 0][valid_mask]
        
        # ìƒìœ„ 3ê°œ ì£¼ê¸°
        top_indices = np.argsort(valid_powers)[-3:]
        dominant_periods = valid_periods[top_indices]
        
        result = {
            'cycles': cycles,
            'dominant_periods': dominant_periods,
            'power_spectrum': power_spectrum,
            'frequencies': freqs
        }
        
        return result
    
    def enhance_signal_to_noise_ratio(self, data: np.ndarray, 
                                    method: str = 'spectral_subtraction') -> np.ndarray:
        """ì‹ í˜¸ ëŒ€ ì¡ìŒ ë¹„ìœ¨ í–¥ìƒ"""
        if method == 'spectral_subtraction':
            return self._spectral_subtraction(data)
        elif method == 'wiener_filter':
            return self._wiener_filter(data)
        else:
            raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ë°©ë²•: {method}")
    
    def _spectral_subtraction(self, data: np.ndarray, alpha: float = 2.0) -> np.ndarray:
        """ìŠ¤í™íŠ¸ëŸ´ ì°¨ê° ë°©ë²•"""
        # FFT
        fft_data = fft(data)
        magnitude = np.abs(fft_data)
        phase = np.angle(fft_data)
        
        # ë…¸ì´ì¦ˆ ì¶”ì • (ë‚®ì€ ì£¼íŒŒìˆ˜ ì„±ë¶„ ì œì™¸í•œ í‰ê· )
        high_freq_mask = np.abs(fftfreq(len(data))) > 0.1
        noise_estimate = np.mean(magnitude[high_freq_mask])
        
        # ìŠ¤í™íŠ¸ëŸ´ ì°¨ê°
        enhanced_magnitude = magnitude - alpha * noise_estimate
        enhanced_magnitude = np.maximum(enhanced_magnitude, 0.1 * magnitude)
        
        # ì¬êµ¬ì„±
        enhanced_fft = enhanced_magnitude * np.exp(1j * phase)
        enhanced_signal = np.real(np.fft.ifft(enhanced_fft))
        
        return enhanced_signal
    
    def _wiener_filter(self, data: np.ndarray) -> np.ndarray:
        """ìœ„ë„ˆ í•„í„° (ë‹¨ìˆœí™”ëœ ë²„ì „)"""
        # ì‹ í˜¸ì˜ ìê¸°ìƒê´€ ì¶”ì •
        signal_power = np.var(data)
        
        # ë…¸ì´ì¦ˆ íŒŒì›Œ ì¶”ì • (ê³ ì£¼íŒŒ ì„±ë¶„)
        diff = np.diff(data)
        noise_power = np.var(diff) / 2
        
        # ìœ„ë„ˆ í•„í„° ê³„ìˆ˜
        wiener_coeff = signal_power / (signal_power + noise_power)
        
        # í•„í„° ì ìš© (ë‹¨ìˆœ ìŠ¤ì¼€ì¼ë§)
        return data * wiener_coeff

class DataQualityEnhancementPipeline:
    """
    ğŸ“ˆ ë°ì´í„° í’ˆì§ˆ í–¥ìƒ íŒŒì´í”„ë¼ì¸
    
    ì£¼ìš” ê¸°ëŠ¥:
    1. ì¢…í•©ì  ì´ìƒì¹˜ íƒì§€ ë° ì²˜ë¦¬
    2. ì§€ëŠ¥í˜• ê²°ì¸¡ì¹˜ ëŒ€ì²´
    3. ë‹¤ì°¨ì› ë…¸ì´ì¦ˆ ê°ì†Œ
    4. ì‹ í˜¸ ì¶”ì¶œ ë° ê°•í™”
    5. í’ˆì§ˆ ë©”íŠ¸ë¦­ í‰ê°€
    """
    
    def __init__(self):
        """íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”"""
        self.logger = logging.getLogger(__name__)
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.outlier_detectors = {
            'statistical': StatisticalOutlierDetector(),
            'isolation_forest': IsolationForestDetector(),
            'timeseries': TimeSeriesOutlierDetector()
        }
        
        self.imputers = {
            'timeseries': TimeSeriesImputer(),
            'knn': KNNTimeSeriesImputer()
        }
        
        self.denoisers = {
            'savgol': SavitzkyGolayDenoiser(),
            'wavelet': WaveletDenoiser(),
            'adaptive': AdaptiveNoiseReducer()
        }
        
        self.signal_extractor = SignalExtractor()
        
        # ì²˜ë¦¬ ì´ë ¥
        self.processing_history = []
        
        self.logger.info("ğŸ“ˆ ë°ì´í„° í’ˆì§ˆ í–¥ìƒ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def assess_data_quality(self, data: pd.DataFrame) -> QualityMetrics:
        """ë°ì´í„° í’ˆì§ˆ í‰ê°€"""
        metrics = {}
        
        # ì™„ì „ì„± (ê²°ì¸¡ì¹˜ ë¹„ìœ¨)
        total_cells = data.size
        missing_cells = data.isnull().sum().sum()
        completeness = 1 - (missing_cells / total_cells) if total_cells > 0 else 0
        
        # ì¼ê´€ì„± (ë°ì´í„° íƒ€ì… ì¼ê´€ì„±)
        consistency_score = 0
        for col in data.columns:
            if data[col].dtype in ['int64', 'float64']:
                # ìˆ«ìí˜• ë°ì´í„°ì˜ ì¼ê´€ì„±
                finite_ratio = np.isfinite(data[col]).sum() / len(data[col])
                consistency_score += finite_ratio
        consistency_score = consistency_score / len(data.columns) if len(data.columns) > 0 else 0
        
        # ì •í™•ì„± (ì´ìƒì¹˜ ë¹„ìœ¨ë¡œ ê·¼ì‚¬)
        outlier_ratios = []
        for col in data.select_dtypes(include=[np.number]).columns:
            try:
                detector = StatisticalOutlierDetector()
                outliers = detector.detect(data[col].dropna().values)
                outlier_ratio = np.sum(outliers) / len(outliers) if len(outliers) > 0 else 0
                outlier_ratios.append(1 - outlier_ratio)
            except:
                outlier_ratios.append(0.9)  # ê¸°ë³¸ê°’
        accuracy = np.mean(outlier_ratios) if outlier_ratios else 0
        
        # ìœ íš¨ì„± (ìœ íš¨í•œ ê°’ì˜ ë¹„ìœ¨)
        validity_scores = []
        for col in data.columns:
            if data[col].dtype in ['int64', 'float64']:
                valid_ratio = (~data[col].isnull() & np.isfinite(data[col])).sum() / len(data[col])
                validity_scores.append(valid_ratio)
        validity = np.mean(validity_scores) if validity_scores else 0
        
        # ìœ ì¼ì„± (ì¤‘ë³µ í–‰ ë¹„ìœ¨)
        uniqueness = 1 - (len(data) - len(data.drop_duplicates())) / len(data) if len(data) > 0 else 0
        
        # ì ì‹œì„± (ì‹œê°„ ê°„ê²© ì¼ê´€ì„±, ì‹œê³„ì—´ ë°ì´í„°ì¸ ê²½ìš°)
        timeliness = 1.0  # ê¸°ë³¸ê°’
        if isinstance(data.index, pd.DatetimeIndex):
            time_diffs = data.index.to_series().diff().dropna()
            if len(time_diffs) > 1:
                mode_diff = time_diffs.mode().iloc[0] if len(time_diffs.mode()) > 0 else time_diffs.median()
                consistent_intervals = (time_diffs == mode_diff).sum()
                timeliness = consistent_intervals / len(time_diffs)
        
        # ì¢…í•© ì ìˆ˜
        overall_score = np.mean([
            completeness * 0.25,
            consistency * 0.20,
            accuracy * 0.25,
            validity * 0.15,
            uniqueness * 0.10,
            timeliness * 0.05
        ])
        
        return QualityMetrics(
            completeness=completeness,
            consistency=consistency,
            accuracy=accuracy,
            validity=validity,
            uniqueness=uniqueness,
            timeliness=timeliness,
            overall_score=overall_score
        )
    
    def detect_outliers(self, data: pd.DataFrame, 
                       methods: List[str] = None) -> Dict[str, pd.DataFrame]:
        """ì¢…í•©ì  ì´ìƒì¹˜ íƒì§€"""
        methods = methods or ['statistical', 'isolation_forest', 'timeseries']
        
        results = {}
        
        for method in methods:
            if method not in self.outlier_detectors:
                continue
            
            detector = self.outlier_detectors[method]
            outlier_data = pd.DataFrame(index=data.index)
            
            for col in data.select_dtypes(include=[np.number]).columns:
                try:
                    outliers = detector.detect(data[col].values)
                    outlier_data[col] = outliers
                except Exception as e:
                    self.logger.warning(f"ì´ìƒì¹˜ íƒì§€ ì˜¤ë¥˜ ({method}, {col}): {e}")
                    outlier_data[col] = False
            
            results[method] = outlier_data
        
        return results
    
    def handle_outliers(self, data: pd.DataFrame, 
                       outliers: Dict[str, pd.DataFrame],
                       strategy: str = 'ensemble') -> pd.DataFrame:
        """ì´ìƒì¹˜ ì²˜ë¦¬"""
        cleaned_data = data.copy()
        
        if strategy == 'ensemble':
            # ì•™ìƒë¸”: ì—¬ëŸ¬ ë°©ë²•ì—ì„œ ê³µí†µìœ¼ë¡œ íƒì§€ëœ ì´ìƒì¹˜ë§Œ ì²˜ë¦¬
            for col in cleaned_data.select_dtypes(include=[np.number]).columns:
                outlier_votes = np.zeros(len(cleaned_data), dtype=int)
                
                for method, outlier_df in outliers.items():
                    if col in outlier_df.columns:
                        outlier_votes += outlier_df[col].astype(int)
                
                # ê³¼ë°˜ìˆ˜ íˆ¬í‘œë¡œ ì´ìƒì¹˜ ê²°ì •
                threshold = len(outliers) // 2 + 1
                final_outliers = outlier_votes >= threshold
                
                if np.any(final_outliers):
                    # ì´ìƒì¹˜ë¥¼ ì¤‘ì•™ê°’ìœ¼ë¡œ ëŒ€ì²´
                    median_value = cleaned_data[col].median()
                    cleaned_data.loc[final_outliers, col] = median_value
                    
                    self.logger.info(f"{col}: {np.sum(final_outliers)}ê°œ ì´ìƒì¹˜ ì²˜ë¦¬")
        
        return cleaned_data
    
    def impute_missing_values(self, data: pd.DataFrame, 
                            method: str = 'adaptive') -> pd.DataFrame:
        """ê²°ì¸¡ì¹˜ ëŒ€ì²´"""
        imputed_data = data.copy()
        
        for col in imputed_data.select_dtypes(include=[np.number]).columns:
            if imputed_data[col].isnull().any():
                missing_count = imputed_data[col].isnull().sum()
                missing_ratio = missing_count / len(imputed_data[col])
                
                try:
                    if method == 'adaptive':
                        # ê²°ì¸¡ì¹˜ ë¹„ìœ¨ì— ë”°ë¼ ì ì‘ì  ë°©ë²• ì„ íƒ
                        if missing_ratio < 0.05:
                            # ì ì€ ê²°ì¸¡ì¹˜: ì„ í˜• ë³´ê°„
                            imputer = TimeSeriesImputer('interpolation')
                        elif missing_ratio < 0.2:
                            # ì¤‘ê°„ ê²°ì¸¡ì¹˜: ê³„ì ˆì„± ê¸°ë°˜
                            imputer = TimeSeriesImputer('seasonal')
                        else:
                            # ë§ì€ ê²°ì¸¡ì¹˜: KNN
                            imputer = KNNTimeSeriesImputer()
                    else:
                        imputer = self.imputers.get(method, TimeSeriesImputer())
                    
                    imputed_values = imputer.impute(imputed_data[col].values)
                    imputed_data[col] = imputed_values
                    
                    self.logger.info(f"{col}: {missing_count}ê°œ ê²°ì¸¡ì¹˜ ëŒ€ì²´")
                    
                except Exception as e:
                    self.logger.warning(f"ê²°ì¸¡ì¹˜ ëŒ€ì²´ ì˜¤ë¥˜ ({col}): {e}")
        
        return imputed_data
    
    def reduce_noise(self, data: pd.DataFrame, 
                   method: str = 'adaptive') -> pd.DataFrame:
        """ë…¸ì´ì¦ˆ ê°ì†Œ"""
        denoised_data = data.copy()
        
        if method not in self.denoisers:
            self.logger.warning(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ë…¸ì´ì¦ˆ ì œê±° ë°©ë²•: {method}")
            return denoised_data
        
        denoiser = self.denoisers[method]
        
        for col in denoised_data.select_dtypes(include=[np.number]).columns:
            try:
                original_data = denoised_data[col].values
                denoised_values = denoiser.reduce(original_data)
                denoised_data[col] = denoised_values
                
                # ë…¸ì´ì¦ˆ ê°ì†Œ íš¨ê³¼ ì¸¡ì •
                noise_reduction = np.std(original_data - denoised_values) / np.std(original_data)
                self.logger.info(f"{col}: ë…¸ì´ì¦ˆ {noise_reduction:.3f} ê°ì†Œ")
                
            except Exception as e:
                self.logger.warning(f"ë…¸ì´ì¦ˆ ê°ì†Œ ì˜¤ë¥˜ ({col}): {e}")
        
        return denoised_data
    
    def extract_signals(self, data: pd.DataFrame) -> Dict[str, pd.DataFrame]:
        """ì‹ í˜¸ ì¶”ì¶œ"""
        signals = {}
        
        # íŠ¸ë Œë“œ ì¶”ì¶œ
        trend_data = pd.DataFrame(index=data.index)
        for col in data.select_dtypes(include=[np.number]).columns:
            try:
                trend = self.signal_extractor.extract_trend(data[col].values)
                trend_data[col] = trend
            except Exception as e:
                self.logger.warning(f"íŠ¸ë Œë“œ ì¶”ì¶œ ì˜¤ë¥˜ ({col}): {e}")
                trend_data[col] = data[col]
        
        signals['trend'] = trend_data
        
        # ì£¼ê¸°ì„± ì¶”ì¶œ
        cycles_data = pd.DataFrame(index=data.index)
        for col in data.select_dtypes(include=[np.number]).columns:
            try:
                cycles_result = self.signal_extractor.extract_cycles(data[col].values)
                cycles_data[col] = cycles_result['cycles']
            except Exception as e:
                self.logger.warning(f"ì£¼ê¸°ì„± ì¶”ì¶œ ì˜¤ë¥˜ ({col}): {e}")
                cycles_data[col] = 0
        
        signals['cycles'] = cycles_data
        
        # ì”ì—¬ ì„±ë¶„ (ì›ë³¸ - íŠ¸ë Œë“œ - ì£¼ê¸°ì„±)
        residual_data = data.select_dtypes(include=[np.number]) - trend_data - cycles_data
        signals['residual'] = residual_data
        
        return signals
    
    def enhance_data_quality(self, data: pd.DataFrame, 
                           config: Dict = None) -> Tuple[pd.DataFrame, Dict]:
        """ì¢…í•©ì  ë°ì´í„° í’ˆì§ˆ í–¥ìƒ"""
        self.logger.info("ğŸš€ ì¢…í•©ì  ë°ì´í„° í’ˆì§ˆ í–¥ìƒ ì‹œì‘...")
        
        config = config or {
            'detect_outliers': True,
            'handle_outliers': True,
            'impute_missing': True,
            'reduce_noise': True,
            'extract_signals': False,
            'enhance_snr': False
        }
        
        enhanced_data = data.copy()
        enhancement_log = {'steps': [], 'metrics': {}}
        
        # ì´ˆê¸° í’ˆì§ˆ í‰ê°€
        initial_quality = self.assess_data_quality(enhanced_data)
        enhancement_log['metrics']['initial'] = initial_quality
        
        # 1. ì´ìƒì¹˜ íƒì§€ ë° ì²˜ë¦¬
        if config.get('detect_outliers', True):
            self.logger.info("ğŸ” ì´ìƒì¹˜ íƒì§€ ì¤‘...")
            outliers = self.detect_outliers(enhanced_data)
            enhancement_log['steps'].append('outlier_detection')
            
            if config.get('handle_outliers', True):
                self.logger.info("ğŸ› ï¸ ì´ìƒì¹˜ ì²˜ë¦¬ ì¤‘...")
                enhanced_data = self.handle_outliers(enhanced_data, outliers)
                enhancement_log['steps'].append('outlier_handling')
        
        # 2. ê²°ì¸¡ì¹˜ ëŒ€ì²´
        if config.get('impute_missing', True):
            self.logger.info("ğŸ”„ ê²°ì¸¡ì¹˜ ëŒ€ì²´ ì¤‘...")
            enhanced_data = self.impute_missing_values(enhanced_data)
            enhancement_log['steps'].append('missing_value_imputation')
        
        # 3. ë…¸ì´ì¦ˆ ê°ì†Œ
        if config.get('reduce_noise', True):
            self.logger.info("ğŸ”‡ ë…¸ì´ì¦ˆ ê°ì†Œ ì¤‘...")
            enhanced_data = self.reduce_noise(enhanced_data)
            enhancement_log['steps'].append('noise_reduction')
        
        # 4. ì‹ í˜¸ ì¶”ì¶œ (ì˜µì…˜)
        if config.get('extract_signals', False):
            self.logger.info("ğŸ“¡ ì‹ í˜¸ ì¶”ì¶œ ì¤‘...")
            signals = self.extract_signals(enhanced_data)
            enhancement_log['signals'] = signals
            enhancement_log['steps'].append('signal_extraction')
        
        # 5. SNR í–¥ìƒ (ì˜µì…˜)
        if config.get('enhance_snr', False):
            self.logger.info("ğŸ“ˆ SNR í–¥ìƒ ì¤‘...")
            for col in enhanced_data.select_dtypes(include=[np.number]).columns:
                try:
                    enhanced_values = self.signal_extractor.enhance_signal_to_noise_ratio(
                        enhanced_data[col].values
                    )
                    enhanced_data[col] = enhanced_values
                except Exception as e:
                    self.logger.warning(f"SNR í–¥ìƒ ì˜¤ë¥˜ ({col}): {e}")
            enhancement_log['steps'].append('snr_enhancement')
        
        # ìµœì¢… í’ˆì§ˆ í‰ê°€
        final_quality = self.assess_data_quality(enhanced_data)
        enhancement_log['metrics']['final'] = final_quality
        
        # ê°œì„  íš¨ê³¼ ê³„ì‚°
        improvement = final_quality.overall_score - initial_quality.overall_score
        enhancement_log['improvement'] = improvement
        
        self.logger.info(f"âœ… ë°ì´í„° í’ˆì§ˆ í–¥ìƒ ì™„ë£Œ! ê°œì„ ë„: {improvement:.3f}")
        
        return enhanced_data, enhancement_log
    
    def generate_quality_report(self, 
                              original_data: pd.DataFrame,
                              enhanced_data: pd.DataFrame,
                              enhancement_log: Dict) -> str:
        """í’ˆì§ˆ í–¥ìƒ ë³´ê³ ì„œ ìƒì„±"""
        initial_metrics = enhancement_log['metrics']['initial']
        final_metrics = enhancement_log['metrics']['final']
        
        html_report = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>ë°ì´í„° í’ˆì§ˆ í–¥ìƒ ë³´ê³ ì„œ</title>
            <meta charset="utf-8">
            <style>
                body {{ font-family: Arial, sans-serif; margin: 20px; }}
                .header {{ background: #2c3e50; color: white; padding: 20px; border-radius: 5px; }}
                .section {{ margin: 20px 0; padding: 15px; border: 1px solid #ddd; border-radius: 5px; }}
                .metric {{ display: inline-block; margin: 10px; padding: 10px; background: #f8f9fa; border-radius: 3px; }}
                .improvement {{ background: #d4edda; color: #155724; }}
                .degradation {{ background: #f8d7da; color: #721c24; }}
                table {{ width: 100%; border-collapse: collapse; margin: 10px 0; }}
                th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
                th {{ background: #f2f2f2; }}
            </style>
        </head>
        <body>
            <div class="header">
                <h1>ğŸ“ˆ ë°ì´í„° í’ˆì§ˆ í–¥ìƒ ë³´ê³ ì„œ</h1>
                <p>ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
            </div>
            
            <div class="section">
                <h2>ğŸ“Š í’ˆì§ˆ ë©”íŠ¸ë¦­ ë¹„êµ</h2>
                <table>
                    <tr><th>ë©”íŠ¸ë¦­</th><th>ì²˜ë¦¬ ì „</th><th>ì²˜ë¦¬ í›„</th><th>ê°œì„ ë„</th></tr>
                    <tr>
                        <td>ì™„ì „ì„±</td>
                        <td>{initial_metrics.completeness:.3f}</td>
                        <td>{final_metrics.completeness:.3f}</td>
                        <td>{final_metrics.completeness - initial_metrics.completeness:.3f}</td>
                    </tr>
                    <tr>
                        <td>ì¼ê´€ì„±</td>
                        <td>{initial_metrics.consistency:.3f}</td>
                        <td>{final_metrics.consistency:.3f}</td>
                        <td>{final_metrics.consistency - initial_metrics.consistency:.3f}</td>
                    </tr>
                    <tr>
                        <td>ì •í™•ì„±</td>
                        <td>{initial_metrics.accuracy:.3f}</td>
                        <td>{final_metrics.accuracy:.3f}</td>
                        <td>{final_metrics.accuracy - initial_metrics.accuracy:.3f}</td>
                    </tr>
                    <tr>
                        <td>ìœ íš¨ì„±</td>
                        <td>{initial_metrics.validity:.3f}</td>
                        <td>{final_metrics.validity:.3f}</td>
                        <td>{final_metrics.validity - initial_metrics.validity:.3f}</td>
                    </tr>
                    <tr>
                        <td>ì¢…í•© ì ìˆ˜</td>
                        <td>{initial_metrics.overall_score:.3f}</td>
                        <td>{final_metrics.overall_score:.3f}</td>
                        <td>{enhancement_log['improvement']:.3f}</td>
                    </tr>
                </table>
            </div>
            
            <div class="section">
                <h2>ğŸ”„ ì²˜ë¦¬ ë‹¨ê³„</h2>
                <ul>
                    {''.join(f'<li>{step}</li>' for step in enhancement_log['steps'])}
                </ul>
            </div>
            
            <div class="section">
                <h2>ğŸ“ˆ ë°ì´í„° í†µê³„</h2>
                <table>
                    <tr><th>í•­ëª©</th><th>ì²˜ë¦¬ ì „</th><th>ì²˜ë¦¬ í›„</th></tr>
                    <tr>
                        <td>í–‰ ìˆ˜</td>
                        <td>{len(original_data):,}</td>
                        <td>{len(enhanced_data):,}</td>
                    </tr>
                    <tr>
                        <td>ì—´ ìˆ˜</td>
                        <td>{len(original_data.columns)}</td>
                        <td>{len(enhanced_data.columns)}</td>
                    </tr>
                    <tr>
                        <td>ê²°ì¸¡ì¹˜ ìˆ˜</td>
                        <td>{original_data.isnull().sum().sum():,}</td>
                        <td>{enhanced_data.isnull().sum().sum():,}</td>
                    </tr>
                </table>
            </div>
        </body>
        </html>
        """
        
        return html_report


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ“ˆ ë°ì´í„° í’ˆì§ˆ í–¥ìƒ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
    
    # íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    pipeline = DataQualityEnhancementPipeline()
    
    # ì˜ˆì œ ë°ì´í„° ìƒì„± (í’ˆì§ˆ ë¬¸ì œê°€ ìˆëŠ” ë°ì´í„°)
    print("\nğŸ“Š ì˜ˆì œ ë°ì´í„° ìƒì„±...")
    np.random.seed(42)
    
    # ì‹œê³„ì—´ ë°ì´í„° ìƒì„±
    n_samples = 1000
    dates = pd.date_range('2024-01-01', periods=n_samples, freq='H')
    
    # ê¸°ë³¸ ì‹ í˜¸ (íŠ¸ë Œë“œ + ì£¼ê¸°ì„± + ë…¸ì´ì¦ˆ)
    trend = np.linspace(100, 120, n_samples)
    seasonal = 10 * np.sin(2 * np.pi * np.arange(n_samples) / 24)  # ì¼ì¼ ì£¼ê¸°
    noise = np.random.normal(0, 2, n_samples)
    signal = trend + seasonal + noise
    
    # í’ˆì§ˆ ë¬¸ì œ ì¶”ê°€
    # 1. ê²°ì¸¡ì¹˜
    missing_indices = np.random.choice(n_samples, size=50, replace=False)
    signal[missing_indices] = np.nan
    
    # 2. ì´ìƒì¹˜
    outlier_indices = np.random.choice(n_samples, size=20, replace=False)
    signal[outlier_indices] += np.random.normal(0, 20, 20)
    
    # 3. ì¶”ê°€ ë…¸ì´ì¦ˆ
    high_noise_indices = np.random.choice(n_samples, size=100, replace=False)
    signal[high_noise_indices] += np.random.normal(0, 5, 100)
    
    # ë°ì´í„°í”„ë ˆì„ ìƒì„±
    data = pd.DataFrame({
        'price': signal,
        'volume': np.random.lognormal(10, 0.5, n_samples),
        'feature1': np.cumsum(np.random.randn(n_samples) * 0.1),
        'feature2': signal * 0.5 + np.random.randn(n_samples)
    }, index=dates)
    
    print(f"ìƒì„±ëœ ë°ì´í„°: {data.shape}")
    print(f"ê²°ì¸¡ì¹˜ ìˆ˜: {data.isnull().sum().sum()}")
    
    # ì´ˆê¸° í’ˆì§ˆ í‰ê°€
    print("\nğŸ“ ì´ˆê¸° í’ˆì§ˆ í‰ê°€...")
    initial_quality = pipeline.assess_data_quality(data)
    print(f"ì´ˆê¸° í’ˆì§ˆ ì ìˆ˜: {initial_quality.overall_score:.3f}")
    
    # í’ˆì§ˆ í–¥ìƒ ì‹¤í–‰
    print("\nğŸš€ í’ˆì§ˆ í–¥ìƒ ì‹¤í–‰...")
    enhanced_data, enhancement_log = pipeline.enhance_data_quality(data, {
        'detect_outliers': True,
        'handle_outliers': True,
        'impute_missing': True,
        'reduce_noise': True,
        'extract_signals': False,
        'enhance_snr': True
    })
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"\nâœ… í’ˆì§ˆ í–¥ìƒ ì™„ë£Œ!")
    print(f"ìµœì¢… í’ˆì§ˆ ì ìˆ˜: {enhancement_log['metrics']['final'].overall_score:.3f}")
    print(f"ê°œì„ ë„: {enhancement_log['improvement']:.3f}")
    print(f"ì²˜ë¦¬ëœ ê²°ì¸¡ì¹˜: {data.isnull().sum().sum() - enhanced_data.isnull().sum().sum()}")
    
    # ë³´ê³ ì„œ ìƒì„±
    print("\nğŸ“‹ í’ˆì§ˆ ë³´ê³ ì„œ ìƒì„±...")
    report_html = pipeline.generate_quality_report(data, enhanced_data, enhancement_log)
    
    with open("data_quality_enhancement_report.html", "w", encoding="utf-8") as f:
        f.write(report_html)
    
    print("\nâœ… ë°ì´í„° í’ˆì§ˆ í–¥ìƒ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
    print("ğŸ“‹ ìƒì„¸ ë³´ê³ ì„œ: data_quality_enhancement_report.html")


if __name__ == "__main__":
    main()