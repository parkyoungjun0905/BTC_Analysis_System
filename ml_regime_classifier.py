#!/usr/bin/env python3
"""
ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ì‹¤ì‹œê°„ ì‹œì¥ ì²´ì œ ë¶„ë¥˜ê¸°
ë‹¤ì–‘í•œ ML ì•Œê³ ë¦¬ì¦˜ì„ ì•™ìƒë¸”í•˜ì—¬ Bitcoin ì‹œì¥ ì²´ì œë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ë¶„ë¥˜

ì§€ì› ì•Œê³ ë¦¬ì¦˜:
1. Random Forest - ë¹„ì„ í˜• íŒ¨í„´ ê°ì§€
2. Gradient Boosting - ìˆœì°¨ì  íŠ¹ì§• í•™ìŠµ  
3. SVM - ê³ ì°¨ì› ê²½ê³„ ë¶„ë¥˜
4. Neural Network - ë³µì¡í•œ ê´€ê³„ ëª¨ë¸ë§
5. XGBoost - ê³ ì„±ëŠ¥ ë¶€ìŠ¤íŒ…
"""

import os
import json
import sqlite3
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Union
import logging
from dataclasses import dataclass, asdict
import asyncio
import pickle
import joblib
from collections import defaultdict, deque
import warnings
warnings.filterwarnings("ignore")

# ë¨¸ì‹ ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.calibration import CalibratedClassifierCV
from sklearn.feature_selection import SelectKBest, f_classif, RFE
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
import xgboost as xgb

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class RegimeClassification:
    """ì²´ì œ ë¶„ë¥˜ ê²°ê³¼"""
    regime_type: str
    confidence: float
    probability_distribution: Dict[str, float]
    feature_importance: Dict[str, float]
    ensemble_votes: Dict[str, str]
    prediction_timestamp: datetime
    
@dataclass
class ModelPerformance:
    """ëª¨ë¸ ì„±ëŠ¥ ì§€í‘œ"""
    model_name: str
    accuracy: float
    precision: Dict[str, float]
    recall: Dict[str, float]
    f1_score: Dict[str, float]
    cross_val_score: float
    training_samples: int
    last_updated: datetime

class MLRegimeClassifier:
    """ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ì‹œì¥ ì²´ì œ ë¶„ë¥˜ê¸°"""
    
    def __init__(self, base_path: str = "/Users/parkyoungjun/Desktop/BTC_Analysis_System"):
        self.base_path = base_path
        self.db_path = os.path.join(base_path, "ml_regime_db.db")
        self.models_path = os.path.join(base_path, "ml_regime_models")
        os.makedirs(self.models_path, exist_ok=True)
        
        # ì²´ì œ ë ˆì´ë¸”
        self.regime_labels = {
            0: "LOW_VOLATILITY_ACCUMULATION",
            1: "BULL_MARKET",
            2: "SIDEWAYS", 
            3: "BEAR_MARKET",
            4: "HIGH_VOLATILITY_SHOCK"
        }
        
        # íŠ¹ì§• ì´ë¦„ë“¤
        self.feature_names = [
            "price_trend_1d", "price_trend_7d", "price_trend_30d", "trend_consistency",
            "volatility_1d", "volatility_7d", "volatility_30d", "volatility_regime_change", 
            "volume_trend", "volume_volatility", "volume_price_correlation",
            "rsi_14", "macd_signal", "bollinger_position",
            "whale_activity", "exchange_flow", "hodler_behavior",
            "futures_basis", "funding_rate", "put_call_ratio", "fear_greed_index",
            "correlation_gold", "correlation_stocks", "dxy_impact"
        ]
        
        # ê°œë³„ ëª¨ë¸ë“¤
        self.models = {}
        self.scalers = {}
        self.feature_selectors = {}
        self.label_encoder = LabelEncoder()
        
        # ì•™ìƒë¸” ëª¨ë¸
        self.ensemble_model = None
        self.meta_classifier = None
        
        # í•™ìŠµ ìƒíƒœ
        self.is_trained = False
        self.training_history = deque(maxlen=100)
        
        # ì‹¤ì‹œê°„ ì˜ˆì¸¡ íˆìŠ¤í† ë¦¬
        self.prediction_history = deque(maxlen=200)
        self.current_regime = None
        self.regime_confidence = 0.0
        
        # ì„±ëŠ¥ ì¶”ì 
        self.performance_metrics = {}
        
        self.init_database()
        self.init_models()
        self.load_trained_models()
    
    def init_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # ML ì˜ˆì¸¡ ê¸°ë¡
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS ml_predictions (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT NOT NULL,
                    predicted_regime TEXT NOT NULL,
                    regime_id INTEGER NOT NULL,
                    confidence REAL NOT NULL,
                    probability_distribution TEXT NOT NULL,
                    feature_importance TEXT NOT NULL,
                    ensemble_votes TEXT NOT NULL,
                    input_features TEXT NOT NULL,
                    model_version TEXT NOT NULL
                )
            ''')
            
            # ëª¨ë¸ ì„±ëŠ¥ ê¸°ë¡
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS ml_model_performance (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    model_name TEXT NOT NULL,
                    accuracy REAL NOT NULL,
                    precision_scores TEXT NOT NULL,
                    recall_scores TEXT NOT NULL,
                    f1_scores TEXT NOT NULL,
                    cross_val_score REAL NOT NULL,
                    training_samples INTEGER NOT NULL,
                    confusion_matrix TEXT NOT NULL,
                    feature_importance TEXT,
                    hyperparameters TEXT,
                    training_date TEXT NOT NULL
                )
            ''')
            
            # í•™ìŠµ ë°ì´í„°
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS ml_training_data (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT NOT NULL,
                    features TEXT NOT NULL,
                    true_regime TEXT NOT NULL,
                    regime_id INTEGER NOT NULL,
                    data_source TEXT,
                    is_validated BOOLEAN DEFAULT FALSE,
                    validation_confidence REAL
                )
            ''')
            
            # ëª¨ë¸ ë¹„êµ ë° ì„ íƒ
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS model_comparison (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    comparison_date TEXT NOT NULL,
                    test_period_days INTEGER NOT NULL,
                    model_performances TEXT NOT NULL,
                    best_model TEXT NOT NULL,
                    ensemble_performance TEXT NOT NULL,
                    recommendation TEXT NOT NULL
                )
            ''')
            
            conn.commit()
            conn.close()
            logger.info("âœ… ML ì²´ì œ ë¶„ë¥˜ê¸° ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ML ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def init_models(self):
        """ML ëª¨ë¸ë“¤ ì´ˆê¸°í™”"""
        try:
            # Random Forest
            self.models['random_forest'] = RandomForestClassifier(
                n_estimators=200,
                max_depth=15,
                min_samples_split=5,
                min_samples_leaf=2,
                random_state=42,
                n_jobs=-1,
                class_weight='balanced'
            )
            
            # Gradient Boosting
            self.models['gradient_boosting'] = GradientBoostingClassifier(
                n_estimators=150,
                learning_rate=0.1,
                max_depth=8,
                min_samples_split=5,
                random_state=42
            )
            
            # SVM
            self.models['svm'] = SVC(
                kernel='rbf',
                C=10.0,
                gamma='scale',
                probability=True,
                random_state=42,
                class_weight='balanced'
            )
            
            # Neural Network
            self.models['neural_network'] = MLPClassifier(
                hidden_layer_sizes=(128, 64, 32),
                activation='relu',
                solver='adam',
                alpha=0.001,
                learning_rate_init=0.001,
                max_iter=500,
                random_state=42,
                early_stopping=True
            )
            
            # XGBoost
            self.models['xgboost'] = xgb.XGBClassifier(
                n_estimators=200,
                learning_rate=0.1,
                max_depth=8,
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=42,
                use_label_encoder=False,
                eval_metric='mlogloss'
            )
            
            # ê° ëª¨ë¸ë³„ ìŠ¤ì¼€ì¼ëŸ¬
            for model_name in self.models.keys():
                if model_name in ['svm', 'neural_network']:
                    self.scalers[model_name] = StandardScaler()
                else:
                    self.scalers[model_name] = RobustScaler()
                    
                # íŠ¹ì§• ì„ íƒê¸°
                self.feature_selectors[model_name] = SelectKBest(
                    score_func=f_classif, 
                    k=min(15, len(self.feature_names))
                )
            
            logger.info(f"âœ… {len(self.models)}ê°œ ML ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ML ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def load_trained_models(self):
        """í•™ìŠµëœ ëª¨ë¸ë“¤ ë¡œë“œ"""
        try:
            models_loaded = 0
            
            for model_name in self.models.keys():
                model_file = os.path.join(self.models_path, f"{model_name}_regime_model.pkl")
                scaler_file = os.path.join(self.models_path, f"{model_name}_scaler.pkl")
                selector_file = os.path.join(self.models_path, f"{model_name}_selector.pkl")
                
                if all(os.path.exists(f) for f in [model_file, scaler_file, selector_file]):
                    try:
                        self.models[model_name] = joblib.load(model_file)
                        self.scalers[model_name] = joblib.load(scaler_file)
                        self.feature_selectors[model_name] = joblib.load(selector_file)
                        models_loaded += 1
                    except:
                        logger.warning(f"ëª¨ë¸ {model_name} ë¡œë“œ ì‹¤íŒ¨")
            
            # ì•™ìƒë¸” ëª¨ë¸ ë¡œë“œ
            ensemble_file = os.path.join(self.models_path, "ensemble_regime_model.pkl")
            if os.path.exists(ensemble_file):
                try:
                    self.ensemble_model = joblib.load(ensemble_file)
                    models_loaded += 1
                except:
                    logger.warning("ì•™ìƒë¸” ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
            
            # ë ˆì´ë¸” ì¸ì½”ë” ë¡œë“œ
            encoder_file = os.path.join(self.models_path, "label_encoder.pkl")
            if os.path.exists(encoder_file):
                try:
                    self.label_encoder = joblib.load(encoder_file)
                except:
                    logger.warning("ë ˆì´ë¸” ì¸ì½”ë” ë¡œë“œ ì‹¤íŒ¨")
            
            if models_loaded > 0:
                self.is_trained = True
                logger.info(f"âœ… {models_loaded}ê°œ í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            else:
                logger.info("í•™ìŠµëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ í•™ìŠµì´ í•„ìš”í•©ë‹ˆë‹¤.")
                
        except Exception as e:
            logger.error(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    async def train_models(self, training_data: List[Dict], test_size: float = 0.2) -> Dict:
        """ëª¨ë“  ML ëª¨ë¸ í•™ìŠµ"""
        try:
            if not training_data or len(training_data) < 50:
                return {"error": "ì¶©ë¶„í•œ í•™ìŠµ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤ (ìµœì†Œ 50ê°œ í•„ìš”)"}
            
            logger.info(f"ğŸ§  ML ëª¨ë¸ í•™ìŠµ ì‹œì‘ (ë°ì´í„°: {len(training_data)}ê°œ)")
            
            # ë°ì´í„° ì¤€ë¹„
            X, y = self.prepare_training_data(training_data)
            if X is None or y is None:
                return {"error": "í•™ìŠµ ë°ì´í„° ì¤€ë¹„ ì‹¤íŒ¨"}
            
            # ë ˆì´ë¸” ì¸ì½”ë”©
            y_encoded = self.label_encoder.fit_transform(y)
            
            # í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í• 
            X_train, X_test, y_train, y_test = train_test_split(
                X, y_encoded, test_size=test_size, stratify=y_encoded, random_state=42
            )
            
            # ê°œë³„ ëª¨ë¸ í•™ìŠµ
            model_results = {}
            for model_name, model in self.models.items():
                logger.info(f"ğŸ“š {model_name} ëª¨ë¸ í•™ìŠµ ì¤‘...")
                result = await self.train_single_model(
                    model_name, model, X_train, X_test, y_train, y_test
                )
                model_results[model_name] = result
            
            # ì•™ìƒë¸” ëª¨ë¸ êµ¬ì„±
            ensemble_result = await self.create_ensemble_model(X_train, X_test, y_train, y_test)
            model_results['ensemble'] = ensemble_result
            
            # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì„ ì •
            best_model = self.select_best_model(model_results)
            
            # ëª¨ë¸ë“¤ ì €ì¥
            await self.save_all_models()
            
            # í•™ìŠµ ê²°ê³¼ ì €ì¥
            await self.save_training_results(model_results, len(training_data))
            
            self.is_trained = True
            
            return {
                "training_completed": True,
                "training_samples": len(training_data),
                "test_samples": len(X_test),
                "model_results": model_results,
                "best_model": best_model,
                "ensemble_accuracy": ensemble_result.get('accuracy', 0)
            }
            
        except Exception as e:
            logger.error(f"ML ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def prepare_training_data(self, training_data: List[Dict]) -> Tuple[Optional[np.ndarray], Optional[List[str]]]:
        """í•™ìŠµ ë°ì´í„° ì¤€ë¹„ ë° ì „ì²˜ë¦¬"""
        try:
            features = []
            labels = []
            
            for data in training_data:
                feature_vector = data.get('features')
                label = data.get('true_regime')
                
                if feature_vector and label and len(feature_vector) == len(self.feature_names):
                    features.append(feature_vector)
                    labels.append(label)
            
            if len(features) < 20:
                logger.error("ìœ íš¨í•œ í•™ìŠµ ë°ì´í„°ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤")
                return None, None
            
            X = np.array(features)
            y = labels
            
            # ì´ìƒì¹˜ ì œê±°
            X_clean, y_clean = self.remove_outliers(X, y)
            
            logger.info(f"âœ… í•™ìŠµ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {len(X_clean)}ê°œ ìƒ˜í”Œ, {len(self.feature_names)}ê°œ íŠ¹ì§•")
            
            return X_clean, y_clean
            
        except Exception as e:
            logger.error(f"í•™ìŠµ ë°ì´í„° ì¤€ë¹„ ì‹¤íŒ¨: {e}")
            return None, None
    
    def remove_outliers(self, X: np.ndarray, y: List[str], threshold: float = 3.0) -> Tuple[np.ndarray, List[str]]:
        """ì´ìƒì¹˜ ì œê±°"""
        try:
            # Z-score ê¸°ë°˜ ì´ìƒì¹˜ íƒì§€
            z_scores = np.abs((X - np.mean(X, axis=0)) / np.std(X, axis=0))
            outlier_mask = np.any(z_scores > threshold, axis=1)
            
            # ì´ìƒì¹˜ê°€ ì•„ë‹Œ ë°ì´í„°ë§Œ ì„ íƒ
            clean_indices = ~outlier_mask
            X_clean = X[clean_indices]
            y_clean = [y[i] for i in range(len(y)) if clean_indices[i]]
            
            removed_count = len(X) - len(X_clean)
            if removed_count > 0:
                logger.info(f"ì´ìƒì¹˜ {removed_count}ê°œ ì œê±°ë¨")
            
            return X_clean, y_clean
            
        except Exception as e:
            logger.error(f"ì´ìƒì¹˜ ì œê±° ì‹¤íŒ¨: {e}")
            return X, y
    
    async def train_single_model(self, model_name: str, model, 
                                X_train: np.ndarray, X_test: np.ndarray,
                                y_train: np.ndarray, y_test: np.ndarray) -> Dict:
        """ê°œë³„ ëª¨ë¸ í•™ìŠµ"""
        try:
            # íŠ¹ì§• ìŠ¤ì¼€ì¼ë§
            scaler = self.scalers[model_name]
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            
            # íŠ¹ì§• ì„ íƒ
            selector = self.feature_selectors[model_name]
            X_train_selected = selector.fit_transform(X_train_scaled, y_train)
            X_test_selected = selector.transform(X_test_scaled)
            
            # í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (ì¼ë¶€ ëª¨ë¸ë§Œ)
            if model_name in ['random_forest', 'svm']:
                model = await self.hyperparameter_tuning(model_name, model, X_train_selected, y_train)
            
            # ëª¨ë¸ í•™ìŠµ
            model.fit(X_train_selected, y_train)
            
            # ì˜ˆì¸¡ ë° í‰ê°€
            y_pred = model.predict(X_test_selected)
            y_pred_proba = model.predict_proba(X_test_selected) if hasattr(model, 'predict_proba') else None
            
            # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
            accuracy = accuracy_score(y_test, y_pred)
            classification_rep = classification_report(y_test, y_pred, output_dict=True, zero_division=0)
            
            # êµì°¨ ê²€ì¦
            cv_scores = cross_val_score(model, X_train_selected, y_train, cv=5, scoring='accuracy')
            
            # íŠ¹ì§• ì¤‘ìš”ë„
            feature_importance = {}
            if hasattr(model, 'feature_importances_'):
                selected_features = selector.get_support()
                feature_names_selected = [self.feature_names[i] for i in range(len(selected_features)) if selected_features[i]]
                importance_values = model.feature_importances_
                feature_importance = dict(zip(feature_names_selected, importance_values))
            
            # ì—…ë°ì´íŠ¸ëœ ëª¨ë¸ ì €ì¥
            self.models[model_name] = model
            
            result = {
                'accuracy': accuracy,
                'cross_val_score': cv_scores.mean(),
                'classification_report': classification_rep,
                'feature_importance': feature_importance,
                'confusion_matrix': confusion_matrix(y_test, y_pred).tolist(),
                'n_features_selected': X_train_selected.shape[1],
                'cv_std': cv_scores.std()
            }
            
            logger.info(f"âœ… {model_name}: ì •í™•ë„ {accuracy:.3f}, CV {cv_scores.mean():.3f}Â±{cv_scores.std():.3f}")
            
            return result
            
        except Exception as e:
            logger.error(f"{model_name} ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    async def hyperparameter_tuning(self, model_name: str, model, X_train: np.ndarray, y_train: np.ndarray):
        """í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹"""
        try:
            if model_name == 'random_forest':
                param_grid = {
                    'n_estimators': [100, 200, 300],
                    'max_depth': [10, 15, 20],
                    'min_samples_split': [2, 5, 10]
                }
            elif model_name == 'svm':
                param_grid = {
                    'C': [1, 10, 100],
                    'gamma': ['scale', 'auto', 0.001, 0.01]
                }
            else:
                return model
            
            # ê·¸ë¦¬ë“œ ì„œì¹˜ (3-fold CVë¡œ ë¹ ë¥´ê²Œ)
            grid_search = GridSearchCV(
                model, param_grid, cv=3, scoring='accuracy', 
                n_jobs=-1, verbose=0
            )
            grid_search.fit(X_train, y_train)
            
            logger.info(f"{model_name} ìµœì  íŒŒë¼ë¯¸í„°: {grid_search.best_params_}")
            
            return grid_search.best_estimator_
            
        except Exception as e:
            logger.error(f"{model_name} í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹¤íŒ¨: {e}")
            return model
    
    async def create_ensemble_model(self, X_train: np.ndarray, X_test: np.ndarray,
                                  y_train: np.ndarray, y_test: np.ndarray) -> Dict:
        """ì•™ìƒë¸” ëª¨ë¸ ìƒì„±"""
        try:
            # ì„±ëŠ¥ ì¢‹ì€ ëª¨ë¸ë“¤ë§Œ ì„ íƒ
            good_models = []
            model_weights = []
            
            for model_name, model in self.models.items():
                if model_name in ['random_forest', 'gradient_boosting', 'xgboost']:
                    scaler = self.scalers[model_name]
                    selector = self.feature_selectors[model_name]
                    
                    X_train_processed = selector.transform(scaler.transform(X_train))
                    X_test_processed = selector.transform(scaler.transform(X_test))
                    
                    # ê°œë³„ ëª¨ë¸ ì„±ëŠ¥ í™•ì¸
                    y_pred = model.predict(X_test_processed)
                    accuracy = accuracy_score(y_test, y_pred)
                    
                    if accuracy > 0.6:  # 60% ì´ìƒ ì •í™•ë„ì¸ ëª¨ë¸ë§Œ
                        good_models.append((model_name, model))
                        model_weights.append(accuracy)
            
            if len(good_models) < 2:
                return {"error": "ì•™ìƒë¸”ì„ êµ¬ì„±í•  ì¶©ë¶„í•œ ì„±ëŠ¥ì˜ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤"}
            
            # ê°€ì¤‘ íˆ¬í‘œ ë¶„ë¥˜ê¸°
            estimators = [(name, model) for name, model in good_models]
            self.ensemble_model = VotingClassifier(
                estimators=estimators,
                voting='soft',  # í™•ë¥  ê¸°ë°˜ íˆ¬í‘œ
                weights=model_weights
            )
            
            # ì•™ìƒë¸” í•™ìŠµ (ì „ì²´ íŠ¹ì§• ì‚¬ìš©)
            X_train_ensemble = StandardScaler().fit_transform(X_train)
            X_test_ensemble = StandardScaler().fit_transform(X_test)
            
            self.ensemble_model.fit(X_train_ensemble, y_train)
            
            # ì•™ìƒë¸” ì„±ëŠ¥ í‰ê°€
            y_pred_ensemble = self.ensemble_model.predict(X_test_ensemble)
            ensemble_accuracy = accuracy_score(y_test, y_pred_ensemble)
            
            # êµì°¨ ê²€ì¦
            cv_scores = cross_val_score(
                self.ensemble_model, X_train_ensemble, y_train, cv=5, scoring='accuracy'
            )
            
            logger.info(f"âœ… ì•™ìƒë¸” ëª¨ë¸ ìƒì„±: ì •í™•ë„ {ensemble_accuracy:.3f}, CV {cv_scores.mean():.3f}")
            
            return {
                'accuracy': ensemble_accuracy,
                'cross_val_score': cv_scores.mean(),
                'cv_std': cv_scores.std(),
                'n_models': len(good_models),
                'model_names': [name for name, _ in good_models],
                'model_weights': model_weights
            }
            
        except Exception as e:
            logger.error(f"ì•™ìƒë¸” ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def select_best_model(self, model_results: Dict) -> str:
        """ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì„ ì •"""
        try:
            best_model = "ensemble"
            best_score = 0.0
            
            for model_name, result in model_results.items():
                if isinstance(result, dict) and not result.get("error"):
                    # ì •í™•ë„ì™€ êµì°¨ê²€ì¦ ì ìˆ˜ì˜ ì¡°í™”í‰ê· 
                    accuracy = result.get('accuracy', 0)
                    cv_score = result.get('cross_val_score', 0)
                    
                    if accuracy > 0 and cv_score > 0:
                        score = 2 * accuracy * cv_score / (accuracy + cv_score)
                        if score > best_score:
                            best_score = score
                            best_model = model_name
            
            logger.info(f"ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model} (ì ìˆ˜: {best_score:.3f})")
            
            return best_model
            
        except Exception as e:
            logger.error(f"ìµœê³  ëª¨ë¸ ì„ ì • ì‹¤íŒ¨: {e}")
            return "ensemble"
    
    async def predict_regime(self, features: np.ndarray) -> RegimeClassification:
        """ì‹œì¥ ì²´ì œ ì˜ˆì¸¡"""
        try:
            if not self.is_trained:
                raise ValueError("ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            
            if len(features) != len(self.feature_names):
                raise ValueError(f"íŠ¹ì§• ê°œìˆ˜ê°€ ë§ì§€ ì•ŠìŠµë‹ˆë‹¤: {len(features)} != {len(self.feature_names)}")
            
            features = features.reshape(1, -1)
            
            # ê°œë³„ ëª¨ë¸ ì˜ˆì¸¡
            model_predictions = {}
            model_probabilities = {}
            
            for model_name, model in self.models.items():
                try:
                    scaler = self.scalers[model_name]
                    selector = self.feature_selectors[model_name]
                    
                    # ì „ì²˜ë¦¬
                    features_scaled = scaler.transform(features)
                    features_selected = selector.transform(features_scaled)
                    
                    # ì˜ˆì¸¡
                    prediction = model.predict(features_selected)[0]
                    proba = model.predict_proba(features_selected)[0] if hasattr(model, 'predict_proba') else None
                    
                    # ë ˆì´ë¸” ë””ì½”ë”©
                    predicted_regime = self.label_encoder.inverse_transform([prediction])[0]
                    
                    model_predictions[model_name] = predicted_regime
                    if proba is not None:
                        model_probabilities[model_name] = dict(zip(
                            self.label_encoder.inverse_transform(range(len(proba))), proba
                        ))
                except Exception as model_error:
                    logger.warning(f"ëª¨ë¸ {model_name} ì˜ˆì¸¡ ì‹¤íŒ¨: {model_error}")
                    continue
            
            # ì•™ìƒë¸” ì˜ˆì¸¡
            ensemble_prediction = None
            ensemble_probabilities = None
            
            if self.ensemble_model:
                try:
                    features_scaled = StandardScaler().fit_transform(features)
                    ensemble_pred = self.ensemble_model.predict(features_scaled)[0]
                    ensemble_proba = self.ensemble_model.predict_proba(features_scaled)[0]
                    
                    ensemble_prediction = self.label_encoder.inverse_transform([ensemble_pred])[0]
                    ensemble_probabilities = dict(zip(
                        self.label_encoder.inverse_transform(range(len(ensemble_proba))), 
                        ensemble_proba
                    ))
                except Exception as ensemble_error:
                    logger.warning(f"ì•™ìƒë¸” ì˜ˆì¸¡ ì‹¤íŒ¨: {ensemble_error}")
            
            # ìµœì¢… ì˜ˆì¸¡ ê²°ì • (ì•™ìƒë¸” ìš°ì„ , ì—†ìœ¼ë©´ ë‹¤ìˆ˜ê²°)
            if ensemble_prediction and ensemble_probabilities:
                final_prediction = ensemble_prediction
                final_probabilities = ensemble_probabilities
                confidence = max(ensemble_probabilities.values())
            else:
                # ë‹¤ìˆ˜ê²° íˆ¬í‘œ
                prediction_votes = list(model_predictions.values())
                if prediction_votes:
                    final_prediction = max(set(prediction_votes), key=prediction_votes.count)
                    
                    # í‰ê·  í™•ë¥  ê³„ì‚°
                    if model_probabilities:
                        final_probabilities = defaultdict(float)
                        for proba_dict in model_probabilities.values():
                            for regime, prob in proba_dict.items():
                                final_probabilities[regime] += prob
                        
                        # í‰ê· í™”
                        n_models = len(model_probabilities)
                        final_probabilities = {k: v/n_models for k, v in final_probabilities.items()}
                        confidence = final_probabilities.get(final_prediction, 0.5)
                    else:
                        final_probabilities = {final_prediction: 0.5}
                        confidence = 0.5
                else:
                    raise ValueError("ëª¨ë“  ëª¨ë¸ ì˜ˆì¸¡ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")
            
            # íŠ¹ì§• ì¤‘ìš”ë„ ê³„ì‚° (Random Forest ê¸°ì¤€)
            feature_importance = {}
            if 'random_forest' in self.models and hasattr(self.models['random_forest'], 'feature_importances_'):
                rf_model = self.models['random_forest']
                rf_selector = self.feature_selectors['random_forest']
                selected_features = rf_selector.get_support()
                feature_names_selected = [self.feature_names[i] for i in range(len(selected_features)) if selected_features[i]]
                
                if len(feature_names_selected) == len(rf_model.feature_importances_):
                    feature_importance = dict(zip(feature_names_selected, rf_model.feature_importances_))
            
            # ì˜ˆì¸¡ ê²°ê³¼ ìƒì„±
            classification_result = RegimeClassification(
                regime_type=final_prediction,
                confidence=confidence,
                probability_distribution=dict(final_probabilities),
                feature_importance=feature_importance,
                ensemble_votes=model_predictions,
                prediction_timestamp=datetime.now()
            )
            
            # ì˜ˆì¸¡ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
            self.prediction_history.append(classification_result)
            self.current_regime = final_prediction
            self.regime_confidence = confidence
            
            # ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
            await self.save_prediction_result(classification_result, features.flatten())
            
            return classification_result
            
        except Exception as e:
            logger.error(f"ì²´ì œ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ ì˜ˆì¸¡ ë°˜í™˜
            return RegimeClassification(
                regime_type="SIDEWAYS",
                confidence=0.5,
                probability_distribution={"SIDEWAYS": 0.5},
                feature_importance={},
                ensemble_votes={"error": "prediction_failed"},
                prediction_timestamp=datetime.now()
            )
    
    async def save_all_models(self):
        """ëª¨ë“  ëª¨ë¸ ì €ì¥"""
        try:
            # ê°œë³„ ëª¨ë¸ë“¤ ì €ì¥
            for model_name, model in self.models.items():
                joblib.dump(model, os.path.join(self.models_path, f"{model_name}_regime_model.pkl"))
                joblib.dump(self.scalers[model_name], os.path.join(self.models_path, f"{model_name}_scaler.pkl"))
                joblib.dump(self.feature_selectors[model_name], os.path.join(self.models_path, f"{model_name}_selector.pkl"))
            
            # ì•™ìƒë¸” ëª¨ë¸ ì €ì¥
            if self.ensemble_model:
                joblib.dump(self.ensemble_model, os.path.join(self.models_path, "ensemble_regime_model.pkl"))
            
            # ë ˆì´ë¸” ì¸ì½”ë” ì €ì¥
            joblib.dump(self.label_encoder, os.path.join(self.models_path, "label_encoder.pkl"))
            
            logger.info("âœ… ëª¨ë“  ëª¨ë¸ ì €ì¥ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    async def save_training_results(self, model_results: Dict, training_samples: int):
        """í•™ìŠµ ê²°ê³¼ ì €ì¥"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            for model_name, result in model_results.items():
                if isinstance(result, dict) and not result.get("error"):
                    cursor.execute('''
                        INSERT INTO ml_model_performance
                        (model_name, accuracy, precision_scores, recall_scores, f1_scores,
                         cross_val_score, training_samples, confusion_matrix, feature_importance,
                         hyperparameters, training_date)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    ''', (
                        model_name,
                        result.get('accuracy', 0),
                        json.dumps(result.get('classification_report', {}).get('weighted avg', {})),
                        json.dumps(result.get('classification_report', {}).get('macro avg', {})),
                        json.dumps(result.get('classification_report', {}).get('macro avg', {})),
                        result.get('cross_val_score', 0),
                        training_samples,
                        json.dumps(result.get('confusion_matrix', [])),
                        json.dumps(result.get('feature_importance', {})),
                        json.dumps({}),  # í•˜ì´í¼íŒŒë¼ë¯¸í„°ëŠ” ë³„ë„ ì €ì¥ í•„ìš”ì‹œ êµ¬í˜„
                        datetime.now().isoformat()
                    ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            logger.error(f"í•™ìŠµ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    async def save_prediction_result(self, classification: RegimeClassification, features: np.ndarray):
        """ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT INTO ml_predictions
                (timestamp, predicted_regime, regime_id, confidence, probability_distribution,
                 feature_importance, ensemble_votes, input_features, model_version)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                classification.prediction_timestamp.isoformat(),
                classification.regime_type,
                list(self.regime_labels.values()).index(classification.regime_type),
                classification.confidence,
                json.dumps(classification.probability_distribution),
                json.dumps(classification.feature_importance),
                json.dumps(classification.ensemble_votes),
                json.dumps(features.tolist()),
                "v1.0"
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            logger.error(f"ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    async def get_model_diagnostics(self) -> Dict:
        """ëª¨ë¸ ì§„ë‹¨ ì •ë³´"""
        try:
            diagnostics = {
                "training_status": "trained" if self.is_trained else "untrained",
                "n_models": len(self.models),
                "current_regime": self.current_regime,
                "regime_confidence": self.regime_confidence,
                "prediction_history_length": len(self.prediction_history)
            }
            
            if self.is_trained:
                # ìµœê·¼ ì„±ëŠ¥ ì¡°íšŒ
                conn = sqlite3.connect(self.db_path)
                cursor = conn.cursor()
                
                cursor.execute('''
                    SELECT model_name, accuracy, cross_val_score, training_date
                    FROM ml_model_performance 
                    ORDER BY training_date DESC 
                    LIMIT 10
                ''')
                
                recent_performance = cursor.fetchall()
                diagnostics["recent_performance"] = [
                    {
                        "model": row[0],
                        "accuracy": row[1],
                        "cross_val_score": row[2],
                        "training_date": row[3]
                    } for row in recent_performance
                ]
                
                # ìµœê·¼ ì˜ˆì¸¡ íˆìŠ¤í† ë¦¬
                if self.prediction_history:
                    recent_predictions = list(self.prediction_history)[-5:]
                    diagnostics["recent_predictions"] = [
                        {
                            "regime": pred.regime_type,
                            "confidence": pred.confidence,
                            "timestamp": pred.prediction_timestamp.isoformat()
                        } for pred in recent_predictions
                    ]
                
                conn.close()
            
            return diagnostics
            
        except Exception as e:
            logger.error(f"ëª¨ë¸ ì§„ë‹¨ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}

# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
async def test_ml_regime_classifier():
    """ML ì²´ì œ ë¶„ë¥˜ê¸° í…ŒìŠ¤íŠ¸"""
    print("ğŸ¤– ë¨¸ì‹ ëŸ¬ë‹ ì²´ì œ ë¶„ë¥˜ê¸° í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    classifier = MLRegimeClassifier()
    
    # ì§„ë‹¨ ì •ë³´
    diagnostics = await classifier.get_model_diagnostics()
    print(f"ğŸ“Š í•™ìŠµ ìƒíƒœ: {diagnostics.get('training_status')}")
    print(f"ğŸ”¢ ëª¨ë¸ ìˆ˜: {diagnostics.get('n_models')}")
    
    if diagnostics.get('training_status') == 'trained':
        # í…ŒìŠ¤íŠ¸ íŠ¹ì§•ê°’
        test_features = np.array([
            0.03,   # price_trend_1d
            0.05,   # price_trend_7d  
            0.08,   # price_trend_30d
            0.7,    # trend_consistency
            0.04,   # volatility_1d
            0.03,   # volatility_7d
            0.035,  # volatility_30d
            0.1,    # volatility_regime_change
            0.2,    # volume_trend
            0.02,   # volume_volatility
            0.4,    # volume_price_correlation
            65,     # rsi_14
            0.05,   # macd_signal
            0.8,    # bollinger_position
            0.7,    # whale_activity
            0.1,    # exchange_flow
            0.6,    # hodler_behavior
            0.02,   # futures_basis
            0.01,   # funding_rate
            0.3,    # put_call_ratio
            65,     # fear_greed_index
            0.1,    # correlation_gold
            0.3,    # correlation_stocks
            -0.05   # dxy_impact
        ])
        
        # ì˜ˆì¸¡ ìˆ˜í–‰
        prediction = await classifier.predict_regime(test_features)
        
        print(f"\nğŸ¯ ì˜ˆì¸¡ëœ ì²´ì œ: {prediction.regime_type}")
        print(f"ğŸ”¥ ì‹ ë¢°ë„: {prediction.confidence:.1%}")
        print(f"ğŸ“Š í™•ë¥  ë¶„í¬:")
        for regime, prob in prediction.probability_distribution.items():
            print(f"   â€¢ {regime}: {prob:.1%}")
        
        print(f"ğŸ—³ï¸ ëª¨ë¸ë³„ íˆ¬í‘œ:")
        for model, vote in prediction.ensemble_votes.items():
            print(f"   â€¢ {model}: {vote}")
        
        if prediction.feature_importance:
            print(f"ğŸ” ì£¼ìš” íŠ¹ì§• (ìƒìœ„ 5ê°œ):")
            sorted_features = sorted(prediction.feature_importance.items(), 
                                   key=lambda x: x[1], reverse=True)
            for feature, importance in sorted_features[:5]:
                print(f"   â€¢ {feature}: {importance:.3f}")
    else:
        print("âš ï¸ ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        print("í•™ìŠµì„ ìœ„í•´ì„œëŠ” ì¶©ë¶„í•œ ë ˆì´ë¸”ëœ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤")
    
    print("\n" + "=" * 50)
    print("ğŸ‰ ML ì²´ì œ ë¶„ë¥˜ê¸° í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

if __name__ == "__main__":
    asyncio.run(test_ml_regime_classifier())