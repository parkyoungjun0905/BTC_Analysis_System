#!/usr/bin/env python3
"""
ğŸš€ ì™„ì „ì²´ ë¬´í•œí•™ìŠµ 95% ì‹œìŠ¤í…œ
- LSTM, Transformer, CNN, AutoEncoder ë“± ëª¨ë“  ë”¥ëŸ¬ë‹ ê¸°ìˆ  ë™ì›
- XGBoost, LightGBM, CatBoost ë“± ëª¨ë“  ë¨¸ì‹ ëŸ¬ë‹ ì•™ìƒë¸”
- ê°•í™”í•™ìŠµ, ë©”íƒ€ëŸ¬ë‹, ì‹ ê²½ì§„í™” ë“± ìµœì²¨ë‹¨ AI ê¸°ìˆ 
- 95% ì„±ê³µë¥  ë‹¬ì„±ê¹Œì§€ ëª¨ë“  ìˆ˜ë‹¨ ë™ì›í•˜ëŠ” ê¶ê·¹ ì‹œìŠ¤í…œ
"""

import pandas as pd
import numpy as np
import json
import os
import warnings
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Union
from collections import defaultdict, Counter
import random
import itertools

# ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬
try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torch.utils.data import DataLoader, TensorDataset
    PYTORCH_AVAILABLE = True
except ImportError:
    PYTORCH_AVAILABLE = False
    print("âš ï¸ PyTorch ì—†ìŒ - ë”¥ëŸ¬ë‹ ê¸°ëŠ¥ ì œí•œ")

try:
    import tensorflow as tf
    from tensorflow.keras.models import Sequential, Model
    from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, MaxPooling1D, Flatten, Input, Attention, MultiHeadAttention
    from tensorflow.keras.optimizers import Adam, RMSprop
    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
    TF_AVAILABLE = True
except ImportError:
    TF_AVAILABLE = False
    print("âš ï¸ TensorFlow ì—†ìŒ - ë”¥ëŸ¬ë‹ ê¸°ëŠ¥ ì œí•œ")

# ê³ ê¸‰ ë¨¸ì‹ ëŸ¬ë‹
try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False

try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False

try:
    import catboost as cb
    CATBOOST_AVAILABLE = True
except ImportError:
    CATBOOST_AVAILABLE = False

# ê¸°ë³¸ ë¨¸ì‹ ëŸ¬ë‹
from sklearn.ensemble import (RandomForestRegressor, GradientBoostingRegressor, 
                              ExtraTreesRegressor, AdaBoostRegressor, BaggingRegressor)
from sklearn.linear_model import (Ridge, Lasso, ElasticNet, BayesianRidge, 
                                  HuberRegressor, SGDRegressor)
from sklearn.neural_network import MLPRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.preprocessing import (RobustScaler, StandardScaler, MinMaxScaler, 
                                   PowerTransformer, QuantileTransformer)
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from sklearn.model_selection import cross_val_score, TimeSeriesSplit
from sklearn.decomposition import PCA, ICA, FastICA
from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression

warnings.filterwarnings('ignore')

class UltimateInfiniteLearningSystem:
    """ì™„ì „ì²´ ë¬´í•œí•™ìŠµ 95% ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.data_path = "/Users/parkyoungjun/Desktop/BTC_Analysis_System"
        self.historical_data = None
        
        # í•™ìŠµ ê¸°ë¡
        self.all_predictions = []
        self.success_patterns = []
        self.failure_patterns = []
        self.discovered_rules = []
        
        # ì„±ëŠ¥ ì¶”ì 
        self.total_attempts = 0
        self.success_count = 0
        self.current_success_rate = 0.0
        self.target_success_rate = 95.0
        
        # ëª¨ë“  ëª¨ë¸ íƒ€ì… ì •ì˜
        self.model_types = self._initialize_all_models()
        
        # ë™ì  í•™ìŠµ íŒŒë¼ë¯¸í„°
        self.learning_weights = defaultdict(float)
        self.model_performance = defaultdict(lambda: {'success': 0, 'attempts': 0, 'avg_accuracy': 0})
        self.feature_importance_history = defaultdict(list)
        
        print("ğŸš€ ì™„ì „ì²´ ë¬´í•œí•™ìŠµ 95% ì‹œìŠ¤í…œ ì´ˆê¸°í™”")
        print(f"ğŸ¯ ëª©í‘œ: {self.target_success_rate}% ì„±ê³µë¥ ")
        print(f"ğŸ¤– ì‚¬ìš© ê°€ëŠ¥ ëª¨ë¸: {len(self.model_types)}ê°€ì§€")
        
    def _initialize_all_models(self) -> Dict:
        """ëª¨ë“  ê°€ëŠ¥í•œ ëª¨ë¸ ì´ˆê¸°í™”"""
        models = {
            # ê¸°ë³¸ ë¨¸ì‹ ëŸ¬ë‹
            'rf': {'type': 'sklearn', 'class': RandomForestRegressor, 'params': {'n_estimators': 100, 'random_state': 42}},
            'gb': {'type': 'sklearn', 'class': GradientBoostingRegressor, 'params': {'n_estimators': 100, 'random_state': 42}},
            'extra_trees': {'type': 'sklearn', 'class': ExtraTreesRegressor, 'params': {'n_estimators': 100, 'random_state': 42}},
            'ada_boost': {'type': 'sklearn', 'class': AdaBoostRegressor, 'params': {'n_estimators': 100, 'random_state': 42}},
            'bagging': {'type': 'sklearn', 'class': BaggingRegressor, 'params': {'n_estimators': 100, 'random_state': 42}},
            
            # ì„ í˜• ëª¨ë¸
            'ridge': {'type': 'sklearn', 'class': Ridge, 'params': {'alpha': 1.0}},
            'lasso': {'type': 'sklearn', 'class': Lasso, 'params': {'alpha': 1.0}},
            'elastic': {'type': 'sklearn', 'class': ElasticNet, 'params': {'alpha': 1.0}},
            'bayesian_ridge': {'type': 'sklearn', 'class': BayesianRidge, 'params': {}},
            'huber': {'type': 'sklearn', 'class': HuberRegressor, 'params': {}},
            'sgd': {'type': 'sklearn', 'class': SGDRegressor, 'params': {'random_state': 42}},
            
            # ì‹ ê²½ë§
            'mlp': {'type': 'sklearn', 'class': MLPRegressor, 'params': {'hidden_layer_sizes': (100, 50), 'random_state': 42, 'max_iter': 500}},
            
            # ê¸°íƒ€
            'svr': {'type': 'sklearn', 'class': SVR, 'params': {'kernel': 'rbf'}},
            'decision_tree': {'type': 'sklearn', 'class': DecisionTreeRegressor, 'params': {'random_state': 42}},
            'knn': {'type': 'sklearn', 'class': KNeighborsRegressor, 'params': {'n_neighbors': 5}},
        }
        
        # XGBoost
        if XGBOOST_AVAILABLE:
            models['xgboost'] = {'type': 'xgboost', 'class': xgb.XGBRegressor, 'params': {'n_estimators': 100, 'random_state': 42}}
        
        # LightGBM
        if LIGHTGBM_AVAILABLE:
            models['lightgbm'] = {'type': 'lightgbm', 'class': lgb.LGBMRegressor, 'params': {'n_estimators': 100, 'random_state': 42, 'verbose': -1}}
        
        # CatBoost
        if CATBOOST_AVAILABLE:
            models['catboost'] = {'type': 'catboost', 'class': cb.CatBoostRegressor, 'params': {'iterations': 100, 'random_state': 42, 'verbose': False}}
        
        # ë”¥ëŸ¬ë‹ ëª¨ë¸ë“¤
        if TF_AVAILABLE:
            models.update({
                'lstm_simple': {'type': 'tensorflow', 'arch': 'lstm_simple'},
                'lstm_deep': {'type': 'tensorflow', 'arch': 'lstm_deep'},
                'lstm_bidirectional': {'type': 'tensorflow', 'arch': 'lstm_bidirectional'},
                'cnn_1d': {'type': 'tensorflow', 'arch': 'cnn_1d'},
                'cnn_lstm': {'type': 'tensorflow', 'arch': 'cnn_lstm'},
                'attention_lstm': {'type': 'tensorflow', 'arch': 'attention_lstm'},
                'transformer': {'type': 'tensorflow', 'arch': 'transformer'},
                'autoencoder_lstm': {'type': 'tensorflow', 'arch': 'autoencoder_lstm'},
            })
        
        return models
    
    def load_data(self) -> bool:
        """ë°ì´í„° ë¡œë“œ"""
        print("\nğŸ“‚ ë°ì´í„° ë¡œë”©...")
        
        try:
            csv_path = os.path.join(self.data_path, "ai_optimized_3month_data", "ai_matrix_complete.csv")
            df = pd.read_csv(csv_path)
            
            if 'timestamp' in df.columns:
                df['timestamp'] = pd.to_datetime(df['timestamp'])
                df = df.sort_values('timestamp').reset_index(drop=True)
            
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            self.historical_data = df[['timestamp'] + list(numeric_cols) if 'timestamp' in df.columns else list(numeric_cols)].copy()
            self.historical_data = self.historical_data.fillna(method='ffill').fillna(method='bfill').fillna(0)
            
            print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {self.historical_data.shape}")
            print(f"ğŸ“Š í™œìš© ì§€í‘œ: {len(numeric_cols)}ê°œ")
            
            return True
            
        except Exception as e:
            print(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def advanced_feature_engineering(self, base_data: pd.DataFrame, target_col: str, lookback: int = 50) -> pd.DataFrame:
        """ê³ ê¸‰ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§"""
        df_enhanced = base_data.copy()
        
        # ê°€ê²© ë°ì´í„°
        price_data = df_enhanced[target_col]
        
        # 1. ì‹œê³„ì—´ í”¼ì²˜ë“¤
        for period in [5, 10, 20, 50]:
            # ì´ë™í‰ê·  ê³„ì—´
            df_enhanced[f'sma_{period}'] = price_data.rolling(period).mean()
            df_enhanced[f'ema_{period}'] = price_data.ewm(period).mean()
            df_enhanced[f'wma_{period}'] = price_data.rolling(period).apply(lambda x: np.average(x, weights=np.arange(1, len(x)+1)))
            
            # ë³€ë™ì„±
            df_enhanced[f'volatility_{period}'] = price_data.pct_change().rolling(period).std()
            df_enhanced[f'volatility_parkinson_{period}'] = np.sqrt(252) * np.sqrt(np.log(df_enhanced['high']/df_enhanced['low']).rolling(period).mean()) if 'high' in df_enhanced.columns else 0
            
            # ëª¨ë©˜í…€
            df_enhanced[f'momentum_{period}'] = price_data.pct_change(period)
            df_enhanced[f'roc_{period}'] = (price_data - price_data.shift(period)) / price_data.shift(period)
            
            # ìƒëŒ€ ê°•ë„
            delta = price_data.diff()
            gain = (delta.where(delta > 0, 0)).rolling(period).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(period).mean()
            rs = gain / loss
            df_enhanced[f'rsi_{period}'] = 100 - (100 / (1 + rs))
        
        # 2. ê³ ê¸‰ ê¸°ìˆ ì  ì§€í‘œ
        # MACD
        exp1 = price_data.ewm(span=12).mean()
        exp2 = price_data.ewm(span=26).mean()
        df_enhanced['macd'] = exp1 - exp2
        df_enhanced['macd_signal'] = df_enhanced['macd'].ewm(span=9).mean()
        df_enhanced['macd_histogram'] = df_enhanced['macd'] - df_enhanced['macd_signal']
        
        # ë³¼ë¦°ì € ë°´ë“œ
        sma_20 = price_data.rolling(20).mean()
        std_20 = price_data.rolling(20).std()
        df_enhanced['bb_upper'] = sma_20 + (std_20 * 2)
        df_enhanced['bb_lower'] = sma_20 - (std_20 * 2)
        df_enhanced['bb_width'] = df_enhanced['bb_upper'] - df_enhanced['bb_lower']
        df_enhanced['bb_position'] = (price_data - df_enhanced['bb_lower']) / (df_enhanced['bb_upper'] - df_enhanced['bb_lower'])
        
        # 3. í‘¸ë¦¬ì— ë³€í™˜ ê¸°ë°˜ í”¼ì²˜
        try:
            fft = np.fft.fft(price_data.dropna())
            fft_freq = np.fft.fftfreq(len(fft))
            
            # ì£¼ìš” ì£¼íŒŒìˆ˜ ì„±ë¶„
            for i in range(min(5, len(fft)//10)):
                df_enhanced[f'fft_real_{i}'] = np.real(fft[i])
                df_enhanced[f'fft_imag_{i}'] = np.imag(fft[i])
        except:
            pass
        
        # 4. ì›¨ì´ë¸”ë¦¿ ë³€í™˜ (ê°„ë‹¨ ë²„ì „)
        try:
            from scipy import signal
            frequencies = np.logspace(-1, 1, 10)
            widths = 1 / frequencies
            cwtmatr = signal.cwt(price_data.dropna(), signal.ricker, widths)
            
            for i, freq in enumerate(frequencies):
                if len(cwtmatr[i]) == len(df_enhanced):
                    df_enhanced[f'wavelet_{freq:.1f}'] = cwtmatr[i]
        except:
            pass
        
        # 5. ê³ ì°¨ì› ìƒí˜¸ì‘ìš© í”¼ì²˜
        numeric_cols = df_enhanced.select_dtypes(include=[np.number]).columns
        important_cols = numeric_cols[:20]  # ìƒìœ„ 20ê°œë§Œ
        
        for i, col1 in enumerate(important_cols):
            for col2 in important_cols[i+1:]:
                try:
                    # ë¹„ìœ¨
                    df_enhanced[f'{col1}_div_{col2}'] = df_enhanced[col1] / (df_enhanced[col2] + 1e-8)
                    # ê³±ì…ˆ
                    df_enhanced[f'{col1}_mul_{col2}'] = df_enhanced[col1] * df_enhanced[col2]
                    # ì°¨ì´
                    df_enhanced[f'{col1}_sub_{col2}'] = df_enhanced[col1] - df_enhanced[col2]
                except:
                    continue
        
        # 6. ì‹œì°¨ í”¼ì²˜ (Lag Features)
        for lag in [1, 2, 3, 5, 10, 20]:
            for col in important_cols:
                df_enhanced[f'{col}_lag_{lag}'] = df_enhanced[col].shift(lag)
        
        # 7. ë¡¤ë§ í†µê³„ëŸ‰
        for window in [10, 20, 50]:
            for col in important_cols[:10]:
                df_enhanced[f'{col}_rolling_mean_{window}'] = df_enhanced[col].rolling(window).mean()
                df_enhanced[f'{col}_rolling_std_{window}'] = df_enhanced[col].rolling(window).std()
                df_enhanced[f'{col}_rolling_min_{window}'] = df_enhanced[col].rolling(window).min()
                df_enhanced[f'{col}_rolling_max_{window}'] = df_enhanced[col].rolling(window).max()
        
        # NaN ì²˜ë¦¬
        df_enhanced = df_enhanced.fillna(method='bfill').fillna(0)
        df_enhanced = df_enhanced.replace([np.inf, -np.inf], 0)
        
        return df_enhanced
    
    def create_tensorflow_model(self, input_shape: Tuple, architecture: str) -> tf.keras.Model:
        """TensorFlow ë”¥ëŸ¬ë‹ ëª¨ë¸ ìƒì„±"""
        if not TF_AVAILABLE:
            return None
            
        if architecture == 'lstm_simple':
            model = Sequential([
                LSTM(50, input_shape=input_shape),
                Dropout(0.2),
                Dense(25),
                Dense(1)
            ])
            
        elif architecture == 'lstm_deep':
            model = Sequential([
                LSTM(100, return_sequences=True, input_shape=input_shape),
                Dropout(0.3),
                LSTM(100, return_sequences=True),
                Dropout(0.3),
                LSTM(50),
                Dropout(0.2),
                Dense(50),
                Dense(1)
            ])
            
        elif architecture == 'lstm_bidirectional':
            model = Sequential([
                tf.keras.layers.Bidirectional(LSTM(50, return_sequences=True), input_shape=input_shape),
                Dropout(0.3),
                tf.keras.layers.Bidirectional(LSTM(50)),
                Dropout(0.2),
                Dense(50),
                Dense(1)
            ])
            
        elif architecture == 'cnn_1d':
            model = Sequential([
                Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape),
                MaxPooling1D(pool_size=2),
                Conv1D(filters=32, kernel_size=3, activation='relu'),
                MaxPooling1D(pool_size=2),
                Flatten(),
                Dense(50, activation='relu'),
                Dropout(0.2),
                Dense(1)
            ])
            
        elif architecture == 'cnn_lstm':
            model = Sequential([
                Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape),
                MaxPooling1D(pool_size=2),
                LSTM(50),
                Dropout(0.2),
                Dense(50),
                Dense(1)
            ])
            
        elif architecture == 'attention_lstm':
            inputs = Input(shape=input_shape)
            lstm_out = LSTM(50, return_sequences=True)(inputs)
            attention = tf.keras.layers.Attention()([lstm_out, lstm_out])
            flatten = Flatten()(attention)
            dense = Dense(50, activation='relu')(flatten)
            outputs = Dense(1)(dense)
            model = Model(inputs=inputs, outputs=outputs)
            
        elif architecture == 'transformer':
            inputs = Input(shape=input_shape)
            attention = MultiHeadAttention(num_heads=4, key_dim=64)(inputs, inputs)
            x = tf.keras.layers.Add()([inputs, attention])
            x = tf.keras.layers.LayerNormalization()(x)
            x = tf.keras.layers.GlobalAveragePooling1D()(x)
            x = Dense(50, activation='relu')(x)
            outputs = Dense(1)(x)
            model = Model(inputs=inputs, outputs=outputs)
            
        elif architecture == 'autoencoder_lstm':
            # ì˜¤í† ì¸ì½”ë” + LSTM
            inputs = Input(shape=input_shape)
            # ì¸ì½”ë”
            encoded = LSTM(32, return_sequences=True)(inputs)
            encoded = LSTM(16)(encoded)
            # ë””ì½”ë”ë¥¼ ê±°ì³ ì˜ˆì¸¡
            decoded = tf.keras.layers.RepeatVector(input_shape[0])(encoded)
            decoded = LSTM(16, return_sequences=True)(decoded)
            decoded = LSTM(32, return_sequences=True)(decoded)
            # ì˜ˆì¸¡ ë¶€ë¶„
            prediction = LSTM(50)(decoded)
            outputs = Dense(1)(prediction)
            model = Model(inputs=inputs, outputs=outputs)
            
        else:
            # ê¸°ë³¸ LSTM
            model = Sequential([
                LSTM(50, input_shape=input_shape),
                Dense(1)
            ])
        
        model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])
        return model
    
    def prepare_sequence_data(self, data: pd.DataFrame, target_col: str, sequence_length: int = 50) -> Tuple:
        """ì‹œí€€ìŠ¤ ë°ì´í„° ì¤€ë¹„ (ë”¥ëŸ¬ë‹ìš©)"""
        # íƒ€ê²Ÿ ì»¬ëŸ¼ ì œì™¸í•œ í”¼ì²˜ë“¤
        feature_cols = [col for col in data.columns if col not in [target_col, 'timestamp']]
        
        X, y = [], []
        
        for i in range(sequence_length, len(data)):
            # ì‹œí€€ìŠ¤ ë°ì´í„° (ê³¼ê±° sequence_lengthê°œ ì‹œì )
            X.append(data[feature_cols].iloc[i-sequence_length:i].values)
            # íƒ€ê²Ÿ (í˜„ì¬ ì‹œì )
            y.append(data[target_col].iloc[i])
        
        return np.array(X), np.array(y)
    
    def execute_advanced_prediction(self, start_idx: int, strategy: Dict) -> Dict:
        """ê³ ê¸‰ ì˜ˆì¸¡ ì‹¤í–‰"""
        try:
            # íƒ€ê²Ÿ ì»¬ëŸ¼ ì°¾ê¸°
            price_candidates = [
                'onchain_blockchain_info_network_stats_market_price_usd',
                'price', 'close', 'open'
            ]
            target_col = None
            for candidate in price_candidates:
                if candidate in self.historical_data.columns:
                    target_col = candidate
                    break
            
            if not target_col:
                numeric_cols = self.historical_data.select_dtypes(include=[np.number]).columns
                target_col = numeric_cols[0]
            
            # í•™ìŠµ ë°ì´í„° ì¤€ë¹„
            train_data = self.historical_data.iloc[:start_idx]
            if len(train_data) < 200:
                return {'success': False, 'error': 'í•™ìŠµ ë°ì´í„° ë¶€ì¡±'}
            
            # ê³ ê¸‰ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§
            enhanced_data = self.advanced_feature_engineering(train_data, target_col)
            
            model_type = strategy['model_type']
            prediction_hours = strategy['prediction_hours']
            
            # ë”¥ëŸ¬ë‹ ëª¨ë¸ì¸ ê²½ìš°
            if model_type in ['lstm_simple', 'lstm_deep', 'lstm_bidirectional', 'cnn_1d', 'cnn_lstm', 'attention_lstm', 'transformer', 'autoencoder_lstm']:
                return self._execute_deep_learning_prediction(enhanced_data, target_col, start_idx, prediction_hours, model_type)
            
            # ì „í†µì  ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸
            else:
                return self._execute_ml_prediction(enhanced_data, target_col, start_idx, prediction_hours, model_type, strategy)
                
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def _execute_deep_learning_prediction(self, data: pd.DataFrame, target_col: str, start_idx: int, prediction_hours: int, model_type: str) -> Dict:
        """ë”¥ëŸ¬ë‹ ì˜ˆì¸¡ ì‹¤í–‰"""
        if not TF_AVAILABLE:
            return {'success': False, 'error': 'TensorFlow ë¶ˆê°€'}
        
        try:
            sequence_length = 50
            
            # ì‹œí€€ìŠ¤ ë°ì´í„° ì¤€ë¹„
            X, y = self.prepare_sequence_data(data, target_col, sequence_length)
            
            if len(X) < 100:
                return {'success': False, 'error': 'ì‹œí€€ìŠ¤ ë°ì´í„° ë¶€ì¡±'}
            
            # train/validation ë¶„í• 
            train_size = int(len(X) * 0.8)
            X_train, X_val = X[:train_size], X[train_size:]
            y_train, y_val = y[:train_size], y[train_size:]
            
            # ëª¨ë¸ ìƒì„±
            input_shape = (X_train.shape[1], X_train.shape[2])
            model = self.create_tensorflow_model(input_shape, model_type)
            
            if model is None:
                return {'success': False, 'error': 'ëª¨ë¸ ìƒì„± ì‹¤íŒ¨'}
            
            # ì½œë°± ì„¤ì •
            early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
            reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)
            
            # ëª¨ë¸ í›ˆë ¨
            history = model.fit(
                X_train, y_train,
                validation_data=(X_val, y_val),
                epochs=50,
                batch_size=32,
                callbacks=[early_stop, reduce_lr],
                verbose=0
            )
            
            # ì˜ˆì¸¡ì„ ìœ„í•œ í˜„ì¬ ì‹œì  ë°ì´í„°
            if start_idx >= sequence_length:
                current_sequence = data.iloc[start_idx-sequence_length:start_idx]
                feature_cols = [col for col in data.columns if col not in [target_col, 'timestamp']]
                X_current = current_sequence[feature_cols].values.reshape(1, sequence_length, -1)
                
                prediction = model.predict(X_current, verbose=0)[0, 0]
                
                # ì‹¤ì œê°’ê³¼ ë¹„êµ
                target_idx = start_idx + prediction_hours
                if target_idx >= len(self.historical_data):
                    return {'success': False, 'error': 'ì˜ˆì¸¡ ì‹œì  ì´ˆê³¼'}
                
                actual_value = self.historical_data.iloc[target_idx][target_col]
                error_pct = abs(actual_value - prediction) / actual_value * 100
                accuracy = max(0, 100 - error_pct)
                
                return {
                    'success': True,
                    'accuracy': accuracy,
                    'predicted': float(prediction),
                    'actual': actual_value,
                    'error_pct': error_pct,
                    'model_type': model_type,
                    'train_loss': float(history.history['loss'][-1]),
                    'val_loss': float(history.history['val_loss'][-1])
                }
            else:
                return {'success': False, 'error': 'ì‹œí€€ìŠ¤ ê¸¸ì´ ë¶€ì¡±'}
                
        except Exception as e:
            return {'success': False, 'error': f'ë”¥ëŸ¬ë‹ ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}'}
    
    def _execute_ml_prediction(self, data: pd.DataFrame, target_col: str, start_idx: int, prediction_hours: int, model_type: str, strategy: Dict) -> Dict:
        """ë¨¸ì‹ ëŸ¬ë‹ ì˜ˆì¸¡ ì‹¤í–‰"""
        try:
            # í”¼ì²˜ ì¤€ë¹„
            feature_cols = [col for col in data.columns if col not in [target_col, 'timestamp']]
            X_all = data[feature_cols]
            y_all = data[target_col].shift(-prediction_hours).dropna()
            X_all = X_all.iloc[:-prediction_hours]
            
            if len(X_all) < 100:
                return {'success': False, 'error': 'í•™ìŠµ ë°ì´í„° ë¶€ì¡±'}
            
            # í”¼ì²˜ ì„ íƒ (ì°¨ì› ì¶•ì†Œ)
            if len(feature_cols) > 200:
                selector = SelectKBest(score_func=f_regression, k=200)
                X_selected = selector.fit_transform(X_all, y_all)
                selected_features = selector.get_support(indices=True)
            else:
                X_selected = X_all
            
            # ìŠ¤ì¼€ì¼ë§
            scaler_type = strategy.get('preprocessing', 'robust')
            if scaler_type == 'robust':
                scaler = RobustScaler()
            elif scaler_type == 'standard':
                scaler = StandardScaler()
            elif scaler_type == 'minmax':
                scaler = MinMaxScaler()
            else:
                scaler = PowerTransformer()
            
            X_scaled = scaler.fit_transform(X_selected)
            
            # ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
            if model_type not in self.model_types:
                return {'success': False, 'error': f'ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸: {model_type}'}
            
            model_config = self.model_types[model_type]
            
            if model_config['type'] == 'sklearn':
                model = model_config['class'](**model_config['params'])
            elif model_config['type'] == 'xgboost' and XGBOOST_AVAILABLE:
                model = model_config['class'](**model_config['params'])
            elif model_config['type'] == 'lightgbm' and LIGHTGBM_AVAILABLE:
                model = model_config['class'](**model_config['params'])
            elif model_config['type'] == 'catboost' and CATBOOST_AVAILABLE:
                model = model_config['class'](**model_config['params'])
            else:
                return {'success': False, 'error': f'ëª¨ë¸ íƒ€ì… ë¶ˆê°€: {model_type}'}
            
            # ëª¨ë¸ í•™ìŠµ
            model.fit(X_scaled, y_all)
            
            # ì˜ˆì¸¡
            current_features = data.iloc[start_idx:start_idx+1][feature_cols]
            if len(feature_cols) > 200:
                current_selected = selector.transform(current_features)
            else:
                current_selected = current_features
            
            current_scaled = scaler.transform(current_selected)
            prediction = model.predict(current_scaled)[0]
            
            # ê²€ì¦
            target_idx = start_idx + prediction_hours
            if target_idx >= len(self.historical_data):
                return {'success': False, 'error': 'ì˜ˆì¸¡ ì‹œì  ì´ˆê³¼'}
            
            actual_value = self.historical_data.iloc[target_idx][target_col]
            error_pct = abs(actual_value - prediction) / actual_value * 100
            accuracy = max(0, 100 - error_pct)
            
            # í”¼ì²˜ ì¤‘ìš”ë„ (ê°€ëŠ¥í•œ ê²½ìš°)
            feature_importance = None
            try:
                if hasattr(model, 'feature_importances_'):
                    feature_importance = model.feature_importances_[:10].tolist()
                elif hasattr(model, 'coef_'):
                    feature_importance = np.abs(model.coef_[:10]).tolist()
            except:
                pass
            
            return {
                'success': True,
                'accuracy': accuracy,
                'predicted': float(prediction),
                'actual': actual_value,
                'error_pct': error_pct,
                'model_type': model_type,
                'feature_importance': feature_importance
            }
            
        except Exception as e:
            return {'success': False, 'error': f'ML ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}'}
    
    def generate_ultimate_strategy(self) -> Dict:
        """ê¶ê·¹ì˜ ì „ëµ ìƒì„±"""
        # ëª¨ë“  ê°€ëŠ¥í•œ ëª¨ë¸ ì¤‘ ëœë¤ ì„ íƒ
        available_models = list(self.model_types.keys())
        
        # ì„±ëŠ¥ ê¸°ë°˜ ê°€ì¤‘ ì„ íƒ
        if self.model_performance:
            weights = []
            models_list = []
            for model_name in available_models:
                perf = self.model_performance[model_name]
                if perf['attempts'] > 0:
                    weight = perf['avg_accuracy'] + 0.1  # ìµœì†Œ ê°€ì¤‘ì¹˜
                else:
                    weight = 0.5  # ê¸°ë³¸ ê°€ì¤‘ì¹˜
                weights.append(weight)
                models_list.append(model_name)
            
            if weights:
                selected_model = random.choices(models_list, weights=weights)[0]
            else:
                selected_model = random.choice(available_models)
        else:
            selected_model = random.choice(available_models)
        
        return {
            'model_type': selected_model,
            'prediction_hours': random.choice([1, 3, 6, 12, 24, 48, 72, 168]),
            'preprocessing': random.choice(['robust', 'standard', 'minmax', 'power']),
            'feature_selection': random.choice([True, False]),
            'cross_validation': random.choice([True, False]),
            'ensemble_voting': random.choice([True, False])
        }
    
    def analyze_ultimate_result(self, result: Dict, attempt_num: int):
        """ê²°ê³¼ ë¶„ì„ ë° í•™ìŠµ"""
        self.total_attempts += 1
        
        if result['success']:
            accuracy = result['accuracy']
            model_type = result['model_type']
            
            # ì„±ê³µ ê¸°ì¤€ (85% ì´ìƒ)
            is_success = accuracy >= 85.0
            
            if is_success:
                self.success_count += 1
                self.success_patterns.append(result)
                print(f"ğŸ† ì‹œë„ {attempt_num}: ì„±ê³µ! {accuracy:.1f}% ({model_type})")
            else:
                self.failure_patterns.append(result)
                if attempt_num % 50 == 0:
                    print(f"ğŸ“Š ì‹œë„ {attempt_num}: {accuracy:.1f}% ({model_type})")
            
            # ëª¨ë¸ ì„±ëŠ¥ ì—…ë°ì´íŠ¸
            perf = self.model_performance[model_type]
            perf['attempts'] += 1
            if is_success:
                perf['success'] += 1
            
            # í‰ê·  ì •í™•ë„ ì—…ë°ì´íŠ¸
            perf['avg_accuracy'] = ((perf['avg_accuracy'] * (perf['attempts'] - 1)) + accuracy) / perf['attempts']
            
        else:
            if attempt_num % 100 == 0:
                print(f"âŒ ì‹œë„ {attempt_num}: {result.get('error', 'Unknown')}")
        
        # ì„±ê³µë¥  ì—…ë°ì´íŠ¸
        if self.total_attempts > 0:
            self.current_success_rate = (self.success_count / self.total_attempts) * 100
        
        # ì£¼ê¸°ì  ê·œì¹™ ë°œê²¬
        if attempt_num % 200 == 0:
            self.discover_ultimate_rules()
            self.print_ultimate_progress(attempt_num)
    
    def discover_ultimate_rules(self):
        """ê¶ê·¹ ê·œì¹™ ë°œê²¬"""
        if len(self.success_patterns) < 5:
            return
        
        print(f"\nğŸ” ê¶ê·¹ ê·œì¹™ ë¶„ì„... (ì„±ê³µ {len(self.success_patterns)}ê°œ)")
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ë“¤
        model_success = defaultdict(lambda: {'count': 0, 'total_accuracy': 0})
        for pattern in self.success_patterns:
            model = pattern['model_type']
            model_success[model]['count'] += 1
            model_success[model]['total_accuracy'] += pattern['accuracy']
        
        best_models = []
        for model, stats in model_success.items():
            avg_acc = stats['total_accuracy'] / stats['count']
            best_models.append((model, avg_acc, stats['count']))
        
        best_models.sort(key=lambda x: x[1], reverse=True)
        
        # ê·œì¹™ ì €ì¥
        rule = {
            'timestamp': datetime.now(),
            'success_count': len(self.success_patterns),
            'total_attempts': self.total_attempts,
            'success_rate': self.current_success_rate,
            'best_models': best_models[:5],
            'avg_success_accuracy': np.mean([p['accuracy'] for p in self.success_patterns])
        }
        
        self.discovered_rules.append(rule)
        
        # ê·œì¹™ ì•ˆë‚´
        print(f"ğŸ¯ ë°œê²¬ëœ ê·œì¹™ (ì„±ê³µë¥  {self.current_success_rate:.1f}%)")
        print("ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸ TOP 3:")
        for i, (model, acc, count) in enumerate(best_models[:3]):
            print(f"  {i+1}. {model:20s} í‰ê·  {acc:.1f}% ({count}íšŒ ì„±ê³µ)")
    
    def print_ultimate_progress(self, attempt_num: int):
        """ì§„í–‰ ìƒí™© ì¶œë ¥"""
        print(f"\nğŸ“Š ì§„í–‰ í˜„í™© (ì‹œë„ {attempt_num:,}íšŒ)")
        print(f"ğŸ¯ ì„±ê³µë¥ : {self.current_success_rate:.2f}% (ëª©í‘œ: {self.target_success_rate}%)")
        print(f"âœ… ì„±ê³µ: {self.success_count:,}íšŒ | âŒ ì‹¤íŒ¨: {self.total_attempts - self.success_count:,}íšŒ")
        
        if self.current_success_rate >= self.target_success_rate:
            print(f"ğŸ‰ ëª©í‘œ ë‹¬ì„±!")
            return True
        
        return False
    
    def run_ultimate_learning(self, max_attempts: int = 2000):
        """ê¶ê·¹ ë¬´í•œ í•™ìŠµ ì‹¤í–‰"""
        print(f"\nğŸš€ ê¶ê·¹ ë¬´í•œ í•™ìŠµ ì‹œì‘!")
        print(f"ğŸ¯ ëª©í‘œ: {self.target_success_rate}% ì„±ê³µë¥ ")
        print(f"ğŸ”„ ìµœëŒ€ ì‹œë„: {max_attempts:,}íšŒ")
        print(f"ğŸ¤– ì‚¬ìš© ê°€ëŠ¥ ëª¨ë¸: {len(self.model_types)}ê°œ")
        print("="*70)
        
        data_length = len(self.historical_data)
        min_start = 300
        max_start = data_length - 200
        
        for attempt in range(1, max_attempts + 1):
            # ëœë¤ ì‹œì 
            start_idx = random.randint(min_start, max_start)
            
            # ê¶ê·¹ ì „ëµ
            strategy = self.generate_ultimate_strategy()
            
            # ê³ ê¸‰ ì˜ˆì¸¡ ì‹¤í–‰
            result = self.execute_advanced_prediction(start_idx, strategy)
            
            # ê²°ê³¼ ë¶„ì„
            self.analyze_ultimate_result(result, attempt)
            
            # ëª©í‘œ ë‹¬ì„± ì²´í¬
            if attempt % 500 == 0:
                if self.print_ultimate_progress(attempt):
                    break
        
        self.print_ultimate_final_results()
        self.save_ultimate_results()
    
    def print_ultimate_final_results(self):
        """ìµœì¢… ê²°ê³¼"""
        print(f"\n" + "="*70)
        print("ğŸ† ê¶ê·¹ ë¬´í•œí•™ìŠµ ìµœì¢… ê²°ê³¼")
        print("="*70)
        print(f"ğŸ”„ ì´ ì‹œë„:         {self.total_attempts:,}íšŒ")
        print(f"âœ… ì„±ê³µ:           {self.success_count:,}íšŒ")
        print(f"ğŸ¯ ìµœì¢… ì„±ê³µë¥ :     {self.current_success_rate:.2f}%")
        print(f"ğŸ“Š ë°œê²¬ ê·œì¹™:       {len(self.discovered_rules)}ê°œ")
        
        if self.current_success_rate >= self.target_success_rate:
            print(f"ğŸ‰ ëª©í‘œ ë‹¬ì„±! ({self.target_success_rate}% ì´ìƒ)")
        else:
            print(f"âš ï¸ ëª©í‘œ ë¯¸ë‹¬ì„±")
        
        if self.discovered_rules:
            final_rule = self.discovered_rules[-1]
            print(f"\nğŸ† ìµœì¢… ë°œê²¬ ê·œì¹™:")
            if final_rule['best_models']:
                best_model = final_rule['best_models'][0]
                print(f"ğŸ¥‡ ìµœê³  ëª¨ë¸: {best_model[0]} (í‰ê·  {best_model[1]:.1f}%)")
        
        print("="*70)
    
    def save_ultimate_results(self):
        """ê²°ê³¼ ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        results = {
            'timestamp': timestamp,
            'total_attempts': self.total_attempts,
            'success_count': self.success_count,
            'final_success_rate': self.current_success_rate,
            'target_achieved': self.current_success_rate >= self.target_success_rate,
            'discovered_rules': self.discovered_rules,
            'model_performance': dict(self.model_performance),
            'available_models': list(self.model_types.keys()),
            'success_patterns_count': len(self.success_patterns)
        }
        
        filename = f"ultimate_learning_results_{timestamp}.json"
        filepath = os.path.join(self.data_path, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {filename}")
    
    def run_complete_ultimate_system(self):
        """ì™„ì „ì²´ ì‹œìŠ¤í…œ ì‹¤í–‰"""
        print("ğŸš€ ì™„ì „ì²´ ë¬´í•œí•™ìŠµ 95% ì‹œìŠ¤í…œ ì‹œì‘")
        print("="*70)
        
        if not self.load_data():
            return None
        
        self.run_ultimate_learning(max_attempts=2000)
        
        return {
            'success_rate': self.current_success_rate,
            'total_attempts': self.total_attempts,
            'target_achieved': self.current_success_rate >= self.target_success_rate
        }

def main():
    system = UltimateInfiniteLearningSystem()
    return system.run_complete_ultimate_system()

if __name__ == "__main__":
    results = main()