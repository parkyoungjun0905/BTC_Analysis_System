#!/usr/bin/env python3
"""
ğŸ¯ ê³ ê¸‰ ì•™ìƒë¸” í•™ìŠµ ì‹œìŠ¤í…œ - 90%+ ì •í™•ë„ ë‹¬ì„±
ë¹„íŠ¸ì½”ì¸ ê°€ê²© ì˜ˆì¸¡ì„ ìœ„í•œ ì¢…í•©ì ì¸ ì•™ìƒë¸” ë¨¸ì‹ ëŸ¬ë‹ ì‹œìŠ¤í…œ

í•µì‹¬ ê¸°ëŠ¥:
- ë‹¤ì–‘í•œ ëª¨ë¸ ì•„í‚¤í…ì²˜ í†µí•© (LSTM, Transformer, XGBoost, CNN ë“±)
- ë™ì  ëª¨ë¸ ê°€ì¤‘ì¹˜ ì¡°ì • ë° ì ì‘ì  ì•™ìƒë¸”
- ë©”íƒ€ í•™ìŠµ ê¸°ë°˜ ëª¨ë¸ ì„ íƒ ë° ìµœì í™”
- ë² ì´ì§€ì•ˆ ëª¨ë¸ í‰ê· í™” ë° ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™”
- ê°•ê±´í•œ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ì‹¤íŒ¨ ê°ì§€
"""

import numpy as np
import pandas as pd
import warnings
import logging
import json
import pickle
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Any, Optional, Union
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
from pathlib import Path
import sqlite3

# ë¨¸ì‹ ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬
from sklearn.ensemble import (
    RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor,
    AdaBoostRegressor, BaggingRegressor, VotingRegressor, StackingRegressor
)
from sklearn.linear_model import (
    Ridge, Lasso, ElasticNet, HuberRegressor, RANSACRegressor, 
    TheilSenRegressor, BayesianRidge
)
from sklearn.svm import SVR, NuSVR
from sklearn.neural_network import MLPRegressor
from sklearn.model_selection import (
    TimeSeriesSplit, cross_val_score, GridSearchCV, RandomizedSearchCV
)
from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler
from sklearn.metrics import (
    mean_squared_error, mean_absolute_error, r2_score,
    mean_absolute_percentage_error
)

# ë”¥ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ (ì„ íƒì  ì„¤ì¹˜)
try:
    import tensorflow as tf
    from tensorflow.keras.models import Sequential, Model
    from tensorflow.keras.layers import (
        LSTM, GRU, Dense, Dropout, Conv1D, MaxPooling1D, 
        MultiHeadAttention, LayerNormalization, GlobalAveragePooling1D,
        Input, Concatenate, BatchNormalization
    )
    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
    TENSORFLOW_AVAILABLE = True
except ImportError:
    TENSORFLOW_AVAILABLE = False
    print("âš ï¸ TensorFlow ë¯¸ì„¤ì¹˜ - LSTM/Transformer ëª¨ë¸ ë¹„í™œì„±í™”")

# XGBoost/LightGBM/CatBoost
try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False
    print("âš ï¸ XGBoost ë¯¸ì„¤ì¹˜")

try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False
    print("âš ï¸ LightGBM ë¯¸ì„¤ì¹˜")

try:
    import catboost as cb
    CATBOOST_AVAILABLE = True
except ImportError:
    CATBOOST_AVAILABLE = False
    print("âš ï¸ CatBoost ë¯¸ì„¤ì¹˜")

# ìµœì í™” ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from scipy.optimize import minimize, differential_evolution
    from scipy.stats import norm
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False
    print("âš ï¸ SciPy ë¯¸ì„¤ì¹˜")

warnings.filterwarnings('ignore')

class AdvancedEnsembleLearningSystem:
    """
    ğŸ§  ê³ ê¸‰ ì•™ìƒë¸” í•™ìŠµ ì‹œìŠ¤í…œ
    
    ë‹¤ì–‘í•œ ëª¨ë¸ì„ í†µí•©í•˜ì—¬ 90%+ ì •í™•ë„ë¥¼ ë‹¬ì„±í•˜ëŠ” ì¢…í•© ì‹œìŠ¤í…œ
    """
    
    def __init__(self, target_accuracy: float = 0.90):
        """ì´ˆê¸°í™”"""
        self.target_accuracy = target_accuracy
        self.models = {}
        self.model_weights = {}
        self.model_performance = {}
        self.meta_learners = {}
        self.scalers = {}
        
        # ì„±ëŠ¥ ì¶”ì 
        self.accuracy_history = []
        self.ensemble_history = []
        self.failure_count = 0
        
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('ensemble_system.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
        # ë°ì´í„° ê²½ë¡œ
        self.data_path = Path("/Users/parkyoungjun/Desktop/BTC_Analysis_System/historical_6month_data")
        
        # ëª¨ë¸ ì €ì¥ ê²½ë¡œ
        self.model_save_path = Path("/Users/parkyoungjun/Desktop/BTC_Analysis_System/ensemble_models")
        self.model_save_path.mkdir(exist_ok=True)
        
        print("ğŸ¯ ê³ ê¸‰ ì•™ìƒë¸” í•™ìŠµ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"ğŸ“Š ëª©í‘œ ì •í™•ë„: {target_accuracy*100:.1f}%")

    def load_comprehensive_data(self) -> pd.DataFrame:
        """
        ğŸ“Š ì¢…í•© ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
        
        Returns:
            pd.DataFrame: í†µí•© ë°ì´í„°ì…‹
        """
        print("ğŸ“Š ì¢…í•© ë°ì´í„° ë¡œë“œ ì‹œì‘...")
        
        all_data = {}
        files_loaded = 0
        
        # ëª¨ë“  CSV íŒŒì¼ ë¡œë“œ
        for csv_file in self.data_path.glob("*.csv"):
            try:
                df = pd.read_csv(csv_file, index_col=0, parse_dates=True)
                if not df.empty:
                    column_name = csv_file.stem
                    all_data[column_name] = df.iloc[:, 0]  # ì²« ë²ˆì§¸ ì»¬ëŸ¼ë§Œ ì‚¬ìš©
                    files_loaded += 1
            except Exception as e:
                self.logger.warning(f"âš ï¸ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {csv_file.name} - {e}")
                continue
        
        # ë°ì´í„° í†µí•©
        if not all_data:
            raise ValueError("âŒ ë¡œë“œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
        
        combined_df = pd.DataFrame(all_data)
        combined_df = combined_df.dropna()
        
        print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {files_loaded}ê°œ íŒŒì¼, {len(combined_df)}ê°œ í–‰")
        print(f"ğŸ“ˆ íŠ¹ì„± ìˆ˜: {len(combined_df.columns)}ê°œ")
        
        return combined_df

    def create_diverse_models(self) -> Dict[str, Any]:
        """
        ğŸ¤– ë‹¤ì–‘í•œ ëª¨ë¸ ì•„í‚¤í…ì²˜ ìƒì„±
        
        Returns:
            Dict[str, Any]: ìƒì„±ëœ ëª¨ë¸ë“¤
        """
        print("ğŸ¤– ë‹¤ì–‘í•œ ëª¨ë¸ ì•„í‚¤í…ì²˜ ìƒì„± ì‹œì‘...")
        
        models = {}
        
        # 1. ì „í†µì ì¸ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸
        models['random_forest'] = {
            'model': RandomForestRegressor(
                n_estimators=200,
                max_depth=15,
                random_state=42,
                n_jobs=-1
            ),
            'type': 'traditional',
            'hyperparams': {
                'n_estimators': [100, 200, 300],
                'max_depth': [10, 15, 20, None],
                'min_samples_split': [2, 5, 10]
            }
        }
        
        models['gradient_boosting'] = {
            'model': GradientBoostingRegressor(
                n_estimators=200,
                learning_rate=0.1,
                max_depth=8,
                random_state=42
            ),
            'type': 'boosting',
            'hyperparams': {
                'n_estimators': [100, 200, 300],
                'learning_rate': [0.05, 0.1, 0.15],
                'max_depth': [6, 8, 10]
            }
        }
        
        models['extra_trees'] = {
            'model': ExtraTreesRegressor(
                n_estimators=200,
                random_state=42,
                n_jobs=-1
            ),
            'type': 'traditional'
        }
        
        # 2. ê°•ê±´í•œ íšŒê·€ ëª¨ë¸
        models['huber_regressor'] = {
            'model': HuberRegressor(epsilon=1.35, max_iter=1000),
            'type': 'robust'
        }
        
        models['ransac_regressor'] = {
            'model': RANSACRegressor(random_state=42, max_trials=1000),
            'type': 'robust'
        }
        
        models['theil_sen'] = {
            'model': TheilSenRegressor(random_state=42, n_jobs=-1),
            'type': 'robust'
        }
        
        # 3. ë² ì´ì§€ì•ˆ ëª¨ë¸
        models['bayesian_ridge'] = {
            'model': BayesianRidge(compute_score=True),
            'type': 'bayesian'
        }
        
        # 4. SVM ëª¨ë¸
        models['svr_rbf'] = {
            'model': SVR(kernel='rbf', C=100, gamma='scale'),
            'type': 'svm'
        }
        
        models['svr_poly'] = {
            'model': SVR(kernel='poly', degree=3, C=100),
            'type': 'svm'
        }
        
        # 5. ì‹ ê²½ë§ ëª¨ë¸
        models['mlp_regressor'] = {
            'model': MLPRegressor(
                hidden_layer_sizes=(200, 100, 50),
                max_iter=2000,
                random_state=42,
                early_stopping=True
            ),
            'type': 'neural'
        }
        
        # 6. XGBoost ëª¨ë¸ (ì„¤ì¹˜ëœ ê²½ìš°)
        if XGBOOST_AVAILABLE:
            models['xgboost'] = {
                'model': xgb.XGBRegressor(
                    n_estimators=200,
                    learning_rate=0.1,
                    max_depth=8,
                    random_state=42,
                    n_jobs=-1
                ),
                'type': 'boosting',
                'hyperparams': {
                    'n_estimators': [100, 200, 300],
                    'learning_rate': [0.05, 0.1, 0.15],
                    'max_depth': [6, 8, 10]
                }
            }
        
        # 7. LightGBM ëª¨ë¸ (ì„¤ì¹˜ëœ ê²½ìš°)
        if LIGHTGBM_AVAILABLE:
            models['lightgbm'] = {
                'model': lgb.LGBMRegressor(
                    n_estimators=200,
                    learning_rate=0.1,
                    max_depth=8,
                    random_state=42,
                    n_jobs=-1,
                    verbose=-1
                ),
                'type': 'boosting'
            }
        
        # 8. CatBoost ëª¨ë¸ (ì„¤ì¹˜ëœ ê²½ìš°)
        if CATBOOST_AVAILABLE:
            models['catboost'] = {
                'model': cb.CatBoostRegressor(
                    iterations=200,
                    learning_rate=0.1,
                    depth=8,
                    random_state=42,
                    silent=True
                ),
                'type': 'boosting'
            }
        
        print(f"âœ… {len(models)}ê°œ ëª¨ë¸ ì•„í‚¤í…ì²˜ ìƒì„± ì™„ë£Œ")
        return models

    def create_deep_learning_models(self, input_shape: Tuple[int, int]) -> Dict[str, Any]:
        """
        ğŸ§  ë”¥ëŸ¬ë‹ ëª¨ë¸ ìƒì„± (TensorFlow ì‚¬ìš© ê°€ëŠ¥ì‹œ)
        
        Args:
            input_shape: ì…ë ¥ ë°ì´í„° í˜•íƒœ
            
        Returns:
            Dict[str, Any]: ë”¥ëŸ¬ë‹ ëª¨ë¸ë“¤
        """
        if not TENSORFLOW_AVAILABLE:
            return {}
        
        print("ğŸ§  ë”¥ëŸ¬ë‹ ëª¨ë¸ ìƒì„± ì‹œì‘...")
        models = {}
        
        # 1. LSTM ëª¨ë¸
        lstm_model = Sequential([
            LSTM(128, return_sequences=True, input_shape=input_shape),
            Dropout(0.3),
            LSTM(64, return_sequences=True),
            Dropout(0.3),
            LSTM(32, return_sequences=False),
            Dropout(0.3),
            Dense(50, activation='relu'),
            Dropout(0.2),
            Dense(1)
        ])
        
        lstm_model.compile(
            optimizer='adam',
            loss='mse',
            metrics=['mae']
        )
        
        models['lstm'] = {
            'model': lstm_model,
            'type': 'deep_learning',
            'sequence_length': input_shape[0]
        }
        
        # 2. GRU ëª¨ë¸
        gru_model = Sequential([
            GRU(128, return_sequences=True, input_shape=input_shape),
            Dropout(0.3),
            GRU(64, return_sequences=True),
            Dropout(0.3),
            GRU(32, return_sequences=False),
            Dropout(0.3),
            Dense(50, activation='relu'),
            Dropout(0.2),
            Dense(1)
        ])
        
        gru_model.compile(
            optimizer='adam',
            loss='mse',
            metrics=['mae']
        )
        
        models['gru'] = {
            'model': gru_model,
            'type': 'deep_learning',
            'sequence_length': input_shape[0]
        }
        
        # 3. CNN-LSTM í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸
        cnn_lstm_model = Sequential([
            Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape),
            BatchNormalization(),
            MaxPooling1D(pool_size=2),
            Conv1D(filters=32, kernel_size=3, activation='relu'),
            BatchNormalization(),
            LSTM(64, return_sequences=True),
            Dropout(0.3),
            LSTM(32, return_sequences=False),
            Dropout(0.3),
            Dense(50, activation='relu'),
            Dense(1)
        ])
        
        cnn_lstm_model.compile(
            optimizer='adam',
            loss='mse',
            metrics=['mae']
        )
        
        models['cnn_lstm'] = {
            'model': cnn_lstm_model,
            'type': 'deep_learning',
            'sequence_length': input_shape[0]
        }
        
        # 4. Transformer ëª¨ë¸ (ê°„ë‹¨í•œ ë²„ì „)
        def create_transformer_model(input_shape):
            inputs = Input(shape=input_shape)
            
            # Multi-Head Attention
            attention = MultiHeadAttention(
                num_heads=8,
                key_dim=64
            )(inputs, inputs)
            
            attention = Dropout(0.3)(attention)
            attention = LayerNormalization()(inputs + attention)
            
            # Feed Forward
            ff = Dense(128, activation='relu')(attention)
            ff = Dropout(0.3)(ff)
            ff = Dense(input_shape[-1])(ff)
            ff = LayerNormalization()(attention + ff)
            
            # Global Average Pooling
            pooled = GlobalAveragePooling1D()(ff)
            
            # Final layers
            outputs = Dense(50, activation='relu')(pooled)
            outputs = Dropout(0.3)(outputs)
            outputs = Dense(1)(outputs)
            
            model = Model(inputs=inputs, outputs=outputs)
            return model
        
        transformer_model = create_transformer_model(input_shape)
        transformer_model.compile(
            optimizer='adam',
            loss='mse',
            metrics=['mae']
        )
        
        models['transformer'] = {
            'model': transformer_model,
            'type': 'deep_learning',
            'sequence_length': input_shape[0]
        }
        
        print(f"âœ… {len(models)}ê°œ ë”¥ëŸ¬ë‹ ëª¨ë¸ ìƒì„± ì™„ë£Œ")
        return models

    def prepare_sequences(self, data: pd.DataFrame, sequence_length: int = 24) -> Tuple[np.ndarray, np.ndarray]:
        """
        ğŸ”„ ì‹œê³„ì—´ ì‹œí€€ìŠ¤ ë°ì´í„° ì¤€ë¹„ (ë”¥ëŸ¬ë‹ìš©)
        
        Args:
            data: ì…ë ¥ ë°ì´í„°
            sequence_length: ì‹œí€€ìŠ¤ ê¸¸ì´
            
        Returns:
            Tuple[np.ndarray, np.ndarray]: X, y ë°°ì—´
        """
        # ê°€ê²© ë°ì´í„°ê°€ ìˆëŠ” ì»¬ëŸ¼ ì°¾ê¸°
        price_col = None
        for col in data.columns:
            if 'price' in col.lower() or 'btc' in col.lower():
                price_col = col
                break
        
        if price_col is None:
            price_col = data.columns[0]  # ì²« ë²ˆì§¸ ì»¬ëŸ¼ ì‚¬ìš©
        
        # íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
        features = data.drop(columns=[price_col])
        target = data[price_col]
        
        # ì‹œí€€ìŠ¤ ìƒì„±
        X, y = [], []
        
        for i in range(sequence_length, len(data)):
            X.append(features.iloc[i-sequence_length:i].values)
            y.append(target.iloc[i])
        
        return np.array(X), np.array(y)

    def train_single_model(self, model_info: Dict, X_train: np.ndarray, 
                          y_train: np.ndarray, X_val: np.ndarray, 
                          y_val: np.ndarray) -> Dict:
        """
        ğŸ¯ ë‹¨ì¼ ëª¨ë¸ í›ˆë ¨
        
        Args:
            model_info: ëª¨ë¸ ì •ë³´
            X_train: í›ˆë ¨ ë°ì´í„°
            y_train: í›ˆë ¨ íƒ€ê²Ÿ
            X_val: ê²€ì¦ ë°ì´í„°
            y_val: ê²€ì¦ íƒ€ê²Ÿ
            
        Returns:
            Dict: í›ˆë ¨ ê²°ê³¼
        """
        model = model_info['model']
        model_type = model_info['type']
        
        try:
            # ë”¥ëŸ¬ë‹ ëª¨ë¸ í›ˆë ¨
            if model_type == 'deep_learning':
                callbacks = [
                    EarlyStopping(patience=20, restore_best_weights=True),
                    ReduceLROnPlateau(factor=0.5, patience=10)
                ]
                
                history = model.fit(
                    X_train, y_train,
                    validation_data=(X_val, y_val),
                    epochs=200,
                    batch_size=32,
                    callbacks=callbacks,
                    verbose=0
                )
                
                # ì˜ˆì¸¡ ë° ì„±ëŠ¥ í‰ê°€
                y_pred = model.predict(X_val).flatten()
                
            else:
                # ì „í†µì ì¸ ML ëª¨ë¸ í›ˆë ¨
                model.fit(X_train, y_train)
                y_pred = model.predict(X_val)
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
            mse = mean_squared_error(y_val, y_pred)
            mae = mean_absolute_error(y_val, y_pred)
            r2 = r2_score(y_val, y_pred)
            
            # ë°©í–¥ì„± ì •í™•ë„ ê³„ì‚°
            direction_actual = np.diff(y_val) > 0
            direction_pred = np.diff(y_pred) > 0
            direction_accuracy = np.mean(direction_actual == direction_pred)
            
            return {
                'model': model,
                'mse': mse,
                'mae': mae,
                'r2': r2,
                'direction_accuracy': direction_accuracy,
                'predictions': y_pred,
                'status': 'success'
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ í›ˆë ¨ ì‹¤íŒ¨: {e}")
            return {
                'model': model,
                'status': 'failed',
                'error': str(e)
            }

    def hyperparameter_optimization(self, model_info: Dict, X_train: np.ndarray, 
                                  y_train: np.ndarray) -> Dict:
        """
        ğŸ”§ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
        
        Args:
            model_info: ëª¨ë¸ ì •ë³´
            X_train: í›ˆë ¨ ë°ì´í„°
            y_train: í›ˆë ¨ íƒ€ê²Ÿ
            
        Returns:
            Dict: ìµœì í™”ëœ ëª¨ë¸ ì •ë³´
        """
        if 'hyperparams' not in model_info or model_info['type'] == 'deep_learning':
            return model_info
        
        try:
            # ì‹œê³„ì—´ ë¶„í• 
            tscv = TimeSeriesSplit(n_splits=5)
            
            # ê·¸ë¦¬ë“œ ì„œì¹˜
            grid_search = RandomizedSearchCV(
                model_info['model'],
                model_info['hyperparams'],
                cv=tscv,
                scoring='neg_mean_squared_error',
                n_iter=20,
                random_state=42,
                n_jobs=-1
            )
            
            grid_search.fit(X_train, y_train)
            
            # ìµœì í™”ëœ ëª¨ë¸ë¡œ ì—…ë°ì´íŠ¸
            model_info['model'] = grid_search.best_estimator_
            model_info['best_params'] = grid_search.best_params_
            model_info['best_score'] = -grid_search.best_score_
            
            return model_info
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹¤íŒ¨: {e}")
            return model_info

    def dynamic_ensemble_weighting(self, model_results: Dict[str, Dict]) -> Dict[str, float]:
        """
        âš–ï¸ ë™ì  ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ê³„ì‚°
        
        Args:
            model_results: ëª¨ë¸ë³„ ê²°ê³¼
            
        Returns:
            Dict[str, float]: ëª¨ë¸ë³„ ê°€ì¤‘ì¹˜
        """
        weights = {}
        
        # ì„±ëŠ¥ ì§€í‘œë³„ ê°€ì¤‘ì¹˜
        performance_weights = {
            'direction_accuracy': 0.4,  # ë°©í–¥ì„±ì´ ê°€ì¥ ì¤‘ìš”
            'r2': 0.3,
            'mse': 0.2,
            'mae': 0.1
        }
        
        # ê° ëª¨ë¸ì˜ ì¢…í•© ì ìˆ˜ ê³„ì‚°
        model_scores = {}
        
        for model_name, result in model_results.items():
            if result['status'] != 'success':
                model_scores[model_name] = 0.0
                continue
            
            # ì •ê·œí™”ëœ ì ìˆ˜ ê³„ì‚°
            score = 0.0
            
            # ë°©í–¥ì„± ì •í™•ë„ (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
            score += result['direction_accuracy'] * performance_weights['direction_accuracy']
            
            # RÂ² ì ìˆ˜ (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ, ìŒìˆ˜ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ 0ê³¼ 1 ì‚¬ì´ë¡œ í´ë¦¬í•‘)
            r2_normalized = max(0, min(1, result['r2']))
            score += r2_normalized * performance_weights['r2']
            
            # MSE (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ, ì—­ìˆ˜ ì‚¬ìš©)
            mse_score = 1 / (1 + result['mse'])
            score += mse_score * performance_weights['mse']
            
            # MAE (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ, ì—­ìˆ˜ ì‚¬ìš©)  
            mae_score = 1 / (1 + result['mae'])
            score += mae_score * performance_weights['mae']
            
            model_scores[model_name] = score
        
        # ì†Œí”„íŠ¸ë§¥ìŠ¤ ë³€í™˜ìœ¼ë¡œ ê°€ì¤‘ì¹˜ ì •ê·œí™”
        if model_scores:
            scores_array = np.array(list(model_scores.values()))
            if np.sum(scores_array) > 0:
                # ì˜¨ë„ íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•œ ì†Œí”„íŠ¸ë§¥ìŠ¤ (ë” ì„ íƒì )
                temperature = 2.0
                exp_scores = np.exp(scores_array / temperature)
                softmax_weights = exp_scores / np.sum(exp_scores)
                
                for i, model_name in enumerate(model_scores.keys()):
                    weights[model_name] = float(softmax_weights[i])
            else:
                # ëª¨ë“  ëª¨ë¸ì´ ì‹¤íŒ¨í•œ ê²½ìš° ê· ë“± ê°€ì¤‘ì¹˜
                n_models = len(model_scores)
                for model_name in model_scores.keys():
                    weights[model_name] = 1.0 / n_models
        
        return weights

    def meta_learning_optimization(self, model_results: Dict[str, Dict], 
                                 X_val: np.ndarray, y_val: np.ndarray) -> Any:
        """
        ğŸ§  ë©”íƒ€ í•™ìŠµ ê¸°ë°˜ ì•™ìƒë¸” ìµœì í™”
        
        Args:
            model_results: ëª¨ë¸ë³„ ê²°ê³¼
            X_val: ê²€ì¦ ë°ì´í„°
            y_val: ê²€ì¦ íƒ€ê²Ÿ
            
        Returns:
            ë©”íƒ€ ëŸ¬ë„ˆ ëª¨ë¸
        """
        # ì„±ê³µí•œ ëª¨ë¸ë“¤ì˜ ì˜ˆì¸¡ê°’ ìˆ˜ì§‘
        meta_features = []
        successful_models = []
        
        for model_name, result in model_results.items():
            if result['status'] == 'success':
                meta_features.append(result['predictions'])
                successful_models.append(model_name)
        
        if len(meta_features) < 2:
            return None
        
        # ë©”íƒ€ íŠ¹ì„± ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±
        meta_X = np.column_stack(meta_features)
        
        # ë©”íƒ€ ëŸ¬ë„ˆ í›ˆë ¨ (ì—¬ëŸ¬ ëª¨ë¸ ì‹œë„)
        meta_learners = {
            'ridge': Ridge(alpha=1.0),
            'elastic_net': ElasticNet(alpha=0.1, l1_ratio=0.5),
            'random_forest': RandomForestRegressor(n_estimators=50, random_state=42),
            'gradient_boosting': GradientBoostingRegressor(n_estimators=50, random_state=42)
        }
        
        best_meta_learner = None
        best_score = float('inf')
        
        for name, learner in meta_learners.items():
            try:
                # ì‹œê³„ì—´ êµì°¨ê²€ì¦
                tscv = TimeSeriesSplit(n_splits=3)
                scores = cross_val_score(learner, meta_X, y_val, cv=tscv, 
                                       scoring='neg_mean_squared_error')
                avg_score = -np.mean(scores)
                
                if avg_score < best_score:
                    best_score = avg_score
                    best_meta_learner = learner
                    
            except Exception as e:
                self.logger.warning(f"âš ï¸ ë©”íƒ€ ëŸ¬ë„ˆ {name} í›ˆë ¨ ì‹¤íŒ¨: {e}")
                continue
        
        # ìµœì  ë©”íƒ€ ëŸ¬ë„ˆ í›ˆë ¨
        if best_meta_learner is not None:
            best_meta_learner.fit(meta_X, y_val)
            
            return {
                'meta_learner': best_meta_learner,
                'successful_models': successful_models,
                'meta_score': best_score
            }
        
        return None

    def bayesian_model_averaging(self, model_results: Dict[str, Dict], 
                                X_val: np.ndarray, y_val: np.ndarray) -> Dict:
        """
        ğŸ“Š ë² ì´ì§€ì•ˆ ëª¨ë¸ í‰ê· í™”
        
        Args:
            model_results: ëª¨ë¸ë³„ ê²°ê³¼
            X_val: ê²€ì¦ ë°ì´í„°  
            y_val: ê²€ì¦ íƒ€ê²Ÿ
            
        Returns:
            Dict: ë² ì´ì§€ì•ˆ í‰ê· í™” ê²°ê³¼
        """
        if not SCIPY_AVAILABLE:
            return None
        
        # ì„±ê³µí•œ ëª¨ë¸ë“¤ì˜ ì˜ˆì¸¡ê°’ê³¼ ì„±ëŠ¥ ìˆ˜ì§‘
        predictions = []
        likelihoods = []
        
        for model_name, result in model_results.items():
            if result['status'] == 'success':
                pred = result['predictions']
                
                # ìš°ë„ ê³„ì‚° (MSE ê¸°ë°˜)
                mse = result['mse']
                likelihood = np.exp(-mse / (2 * np.var(y_val)))
                
                predictions.append(pred)
                likelihoods.append(likelihood)
        
        if len(predictions) < 2:
            return None
        
        # ë² ì´ì§€ì•ˆ ê°€ì¤‘ì¹˜ ê³„ì‚°
        likelihoods = np.array(likelihoods)
        bayesian_weights = likelihoods / np.sum(likelihoods)
        
        # ê°€ì¤‘ í‰ê·  ì˜ˆì¸¡
        weighted_predictions = np.average(predictions, axis=0, weights=bayesian_weights)
        
        # ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™” (ì˜ˆì¸¡ ë¶„ì‚°)
        prediction_variance = np.var(predictions, axis=0)
        uncertainty = np.sqrt(prediction_variance)
        
        return {
            'predictions': weighted_predictions,
            'weights': bayesian_weights,
            'uncertainty': uncertainty,
            'confidence_intervals': {
                'lower': weighted_predictions - 1.96 * uncertainty,
                'upper': weighted_predictions + 1.96 * uncertainty
            }
        }

    def ensemble_prediction(self, models: Dict, weights: Dict[str, float], 
                          X_test: np.ndarray, method: str = 'weighted_average') -> np.ndarray:
        """
        ğŸ¯ ì•™ìƒë¸” ì˜ˆì¸¡ ìˆ˜í–‰
        
        Args:
            models: í›ˆë ¨ëœ ëª¨ë¸ë“¤
            weights: ëª¨ë¸ ê°€ì¤‘ì¹˜
            X_test: í…ŒìŠ¤íŠ¸ ë°ì´í„°
            method: ì•™ìƒë¸” ë°©ë²•
            
        Returns:
            np.ndarray: ì•™ìƒë¸” ì˜ˆì¸¡ê°’
        """
        predictions = []
        model_weights = []
        
        for model_name, model_info in models.items():
            if model_name in weights and weights[model_name] > 0:
                try:
                    if model_info['type'] == 'deep_learning':
                        pred = model_info['model'].predict(X_test).flatten()
                    else:
                        pred = model_info['model'].predict(X_test)
                    
                    predictions.append(pred)
                    model_weights.append(weights[model_name])
                    
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ëª¨ë¸ {model_name} ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
                    continue
        
        if not predictions:
            raise ValueError("âŒ ì˜ˆì¸¡ ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")
        
        predictions = np.array(predictions)
        model_weights = np.array(model_weights)
        
        if method == 'weighted_average':
            # ê°€ì¤‘ í‰ê· 
            ensemble_pred = np.average(predictions, axis=0, weights=model_weights)
            
        elif method == 'median':
            # ì¤‘ìœ„ìˆ˜ (ì´ìƒì¹˜ì— ê°•ê±´)
            ensemble_pred = np.median(predictions, axis=0)
            
        elif method == 'trimmed_mean':
            # ì ˆì‚¬ í‰ê·  (ìƒí•˜ìœ„ 20% ì œê±°)
            sorted_preds = np.sort(predictions, axis=0)
            n_models = len(predictions)
            trim_count = max(1, int(0.2 * n_models))
            trimmed = sorted_preds[trim_count:-trim_count] if n_models > 2*trim_count else sorted_preds
            ensemble_pred = np.mean(trimmed, axis=0)
            
        else:
            ensemble_pred = np.mean(predictions, axis=0)
        
        return ensemble_pred

    def performance_monitoring(self, y_true: np.ndarray, y_pred: np.ndarray) -> Dict:
        """
        ğŸ“Š ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ë¶„ì„
        
        Args:
            y_true: ì‹¤ì œ ê°’
            y_pred: ì˜ˆì¸¡ ê°’
            
        Returns:
            Dict: ì„±ëŠ¥ ë¶„ì„ ê²°ê³¼
        """
        # ê¸°ë³¸ ë©”íŠ¸ë¦­
        mse = mean_squared_error(y_true, y_pred)
        mae = mean_absolute_error(y_true, y_pred)
        r2 = r2_score(y_true, y_pred)
        
        # ë°©í–¥ì„± ì •í™•ë„
        if len(y_true) > 1:
            direction_actual = np.diff(y_true) > 0
            direction_pred = np.diff(y_pred) > 0
            direction_accuracy = np.mean(direction_actual == direction_pred)
        else:
            direction_accuracy = 0.0
        
        # MAPE (Mean Absolute Percentage Error)
        mape = mean_absolute_percentage_error(y_true, y_pred) * 100
        
        # ì”ì°¨ ë¶„ì„
        residuals = y_true - y_pred
        residual_std = np.std(residuals)
        residual_mean = np.mean(residuals)
        
        # ì„±ëŠ¥ ë“±ê¸‰ ê³„ì‚°
        if direction_accuracy >= 0.90:
            grade = 'A+'
        elif direction_accuracy >= 0.85:
            grade = 'A'
        elif direction_accuracy >= 0.80:
            grade = 'B+'
        elif direction_accuracy >= 0.75:
            grade = 'B'
        elif direction_accuracy >= 0.70:
            grade = 'C+'
        else:
            grade = 'C'
        
        return {
            'mse': mse,
            'mae': mae,
            'r2': r2,
            'direction_accuracy': direction_accuracy,
            'mape': mape,
            'residual_std': residual_std,
            'residual_mean': residual_mean,
            'grade': grade,
            'target_achieved': direction_accuracy >= self.target_accuracy
        }

    def failure_detection_and_recovery(self, performance: Dict) -> Dict:
        """
        ğŸ”§ ëª¨ë¸ ì‹¤íŒ¨ ê°ì§€ ë° ë³µêµ¬
        
        Args:
            performance: ì„±ëŠ¥ ë¶„ì„ ê²°ê³¼
            
        Returns:
            Dict: ë³µêµ¬ ì „ëµ
        """
        issues = []
        recovery_actions = []
        
        # ì„±ëŠ¥ ì €í•˜ ê°ì§€
        if performance['direction_accuracy'] < 0.55:
            issues.append("ì‹¬ê°í•œ ì„±ëŠ¥ ì €í•˜ (< 55%)")
            recovery_actions.append("ëª¨ë“  ëª¨ë¸ ì¬í›ˆë ¨")
            
        elif performance['direction_accuracy'] < 0.65:
            issues.append("ì„±ëŠ¥ ì €í•˜ ê°ì§€ (< 65%)")
            recovery_actions.append("ì €ì„±ëŠ¥ ëª¨ë¸ ì œê±° ë° ê°€ì¤‘ì¹˜ ì¬ì¡°ì •")
        
        # ê³¼ì í•© ê°ì§€
        if abs(performance['residual_mean']) > 0.1 * np.mean(np.abs(performance.get('y_true', [1]))):
            issues.append("í¸í–¥ì„± ê°ì§€")
            recovery_actions.append("ì •ê·œí™” ê°•í™”")
        
        # RÂ² ì ìˆ˜ í™•ì¸
        if performance['r2'] < 0.3:
            issues.append("ë‚®ì€ ì„¤ëª…ë ¥")
            recovery_actions.append("íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ê°œì„ ")
        
        # ë³µêµ¬ ì „ëµ ì‹¤í–‰
        if issues:
            self.failure_count += 1
            self.logger.warning(f"âš ï¸ ê°ì§€ëœ ë¬¸ì œ: {', '.join(issues)}")
            self.logger.info(f"ğŸ”§ ë³µêµ¬ ì•¡ì…˜: {', '.join(recovery_actions)}")
        
        return {
            'issues_detected': issues,
            'recovery_actions': recovery_actions,
            'failure_count': self.failure_count,
            'needs_retraining': len(issues) > 0
        }

    def save_ensemble_system(self, models: Dict, weights: Dict, 
                           meta_learner: Any = None) -> str:
        """
        ğŸ’¾ ì•™ìƒë¸” ì‹œìŠ¤í…œ ì €ì¥
        
        Args:
            models: í›ˆë ¨ëœ ëª¨ë¸ë“¤
            weights: ëª¨ë¸ ê°€ì¤‘ì¹˜
            meta_learner: ë©”íƒ€ ëŸ¬ë„ˆ (ì„ íƒì )
            
        Returns:
            str: ì €ì¥ ê²½ë¡œ
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        save_path = self.model_save_path / f"ensemble_system_{timestamp}.pkl"
        
        ensemble_data = {
            'models': models,
            'weights': weights,
            'meta_learner': meta_learner,
            'scalers': self.scalers,
            'target_accuracy': self.target_accuracy,
            'accuracy_history': self.accuracy_history,
            'timestamp': timestamp
        }
        
        with open(save_path, 'wb') as f:
            pickle.dump(ensemble_data, f)
        
        self.logger.info(f"ğŸ’¾ ì•™ìƒë¸” ì‹œìŠ¤í…œ ì €ì¥ ì™„ë£Œ: {save_path}")
        return str(save_path)

    def train_ensemble_system(self) -> Dict:
        """
        ğŸ¯ ì „ì²´ ì•™ìƒë¸” ì‹œìŠ¤í…œ í›ˆë ¨
        
        Returns:
            Dict: í›ˆë ¨ ê²°ê³¼
        """
        print("\nğŸ¯ ê³ ê¸‰ ì•™ìƒë¸” í•™ìŠµ ì‹œìŠ¤í…œ í›ˆë ¨ ì‹œì‘")
        print("=" * 50)
        
        start_time = datetime.now()
        
        try:
            # 1. ë°ì´í„° ë¡œë“œ
            data = self.load_comprehensive_data()
            
            # 2. ë°ì´í„° ì „ì²˜ë¦¬
            print("ğŸ”„ ë°ì´í„° ì „ì²˜ë¦¬...")
            
            # ê°€ê²© ì»¬ëŸ¼ ì°¾ê¸°
            price_col = None
            for col in data.columns:
                if 'price' in col.lower():
                    price_col = col
                    break
            
            if price_col is None:
                price_col = data.columns[0]
            
            # íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
            features = data.drop(columns=[price_col])
            target = data[price_col]
            
            # ë°ì´í„° ë¶„í•  (ì‹œê³„ì—´ì´ë¯€ë¡œ ìˆœì°¨ì )
            split_idx = int(len(data) * 0.8)
            
            X_train = features.iloc[:split_idx]
            y_train = target.iloc[:split_idx]
            X_test = features.iloc[split_idx:]
            y_test = target.iloc[split_idx:]
            
            # ê²€ì¦ ì„¸íŠ¸ ë¶„í• 
            val_split_idx = int(len(X_train) * 0.8)
            X_val = X_train.iloc[val_split_idx:]
            y_val = y_train.iloc[val_split_idx:]
            X_train = X_train.iloc[:val_split_idx]
            y_train = y_train.iloc[:val_split_idx]
            
            # ë°ì´í„° ìŠ¤ì¼€ì¼ë§
            scaler = RobustScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_val_scaled = scaler.transform(X_val)
            X_test_scaled = scaler.transform(X_test)
            
            self.scalers['feature_scaler'] = scaler
            
            print(f"ğŸ“Š í›ˆë ¨ ë°ì´í„°: {len(X_train)} ìƒ˜í”Œ")
            print(f"ğŸ“Š ê²€ì¦ ë°ì´í„°: {len(X_val)} ìƒ˜í”Œ") 
            print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(X_test)} ìƒ˜í”Œ")
            
            # 3. ë‹¤ì–‘í•œ ëª¨ë¸ ìƒì„±
            models = self.create_diverse_models()
            
            # ë”¥ëŸ¬ë‹ ëª¨ë¸ ì¶”ê°€ (ë°ì´í„°ê°€ ì¶©ë¶„í•œ ê²½ìš°)
            if len(X_train) > 500 and TENSORFLOW_AVAILABLE:
                sequence_length = min(24, len(X_train) // 20)
                X_seq_train, y_seq_train = self.prepare_sequences(
                    pd.concat([X_train, y_train.to_frame()], axis=1), 
                    sequence_length
                )
                X_seq_val, y_seq_val = self.prepare_sequences(
                    pd.concat([X_val, y_val.to_frame()], axis=1), 
                    sequence_length
                )
                
                if len(X_seq_train) > 100:
                    input_shape = (sequence_length, X_train.shape[1])
                    deep_models = self.create_deep_learning_models(input_shape)
                    models.update(deep_models)
            
            print(f"ğŸ¤– ì´ {len(models)}ê°œ ëª¨ë¸ ìƒì„± ì™„ë£Œ")
            
            # 4. ëª¨ë¸ë³„ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
            print("ğŸ”§ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”...")
            optimized_models = {}
            
            with ThreadPoolExecutor(max_workers=4) as executor:
                futures = {}
                for name, model_info in models.items():
                    if model_info['type'] != 'deep_learning':  # ì „í†µì  MLë§Œ
                        future = executor.submit(
                            self.hyperparameter_optimization, 
                            model_info, X_train_scaled, y_train.values
                        )
                        futures[name] = future
                    else:
                        optimized_models[name] = model_info
                
                # ê²°ê³¼ ìˆ˜ì§‘
                for name, future in futures.items():
                    try:
                        optimized_models[name] = future.result(timeout=300)
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ {name} ìµœì í™” ì‹¤íŒ¨: {e}")
                        optimized_models[name] = models[name]
            
            models = optimized_models
            
            # 5. ëª¨ë¸ í›ˆë ¨
            print("ğŸ¯ ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")
            model_results = {}
            
            for model_name, model_info in models.items():
                print(f"  ğŸ“ˆ {model_name} í›ˆë ¨ ì¤‘...")
                
                if model_info['type'] == 'deep_learning':
                    # ì‹œí€€ìŠ¤ ë°ì´í„° ì‚¬ìš©
                    if 'X_seq_train' in locals():
                        result = self.train_single_model(
                            model_info, X_seq_train, y_seq_train,
                            X_seq_val, y_seq_val
                        )
                    else:
                        continue
                else:
                    # ì¼ë°˜ ë°ì´í„° ì‚¬ìš©
                    result = self.train_single_model(
                        model_info, X_train_scaled, y_train.values,
                        X_val_scaled, y_val.values
                    )
                
                model_results[model_name] = result
                
                if result['status'] == 'success':
                    print(f"    âœ… ë°©í–¥ì„± ì •í™•ë„: {result['direction_accuracy']:.3f}")
                    print(f"    âœ… RÂ² ì ìˆ˜: {result['r2']:.3f}")
                else:
                    print(f"    âŒ í›ˆë ¨ ì‹¤íŒ¨: {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
            
            # ì„±ê³µí•œ ëª¨ë¸ ê°œìˆ˜ í™•ì¸
            successful_models = [name for name, result in model_results.items() 
                               if result['status'] == 'success']
            print(f"\nâœ… ì„±ê³µí•œ ëª¨ë¸: {len(successful_models)}ê°œ")
            
            if len(successful_models) == 0:
                raise ValueError("âŒ í›ˆë ¨ì— ì„±ê³µí•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")
            
            # 6. ë™ì  ê°€ì¤‘ì¹˜ ê³„ì‚°
            print("âš–ï¸ ë™ì  ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ê³„ì‚°...")
            weights = self.dynamic_ensemble_weighting(model_results)
            
            print("ğŸ“Š ëª¨ë¸ë³„ ê°€ì¤‘ì¹˜:")
            for model_name, weight in sorted(weights.items(), key=lambda x: x[1], reverse=True):
                if weight > 0.01:  # 1% ì´ìƒì¸ ê²½ìš°ë§Œ ì¶œë ¥
                    print(f"  {model_name}: {weight:.3f}")
            
            # 7. ë©”íƒ€ í•™ìŠµ ìµœì í™”
            print("ğŸ§  ë©”íƒ€ í•™ìŠµ ìµœì í™”...")
            meta_learner_result = self.meta_learning_optimization(
                model_results, X_val_scaled, y_val.values
            )
            
            # 8. ë² ì´ì§€ì•ˆ ëª¨ë¸ í‰ê· í™”
            print("ğŸ“Š ë² ì´ì§€ì•ˆ ëª¨ë¸ í‰ê· í™”...")
            bayesian_result = self.bayesian_model_averaging(
                model_results, X_val_scaled, y_val.values
            )
            
            # 9. ìµœì¢… í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡
            print("ğŸ¯ ìµœì¢… í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡...")
            
            # ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ìœ„í•œ ì‹œí€€ìŠ¤ ë°ì´í„° ì¤€ë¹„
            test_predictions = {}
            
            for model_name, model_info in models.items():
                if (model_name in model_results and 
                    model_results[model_name]['status'] == 'success'):
                    
                    try:
                        if model_info['type'] == 'deep_learning':
                            if 'X_seq_train' in locals():
                                # í…ŒìŠ¤íŠ¸ìš© ì‹œí€€ìŠ¤ ë°ì´í„° ì¤€ë¹„
                                X_seq_test, y_seq_test = self.prepare_sequences(
                                    pd.concat([X_test, y_test.to_frame()], axis=1),
                                    sequence_length
                                )
                                pred = model_results[model_name]['model'].predict(X_seq_test).flatten()
                                # ê¸¸ì´ ë§ì¶”ê¸°
                                if len(pred) != len(y_test):
                                    pred = np.pad(pred, (len(y_test) - len(pred), 0), 
                                                mode='edge')[:len(y_test)]
                            else:
                                continue
                        else:
                            pred = model_results[model_name]['model'].predict(X_test_scaled)
                        
                        test_predictions[model_name] = pred
                        
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ {model_name} í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
                        continue
            
            # ì•™ìƒë¸” ì˜ˆì¸¡
            if test_predictions:
                # ê°€ì¤‘ í‰ê·  ì•™ìƒë¸”
                ensemble_pred = self.ensemble_prediction(
                    {name: {'model': model_results[name]['model'], 
                           'type': models[name]['type']} 
                     for name in test_predictions.keys()},
                    weights, X_test_scaled, method='weighted_average'
                )
                
                # ì„±ëŠ¥ í‰ê°€
                performance = self.performance_monitoring(y_test.values, ensemble_pred)
                
                # ì‹¤íŒ¨ ê°ì§€ ë° ë³µêµ¬
                failure_analysis = self.failure_detection_and_recovery(performance)
                
                # ê²°ê³¼ ì €ì¥
                self.accuracy_history.append({
                    'timestamp': datetime.now(),
                    'accuracy': performance['direction_accuracy'],
                    'r2': performance['r2'],
                    'grade': performance['grade']
                })
                
                # ì‹œìŠ¤í…œ ì €ì¥
                save_path = self.save_ensemble_system(
                    {name: {'model': model_results[name]['model'],
                           'type': models[name]['type']}
                     for name in successful_models},
                    weights,
                    meta_learner_result
                )
                
                # ìµœì¢… ê²°ê³¼
                end_time = datetime.now()
                training_time = (end_time - start_time).total_seconds()
                
                result = {
                    'success': True,
                    'training_time_seconds': training_time,
                    'models_trained': len(models),
                    'successful_models': len(successful_models),
                    'ensemble_performance': performance,
                    'model_weights': weights,
                    'meta_learner': meta_learner_result is not None,
                    'bayesian_averaging': bayesian_result is not None,
                    'failure_analysis': failure_analysis,
                    'save_path': save_path,
                    'target_achieved': performance['target_achieved']
                }
                
                # ê²°ê³¼ ì¶œë ¥
                print("\n" + "="*50)
                print("ğŸ¯ ì•™ìƒë¸” í•™ìŠµ ì‹œìŠ¤í…œ í›ˆë ¨ ì™„ë£Œ!")
                print("="*50)
                print(f"â±ï¸  ì´ í›ˆë ¨ ì‹œê°„: {training_time:.1f}ì´ˆ")
                print(f"ğŸ¤– í›ˆë ¨ëœ ëª¨ë¸: {len(models)}ê°œ")
                print(f"âœ… ì„±ê³µí•œ ëª¨ë¸: {len(successful_models)}ê°œ")
                print(f"ğŸ“Š ë°©í–¥ì„± ì •í™•ë„: {performance['direction_accuracy']:.3f} ({performance['direction_accuracy']*100:.1f}%)")
                print(f"ğŸ“ˆ RÂ² ì ìˆ˜: {performance['r2']:.3f}")
                print(f"ğŸ† ì„±ëŠ¥ ë“±ê¸‰: {performance['grade']}")
                print(f"ğŸ¯ ëª©í‘œ ë‹¬ì„±: {'âœ… YES' if performance['target_achieved'] else 'âŒ NO'}")
                print(f"ğŸ’¾ ì €ì¥ ê²½ë¡œ: {save_path}")
                
                return result
            
            else:
                raise ValueError("âŒ í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ì— ì„±ê³µí•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")
                
        except Exception as e:
            self.logger.error(f"âŒ ì•™ìƒë¸” ì‹œìŠ¤í…œ í›ˆë ¨ ì‹¤íŒ¨: {e}")
            return {
                'success': False,
                'error': str(e),
                'training_time_seconds': (datetime.now() - start_time).total_seconds()
            }

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ¯ ê³ ê¸‰ ì•™ìƒë¸” í•™ìŠµ ì‹œìŠ¤í…œ ì‹œì‘")
    
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    ensemble_system = AdvancedEnsembleLearningSystem(target_accuracy=0.90)
    
    # ì•™ìƒë¸” ì‹œìŠ¤í…œ í›ˆë ¨
    result = ensemble_system.train_ensemble_system()
    
    # ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥
    result_path = "/Users/parkyoungjun/Desktop/BTC_Analysis_System/ensemble_learning_results.json"
    
    # datetime ê°ì²´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
    if 'accuracy_history' in result:
        for item in result.get('accuracy_history', []):
            if 'timestamp' in item and hasattr(item['timestamp'], 'isoformat'):
                item['timestamp'] = item['timestamp'].isoformat()
    
    with open(result_path, 'w', encoding='utf-8') as f:
        json.dump(result, f, indent=2, ensure_ascii=False, default=str)
    
    print(f"\nğŸ“„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {result_path}")
    
    return result

if __name__ == "__main__":
    result = main()