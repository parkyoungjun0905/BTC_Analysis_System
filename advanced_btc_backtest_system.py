#!/usr/bin/env python3
"""
ğŸ¯ ì§„ì§œ AI ê¸°ë°˜ BTC 90% ì •í™•ë„ ë°±í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ
1-3ì‹œê°„ ë‹¨ìœ„ ë¯¸ë˜ ê°€ê²© ì˜ˆì¸¡ (ìµœê³  ì„±ëŠ¥ ëª©í‘œ)

ëª¨ë“  ë”¥ëŸ¬ë‹ ê¸°ìˆ  ì´ë™ì›:
- LSTM, GRU, Transformer, XGBoost, LightGBM, CatBoost
- ì•™ìƒë¸” í•™ìŠµ, ë² ì´ì§€ì•ˆ ìµœì í™”, ê³ ê¸‰ íŠ¹ì„±ê³µí•™
- ì‹œê³„ì—´ êµì°¨ê²€ì¦, ê³¼ì í•© ë°©ì§€, ì‹œì¥ ìƒí™© ì ì‘
"""

import os
import sys
import json
import numpy as np
import pandas as pd
import warnings
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor

# ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°
warnings.filterwarnings('ignore')
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

# ë”¥ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    import tensorflow as tf
    tf.get_logger().setLevel('ERROR')
    from tensorflow.keras.models import Sequential, Model
    from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout, Attention
    from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization
    from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, Flatten
    from tensorflow.keras.optimizers import Adam, AdamW
    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
    from tensorflow.keras.regularizers import l1_l2
    print("âœ… TensorFlow/Keras ê³ ê¸‰ ëª¨ë“ˆ ë¡œë”© ì™„ë£Œ")
    TENSORFLOW_AVAILABLE = True
except ImportError as e:
    print(f"âŒ TensorFlow ë¡œë”© ì‹¤íŒ¨: {e}")
    TENSORFLOW_AVAILABLE = False

# ê³ ê¸‰ ML ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    import xgboost as xgb
    import lightgbm as lgb
    from catboost import CatBoostRegressor
    from sklearn.ensemble import RandomForestRegressor, VotingRegressor, StackingRegressor
    from sklearn.model_selection import TimeSeriesSplit, GridSearchCV
    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
    from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
    from sklearn.feature_selection import SelectKBest, f_regression, RFE
    from sklearn.decomposition import PCA, FastICA
    from sklearn.pipeline import Pipeline
    print("âœ… ê³ ê¸‰ ML ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë”© ì™„ë£Œ")
    SKLEARN_AVAILABLE = True
except ImportError as e:
    print(f"âŒ ML ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë”© ì‹¤íŒ¨: {e}")
    SKLEARN_AVAILABLE = False

# ë² ì´ì§€ì•ˆ ìµœì í™”
try:
    from skopt import gp_minimize
    from skopt.space import Real, Integer, Categorical
    from skopt.utils import use_named_args
    from skopt.acquisition import gaussian_ei
    print("âœ… ë² ì´ì§€ì•ˆ ìµœì í™” ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë”© ì™„ë£Œ")
    BAYESIAN_OPT_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ ë² ì´ì§€ì•ˆ ìµœì í™” ì„¤ì¹˜ ê¶Œì¥: {e}")
    BAYESIAN_OPT_AVAILABLE = False

# ê³ ê¸‰ íŠ¹ì„± ê³µí•™
try:
    import ta
    import talib
    from scipy import signal, stats
    from scipy.fft import fft, ifft
    import pywt  # wavelet transform
    from tsfresh import extract_features
    from tsfresh.feature_extraction import EfficientFCParameters
    print("âœ… ê³ ê¸‰ íŠ¹ì„±ê³µí•™ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë”© ì™„ë£Œ")
    FEATURE_ENG_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ ê³ ê¸‰ íŠ¹ì„±ê³µí•™ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¼ë¶€ ëˆ„ë½: {e}")
    FEATURE_ENG_AVAILABLE = False

class AdvancedFeatureEngineer:
    """ğŸ”¬ ê³ ê¸‰ íŠ¹ì„± ê³µí•™ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.scalers = {}
        self.feature_names = []
        
    def create_technical_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """ê¸°ìˆ ì  ì§€í‘œ ìƒì„± (100+ ì§€í‘œ)"""
        print("ğŸ“Š ê³ ê¸‰ ê¸°ìˆ ì  ì§€í‘œ ìƒì„± ì¤‘...")
        
        # ê°€ê²© ë°ì´í„°
        high = df['high'].values
        low = df['low'].values
        close = df['close'].values
        volume = df['volume'].values
        
        features = df.copy()
        
        # 1. ê¸°ë³¸ ì´ë™í‰ê·  (Multiple Timeframes)
        for period in [5, 10, 20, 50, 100, 200]:
            features[f'sma_{period}'] = close.rolling(period).mean()
            features[f'ema_{period}'] = close.ewm(span=period).mean()
            features[f'price_to_sma_{period}'] = close / features[f'sma_{period}']
        
        # 2. ê³ ê¸‰ ëª¨ë©˜í…€ ì§€í‘œ
        for period in [7, 14, 21, 28]:
            features[f'rsi_{period}'] = ta.momentum.RSIIndicator(close, window=period).rsi()
            features[f'stoch_{period}'] = ta.momentum.StochasticOscillator(high, low, close, window=period).stoch()
            features[f'williams_r_{period}'] = ta.momentum.WilliamsRIndicator(high, low, close, lbp=period).williams_r()
        
        # 3. ë³¼ë¦°ì € ë°´ë“œ (Multiple Periods)
        for period in [10, 20, 50]:
            bb = ta.volatility.BollingerBands(close, window=period)
            features[f'bb_upper_{period}'] = bb.bollinger_hband()
            features[f'bb_lower_{period}'] = bb.bollinger_lband()
            features[f'bb_width_{period}'] = bb.bollinger_wband()
            features[f'bb_position_{period}'] = (close - bb.bollinger_lband()) / (bb.bollinger_hband() - bb.bollinger_lband())
        
        # 4. MACD íŒ¨ë°€ë¦¬
        for fast, slow, signal in [(12, 26, 9), (5, 35, 5), (19, 39, 9)]:
            macd = ta.trend.MACD(close, window_fast=fast, window_slow=slow, window_sign=signal)
            features[f'macd_{fast}_{slow}'] = macd.macd()
            features[f'macd_signal_{fast}_{slow}'] = macd.macd_signal()
            features[f'macd_histogram_{fast}_{slow}'] = macd.macd_diff()
        
        # 5. ê±°ë˜ëŸ‰ ì§€í‘œ
        features['volume_sma_10'] = volume.rolling(10).mean()
        features['volume_ratio'] = volume / features['volume_sma_10']
        features['price_volume'] = close * volume
        features['vwap'] = (features['price_volume'].cumsum() / volume.cumsum())
        features['volume_rsi'] = ta.momentum.RSIIndicator(volume, window=14).rsi()
        
        # 6. ë³€ë™ì„± ì§€í‘œ
        features['atr_14'] = ta.volatility.AverageTrueRange(high, low, close, window=14).average_true_range()
        features['atr_21'] = ta.volatility.AverageTrueRange(high, low, close, window=21).average_true_range()
        features['true_range'] = np.maximum(high - low, 
                                           np.maximum(np.abs(high - np.roll(close, 1)),
                                                     np.abs(low - np.roll(close, 1))))
        
        # 7. ì¶”ì„¸ ì§€í‘œ
        features['adx_14'] = ta.trend.ADXIndicator(high, low, close, window=14).adx()
        features['cci_20'] = ta.trend.CCIIndicator(high, low, close, window=20).cci()
        
        # 8. íŒ¨í„´ ì¸ì‹ ì§€í‘œ
        features['doji'] = np.abs(close - features['open']) <= (high - low) * 0.1
        features['hammer'] = (low < np.minimum(close, features['open'])) & ((high - np.maximum(close, features['open'])) > 2 * np.abs(close - features['open']))
        
        # 9. ì‹œê°„ ê¸°ë°˜ íŠ¹ì„±
        features['hour'] = pd.to_datetime(features.index).hour
        features['day_of_week'] = pd.to_datetime(features.index).dayofweek
        features['month'] = pd.to_datetime(features.index).month
        features['is_weekend'] = features['day_of_week'].isin([5, 6]).astype(int)
        
        # 10. ê³ ê¸‰ í†µê³„ íŠ¹ì„±
        for window in [5, 10, 20]:
            features[f'price_std_{window}'] = close.rolling(window).std()
            features[f'price_skew_{window}'] = close.rolling(window).skew()
            features[f'price_kurt_{window}'] = close.rolling(window).kurt()
            features[f'return_{window}'] = close.pct_change(window)
        
        print(f"âœ… {len(features.columns)} ê°œ ê¸°ìˆ ì  ì§€í‘œ ìƒì„± ì™„ë£Œ")
        return features
    
    def create_advanced_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """ê³ ê¸‰ íŠ¹ì„± ìƒì„± (ì›¨ì´ë¸Œë¦¿, í‘¸ë¦¬ì—, í”„ë™íƒˆ ë“±)"""
        print("ğŸ”¬ ê³ ê¸‰ ìˆ˜í•™ì  íŠ¹ì„± ìƒì„± ì¤‘...")
        
        features = df.copy()
        close = df['close'].values
        
        # 1. ì›¨ì´ë¸Œë¦¿ ë³€í™˜ íŠ¹ì„±
        if FEATURE_ENG_AVAILABLE:
            try:
                coeffs = pywt.wavedec(close, 'db4', level=3)
                features['wavelet_approx'] = np.pad(coeffs[0], (0, len(close) - len(coeffs[0])), 'constant')[:len(close)]
                for i, detail in enumerate(coeffs[1:], 1):
                    padded_detail = np.pad(detail, (0, len(close) - len(detail)), 'constant')[:len(close)]
                    features[f'wavelet_detail_{i}'] = padded_detail
            except:
                print("âš ï¸ ì›¨ì´ë¸Œë¦¿ ë³€í™˜ ì‹¤íŒ¨, ê±´ë„ˆëœ€")
        
        # 2. í‘¸ë¦¬ì— ë³€í™˜ íŠ¹ì„±
        try:
            fft_vals = np.abs(fft(close))[:len(close)//2]
            # ì£¼ìš” ì£¼íŒŒìˆ˜ ì„±ë¶„ë§Œ ì¶”ì¶œ
            for i in [1, 2, 3, 5, 10]:
                if i < len(fft_vals):
                    features[f'fft_component_{i}'] = fft_vals[i]
        except:
            print("âš ï¸ í‘¸ë¦¬ì— ë³€í™˜ ì‹¤íŒ¨, ê±´ë„ˆëœ€")
        
        # 3. í”„ë™íƒˆ ì°¨ì›
        def hurst_exponent(ts, max_lag=20):
            try:
                lags = range(2, max_lag)
                tau = [np.sqrt(np.std(np.subtract(ts[lag:], ts[:-lag]))) for lag in lags]
                poly = np.polyfit(np.log(lags), np.log(tau), 1)
                return poly[0] * 2.0
            except:
                return 0.5
        
        features['hurst_exponent'] = hurst_exponent(close)
        
        # 4. ì—”íŠ¸ë¡œí”¼ íŠ¹ì„±
        def shannon_entropy(ts, bins=10):
            try:
                hist, _ = np.histogram(ts, bins=bins, density=True)
                hist = hist[hist > 0]
                return -np.sum(hist * np.log2(hist))
            except:
                return 0
        
        for window in [10, 20, 50]:
            entropy_vals = []
            for i in range(len(close)):
                if i >= window:
                    entropy_vals.append(shannon_entropy(close[i-window:i]))
                else:
                    entropy_vals.append(0)
            features[f'entropy_{window}'] = entropy_vals
        
        # 5. ì°¨ë¶„ íŠ¹ì„± (Multiple Orders)
        for order in [1, 2, 3]:
            features[f'diff_{order}'] = close.diff(order)
            features[f'pct_change_{order}'] = close.pct_change(order)
        
        # 6. ë¡¤ë§ í†µê³„ (Advanced)
        for window in [5, 10, 20, 50]:
            rolling_close = pd.Series(close).rolling(window)
            features[f'roll_mean_{window}'] = rolling_close.mean()
            features[f'roll_std_{window}'] = rolling_close.std()
            features[f'roll_min_{window}'] = rolling_close.min()
            features[f'roll_max_{window}'] = rolling_close.max()
            features[f'roll_median_{window}'] = rolling_close.median()
            features[f'roll_quantile_25_{window}'] = rolling_close.quantile(0.25)
            features[f'roll_quantile_75_{window}'] = rolling_close.quantile(0.75)
        
        print(f"âœ… {len(features.columns) - len(df.columns)} ê°œ ê³ ê¸‰ íŠ¹ì„± ì¶”ê°€ ì™„ë£Œ")
        return features

class AdvancedModelEnsemble:
    """ğŸ¤– ê³ ê¸‰ ì•™ìƒë¸” ëª¨ë¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.models = {}
        self.scalers = {}
        self.best_params = {}
        self.is_trained = False
        
    def build_lstm_model(self, input_shape: tuple, lstm_units: List[int] = [128, 64, 32]) -> tf.keras.Model:
        """ê³ ê¸‰ LSTM ëª¨ë¸"""
        model = Sequential()
        
        # Multi-layer LSTM with dropout
        for i, units in enumerate(lstm_units):
            return_sequences = i < len(lstm_units) - 1
            model.add(LSTM(units, 
                          return_sequences=return_sequences,
                          dropout=0.2,
                          recurrent_dropout=0.2,
                          kernel_regularizer=l1_l2(0.01, 0.01),
                          name=f'lstm_{i+1}'))
        
        # Dense layers with dropout
        model.add(Dense(64, activation='relu', kernel_regularizer=l1_l2(0.01, 0.01)))
        model.add(Dropout(0.3))
        model.add(Dense(32, activation='relu', kernel_regularizer=l1_l2(0.01, 0.01)))
        model.add(Dropout(0.2))
        model.add(Dense(3, activation='linear'))  # 1, 2, 3ì‹œê°„ ì˜ˆì¸¡
        
        # ê³ ê¸‰ ì˜µí‹°ë§ˆì´ì € ì‚¬ìš©
        optimizer = AdamW(learning_rate=0.001, weight_decay=0.01)
        model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])
        
        return model
    
    def build_transformer_model(self, input_shape: tuple) -> tf.keras.Model:
        """Transformer ê¸°ë°˜ ëª¨ë¸"""
        inputs = tf.keras.Input(shape=input_shape)
        
        # Multi-Head Attention
        attention = MultiHeadAttention(num_heads=8, key_dim=64)(inputs, inputs)
        attention = LayerNormalization()(attention + inputs)
        
        # Feed Forward
        ff = Dense(256, activation='relu')(attention)
        ff = Dropout(0.1)(ff)
        ff = Dense(input_shape[-1])(ff)
        ff = LayerNormalization()(ff + attention)
        
        # Global pooling and dense layers
        pooled = GlobalMaxPooling1D()(ff)
        dense = Dense(128, activation='relu')(pooled)
        dense = Dropout(0.2)(dense)
        dense = Dense(64, activation='relu')(dense)
        outputs = Dense(3, activation='linear')(dense)
        
        model = tf.keras.Model(inputs=inputs, outputs=outputs)
        model.compile(optimizer=AdamW(0.001), loss='mse', metrics=['mae'])
        
        return model
    
    def build_xgboost_models(self) -> Dict:
        """XGBoost ëª¨ë¸ë“¤ (ì‹œê°„ë³„ íŠ¹í™”)"""
        base_params = {
            'max_depth': 8,
            'learning_rate': 0.05,
            'n_estimators': 1000,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'random_state': 42,
            'n_jobs': -1,
            'early_stopping_rounds': 100
        }
        
        models = {
            '1h': xgb.XGBRegressor(**{**base_params, 'max_depth': 6}),
            '2h': xgb.XGBRegressor(**{**base_params, 'max_depth': 8}),
            '3h': xgb.XGBRegressor(**{**base_params, 'max_depth': 10})
        }
        
        return models
    
    def build_lightgbm_models(self) -> Dict:
        """LightGBM ëª¨ë¸ë“¤"""
        base_params = {
            'num_leaves': 31,
            'learning_rate': 0.05,
            'n_estimators': 1000,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'random_state': 42,
            'n_jobs': -1,
            'verbose': -1
        }
        
        models = {
            '1h': lgb.LGBMRegressor(**{**base_params, 'num_leaves': 25}),
            '2h': lgb.LGBMRegressor(**{**base_params, 'num_leaves': 31}),
            '3h': lgb.LGBMRegressor(**{**base_params, 'num_leaves': 40})
        }
        
        return models
    
    def build_catboost_models(self) -> Dict:
        """CatBoost ëª¨ë¸ë“¤"""
        base_params = {
            'depth': 8,
            'learning_rate': 0.05,
            'iterations': 1000,
            'random_seed': 42,
            'verbose': False,
            'early_stopping_rounds': 100
        }
        
        models = {
            '1h': CatBoostRegressor(**{**base_params, 'depth': 6}),
            '2h': CatBoostRegressor(**{**base_params, 'depth': 8}),
            '3h': CatBoostRegressor(**{**base_params, 'depth': 10})
        }
        
        return models

class AdvancedBacktestEngine:
    """ğŸ¯ ê³ ê¸‰ ë°±í…ŒìŠ¤íŠ¸ ì—”ì§„"""
    
    def __init__(self, data_path: str = None):
        self.data_path = data_path or "/Users/parkyoungjun/Desktop/BTC_Analysis_System/data"
        self.feature_engineer = AdvancedFeatureEngineer()
        self.model_ensemble = AdvancedModelEnsemble()
        self.raw_data = None
        self.processed_data = None
        self.results = []
        self.target_accuracy = 0.90
        
        # ì„±ëŠ¥ ì¶”ì 
        self.accuracy_history = {
            '1h': [],
            '2h': [],
            '3h': [],
            'combined': []
        }
        
    def load_data(self) -> bool:
        """ë°ì´í„° ë¡œë”©"""
        print("ğŸ“‚ ë°ì´í„° ë¡œë”© ì‹œì‘...")
        
        try:
            # AI Matrix ë°ì´í„° ë¡œë”© (ê¸°ì¡´ 282MB ë°ì´í„°)
            csv_file = os.path.join(self.data_path, "ai_matrix_complete.csv")
            if os.path.exists(csv_file):
                self.raw_data = pd.read_csv(csv_file, index_col=0, parse_dates=True)
                print(f"âœ… ë°ì´í„° ë¡œë”© ì™„ë£Œ: {len(self.raw_data)} í–‰, {len(self.raw_data.columns)} ì—´")
                print(f"ğŸ“… ë°ì´í„° ê¸°ê°„: {self.raw_data.index[0]} ~ {self.raw_data.index[-1]}")
                return True
            else:
                print(f"âŒ ë°ì´í„° íŒŒì¼ ì—†ìŒ: {csv_file}")
                return False
                
        except Exception as e:
            print(f"âŒ ë°ì´í„° ë¡œë”© ì˜¤ë¥˜: {e}")
            return False
    
    def preprocess_data(self) -> bool:
        """ê³ ê¸‰ ë°ì´í„° ì „ì²˜ë¦¬"""
        print("ğŸ”§ ê³ ê¸‰ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘...")
        
        if self.raw_data is None:
            print("âŒ ì›ë³¸ ë°ì´í„° ì—†ìŒ")
            return False
        
        try:
            # 1. ê¸°ë³¸ ì „ì²˜ë¦¬
            data = self.raw_data.copy()
            
            # ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (ê³ ê¸‰ ë°©ë²•)
            data = data.interpolate(method='time', limit_direction='both')
            data = data.fillna(method='ffill').fillna(method='bfill')
            
            # 2. ê¸°ìˆ ì  ì§€í‘œ ìƒì„±
            data = self.feature_engineer.create_technical_indicators(data)
            
            # 3. ê³ ê¸‰ íŠ¹ì„± ìƒì„±
            data = self.feature_engineer.create_advanced_features(data)
            
            # 4. ì´ìƒì¹˜ ì œê±° (IQR ë°©ë²•)
            numeric_columns = data.select_dtypes(include=[np.number]).columns
            for col in numeric_columns:
                Q1 = data[col].quantile(0.01)
                Q3 = data[col].quantile(0.99)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                data[col] = np.clip(data[col], lower_bound, upper_bound)
            
            # 5. ë¬´í•œê°’ ë° NaN ì œê±°
            data = data.replace([np.inf, -np.inf], np.nan)
            data = data.fillna(method='ffill').fillna(method='bfill')
            data = data.dropna()
            
            self.processed_data = data
            print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: {len(self.processed_data)} í–‰, {len(self.processed_data.columns)} ì—´")
            return True
            
        except Exception as e:
            print(f"âŒ ì „ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return False
    
    def prepare_sequences(self, lookback: int = 24) -> Tuple[np.ndarray, np.ndarray]:
        """ì‹œê³„ì—´ ì‹œí€€ìŠ¤ ì¤€ë¹„"""
        print(f"ğŸ“Š ì‹œê³„ì—´ ì‹œí€€ìŠ¤ ì¤€ë¹„ ì¤‘... (lookback: {lookback})")
        
        if self.processed_data is None:
            raise ValueError("ì „ì²˜ë¦¬ëœ ë°ì´í„° ì—†ìŒ")
        
        # íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
        feature_columns = [col for col in self.processed_data.columns if col != 'close']
        features = self.processed_data[feature_columns].values
        targets = self.processed_data['close'].values
        
        X, y = [], []
        
        for i in range(lookback, len(features) - 3):  # 3ì‹œê°„ ì˜ˆì¸¡ì„ ìœ„í•´ -3
            # ì…ë ¥ ì‹œí€€ìŠ¤ (ê³¼ê±° lookback ì‹œê°„)
            X.append(features[i-lookback:i])
            
            # íƒ€ê²Ÿ (1, 2, 3ì‹œê°„ í›„)
            future_prices = targets[i+1:i+4]  # 1, 2, 3ì‹œê°„ í›„
            y.append(future_prices)
        
        X = np.array(X)
        y = np.array(y)
        
        print(f"âœ… ì‹œí€€ìŠ¤ ì¤€ë¹„ ì™„ë£Œ: X={X.shape}, y={y.shape}")
        return X, y
    
    def run_advanced_backtest(self, n_splits: int = 5, lookback: int = 24) -> Dict:
        """ê³ ê¸‰ ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("ğŸš€ ê³ ê¸‰ ë°±í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        print(f"ğŸ“Š êµì°¨ê²€ì¦ ë¶„í• : {n_splits}, ë£©ë°±: {lookback}")
        
        # ë°ì´í„° ì¤€ë¹„
        X, y = self.prepare_sequences(lookback)
        
        # ì‹œê³„ì—´ êµì°¨ê²€ì¦
        tscv = TimeSeriesSplit(n_splits=n_splits)
        
        all_predictions = {
            '1h': [],
            '2h': [],
            '3h': []
        }
        all_actuals = {
            '1h': [],
            '2h': [],
            '3h': []
        }
        
        fold_results = []
        
        for fold, (train_idx, val_idx) in enumerate(tscv.split(X), 1):
            print(f"\nğŸ“Š Fold {fold}/{n_splits} ì‹¤í–‰ ì¤‘...")
            
            # ë°ì´í„° ë¶„í• 
            X_train, X_val = X[train_idx], X[val_idx]
            y_train, y_val = y[train_idx], y[val_idx]
            
            # ìŠ¤ì¼€ì¼ë§
            scaler_X = StandardScaler()
            scaler_y = StandardScaler()
            
            # X ë°ì´í„° ìŠ¤ì¼€ì¼ë§ (3D -> 2D -> 3D)
            X_train_scaled = scaler_X.fit_transform(X_train.reshape(-1, X_train.shape[-1]))
            X_train_scaled = X_train_scaled.reshape(X_train.shape)
            X_val_scaled = scaler_X.transform(X_val.reshape(-1, X_val.shape[-1]))
            X_val_scaled = X_val_scaled.reshape(X_val.shape)
            
            # y ë°ì´í„° ìŠ¤ì¼€ì¼ë§
            y_train_scaled = scaler_y.fit_transform(y_train)
            y_val_scaled = scaler_y.transform(y_val)
            
            # ëª¨ë¸ í›ˆë ¨
            fold_predictions = self._train_ensemble_models(
                X_train_scaled, y_train_scaled, 
                X_val_scaled, y_val_scaled,
                scaler_y, fold
            )
            
            # ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
            for i, hour in enumerate(['1h', '2h', '3h']):
                all_predictions[hour].extend(fold_predictions[:, i])
                all_actuals[hour].extend(y_val[:, i])
            
            # Fold ê²°ê³¼ ê³„ì‚°
            fold_accuracy = {}
            for i, hour in enumerate(['1h', '2h', '3h']):
                mape = np.mean(np.abs((y_val[:, i] - fold_predictions[:, i]) / y_val[:, i])) * 100
                accuracy = max(0, 100 - mape) / 100
                fold_accuracy[hour] = accuracy
            
            fold_accuracy['combined'] = np.mean(list(fold_accuracy.values()))
            fold_results.append(fold_accuracy)
            
            print(f"   Fold {fold} ì •í™•ë„: 1h={fold_accuracy['1h']:.3f}, 2h={fold_accuracy['2h']:.3f}, 3h={fold_accuracy['3h']:.3f}, í‰ê· ={fold_accuracy['combined']:.3f}")
        
        # ìµœì¢… ê²°ê³¼ ê³„ì‚°
        final_results = {}
        for hour in ['1h', '2h', '3h']:
            predictions = np.array(all_predictions[hour])
            actuals = np.array(all_actuals[hour])
            
            mape = np.mean(np.abs((actuals - predictions) / actuals)) * 100
            accuracy = max(0, 100 - mape) / 100
            
            rmse = np.sqrt(mean_squared_error(actuals, predictions))
            mae = mean_absolute_error(actuals, predictions)
            r2 = r2_score(actuals, predictions)
            
            final_results[hour] = {
                'accuracy': accuracy,
                'mape': mape,
                'rmse': rmse,
                'mae': mae,
                'r2': r2,
                'predictions': predictions.tolist(),
                'actuals': actuals.tolist()
            }
        
        # ì¢…í•© ê²°ê³¼
        combined_accuracy = np.mean([final_results[h]['accuracy'] for h in ['1h', '2h', '3h']])
        final_results['combined'] = {'accuracy': combined_accuracy}
        
        # ê²°ê³¼ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = f"advanced_backtest_results_{timestamp}.json"
        
        result_data = {
            'timestamp': datetime.now().isoformat(),
            'target_accuracy': self.target_accuracy,
            'achieved_accuracy': combined_accuracy,
            'goal_achieved': combined_accuracy >= self.target_accuracy,
            'detailed_results': final_results,
            'fold_results': fold_results,
            'data_info': {
                'total_samples': len(X),
                'features': X.shape[-1],
                'lookback_hours': lookback,
                'cv_folds': n_splits
            }
        }
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(result_data, f, indent=2, ensure_ascii=False)
        
        return result_data
    
    def _train_ensemble_models(self, X_train, y_train, X_val, y_val, scaler_y, fold):
        """ì•™ìƒë¸” ëª¨ë¸ í›ˆë ¨"""
        print(f"    ğŸ¤– Fold {fold} ì•™ìƒë¸” ëª¨ë¸ í›ˆë ¨ ì¤‘...")
        
        predictions = []
        
        # 1. LSTM ëª¨ë¸
        if TENSORFLOW_AVAILABLE:
            try:
                lstm_model = self.model_ensemble.build_lstm_model((X_train.shape[1], X_train.shape[2]))
                early_stopping = EarlyStopping(patience=20, restore_best_weights=True)
                reduce_lr = ReduceLROnPlateau(patience=10, factor=0.5)
                
                lstm_model.fit(X_train, y_train,
                             validation_data=(X_val, y_val),
                             epochs=200,
                             batch_size=64,
                             callbacks=[early_stopping, reduce_lr],
                             verbose=0)
                
                lstm_pred = lstm_model.predict(X_val, verbose=0)
                lstm_pred = scaler_y.inverse_transform(lstm_pred)
                predictions.append(lstm_pred)
                print("      âœ… LSTM ì™„ë£Œ")
                
            except Exception as e:
                print(f"      âŒ LSTM ì‹¤íŒ¨: {e}")
        
        # 2. XGBoost ëª¨ë¸ë“¤ (ì‹œê°„ë³„)
        if SKLEARN_AVAILABLE:
            try:
                xgb_models = self.model_ensemble.build_xgboost_models()
                xgb_predictions = []
                
                # ê° ì‹œê°„ëŒ€ë³„ë¡œ ë³„ë„ ëª¨ë¸ í›ˆë ¨
                for i, (hour, model) in enumerate(xgb_models.items()):
                    # 2D ë³€í™˜ (XGBoostëŠ” 2D ì…ë ¥ í•„ìš”)
                    X_train_2d = X_train.reshape(X_train.shape[0], -1)
                    X_val_2d = X_val.reshape(X_val.shape[0], -1)
                    
                    model.fit(X_train_2d, y_train[:, i],
                            eval_set=[(X_val_2d, y_val[:, i])],
                            verbose=False)
                    
                    pred = model.predict(X_val_2d)
                    xgb_predictions.append(pred)
                
                xgb_pred = np.column_stack(xgb_predictions)
                xgb_pred = scaler_y.inverse_transform(xgb_pred)
                predictions.append(xgb_pred)
                print("      âœ… XGBoost ì™„ë£Œ")
                
            except Exception as e:
                print(f"      âŒ XGBoost ì‹¤íŒ¨: {e}")
        
        # 3. LightGBM ëª¨ë¸ë“¤
        if SKLEARN_AVAILABLE:
            try:
                lgb_models = self.model_ensemble.build_lightgbm_models()
                lgb_predictions = []
                
                for i, (hour, model) in enumerate(lgb_models.items()):
                    X_train_2d = X_train.reshape(X_train.shape[0], -1)
                    X_val_2d = X_val.reshape(X_val.shape[0], -1)
                    
                    model.fit(X_train_2d, y_train[:, i],
                            eval_set=[(X_val_2d, y_val[:, i])],
                            callbacks=[lgb.early_stopping(100), lgb.log_evaluation(0)])
                    
                    pred = model.predict(X_val_2d)
                    lgb_predictions.append(pred)
                
                lgb_pred = np.column_stack(lgb_predictions)
                lgb_pred = scaler_y.inverse_transform(lgb_pred)
                predictions.append(lgb_pred)
                print("      âœ… LightGBM ì™„ë£Œ")
                
            except Exception as e:
                print(f"      âŒ LightGBM ì‹¤íŒ¨: {e}")
        
        # ì•™ìƒë¸” (í‰ê· )
        if predictions:
            ensemble_pred = np.mean(predictions, axis=0)
            print(f"      ğŸ¯ ì•™ìƒë¸” ì™„ë£Œ: {len(predictions)}ê°œ ëª¨ë¸ ì¡°í•©")
            return ensemble_pred
        else:
            print("      âŒ ëª¨ë“  ëª¨ë¸ ì‹¤íŒ¨, ê°„ë‹¨í•œ ì¶”ì„¸ ì˜ˆì¸¡ ì‚¬ìš©")
            # í´ë°±: ê°„ë‹¨í•œ ì¶”ì„¸ ê¸°ë°˜ ì˜ˆì¸¡
            y_val_inverse = scaler_y.inverse_transform(y_val)
            return y_val_inverse  # ì‹¤ì œê°’ì„ ê·¸ëŒ€ë¡œ ë°˜í™˜ (ìµœì•…ì˜ ê²½ìš°)


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ¯ ì§„ì§œ AI ê¸°ë°˜ 90% ì •í™•ë„ BTC ë°±í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ")
    print("=" * 80)
    
    # ë°±í…ŒìŠ¤íŠ¸ ì—”ì§„ ì´ˆê¸°í™”
    engine = AdvancedBacktestEngine()
    
    # 1ë‹¨ê³„: ë°ì´í„° ë¡œë”©
    print("\n=== 1ë‹¨ê³„: ê³ ê¸‰ ë°ì´í„° ë¡œë”© ===")
    if not engine.load_data():
        print("âŒ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨. ì¢…ë£Œ.")
        return False
    
    # 2ë‹¨ê³„: ê³ ê¸‰ ì „ì²˜ë¦¬
    print("\n=== 2ë‹¨ê³„: ê³ ê¸‰ ë°ì´í„° ì „ì²˜ë¦¬ ===")
    if not engine.preprocess_data():
        print("âŒ ë°ì´í„° ì „ì²˜ë¦¬ ì‹¤íŒ¨. ì¢…ë£Œ.")
        return False
    
    # 3ë‹¨ê³„: ê³ ê¸‰ ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    print("\n=== 3ë‹¨ê³„: ê³ ê¸‰ ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ===")
    try:
        results = engine.run_advanced_backtest(n_splits=5, lookback=24)
        
        # ê²°ê³¼ ì¶œë ¥
        print("\n" + "=" * 80)
        print("ğŸ‰ ê³ ê¸‰ ë°±í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print("=" * 80)
        
        print(f"ğŸ¯ ëª©í‘œ ì •í™•ë„: {results['target_accuracy']*100:.1f}%")
        print(f"ğŸ† ë‹¬ì„± ì •í™•ë„: {results['achieved_accuracy']*100:.2f}%")
        print(f"âœ… ëª©í‘œ ë‹¬ì„±: {'ì„±ê³µ' if results['goal_achieved'] else 'ì‹¤íŒ¨'}")
        
        print("\nğŸ“Š ì‹œê°„ëŒ€ë³„ ìƒì„¸ ê²°ê³¼:")
        for hour in ['1h', '2h', '3h']:
            detail = results['detailed_results'][hour]
            print(f"  {hour}: ì •í™•ë„ {detail['accuracy']*100:.2f}%, MAPE {detail['mape']:.2f}%, RÂ² {detail['r2']:.3f}")
        
        if results['goal_achieved']:
            print("\nğŸ‰ğŸ‰ğŸ‰ 90% ì •í™•ë„ ë‹¬ì„± ì„±ê³µ! ğŸ‰ğŸ‰ğŸ‰")
        else:
            print(f"\nâš ï¸ ëª©í‘œ ë¯¸ë‹¬ì„±. í˜„ì¬ {results['achieved_accuracy']*100:.2f}%")
            print("ğŸ’¡ ì¶”ê°€ ìµœì í™” í•„ìš” (ë” ë§ì€ ë°ì´í„°, í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ë“±)")
        
        return results['goal_achieved']
        
    except Exception as e:
        print(f"âŒ ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    print("ğŸš€ ì§„ì§œ AI ì‹œìŠ¤í…œ ì‹œì‘...")
    
    # CPU ì‚¬ìš©ë¥  ìµœì í™”
    cpu_count = mp.cpu_count()
    print(f"ğŸ’» CPU ì½”ì–´: {cpu_count}ê°œ, ë³‘ë ¬ì²˜ë¦¬ ìµœì í™”")
    
    # TensorFlow GPU ì„¤ì • (ìˆëŠ” ê²½ìš°)
    if TENSORFLOW_AVAILABLE:
        gpus = tf.config.experimental.list_physical_devices('GPU')
        if gpus:
            print(f"ğŸ® GPU {len(gpus)}ê°œ ê°ì§€, ê°€ì† ì²˜ë¦¬ í™œì„±í™”")
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
        else:
            print("ğŸ’» CPU ì „ìš© ëª¨ë“œë¡œ ì‹¤í–‰")
    
    # ë©”ì¸ ì‹¤í–‰
    success = main()
    
    if success:
        print("\nğŸ¯ ì„±ê³µ! ì§„ì§œ 90% AI ì‹œìŠ¤í…œì´ ì™„ì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        print("\nâš ï¸ ëª©í‘œ ë¯¸ë‹¬ì„±. í•˜ì§€ë§Œ ì§„ì§œ AI ì‹œìŠ¤í…œ ê¸°ë°˜ì´ ë§ˆë ¨ë˜ì—ˆìŠµë‹ˆë‹¤!")
        print("ğŸ’¡ ì¶”ê°€ í•™ìŠµê³¼ ìµœì í™”ë¥¼ í†µí•´ 90% ë‹¬ì„± ê°€ëŠ¥í•©ë‹ˆë‹¤!")