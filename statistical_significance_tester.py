#!/usr/bin/env python3
"""
ğŸ¯ í†µê³„ì  ìœ ì˜ì„± ê²€ì • ì‹œìŠ¤í…œ
- ë¶€íŠ¸ìŠ¤íŠ¸ë© ì‹ ë¢°êµ¬ê°„ (Bootstrap Confidence Intervals)
- ê°€ì„¤ê²€ì • (t-test, Wilcoxon, Kolmogorov-Smirnov)
- ë‹¤ì¤‘ë¹„êµ ë³´ì • (Bonferroni, Holm, FDR)
- ë² ì´ì§€ì•ˆ í†µê³„ (ë² ì´ì§€ì•ˆ A/B í…ŒìŠ¤íŠ¸)
- ì‹œê³„ì—´ íŠ¹í™” ê²€ì • (ìê¸°ìƒê´€, ì •ìƒì„±)
- íš¨ê³¼ í¬ê¸° ë¶„ì„ (Cohen's d, Hedges' g)
- ê²€ì •ë ¥ ë¶„ì„ (Power Analysis)
- ê°•ê±´ì„± ê²€ì • (Robustness Testing)
"""

import numpy as np
import pandas as pd
import warnings
from datetime import datetime
import json
import os
from typing import Dict, List, Tuple, Optional, Union, Callable
from dataclasses import dataclass, field
import logging
from enum import Enum

# í†µê³„ ë¼ì´ë¸ŒëŸ¬ë¦¬
from scipy import stats
from scipy.stats import normaltest, shapiro, kstest, jarque_bera
from scipy.stats import ttest_ind, ttest_rel, wilcoxon, ranksums
from scipy.stats import pearsonr, spearmanr, kendalltau
from scipy.stats import chi2_contingency, fisher_exact
import statsmodels.api as sm
from statsmodels.stats.power import TTestPower, ttest_power
from statsmodels.stats.multitest import multipletests
from statsmodels.tsa.stattools import adfuller, kpss
from statsmodels.stats.diagnostic import acorr_ljungbox

# ë² ì´ì§€ì•ˆ í†µê³„ (ì˜µì…”ë„)
try:
    import pymc3 as pm
    import arviz as az
    BAYESIAN_AVAILABLE = True
except ImportError:
    try:
        import pymc as pm
        import arviz as az
        BAYESIAN_AVAILABLE = True
    except ImportError:
        BAYESIAN_AVAILABLE = False
        print("âš ï¸ PyMC ë¯¸ì„¤ì¹˜: ë² ì´ì§€ì•ˆ ë¶„ì„ ë¶ˆê°€")

# ì‹œê°í™”
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.express as px

warnings.filterwarnings('ignore')

class TestType(Enum):
    """ê²€ì • ìœ í˜•"""
    PARAMETRIC = "parametric"
    NONPARAMETRIC = "nonparametric"
    BAYESIAN = "bayesian"
    BOOTSTRAP = "bootstrap"

@dataclass
class SignificanceConfig:
    """í†µê³„ì  ìœ ì˜ì„± ê²€ì • ì„¤ì •"""
    # ê¸°ë³¸ ì„¤ì •
    alpha: float = 0.05              # ìœ ì˜ìˆ˜ì¤€
    confidence_level: float = 0.95   # ì‹ ë¢°ìˆ˜ì¤€
    
    # ë¶€íŠ¸ìŠ¤íŠ¸ë© ì„¤ì •
    bootstrap_samples: int = 10000   # ë¶€íŠ¸ìŠ¤íŠ¸ë© ìƒ˜í”Œ ìˆ˜
    bootstrap_method: str = 'percentile'  # 'percentile', 'bias_corrected', 'accelerated'
    
    # ë‹¤ì¤‘ë¹„êµ ë³´ì •
    multiple_comparison_method: str = 'holm'  # 'bonferroni', 'holm', 'fdr_bh'
    
    # íš¨ê³¼ í¬ê¸° ì„ê³„ê°’
    small_effect_size: float = 0.2
    medium_effect_size: float = 0.5
    large_effect_size: float = 0.8
    
    # ê²€ì •ë ¥ ë¶„ì„
    desired_power: float = 0.8       # ì›í•˜ëŠ” ê²€ì •ë ¥
    effect_sizes: List[float] = field(default_factory=lambda: [0.1, 0.2, 0.5, 0.8])
    
    # ì‹œê³„ì—´ ê²€ì •
    max_lags: int = 10               # ìµœëŒ€ ì§€ì—° ì°¨ìˆ˜
    trend_methods: List[str] = field(default_factory=lambda: ['c', 'ct', 'ctt'])  # ADF ê²€ì • ë°©ë²•
    
    # ë² ì´ì§€ì•ˆ ì„¤ì •
    mcmc_samples: int = 5000         # MCMC ìƒ˜í”Œ ìˆ˜
    bayesian_chains: int = 4         # ë² ì´ì§€ì•ˆ ì²´ì¸ ìˆ˜
    
    # ê°•ê±´ì„± ê²€ì •
    contamination_levels: List[float] = field(default_factory=lambda: [0.05, 0.1, 0.15])
    outlier_methods: List[str] = field(default_factory=lambda: ['iqr', 'zscore', 'isolation'])

class StatisticalSignificanceTester:
    """í†µê³„ì  ìœ ì˜ì„± ê²€ì • ì‹œìŠ¤í…œ"""
    
    def __init__(self, config: SignificanceConfig = None):
        self.config = config or SignificanceConfig()
        self.data_path = "/Users/parkyoungjun/Desktop/BTC_Analysis_System"
        
        # ê²€ì • ê²°ê³¼ ì €ì¥
        self.test_results = {}
        self.effect_sizes = {}
        self.power_analysis_results = {}
        
        # ë¡œê¹… ì„¤ì •
        self.setup_logging()
        
    def setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(os.path.join(self.data_path, 'statistical_tests.log'), encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def comprehensive_significance_testing(self, strategy_returns: pd.Series, 
                                         benchmark_returns: pd.Series = None,
                                         additional_strategies: Dict[str, pd.Series] = None) -> Dict:
        """ì¢…í•©ì ì¸ í†µê³„ì  ìœ ì˜ì„± ê²€ì •"""
        self.logger.info("ğŸ§ª ì¢…í•©ì ì¸ í†µê³„ì  ìœ ì˜ì„± ê²€ì • ì‹œì‘...")
        
        try:
            # 1. ê¸°ë³¸ ê°€ì • ê²€ì •
            assumption_tests = self.test_statistical_assumptions(strategy_returns, benchmark_returns)
            
            # 2. ì „ëµ vs ë²¤ì¹˜ë§ˆí¬ ë¹„êµ ê²€ì •
            comparison_tests = {}
            if benchmark_returns is not None:
                comparison_tests = self.compare_strategies(strategy_returns, benchmark_returns)
            
            # 3. ë‹¤ì¤‘ ì „ëµ ë¹„êµ (ìˆëŠ” ê²½ìš°)
            multiple_comparison_tests = {}
            if additional_strategies:
                multiple_comparison_tests = self.multiple_strategy_comparison(
                    {'main_strategy': strategy_returns, **additional_strategies}
                )
            
            # 4. ë¶€íŠ¸ìŠ¤íŠ¸ë© ì‹ ë¢°êµ¬ê°„
            bootstrap_results = self.bootstrap_confidence_intervals(strategy_returns)
            
            # 5. ë² ì´ì§€ì•ˆ ë¶„ì„ (ê°€ëŠ¥í•œ ê²½ìš°)
            bayesian_results = {}
            if BAYESIAN_AVAILABLE and benchmark_returns is not None:
                bayesian_results = self.bayesian_comparison(strategy_returns, benchmark_returns)
            
            # 6. ì‹œê³„ì—´ íŠ¹í™” ê²€ì •
            time_series_tests = self.time_series_specific_tests(strategy_returns)
            
            # 7. íš¨ê³¼ í¬ê¸° ë¶„ì„
            effect_size_analysis = self.analyze_effect_sizes(strategy_returns, benchmark_returns)
            
            # 8. ê²€ì •ë ¥ ë¶„ì„
            power_analysis = self.conduct_power_analysis(strategy_returns, benchmark_returns)
            
            # 9. ê°•ê±´ì„± ê²€ì •
            robustness_tests = self.conduct_robustness_tests(strategy_returns, benchmark_returns)
            
            # 10. ì¢…í•© ê²°ë¡ 
            final_conclusions = self.generate_statistical_conclusions(
                assumption_tests, comparison_tests, multiple_comparison_tests,
                bootstrap_results, bayesian_results, time_series_tests,
                effect_size_analysis, power_analysis, robustness_tests
            )
            
            comprehensive_results = {
                'test_timestamp': datetime.now().isoformat(),
                'configuration': self.config.__dict__,
                'data_summary': {
                    'strategy_observations': len(strategy_returns),
                    'benchmark_observations': len(benchmark_returns) if benchmark_returns is not None else 0,
                    'additional_strategies': len(additional_strategies) if additional_strategies else 0,
                    'time_period': {
                        'start': strategy_returns.index[0] if hasattr(strategy_returns, 'index') else 'N/A',
                        'end': strategy_returns.index[-1] if hasattr(strategy_returns, 'index') else 'N/A'
                    }
                },
                'assumption_tests': assumption_tests,
                'strategy_comparison': comparison_tests,
                'multiple_comparisons': multiple_comparison_tests,
                'bootstrap_analysis': bootstrap_results,
                'bayesian_analysis': bayesian_results,
                'time_series_tests': time_series_tests,
                'effect_size_analysis': effect_size_analysis,
                'power_analysis': power_analysis,
                'robustness_tests': robustness_tests,
                'statistical_conclusions': final_conclusions
            }
            
            # ê²°ê³¼ ì €ì¥
            self.save_test_results(comprehensive_results)
            
            # ì‹œê°í™”
            self.create_significance_testing_dashboard(comprehensive_results)
            
            return comprehensive_results
            
        except Exception as e:
            self.logger.error(f"í†µê³„ì  ìœ ì˜ì„± ê²€ì • ì‹¤íŒ¨: {e}")
            raise
    
    def test_statistical_assumptions(self, data1: pd.Series, data2: pd.Series = None) -> Dict:
        """í†µê³„ì  ê°€ì • ê²€ì •"""
        self.logger.info("ğŸ“Š í†µê³„ì  ê°€ì • ê²€ì • ì¤‘...")
        
        assumptions = {}
        
        # 1. ì •ê·œì„± ê²€ì •
        assumptions['normality_tests'] = self._test_normality(data1, data2)
        
        # 2. ë“±ë¶„ì‚°ì„± ê²€ì •
        if data2 is not None:
            assumptions['equal_variance_tests'] = self._test_equal_variance(data1, data2)
        
        # 3. ë…ë¦½ì„± ê²€ì •
        assumptions['independence_tests'] = self._test_independence(data1, data2)
        
        # 4. ì´ìƒì¹˜ ê²€ì •
        assumptions['outlier_tests'] = self._detect_outliers(data1, data2)
        
        # 5. ì•ˆì •ì„± ê²€ì • (ì‹œê³„ì—´)
        assumptions['stationarity_tests'] = self._test_stationarity(data1, data2)
        
        return assumptions
    
    def _test_normality(self, data1: pd.Series, data2: pd.Series = None) -> Dict:
        """ì •ê·œì„± ê²€ì •"""
        results = {}
        
        # ë°ì´í„°1 ì •ê·œì„± ê²€ì •
        data1_clean = data1.dropna()
        if len(data1_clean) > 3:
            # Shapiro-Wilk ê²€ì • (ìƒ˜í”Œ í¬ê¸° < 5000)
            if len(data1_clean) < 5000:
                shapiro_stat, shapiro_p = shapiro(data1_clean)
                results['data1_shapiro'] = {
                    'statistic': float(shapiro_stat),
                    'p_value': float(shapiro_p),
                    'is_normal': shapiro_p > self.config.alpha
                }
            
            # Jarque-Bera ê²€ì •
            jb_stat, jb_p = jarque_bera(data1_clean)
            results['data1_jarque_bera'] = {
                'statistic': float(jb_stat),
                'p_value': float(jb_p),
                'is_normal': jb_p > self.config.alpha
            }
            
            # D'Agostino and Pearson's ê²€ì •
            try:
                da_stat, da_p = normaltest(data1_clean)
                results['data1_dagostino'] = {
                    'statistic': float(da_stat),
                    'p_value': float(da_p),
                    'is_normal': da_p > self.config.alpha
                }
            except:
                pass
        
        # ë°ì´í„°2 ì •ê·œì„± ê²€ì •
        if data2 is not None:
            data2_clean = data2.dropna()
            if len(data2_clean) > 3:
                if len(data2_clean) < 5000:
                    shapiro_stat, shapiro_p = shapiro(data2_clean)
                    results['data2_shapiro'] = {
                        'statistic': float(shapiro_stat),
                        'p_value': float(shapiro_p),
                        'is_normal': shapiro_p > self.config.alpha
                    }
                
                jb_stat, jb_p = jarque_bera(data2_clean)
                results['data2_jarque_bera'] = {
                    'statistic': float(jb_stat),
                    'p_value': float(jb_p),
                    'is_normal': jb_p > self.config.alpha
                }
        
        return results
    
    def _test_equal_variance(self, data1: pd.Series, data2: pd.Series) -> Dict:
        """ë“±ë¶„ì‚°ì„± ê²€ì •"""
        results = {}
        
        data1_clean = data1.dropna()
        data2_clean = data2.dropna()
        
        if len(data1_clean) > 1 and len(data2_clean) > 1:
            # Levene ê²€ì •
            try:
                levene_stat, levene_p = stats.levene(data1_clean, data2_clean)
                results['levene'] = {
                    'statistic': float(levene_stat),
                    'p_value': float(levene_p),
                    'equal_variance': levene_p > self.config.alpha
                }
            except:
                pass
            
            # Bartlett ê²€ì •
            try:
                bartlett_stat, bartlett_p = stats.bartlett(data1_clean, data2_clean)
                results['bartlett'] = {
                    'statistic': float(bartlett_stat),
                    'p_value': float(bartlett_p),
                    'equal_variance': bartlett_p > self.config.alpha
                }
            except:
                pass
            
            # F-ê²€ì • (ë¶„ì‚°ë¹„ ê²€ì •)
            try:
                f_stat = np.var(data1_clean, ddof=1) / np.var(data2_clean, ddof=1)
                f_p = 2 * min(stats.f.cdf(f_stat, len(data1_clean)-1, len(data2_clean)-1),
                             1 - stats.f.cdf(f_stat, len(data1_clean)-1, len(data2_clean)-1))
                results['f_test'] = {
                    'statistic': float(f_stat),
                    'p_value': float(f_p),
                    'equal_variance': f_p > self.config.alpha
                }
            except:
                pass
        
        return results
    
    def _test_independence(self, data1: pd.Series, data2: pd.Series = None) -> Dict:
        """ë…ë¦½ì„± ê²€ì •"""
        results = {}
        
        # ìê¸°ìƒê´€ ê²€ì • (Ljung-Box)
        data1_clean = data1.dropna()
        if len(data1_clean) > 10:
            try:
                ljung_box = acorr_ljungbox(data1_clean, lags=min(self.config.max_lags, len(data1_clean)//4))
                results['data1_ljung_box'] = {
                    'statistics': ljung_box['lb_stat'].to_dict(),
                    'p_values': ljung_box['lb_pvalue'].to_dict(),
                    'is_independent': all(ljung_box['lb_pvalue'] > self.config.alpha)
                }
            except:
                pass
        
        # ë‘ ì‹œë¦¬ì¦ˆ ê°„ ë…ë¦½ì„± (ìƒê´€ê´€ê³„)
        if data2 is not None:
            data2_clean = data2.dropna()
            common_index = data1_clean.index.intersection(data2_clean.index)
            
            if len(common_index) > 3:
                aligned_data1 = data1_clean[common_index]
                aligned_data2 = data2_clean[common_index]
                
                # Pearson ìƒê´€ê³„ìˆ˜
                try:
                    pearson_corr, pearson_p = pearsonr(aligned_data1, aligned_data2)
                    results['pearson_correlation'] = {
                        'correlation': float(pearson_corr),
                        'p_value': float(pearson_p),
                        'is_independent': pearson_p > self.config.alpha
                    }
                except:
                    pass
                
                # Spearman ìˆœìœ„ ìƒê´€ê³„ìˆ˜
                try:
                    spearman_corr, spearman_p = spearmanr(aligned_data1, aligned_data2)
                    results['spearman_correlation'] = {
                        'correlation': float(spearman_corr),
                        'p_value': float(spearman_p),
                        'is_independent': spearman_p > self.config.alpha
                    }
                except:
                    pass
        
        return results
    
    def _detect_outliers(self, data1: pd.Series, data2: pd.Series = None) -> Dict:
        """ì´ìƒì¹˜ íƒì§€"""
        results = {}
        
        for name, data in [('data1', data1), ('data2', data2)]:
            if data is None:
                continue
                
            data_clean = data.dropna()
            if len(data_clean) == 0:
                continue
            
            outlier_results = {}
            
            # IQR ë°©ë²•
            Q1 = data_clean.quantile(0.25)
            Q3 = data_clean.quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            iqr_outliers = data_clean[(data_clean < lower_bound) | (data_clean > upper_bound)]
            outlier_results['iqr_method'] = {
                'outlier_count': len(iqr_outliers),
                'outlier_percentage': len(iqr_outliers) / len(data_clean) * 100,
                'lower_bound': float(lower_bound),
                'upper_bound': float(upper_bound)
            }
            
            # Z-score ë°©ë²•
            z_scores = np.abs(stats.zscore(data_clean))
            z_outliers = data_clean[z_scores > 3]
            outlier_results['zscore_method'] = {
                'outlier_count': len(z_outliers),
                'outlier_percentage': len(z_outliers) / len(data_clean) * 100,
                'threshold': 3.0
            }
            
            # Modified Z-score ë°©ë²•
            median = data_clean.median()
            mad = np.median(np.abs(data_clean - median))
            modified_z_scores = 0.6745 * (data_clean - median) / mad
            modified_z_outliers = data_clean[np.abs(modified_z_scores) > 3.5]
            outlier_results['modified_zscore_method'] = {
                'outlier_count': len(modified_z_outliers),
                'outlier_percentage': len(modified_z_outliers) / len(data_clean) * 100,
                'threshold': 3.5
            }
            
            results[name] = outlier_results
        
        return results
    
    def _test_stationarity(self, data1: pd.Series, data2: pd.Series = None) -> Dict:
        """ì•ˆì •ì„± ê²€ì •"""
        results = {}
        
        for name, data in [('data1', data1), ('data2', data2)]:
            if data is None:
                continue
                
            data_clean = data.dropna()
            if len(data_clean) < 10:
                continue
            
            stationarity_results = {}
            
            # Augmented Dickey-Fuller ê²€ì •
            for trend in self.config.trend_methods:
                try:
                    adf_result = adfuller(data_clean, regression=trend)
                    stationarity_results[f'adf_{trend}'] = {
                        'statistic': float(adf_result[0]),
                        'p_value': float(adf_result[1]),
                        'critical_values': {k: float(v) for k, v in adf_result[4].items()},
                        'is_stationary': adf_result[1] < self.config.alpha
                    }
                except:
                    continue
            
            # KPSS ê²€ì •
            try:
                kpss_result = kpss(data_clean)
                stationarity_results['kpss'] = {
                    'statistic': float(kpss_result[0]),
                    'p_value': float(kpss_result[1]),
                    'critical_values': {k: float(v) for k, v in kpss_result[3].items()},
                    'is_stationary': kpss_result[1] > self.config.alpha
                }
            except:
                pass
            
            results[name] = stationarity_results
        
        return results
    
    def compare_strategies(self, strategy_returns: pd.Series, 
                          benchmark_returns: pd.Series) -> Dict:
        """ì „ëµ vs ë²¤ì¹˜ë§ˆí¬ ë¹„êµ ê²€ì •"""
        self.logger.info("âš–ï¸ ì „ëµ vs ë²¤ì¹˜ë§ˆí¬ ë¹„êµ ê²€ì • ì¤‘...")
        
        # ë°ì´í„° ì •ë ¬
        common_index = strategy_returns.index.intersection(benchmark_returns.index)
        if len(common_index) == 0:
            return {'error': 'ê³µí†µ ì‹œì  ì—†ìŒ'}
        
        strategy_aligned = strategy_returns[common_index]
        benchmark_aligned = benchmark_returns[common_index]
        
        comparison_results = {}
        
        # 1. ë…ë¦½ t-ê²€ì •
        try:
            t_stat, t_p = ttest_ind(strategy_aligned, benchmark_aligned)
            comparison_results['independent_ttest'] = {
                'statistic': float(t_stat),
                'p_value': float(t_p),
                'significant': t_p < self.config.alpha,
                'strategy_better': t_stat > 0
            }
        except:
            pass
        
        # 2. ëŒ€ì‘ t-ê²€ì • (ìŒì²´ ë¹„êµ)
        try:
            paired_t_stat, paired_t_p = ttest_rel(strategy_aligned, benchmark_aligned)
            comparison_results['paired_ttest'] = {
                'statistic': float(paired_t_stat),
                'p_value': float(paired_t_p),
                'significant': paired_t_p < self.config.alpha,
                'strategy_better': paired_t_stat > 0
            }
        except:
            pass
        
        # 3. Wilcoxon ìˆœìœ„í•© ê²€ì • (ë¹„ëª¨ìˆ˜)
        try:
            wilcoxon_stat, wilcoxon_p = ranksums(strategy_aligned, benchmark_aligned)
            comparison_results['wilcoxon_ranksum'] = {
                'statistic': float(wilcoxon_stat),
                'p_value': float(wilcoxon_p),
                'significant': wilcoxon_p < self.config.alpha,
                'strategy_better': wilcoxon_stat > 0
            }
        except:
            pass
        
        # 4. Wilcoxon ë¶€í˜¸ìˆœìœ„ ê²€ì • (ìŒì²´ ë¹„êµ)
        try:
            differences = strategy_aligned - benchmark_aligned
            wilcoxon_signed_stat, wilcoxon_signed_p = wilcoxon(differences)
            comparison_results['wilcoxon_signed_rank'] = {
                'statistic': float(wilcoxon_signed_stat),
                'p_value': float(wilcoxon_signed_p),
                'significant': wilcoxon_signed_p < self.config.alpha,
                'strategy_better': np.median(differences) > 0
            }
        except:
            pass
        
        # 5. Kolmogorov-Smirnov ê²€ì •
        try:
            ks_stat, ks_p = stats.ks_2samp(strategy_aligned, benchmark_aligned)
            comparison_results['kolmogorov_smirnov'] = {
                'statistic': float(ks_stat),
                'p_value': float(ks_p),
                'significant': ks_p < self.config.alpha,
                'distributions_different': ks_p < self.config.alpha
            }
        except:
            pass
        
        return comparison_results
    
    def multiple_strategy_comparison(self, strategies: Dict[str, pd.Series]) -> Dict:
        """ë‹¤ì¤‘ ì „ëµ ë¹„êµ"""
        self.logger.info("ğŸ” ë‹¤ì¤‘ ì „ëµ ë¹„êµ ê²€ì • ì¤‘...")
        
        strategy_names = list(strategies.keys())
        n_strategies = len(strategy_names)
        
        if n_strategies < 2:
            return {'error': 'ìµœì†Œ 2ê°œ ì „ëµ í•„ìš”'}
        
        # ë°ì´í„° ì •ë ¬
        common_index = strategies[strategy_names[0]].index
        for name in strategy_names[1:]:
            common_index = common_index.intersection(strategies[name].index)
        
        if len(common_index) == 0:
            return {'error': 'ê³µí†µ ì‹œì  ì—†ìŒ'}
        
        aligned_strategies = {name: strategies[name][common_index] for name in strategy_names}
        
        multiple_results = {}
        
        # 1. ì¼ì›ë¶„ì‚°ë¶„ì„ (ANOVA)
        try:
            strategy_arrays = [aligned_strategies[name].values for name in strategy_names]
            f_stat, f_p = stats.f_oneway(*strategy_arrays)
            multiple_results['anova'] = {
                'f_statistic': float(f_stat),
                'p_value': float(f_p),
                'significant': f_p < self.config.alpha,
                'strategies_different': f_p < self.config.alpha
            }
        except:
            pass
        
        # 2. Kruskal-Wallis ê²€ì • (ë¹„ëª¨ìˆ˜)
        try:
            kw_stat, kw_p = stats.kruskal(*strategy_arrays)
            multiple_results['kruskal_wallis'] = {
                'statistic': float(kw_stat),
                'p_value': float(kw_p),
                'significant': kw_p < self.config.alpha,
                'strategies_different': kw_p < self.config.alpha
            }
        except:
            pass
        
        # 3. ìŒë³„ ë¹„êµ (ëª¨ë“  ì „ëµ ìŒ)
        pairwise_results = {}
        p_values = []
        
        for i in range(n_strategies):
            for j in range(i+1, n_strategies):
                name1, name2 = strategy_names[i], strategy_names[j]
                
                # t-ê²€ì •
                try:
                    t_stat, t_p = ttest_ind(aligned_strategies[name1], aligned_strategies[name2])
                    pairwise_results[f'{name1}_vs_{name2}'] = {
                        'ttest_statistic': float(t_stat),
                        'ttest_p_value': float(t_p),
                        'significant': t_p < self.config.alpha
                    }
                    p_values.append(t_p)
                except:
                    pass
        
        # 4. ë‹¤ì¤‘ë¹„êµ ë³´ì •
        if p_values:
            rejected, p_adjusted, alpha_sidak, alpha_bonf = multipletests(
                p_values, alpha=self.config.alpha, method=self.config.multiple_comparison_method
            )
            
            multiple_results['multiple_comparison_correction'] = {
                'method': self.config.multiple_comparison_method,
                'original_alpha': self.config.alpha,
                'adjusted_alpha_bonferroni': float(alpha_bonf),
                'adjusted_alpha_sidak': float(alpha_sidak),
                'rejected_hypotheses': rejected.tolist(),
                'adjusted_p_values': p_adjusted.tolist(),
                'significant_comparisons': int(np.sum(rejected))
            }
        
        multiple_results['pairwise_comparisons'] = pairwise_results
        
        return multiple_results
    
    def bootstrap_confidence_intervals(self, data: pd.Series, 
                                     statistic_functions: Dict[str, Callable] = None) -> Dict:
        """ë¶€íŠ¸ìŠ¤íŠ¸ë© ì‹ ë¢°êµ¬ê°„"""
        self.logger.info("ğŸ”„ ë¶€íŠ¸ìŠ¤íŠ¸ë© ì‹ ë¢°êµ¬ê°„ ê³„ì‚° ì¤‘...")
        
        if statistic_functions is None:
            statistic_functions = {
                'mean': np.mean,
                'median': np.median,
                'std': np.std,
                'sharpe_ratio': lambda x: np.mean(x) / np.std(x) * np.sqrt(252) if np.std(x) > 0 else 0,
                'skewness': lambda x: stats.skew(x),
                'kurtosis': lambda x: stats.kurtosis(x)
            }
        
        data_clean = data.dropna()
        n_samples = len(data_clean)
        
        if n_samples < 10:
            return {'error': 'ë¶€íŠ¸ìŠ¤íŠ¸ë©ì— ì¶©ë¶„í•˜ì§€ ì•Šì€ ë°ì´í„°'}
        
        bootstrap_results = {}
        
        for stat_name, stat_func in statistic_functions.items():
            try:
                # ì›ë³¸ í†µê³„ëŸ‰
                original_stat = stat_func(data_clean)
                
                # ë¶€íŠ¸ìŠ¤íŠ¸ë© ìƒ˜í”Œë§
                bootstrap_stats = []
                np.random.seed(42)  # ì¬í˜„ ê°€ëŠ¥ì„±
                
                for _ in range(self.config.bootstrap_samples):
                    bootstrap_sample = np.random.choice(data_clean, size=n_samples, replace=True)
                    bootstrap_stat = stat_func(bootstrap_sample)
                    bootstrap_stats.append(bootstrap_stat)
                
                bootstrap_stats = np.array(bootstrap_stats)
                bootstrap_stats = bootstrap_stats[~np.isnan(bootstrap_stats)]  # NaN ì œê±°
                
                if len(bootstrap_stats) == 0:
                    continue
                
                # ì‹ ë¢°êµ¬ê°„ ê³„ì‚°
                alpha = 1 - self.config.confidence_level
                lower_percentile = (alpha / 2) * 100
                upper_percentile = (1 - alpha / 2) * 100
                
                ci_lower = np.percentile(bootstrap_stats, lower_percentile)
                ci_upper = np.percentile(bootstrap_stats, upper_percentile)
                
                # í¸í–¥ ë³´ì •ëœ ì‹ ë¢°êµ¬ê°„ (BCa)
                bias_correction = self._calculate_bias_correction(original_stat, bootstrap_stats)
                acceleration = self._calculate_acceleration(data_clean, stat_func, original_stat)
                
                bca_lower, bca_upper = self._calculate_bca_interval(
                    bootstrap_stats, bias_correction, acceleration, alpha
                )
                
                bootstrap_results[stat_name] = {
                    'original_statistic': float(original_stat),
                    'bootstrap_mean': float(np.mean(bootstrap_stats)),
                    'bootstrap_std': float(np.std(bootstrap_stats)),
                    'confidence_interval_percentile': {
                        'lower': float(ci_lower),
                        'upper': float(ci_upper)
                    },
                    'confidence_interval_bca': {
                        'lower': float(bca_lower),
                        'upper': float(bca_upper)
                    },
                    'bias': float(np.mean(bootstrap_stats) - original_stat),
                    'bootstrap_distribution': bootstrap_stats.tolist()[:1000]  # ì²˜ìŒ 1000ê°œë§Œ ì €ì¥
                }
            except Exception as e:
                self.logger.warning(f"ë¶€íŠ¸ìŠ¤íŠ¸ë© ì‹¤íŒ¨ ({stat_name}): {e}")
                continue
        
        return bootstrap_results
    
    def _calculate_bias_correction(self, original_stat: float, bootstrap_stats: np.ndarray) -> float:
        """í¸í–¥ ë³´ì • ê³„ì‚°"""
        proportion = np.mean(bootstrap_stats < original_stat)
        if proportion == 0:
            proportion = 1e-7
        elif proportion == 1:
            proportion = 1 - 1e-7
        
        bias_correction = stats.norm.ppf(proportion)
        return bias_correction
    
    def _calculate_acceleration(self, data: np.ndarray, stat_func: Callable, original_stat: float) -> float:
        """ê°€ì†ë„ ê³„ì‚° (Jackknife)"""
        n = len(data)
        jackknife_stats = []
        
        for i in range(n):
            jackknife_sample = np.concatenate([data[:i], data[i+1:]])
            jackknife_stat = stat_func(jackknife_sample)
            jackknife_stats.append(jackknife_stat)
        
        jackknife_stats = np.array(jackknife_stats)
        jackknife_mean = np.mean(jackknife_stats)
        
        numerator = np.sum((jackknife_mean - jackknife_stats) ** 3)
        denominator = 6 * (np.sum((jackknife_mean - jackknife_stats) ** 2)) ** 1.5
        
        if denominator == 0:
            return 0
        
        acceleration = numerator / denominator
        return acceleration
    
    def _calculate_bca_interval(self, bootstrap_stats: np.ndarray, bias_correction: float, 
                              acceleration: float, alpha: float) -> Tuple[float, float]:
        """í¸í–¥ ë³´ì • ê°€ì†ë„ ì‹ ë¢°êµ¬ê°„ ê³„ì‚°"""
        z_alpha_2 = stats.norm.ppf(alpha / 2)
        z_1_alpha_2 = stats.norm.ppf(1 - alpha / 2)
        
        # í•˜í•œ
        numerator_lower = bias_correction + z_alpha_2
        denominator_lower = 1 - acceleration * (bias_correction + z_alpha_2)
        alpha_1 = stats.norm.cdf(bias_correction + numerator_lower / denominator_lower)
        
        # ìƒí•œ
        numerator_upper = bias_correction + z_1_alpha_2
        denominator_upper = 1 - acceleration * (bias_correction + z_1_alpha_2)
        alpha_2 = stats.norm.cdf(bias_correction + numerator_upper / denominator_upper)
        
        # ë°±ë¶„ìœ„ìˆ˜ ê³„ì‚°
        alpha_1 = max(0, min(1, alpha_1))
        alpha_2 = max(0, min(1, alpha_2))
        
        ci_lower = np.percentile(bootstrap_stats, alpha_1 * 100)
        ci_upper = np.percentile(bootstrap_stats, alpha_2 * 100)
        
        return ci_lower, ci_upper
    
    def bayesian_comparison(self, strategy_returns: pd.Series, 
                          benchmark_returns: pd.Series) -> Dict:
        """ë² ì´ì§€ì•ˆ ì „ëµ ë¹„êµ"""
        if not BAYESIAN_AVAILABLE:
            return {'error': 'PyMC ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ'}
        
        self.logger.info("ğŸ”® ë² ì´ì§€ì•ˆ ì „ëµ ë¹„êµ ì¤‘...")
        
        try:
            # ë°ì´í„° ì •ë ¬
            common_index = strategy_returns.index.intersection(benchmark_returns.index)
            strategy_aligned = strategy_returns[common_index].values
            benchmark_aligned = benchmark_returns[common_index].values
            
            # ë² ì´ì§€ì•ˆ ëª¨ë¸ êµ¬ì¶•
            with pm.Model() as model:
                # ì „ëµ ìˆ˜ìµë¥  ëª¨ë¸
                strategy_mu = pm.Normal('strategy_mu', mu=0, sigma=0.1)
                strategy_sigma = pm.HalfNormal('strategy_sigma', sigma=0.1)
                
                # ë²¤ì¹˜ë§ˆí¬ ìˆ˜ìµë¥  ëª¨ë¸
                benchmark_mu = pm.Normal('benchmark_mu', mu=0, sigma=0.1)
                benchmark_sigma = pm.HalfNormal('benchmark_sigma', sigma=0.1)
                
                # ê´€ì¸¡ ë°ì´í„°
                strategy_obs = pm.Normal('strategy_obs', mu=strategy_mu, sigma=strategy_sigma, observed=strategy_aligned)
                benchmark_obs = pm.Normal('benchmark_obs', mu=benchmark_mu, sigma=benchmark_sigma, observed=benchmark_aligned)
                
                # ì°¨ì´
                mu_diff = pm.Deterministic('mu_diff', strategy_mu - benchmark_mu)
                
                # MCMC ìƒ˜í”Œë§
                trace = pm.sample(self.config.mcmc_samples, chains=self.config.bayesian_chains, return_inferencedata=True)
            
            # ê²°ê³¼ ë¶„ì„
            posterior_summary = az.summary(trace)
            
            # ì „ëµì´ ë” ì¢‹ì„ í™•ë¥ 
            mu_diff_samples = trace.posterior['mu_diff'].values.flatten()
            prob_strategy_better = np.mean(mu_diff_samples > 0)
            
            # 95% ì‹ ë¢°êµ¬ê°„
            hdi_95 = az.hdi(trace, hdi_prob=0.95)
            
            bayesian_results = {
                'probability_strategy_better': float(prob_strategy_better),
                'mu_difference_posterior': {
                    'mean': float(np.mean(mu_diff_samples)),
                    'std': float(np.std(mu_diff_samples)),
                    'hdi_95': {
                        'lower': float(hdi_95['mu_diff'].values[0]),
                        'upper': float(hdi_95['mu_diff'].values[1])
                    }
                },
                'posterior_summary': {
                    param: {
                        'mean': float(row['mean']),
                        'std': float(row['sd']),
                        'hdi_3%': float(row['hdi_3%']),
                        'hdi_97%': float(row['hdi_97%'])
                    }
                    for param, row in posterior_summary.iterrows()
                },
                'convergence_diagnostics': {
                    'r_hat_max': float(posterior_summary['r_hat'].max()),
                    'ess_bulk_min': float(posterior_summary['ess_bulk'].min()),
                    'ess_tail_min': float(posterior_summary['ess_tail'].min())
                }
            }
            
            return bayesian_results
            
        except Exception as e:
            self.logger.error(f"ë² ì´ì§€ì•ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {'error': str(e)}
    
    def time_series_specific_tests(self, data: pd.Series) -> Dict:
        """ì‹œê³„ì—´ íŠ¹í™” ê²€ì •"""
        self.logger.info("â±ï¸ ì‹œê³„ì—´ íŠ¹í™” ê²€ì • ì¤‘...")
        
        data_clean = data.dropna()
        ts_results = {}
        
        # 1. ìê¸°ìƒê´€ ê²€ì • (ì´ë¯¸ ë…ë¦½ì„± ê²€ì •ì— í¬í•¨ë¨)
        
        # 2. ARCH íš¨ê³¼ ê²€ì • (ì´ë¶„ì‚°ì„±)
        try:
            # Engleì˜ ARCH ê²€ì •
            residuals = data_clean - data_clean.mean()
            squared_residuals = residuals ** 2
            
            # íšŒê·€ë¶„ì„: squared_residuals ~ lagged_squared_residuals
            lags = min(self.config.max_lags, len(squared_residuals) // 4)
            if lags > 0:
                lagged_data = np.column_stack([squared_residuals.shift(i).dropna() for i in range(1, lags+1)])
                y = squared_residuals[lags:]
                
                if len(y) > lags:
                    # OLS íšŒê·€
                    X = sm.add_constant(lagged_data)
                    model = sm.OLS(y, X).fit()
                    
                    # LM ê²€ì • í†µê³„ëŸ‰
                    lm_stat = len(y) * model.rsquared
                    lm_p = 1 - stats.chi2.cdf(lm_stat, lags)
                    
                    ts_results['arch_test'] = {
                        'lm_statistic': float(lm_stat),
                        'p_value': float(lm_p),
                        'has_arch_effect': lm_p < self.config.alpha,
                        'lags_tested': lags
                    }
        except Exception as e:
            self.logger.warning(f"ARCH ê²€ì • ì‹¤íŒ¨: {e}")
        
        # 3. ë‹¨ìœ„ê·¼ ê²€ì • (ì´ë¯¸ ì•ˆì •ì„± ê²€ì •ì— í¬í•¨ë¨)
        
        # 4. ê³µì ë¶„ ê²€ì • (ë‘ ì‹œë¦¬ì¦ˆê°€ ìˆëŠ” ê²½ìš°ë§Œ)
        
        # 5. ë³€ë™ì  ê²€ì •
        try:
            ts_results['change_point_detection'] = self._detect_change_points(data_clean)
        except Exception as e:
            self.logger.warning(f"ë³€ë™ì  ê²€ì • ì‹¤íŒ¨: {e}")
        
        return ts_results
    
    def _detect_change_points(self, data: pd.Series, min_size: int = 30) -> Dict:
        """ë³€ë™ì  íƒì§€ (PELT ì•Œê³ ë¦¬ì¦˜ ê°„ë‹¨ ë²„ì „)"""
        n = len(data)
        if n < min_size * 2:
            return {'change_points': [], 'method': 'insufficient_data'}
        
        # ê°„ë‹¨í•œ ë¶„ì‚° ë³€í™” íƒì§€
        change_points = []
        
        for i in range(min_size, n - min_size):
            before = data[:i]
            after = data[i:]
            
            # F-ê²€ì •ìœ¼ë¡œ ë¶„ì‚° ë³€í™” ê²€ì •
            f_stat = np.var(after) / np.var(before) if np.var(before) > 0 else 1
            p_value = 2 * min(stats.f.cdf(f_stat, len(after)-1, len(before)-1),
                             1 - stats.f.cdf(f_stat, len(after)-1, len(before)-1))
            
            if p_value < 0.01:  # ì—„ê²©í•œ ê¸°ì¤€
                change_points.append({
                    'position': i,
                    'date': data.index[i] if hasattr(data, 'index') else i,
                    'f_statistic': float(f_stat),
                    'p_value': float(p_value)
                })
        
        return {
            'change_points': change_points,
            'method': 'variance_change_f_test',
            'total_change_points': len(change_points)
        }
    
    def analyze_effect_sizes(self, strategy_returns: pd.Series, 
                           benchmark_returns: pd.Series = None) -> Dict:
        """íš¨ê³¼ í¬ê¸° ë¶„ì„"""
        self.logger.info("ğŸ“ íš¨ê³¼ í¬ê¸° ë¶„ì„ ì¤‘...")
        
        effect_size_results = {}
        
        if benchmark_returns is not None:
            # ë°ì´í„° ì •ë ¬
            common_index = strategy_returns.index.intersection(benchmark_returns.index)
            strategy_aligned = strategy_returns[common_index]
            benchmark_aligned = benchmark_returns[common_index]
            
            # Cohen's d
            pooled_std = np.sqrt(((len(strategy_aligned) - 1) * np.var(strategy_aligned, ddof=1) + 
                                 (len(benchmark_aligned) - 1) * np.var(benchmark_aligned, ddof=1)) / 
                                (len(strategy_aligned) + len(benchmark_aligned) - 2))
            
            cohens_d = (np.mean(strategy_aligned) - np.mean(benchmark_aligned)) / pooled_std if pooled_std > 0 else 0
            
            # Hedges' g (í¸í–¥ ë³´ì •ëœ Cohen's d)
            correction_factor = 1 - (3 / (4 * (len(strategy_aligned) + len(benchmark_aligned) - 2) - 1))
            hedges_g = cohens_d * correction_factor
            
            # Glass's delta
            glass_delta = (np.mean(strategy_aligned) - np.mean(benchmark_aligned)) / np.std(benchmark_aligned, ddof=1)
            
            # íš¨ê³¼ í¬ê¸° í•´ì„
            def interpret_effect_size(effect_size):
                abs_effect = abs(effect_size)
                if abs_effect < self.config.small_effect_size:
                    return 'negligible'
                elif abs_effect < self.config.medium_effect_size:
                    return 'small'
                elif abs_effect < self.config.large_effect_size:
                    return 'medium'
                else:
                    return 'large'
            
            effect_size_results['cohens_d'] = {
                'value': float(cohens_d),
                'interpretation': interpret_effect_size(cohens_d),
                'favors': 'strategy' if cohens_d > 0 else 'benchmark'
            }
            
            effect_size_results['hedges_g'] = {
                'value': float(hedges_g),
                'interpretation': interpret_effect_size(hedges_g),
                'favors': 'strategy' if hedges_g > 0 else 'benchmark'
            }
            
            effect_size_results['glass_delta'] = {
                'value': float(glass_delta),
                'interpretation': interpret_effect_size(glass_delta),
                'favors': 'strategy' if glass_delta > 0 else 'benchmark'
            }
        
        # ì „ëµ ìì²´ì˜ íš¨ê³¼ í¬ê¸° (ë¦¬ìŠ¤í¬ ëŒ€ë¹„ ìˆ˜ìµë¥ )
        strategy_clean = strategy_returns.dropna()
        if len(strategy_clean) > 0:
            # ìƒ¤í”„ ë¹„ìœ¨ì„ íš¨ê³¼ í¬ê¸°ë¡œ ì‚¬ìš©
            sharpe_ratio = np.mean(strategy_clean) / np.std(strategy_clean) * np.sqrt(252) if np.std(strategy_clean) > 0 else 0
            
            effect_size_results['strategy_sharpe_ratio'] = {
                'value': float(sharpe_ratio),
                'interpretation': interpret_effect_size(sharpe_ratio),
                'annualized': True
            }
        
        return effect_size_results
    
    def conduct_power_analysis(self, strategy_returns: pd.Series, 
                             benchmark_returns: pd.Series = None) -> Dict:
        """ê²€ì •ë ¥ ë¶„ì„"""
        self.logger.info("âš¡ ê²€ì •ë ¥ ë¶„ì„ ì¤‘...")
        
        power_results = {}
        
        if benchmark_returns is not None:
            # í˜„ì¬ ìƒ˜í”Œ í¬ê¸°
            common_index = strategy_returns.index.intersection(benchmark_returns.index)
            n_samples = len(common_index)
            
            if n_samples > 0:
                strategy_aligned = strategy_returns[common_index]
                benchmark_aligned = benchmark_returns[common_index]
                
                # í’€ë§ëœ í‘œì¤€í¸ì°¨
                pooled_std = np.sqrt(((len(strategy_aligned) - 1) * np.var(strategy_aligned, ddof=1) + 
                                     (len(benchmark_aligned) - 1) * np.var(benchmark_aligned, ddof=1)) / 
                                    (len(strategy_aligned) + len(benchmark_aligned) - 2))
                
                # ë‹¤ì–‘í•œ íš¨ê³¼ í¬ê¸°ì— ëŒ€í•œ ê²€ì •ë ¥ ê³„ì‚°
                power_analysis = {}
                for effect_size in self.config.effect_sizes:
                    try:
                        power = TTestPower().solve_power(effect_size=effect_size, 
                                                        nobs=n_samples, 
                                                        alpha=self.config.alpha)
                        power_analysis[f'effect_size_{effect_size}'] = {
                            'power': float(power),
                            'adequate_power': power >= self.config.desired_power
                        }
                    except:
                        pass
                
                power_results['current_sample_power'] = power_analysis
                
                # ì›í•˜ëŠ” ê²€ì •ë ¥ì„ ìœ„í•œ í•„ìš” ìƒ˜í”Œ í¬ê¸°
                required_samples = {}
                for effect_size in self.config.effect_sizes:
                    if effect_size > 0:  # 0 íš¨ê³¼ í¬ê¸°ëŠ” ì œì™¸
                        try:
                            required_n = TTestPower().solve_power(effect_size=effect_size,
                                                                 power=self.config.desired_power,
                                                                 alpha=self.config.alpha)
                            required_samples[f'effect_size_{effect_size}'] = {
                                'required_n': int(required_n),
                                'current_n': n_samples,
                                'additional_samples_needed': max(0, int(required_n) - n_samples)
                            }
                        except:
                            pass
                
                power_results['required_sample_sizes'] = required_samples
                
                # í˜„ì¬ ë°ì´í„°ë¡œ íƒì§€ ê°€ëŠ¥í•œ ìµœì†Œ íš¨ê³¼ í¬ê¸°
                try:
                    min_detectable_effect = TTestPower().solve_power(power=self.config.desired_power,
                                                                    nobs=n_samples,
                                                                    alpha=self.config.alpha)
                    power_results['minimum_detectable_effect'] = {
                        'effect_size': float(min_detectable_effect),
                        'interpretation': 'small' if min_detectable_effect < 0.5 else 'medium' if min_detectable_effect < 0.8 else 'large'
                    }
                except:
                    pass
        
        return power_results
    
    def conduct_robustness_tests(self, strategy_returns: pd.Series, 
                               benchmark_returns: pd.Series = None) -> Dict:
        """ê°•ê±´ì„± ê²€ì •"""
        self.logger.info("ğŸ›¡ï¸ ê°•ê±´ì„± ê²€ì • ì¤‘...")
        
        robustness_results = {}
        
        # 1. ì•„ì›ƒë¼ì´ì–´ ì œê±° í›„ ê²€ì •
        robustness_results['outlier_removal_tests'] = self._test_with_outlier_removal(
            strategy_returns, benchmark_returns
        )
        
        # 2. ë¶€íŠ¸ìŠ¤íŠ¸ë© ì•ˆì •ì„± ê²€ì •
        robustness_results['bootstrap_stability'] = self._bootstrap_stability_test(strategy_returns)
        
        # 3. ì„œë¸Œìƒ˜í”Œ ê²€ì •
        robustness_results['subsample_tests'] = self._subsample_robustness_test(
            strategy_returns, benchmark_returns
        )
        
        return robustness_results
    
    def _test_with_outlier_removal(self, strategy_returns: pd.Series, 
                                 benchmark_returns: pd.Series = None) -> Dict:
        """ì´ìƒì¹˜ ì œê±° í›„ ê²€ì •"""
        results = {}
        
        for method in self.config.outlier_methods:
            try:
                if method == 'iqr':
                    strategy_clean = self._remove_outliers_iqr(strategy_returns)
                    benchmark_clean = self._remove_outliers_iqr(benchmark_returns) if benchmark_returns is not None else None
                elif method == 'zscore':
                    strategy_clean = self._remove_outliers_zscore(strategy_returns)
                    benchmark_clean = self._remove_outliers_zscore(benchmark_returns) if benchmark_returns is not None else None
                else:
                    continue  # isolation forestëŠ” êµ¬í˜„ ìƒëµ
                
                # ì´ìƒì¹˜ ì œê±° í›„ ë¹„êµ ê²€ì •
                if benchmark_clean is not None:
                    comparison_results = self.compare_strategies(strategy_clean, benchmark_clean)
                    results[f'{method}_removal'] = {
                        'strategy_observations_removed': len(strategy_returns) - len(strategy_clean),
                        'benchmark_observations_removed': len(benchmark_returns) - len(benchmark_clean) if benchmark_returns is not None else 0,
                        'test_results': comparison_results
                    }
            except Exception as e:
                self.logger.warning(f"ì´ìƒì¹˜ ì œê±° ê²€ì • ì‹¤íŒ¨ ({method}): {e}")
                continue
        
        return results
    
    def _remove_outliers_iqr(self, data: pd.Series) -> pd.Series:
        """IQR ë°©ë²•ìœ¼ë¡œ ì´ìƒì¹˜ ì œê±°"""
        Q1 = data.quantile(0.25)
        Q3 = data.quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        return data[(data >= lower_bound) & (data <= upper_bound)]
    
    def _remove_outliers_zscore(self, data: pd.Series, threshold: float = 3) -> pd.Series:
        """Z-score ë°©ë²•ìœ¼ë¡œ ì´ìƒì¹˜ ì œê±°"""
        z_scores = np.abs(stats.zscore(data))
        return data[z_scores < threshold]
    
    def _bootstrap_stability_test(self, data: pd.Series, n_bootstrap: int = 1000) -> Dict:
        """ë¶€íŠ¸ìŠ¤íŠ¸ë© ì•ˆì •ì„± ê²€ì •"""
        data_clean = data.dropna()
        
        if len(data_clean) < 30:
            return {'error': 'ë¶€íŠ¸ìŠ¤íŠ¸ë©ì— ì¶©ë¶„í•˜ì§€ ì•Šì€ ë°ì´í„°'}
        
        # ë‹¤ì–‘í•œ í†µê³„ëŸ‰ì˜ ë¶€íŠ¸ìŠ¤íŠ¸ë© ì•ˆì •ì„±
        statistics = {
            'mean': np.mean,
            'std': np.std,
            'skewness': lambda x: stats.skew(x),
            'kurtosis': lambda x: stats.kurtosis(x)
        }
        
        stability_results = {}
        
        for stat_name, stat_func in statistics.items():
            bootstrap_stats = []
            np.random.seed(42)
            
            for _ in range(n_bootstrap):
                bootstrap_sample = np.random.choice(data_clean, size=len(data_clean), replace=True)
                bootstrap_stat = stat_func(bootstrap_sample)
                bootstrap_stats.append(bootstrap_stat)
            
            bootstrap_stats = np.array(bootstrap_stats)
            bootstrap_stats = bootstrap_stats[~np.isnan(bootstrap_stats)]
            
            if len(bootstrap_stats) > 0:
                # ì•ˆì •ì„± ì¸¡ì • (ë³€ë™ê³„ìˆ˜)
                stability_cv = np.std(bootstrap_stats) / abs(np.mean(bootstrap_stats)) if np.mean(bootstrap_stats) != 0 else np.inf
                
                stability_results[stat_name] = {
                    'bootstrap_mean': float(np.mean(bootstrap_stats)),
                    'bootstrap_std': float(np.std(bootstrap_stats)),
                    'coefficient_of_variation': float(stability_cv),
                    'is_stable': stability_cv < 0.1  # 10% ë¯¸ë§Œì´ë©´ ì•ˆì •
                }
        
        return stability_results
    
    def _subsample_robustness_test(self, strategy_returns: pd.Series, 
                                 benchmark_returns: pd.Series = None) -> Dict:
        """ì„œë¸Œìƒ˜í”Œ ê°•ê±´ì„± ê²€ì •"""
        if benchmark_returns is None:
            return {'error': 'ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° í•„ìš”'}
        
        common_index = strategy_returns.index.intersection(benchmark_returns.index)
        if len(common_index) < 100:
            return {'error': 'ì„œë¸Œìƒ˜í”Œë§ì— ì¶©ë¶„í•˜ì§€ ì•Šì€ ë°ì´í„°'}
        
        strategy_aligned = strategy_returns[common_index]
        benchmark_aligned = benchmark_returns[common_index]
        
        # ë‹¤ì–‘í•œ í¬ê¸°ì˜ ì„œë¸Œìƒ˜í”Œë¡œ ê²€ì •
        sample_sizes = [0.5, 0.7, 0.9]  # 50%, 70%, 90%
        subsample_results = {}
        
        for sample_ratio in sample_sizes:
            sample_size = int(len(common_index) * sample_ratio)
            
            # ì—¬ëŸ¬ ë²ˆì˜ ëœë¤ ì„œë¸Œìƒ˜í”Œë§
            n_subsamples = 50
            significant_tests = 0
            
            np.random.seed(42)
            for _ in range(n_subsamples):
                subsample_idx = np.random.choice(len(common_index), size=sample_size, replace=False)
                subsample_strategy = strategy_aligned.iloc[subsample_idx]
                subsample_benchmark = benchmark_aligned.iloc[subsample_idx]
                
                # t-ê²€ì • ìˆ˜í–‰
                try:
                    _, p_value = ttest_ind(subsample_strategy, subsample_benchmark)
                    if p_value < self.config.alpha:
                        significant_tests += 1
                except:
                    continue
            
            subsample_results[f'sample_ratio_{sample_ratio}'] = {
                'sample_size': sample_size,
                'n_subsamples': n_subsamples,
                'significant_tests': significant_tests,
                'significance_ratio': significant_tests / n_subsamples,
                'is_robust': significant_tests / n_subsamples >= 0.8  # 80% ì´ìƒì—ì„œ ìœ ì˜í•˜ë©´ ê°•ê±´
            }
        
        return subsample_results
    
    def generate_statistical_conclusions(self, *test_results) -> Dict:
        """í†µê³„ì  ê²°ë¡  ìƒì„±"""
        self.logger.info("ğŸ“‹ í†µê³„ì  ê²°ë¡  ìƒì„± ì¤‘...")
        
        conclusions = {
            'overall_significance': 'inconclusive',
            'confidence_level': 'low',
            'recommendations': [],
            'limitations': [],
            'key_findings': []
        }
        
        # ê° ê²€ì • ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ê²°ë¡  ë„ì¶œ
        # (êµ¬í˜„ ìƒëµ - ì‹¤ì œë¡œëŠ” ë³µì¡í•œ ë…¼ë¦¬ê°€ í•„ìš”)
        
        return conclusions
    
    def create_significance_testing_dashboard(self, results: Dict) -> str:
        """í†µê³„ì  ìœ ì˜ì„± ê²€ì • ëŒ€ì‹œë³´ë“œ ìƒì„±"""
        self.logger.info("ğŸ“Š í†µê³„ì  ìœ ì˜ì„± ê²€ì • ëŒ€ì‹œë³´ë“œ ìƒì„± ì¤‘...")
        
        fig = make_subplots(
            rows=3, cols=3,
            subplot_titles=(
                "ê°€ì • ê²€ì • ê²°ê³¼", "íš¨ê³¼ í¬ê¸° ë¶„ì„", "ê²€ì •ë ¥ ë¶„ì„",
                "ë¶€íŠ¸ìŠ¤íŠ¸ë© ì‹ ë¢°êµ¬ê°„", "ë² ì´ì§€ì•ˆ ë¶„ì„", "ê°•ê±´ì„± ê²€ì •",
                "p-value ë¶„í¬", "ë‹¤ì¤‘ë¹„êµ ê²°ê³¼", "ì‹œê³„ì—´ íŠ¹í™” ê²€ì •"
            ),
            specs=[[{"type": "table"}, {"type": "bar"}, {"type": "scatter"}],
                   [{"type": "box"}, {"type": "violin"}, {"type": "heatmap"}],
                   [{"type": "histogram"}, {"type": "bar"}, {"type": "indicator"}]]
        )
        
        # ëŒ€ì‹œë³´ë“œ êµ¬í˜„ (ì°¨íŠ¸ë³„ë¡œ êµ¬í˜„)
        # ...
        
        fig.update_layout(
            title="ğŸ§ª í†µê³„ì  ìœ ì˜ì„± ê²€ì • ì¢…í•© ëŒ€ì‹œë³´ë“œ",
            height=1200,
            showlegend=True,
            template='plotly_dark'
        )
        
        dashboard_path = os.path.join(self.data_path, 'statistical_significance_dashboard.html')
        fig.write_html(dashboard_path, include_plotlyjs=True)
        
        return dashboard_path
    
    def save_test_results(self, results: Dict):
        """ê²€ì • ê²°ê³¼ ì €ì¥"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # JSON ì €ì¥
        json_path = os.path.join(self.data_path, f'statistical_significance_results_{timestamp}.json')
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)
        
        self.logger.info(f"ê²€ì • ê²°ê³¼ ì €ì¥: {json_path}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ§ª í†µê³„ì  ìœ ì˜ì„± ê²€ì • ì‹œìŠ¤í…œ")
    print("=" * 50)
    
    # ì„¤ì •
    config = SignificanceConfig(
        alpha=0.05,
        bootstrap_samples=5000,
        multiple_comparison_method='holm',
        desired_power=0.8
    )
    
    # ê²€ì • ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    tester = StatisticalSignificanceTester(config)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    np.random.seed(42)
    n_days = 500
    
    # ì „ëµ ìˆ˜ìµë¥  (ì•½ê°„ ë” ì¢‹ì€ ì„±ëŠ¥)
    strategy_returns = pd.Series(
        np.random.normal(0.0015, 0.02, n_days),
        index=pd.date_range('2023-01-01', periods=n_days)
    )
    
    # ë²¤ì¹˜ë§ˆí¬ ìˆ˜ìµë¥ 
    benchmark_returns = pd.Series(
        np.random.normal(0.001, 0.02, n_days),
        index=pd.date_range('2023-01-01', periods=n_days)
    )
    
    # ì¢…í•© í†µê³„ì  ìœ ì˜ì„± ê²€ì •
    results = tester.comprehensive_significance_testing(strategy_returns, benchmark_returns)
    
    print(f"\nğŸ§ª í†µê³„ì  ê²€ì • ì™„ë£Œ!")
    print(f"ë°ì´í„° ê¸°ê°„: {results['data_summary']['time_period']['start']} ~ {results['data_summary']['time_period']['end']}")
    print(f"ì „ëµ ê´€ì°°ê°’: {results['data_summary']['strategy_observations']}ê°œ")
    
    # ì£¼ìš” ê²°ê³¼ ì¶œë ¥
    if 'strategy_comparison' in results:
        comparison = results['strategy_comparison']
        if 'paired_ttest' in comparison:
            t_test = comparison['paired_ttest']
            print(f"ëŒ€ì‘ t-ê²€ì •: t={t_test['statistic']:.3f}, p={t_test['p_value']:.4f}")
            print(f"ì „ëµì´ ë” ìš°ìˆ˜í•¨: {'ì˜ˆ' if t_test['strategy_better'] and t_test['significant'] else 'ì•„ë‹ˆì˜¤'}")

if __name__ == "__main__":
    main()