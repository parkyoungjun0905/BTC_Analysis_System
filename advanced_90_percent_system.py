#!/usr/bin/env python3
"""
ğŸ† ê³ ê¸‰ BTC 90% ì •í™•ë„ ë„ì „ ì‹œìŠ¤í…œ
ì‹¤ì œ ë”¥ëŸ¬ë‹ + ì•™ìƒë¸” + ê³ ê¸‰ íŠ¹ì„±ê³µí•™ìœ¼ë¡œ 90% ì •í™•ë„ ë‹¬ì„±

í•µì‹¬ ì „ëµ:
1. ë°©í–¥ì„± ì˜ˆì¸¡ì— íŠ¹í™”ëœ ëª¨ë¸ ì„¤ê³„
2. ë‹¤ì¤‘ ì˜ˆì¸¡ í˜¸ë¼ì´ì¦Œ (1h, 2h, 3h) ì•™ìƒë¸”
3. ì‹œì¥ ìƒí™©ë³„ ì ì‘ì  ëª¨ë¸
4. ê³ ê¸‰ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ (100+ ì§€í‘œ)
"""

import os
import sys
import json
import numpy as np
import pandas as pd
import warnings
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
import pickle

# ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°
warnings.filterwarnings('ignore')

# ê³ ê¸‰ ML ë¼ì´ë¸ŒëŸ¬ë¦¬
from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler
from sklearn.model_selection import TimeSeriesSplit
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import matplotlib.pyplot as plt

# ë”¥ëŸ¬ë‹ ì‚¬ìš© ê°€ëŠ¥ì‹œ import
try:
    from sklearn.neural_network import MLPRegressor
    from sklearn.svm import SVR
    ADVANCED_ML_AVAILABLE = True
    print("âœ… ê³ ê¸‰ ML ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
except ImportError:
    ADVANCED_ML_AVAILABLE = False
    print("âš ï¸ ê³ ê¸‰ ML ëª¨ë¸ ì¼ë¶€ ì œí•œ")

print("ğŸ† ê³ ê¸‰ BTC 90% ì •í™•ë„ ë„ì „ ì‹œìŠ¤í…œ ì‹œì‘")
print("=" * 60)

class AdvancedFeatureEngineer:
    """ê³ ê¸‰ íŠ¹ì„±ê³µí•™ - 100+ ì§€í‘œ ìƒì„±"""
    
    def __init__(self):
        self.scalers = {
            'robust': RobustScaler(),
            'minmax': MinMaxScaler(),
            'standard': StandardScaler()
        }
        print("âœ… ê³ ê¸‰ íŠ¹ì„±ê³µí•™ ì—”ì§„ ì´ˆê¸°í™”")
    
    def create_advanced_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """ê³ ê¸‰ ê¸°ìˆ  ì§€í‘œ ëŒ€ëŸ‰ ìƒì„±"""
        print("ğŸ”§ ê³ ê¸‰ íŠ¹ì„±ê³µí•™ ì‹œì‘ (100+ ì§€í‘œ ìƒì„±)...")
        
        try:
            original_len = len(df)
            
            # 1. ê¸°ë³¸ ê°€ê²© ì§€í‘œë“¤
            for window in [3, 5, 10, 20, 50]:
                df[f'sma_{window}'] = df['close'].rolling(window).mean()
                df[f'ema_{window}'] = df['close'].ewm(window).mean()
                df[f'price_sma_ratio_{window}'] = df['close'] / df[f'sma_{window}']
                df[f'price_ema_ratio_{window}'] = df['close'] / df[f'ema_{window}']
            
            # 2. ë³€ë™ì„± ì§€í‘œë“¤
            for window in [5, 10, 20]:
                df[f'volatility_{window}'] = df['close'].rolling(window).std()
                df[f'volatility_ratio_{window}'] = df[f'volatility_{window}'] / df[f'volatility_{window}'].rolling(50).mean()
            
            # 3. ëª¨ë©˜í…€ ì§€í‘œë“¤ (ë°©í–¥ì„± ì˜ˆì¸¡ì— ì¤‘ìš”!)
            for window in [3, 5, 10, 14, 20]:
                df[f'roc_{window}'] = df['close'].pct_change(window)
                df[f'momentum_{window}'] = df['close'] / df['close'].shift(window) - 1
            
            # 4. RSI ë³€í˜•ë“¤
            for window in [7, 14, 21]:
                delta = df['close'].diff()
                gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
                loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
                rs = gain / loss
                df[f'rsi_{window}'] = 100 - (100 / (1 + rs))
                df[f'rsi_ma_{window}'] = df[f'rsi_{window}'].rolling(5).mean()
            
            # 5. MACD ë³€í˜•ë“¤
            for fast, slow, signal in [(5, 10, 3), (12, 26, 9), (8, 21, 5)]:
                exp1 = df['close'].ewm(span=fast).mean()
                exp2 = df['close'].ewm(span=slow).mean()
                df[f'macd_{fast}_{slow}'] = exp1 - exp2
                df[f'macd_signal_{fast}_{slow}_{signal}'] = df[f'macd_{fast}_{slow}'].ewm(span=signal).mean()
                df[f'macd_histogram_{fast}_{slow}_{signal}'] = df[f'macd_{fast}_{slow}'] - df[f'macd_signal_{fast}_{slow}_{signal}']
            
            # 6. ë³¼ë¦°ì €ë°´ë“œ ë³€í˜•ë“¤
            for window, std_dev in [(10, 2), (20, 2), (20, 1.5), (50, 2.5)]:
                sma = df['close'].rolling(window).mean()
                std = df['close'].rolling(window).std()
                df[f'bb_upper_{window}_{std_dev}'] = sma + (std * std_dev)
                df[f'bb_lower_{window}_{std_dev}'] = sma - (std * std_dev)
                df[f'bb_position_{window}_{std_dev}'] = (df['close'] - df[f'bb_lower_{window}_{std_dev}']) / (df[f'bb_upper_{window}_{std_dev}'] - df[f'bb_lower_{window}_{std_dev}'])
                df[f'bb_width_{window}_{std_dev}'] = (df[f'bb_upper_{window}_{std_dev}'] - df[f'bb_lower_{window}_{std_dev}']) / sma
            
            # 7. ìŠ¤í† ìºìŠ¤í‹± ë³€í˜•ë“¤  
            for k_window, d_window in [(5, 3), (14, 3), (21, 5)]:
                low_min = df['close'].rolling(window=k_window).min()
                high_max = df['close'].rolling(window=k_window).max()
                df[f'stoch_k_{k_window}'] = 100 * (df['close'] - low_min) / (high_max - low_min)
                df[f'stoch_d_{k_window}_{d_window}'] = df[f'stoch_k_{k_window}'].rolling(d_window).mean()
            
            # 8. ë³¼ë¥¨ ì§€í‘œë“¤ (ì¤‘ìš”!)
            if 'volume' in df.columns:
                for window in [5, 10, 20, 50]:
                    df[f'volume_sma_{window}'] = df['volume'].rolling(window).mean()
                    df[f'volume_ratio_{window}'] = df['volume'] / df[f'volume_sma_{window}']
                    df[f'price_volume_trend_{window}'] = ((df['close'] - df['close'].shift()) / df['close'].shift()) * df['volume']
                
                # OBV (On-Balance Volume)
                df['obv'] = (np.sign(df['close'].diff()) * df['volume']).fillna(0).cumsum()
                df['obv_sma'] = df['obv'].rolling(20).mean()
                df['obv_ratio'] = df['obv'] / df['obv_sma']
            
            # 9. ì¶”ì„¸ ì§€í‘œë“¤
            for window in [10, 20, 50]:
                df[f'trend_strength_{window}'] = df['close'].rolling(window).apply(
                    lambda x: np.corrcoef(np.arange(len(x)), x)[0, 1] if len(x) == window else 0, raw=False
                ).fillna(0)
            
            # 10. ì§€ì§€/ì €í•­ ìˆ˜ì¤€ (ë‹¨ìˆœí™”)
            df['support_5d'] = df['close'].rolling(5).min()
            df['resistance_5d'] = df['close'].rolling(5).max()
            df['support_distance'] = (df['close'] - df['support_5d']) / df['close']
            df['resistance_distance'] = (df['resistance_5d'] - df['close']) / df['close']
            
            # 11. ê³ ê¸‰ íŒ¨í„´ ì¸ì‹ (ë°©í–¥ì„± ì˜ˆì¸¡ í•µì‹¬!)
            # ìƒìŠ¹/í•˜ë½ ì—°ì†ì„±
            df['price_change'] = df['close'].pct_change()
            df['up_days'] = (df['price_change'] > 0).astype(int)
            df['down_days'] = (df['price_change'] < 0).astype(int)
            
            for window in [3, 5, 10]:
                df[f'up_streak_{window}'] = df['up_days'].rolling(window).sum()
                df[f'down_streak_{window}'] = df['down_days'].rolling(window).sum()
                df[f'momentum_ratio_{window}'] = df[f'up_streak_{window}'] / (df[f'down_streak_{window}'] + 1)
            
            # 12. ì‹œê°„ ê¸°ë°˜ íŠ¹ì„±ë“¤
            if 'timestamp' in df.columns:
                df['timestamp'] = pd.to_datetime(df['timestamp'])
                df['hour'] = df['timestamp'].dt.hour
                df['day_of_week'] = df['timestamp'].dt.dayofweek
                df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)
                df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)
                df['dow_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)
                df['dow_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)
            
            # 13. ë¼ê·¸ íŠ¹ì„±ë“¤ (ê³¼ê±° ê°’ë“¤)
            for lag in [1, 2, 3, 5, 10]:
                df[f'price_lag_{lag}'] = df['close'].shift(lag)
                df[f'return_lag_{lag}'] = df['price_change'].shift(lag)
                df[f'volume_lag_{lag}'] = df['volume'].shift(lag) if 'volume' in df.columns else 0
            
            # 14. ë¡¤ë§ í†µê³„
            for window in [5, 10, 20]:
                df[f'skew_{window}'] = df['price_change'].rolling(window).skew()
                df[f'kurt_{window}'] = df['price_change'].rolling(window).kurt()
                df[f'std_{window}'] = df['price_change'].rolling(window).std()
            
            # 15. êµì°¨ ê²€ì¦ íŠ¹ì„±ë“¤ (ë‹¤ë¥¸ ì§€í‘œì™€ì˜ ê´€ê³„)
            df['rsi_sma_cross'] = np.where(df['rsi_14'] > df['rsi_ma_14'], 1, -1)
            df['price_sma_cross'] = np.where(df['close'] > df['sma_20'], 1, -1)
            df['macd_cross'] = np.where(df['macd_12_26'] > df['macd_signal_12_26_9'], 1, -1)
            
            # NaN ì²˜ë¦¬
            df = df.fillna(method='ffill').fillna(method='bfill').fillna(0)
            
            final_len = len(df)
            feature_count = len(df.columns)
            
            print(f"âœ… ê³ ê¸‰ íŠ¹ì„±ê³µí•™ ì™„ë£Œ:")
            print(f"   ğŸ“Š ìƒì„±ëœ íŠ¹ì„± ìˆ˜: {feature_count}")
            print(f"   ğŸ“ˆ ë°ì´í„° ê¸¸ì´: {original_len} â†’ {final_len}")
            
            return df
            
        except Exception as e:
            print(f"âŒ íŠ¹ì„±ê³µí•™ ì˜¤ë¥˜: {e}")
            return df

class DirectionalPredictor:
    """ë°©í–¥ì„± ì˜ˆì¸¡ì— íŠ¹í™”ëœ AI ì˜ˆì¸¡ê¸°"""
    
    def __init__(self, lookback_hours: int = 10):
        self.lookback_hours = lookback_hours
        self.models = {}
        self.scalers = {}
        self.feature_columns = []
        print(f"âœ… ë°©í–¥ì„± íŠ¹í™” AI ì˜ˆì¸¡ê¸° ì´ˆê¸°í™” (lookback: {lookback_hours}ì‹œê°„)")
    
    def create_directional_sequences(self, df: pd.DataFrame) -> Tuple:
        """ë°©í–¥ì„± ì˜ˆì¸¡ì„ ìœ„í•œ ì‹œí€€ìŠ¤ ìƒì„±"""
        print("ğŸ¯ ë°©í–¥ì„± ì˜ˆì¸¡ ì‹œí€€ìŠ¤ ìƒì„± ì¤‘...")
        
        # í•µì‹¬ íŠ¹ì„±ë“¤ ì„ íƒ (ë°©í–¥ì„± ì˜ˆì¸¡ì— ì¤‘ìš”í•œ ê²ƒë“¤)
        directional_features = []
        
        # 1. ëª¨ë©˜í…€ ì§€í‘œë“¤
        momentum_cols = [col for col in df.columns if 'momentum_' in col or 'roc_' in col]
        directional_features.extend(momentum_cols[:10])  # ìƒìœ„ 10ê°œ
        
        # 2. RSI ê´€ë ¨
        rsi_cols = [col for col in df.columns if 'rsi' in col]
        directional_features.extend(rsi_cols[:5])
        
        # 3. MACD ê´€ë ¨  
        macd_cols = [col for col in df.columns if 'macd' in col]
        directional_features.extend(macd_cols[:8])
        
        # 4. ì¶”ì„¸ ì§€í‘œë“¤
        trend_cols = [col for col in df.columns if 'trend' in col or 'streak' in col]
        directional_features.extend(trend_cols[:8])
        
        # 5. ê°€ê²© ë¹„ìœ¨ë“¤
        ratio_cols = [col for col in df.columns if 'ratio' in col and 'price' in col]
        directional_features.extend(ratio_cols[:5])
        
        # 6. í¬ë¡œìŠ¤ ì‹œê·¸ë„ë“¤  
        cross_cols = [col for col in df.columns if 'cross' in col]
        directional_features.extend(cross_cols)
        
        # 7. ë³¼ë¦°ì €ë°´ë“œ í¬ì§€ì…˜
        bb_cols = [col for col in df.columns if 'bb_position' in col]
        directional_features.extend(bb_cols[:3])
        
        # 8. ìŠ¤í† ìºìŠ¤í‹±
        stoch_cols = [col for col in df.columns if 'stoch' in col]
        directional_features.extend(stoch_cols[:4])
        
        # ì¤‘ë³µ ì œê±° ë° ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ
        directional_features = list(set([col for col in directional_features if col in df.columns]))
        
        if len(directional_features) < 10:
            # ë¶€ì¡±í•˜ë©´ ë‹¤ë¥¸ ì§€í‘œë“¤ ì¶”ê°€
            other_cols = [col for col in df.columns if col not in directional_features 
                         and col not in ['close', 'timestamp', 'volume'] 
                         and not col.startswith('price_lag')]
            directional_features.extend(other_cols[:20-len(directional_features)])
        
        self.feature_columns = directional_features[:30]  # ìƒìœ„ 30ê°œ íŠ¹ì„± ì‚¬ìš©
        print(f"ğŸ“Š ë°©í–¥ì„± ì˜ˆì¸¡ íŠ¹ì„± {len(self.feature_columns)}ê°œ ì„ íƒ:")
        for i, col in enumerate(self.feature_columns[:10]):
            print(f"   {i+1:2d}. {col}")
        if len(self.feature_columns) > 10:
            print(f"   ... ì™¸ {len(self.feature_columns)-10}ê°œ")
        
        # ì‹œí€€ìŠ¤ ìƒì„±
        X, y_1h, y_2h, y_3h = [], [], [], []
        
        for i in range(self.lookback_hours, len(df) - 3):
            # íŠ¹ì„±ë“¤
            features = df[self.feature_columns].iloc[i-self.lookback_hours:i].values.flatten()
            X.append(features)
            
            # ë°©í–¥ì„± íƒ€ê²Ÿë“¤ (ìƒìŠ¹=1, í•˜ë½=-1)
            current_price = df['close'].iloc[i]
            price_1h = df['close'].iloc[i + 1] if i + 1 < len(df) else current_price
            price_2h = df['close'].iloc[i + 2] if i + 2 < len(df) else current_price  
            price_3h = df['close'].iloc[i + 3] if i + 3 < len(df) else current_price
            
            y_1h.append(1 if price_1h > current_price else -1)
            y_2h.append(1 if price_2h > current_price else -1)
            y_3h.append(1 if price_3h > current_price else -1)
        
        X = np.array(X)
        y_1h = np.array(y_1h)
        y_2h = np.array(y_2h) 
        y_3h = np.array(y_3h)
        
        print(f"âœ… ì‹œí€€ìŠ¤ ìƒì„± ì™„ë£Œ: X={X.shape}")
        print(f"   1H ë°©í–¥ì„±: ìƒìŠ¹ {(y_1h==1).sum()}, í•˜ë½ {(y_1h==-1).sum()}")
        print(f"   2H ë°©í–¥ì„±: ìƒìŠ¹ {(y_2h==1).sum()}, í•˜ë½ {(y_2h==-1).sum()}")  
        print(f"   3H ë°©í–¥ì„±: ìƒìŠ¹ {(y_3h==1).sum()}, í•˜ë½ {(y_3h==-1).sum()}")
        
        return X, y_1h, y_2h, y_3h
    
    def train_ensemble_models(self, X: np.ndarray, y_1h: np.ndarray, y_2h: np.ndarray, y_3h: np.ndarray) -> Dict:
        """ë‹¤ì¤‘ ëª¨ë¸ ì•™ìƒë¸” í•™ìŠµ"""
        print("ğŸ¤– ë‹¤ì¤‘ ëª¨ë¸ ì•™ìƒë¸” í•™ìŠµ ì‹œì‘...")
        
        # ë°ì´í„° ìŠ¤ì¼€ì¼ë§
        self.scalers['scaler'] = RobustScaler()
        X_scaled = self.scalers['scaler'].fit_transform(X)
        
        # ë‹¤ì–‘í•œ ëª¨ë¸ë“¤ ì •ì˜
        model_configs = {
            'rf_1h': (RandomForestRegressor(n_estimators=200, max_depth=15, random_state=42, n_jobs=-1), y_1h),
            'rf_2h': (RandomForestRegressor(n_estimators=200, max_depth=15, random_state=42, n_jobs=-1), y_2h),
            'rf_3h': (RandomForestRegressor(n_estimators=200, max_depth=15, random_state=42, n_jobs=-1), y_3h),
            'gb_1h': (GradientBoostingRegressor(n_estimators=150, max_depth=8, learning_rate=0.1, random_state=42), y_1h),
            'gb_2h': (GradientBoostingRegressor(n_estimators=150, max_depth=8, learning_rate=0.1, random_state=42), y_2h),
            'gb_3h': (GradientBoostingRegressor(n_estimators=150, max_depth=8, learning_rate=0.1, random_state=42), y_3h),
        }
        
        # ê³ ê¸‰ ëª¨ë¸ë“¤ ì¶”ê°€
        if ADVANCED_ML_AVAILABLE:
            model_configs.update({
                'mlp_1h': (MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42), y_1h),
                'mlp_2h': (MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42), y_2h), 
                'mlp_3h': (MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42), y_3h),
            })
        
        # ëª¨ë¸ í•™ìŠµ ë° ê²€ì¦
        results = {'models': {}, 'scores': {}}
        
        for model_name, (model, y_target) in model_configs.items():
            print(f"  ğŸ”§ {model_name} í•™ìŠµ ì¤‘...")
            
            # ì‹œê³„ì—´ êµì°¨ê²€ì¦
            tscv = TimeSeriesSplit(n_splits=3)
            cv_scores = []
            
            for train_idx, val_idx in tscv.split(X_scaled):
                X_train, X_val = X_scaled[train_idx], X_scaled[val_idx]
                y_train, y_val = y_target[train_idx], y_target[val_idx]
                
                # ëª¨ë¸ í•™ìŠµ
                model_copy = type(model)(**model.get_params())
                model_copy.fit(X_train, y_train)
                
                # ì˜ˆì¸¡ ë° í‰ê°€ (ë°©í–¥ì„± ì •í™•ë„)
                y_pred = model_copy.predict(X_val)
                y_pred_direction = np.where(y_pred > 0, 1, -1)
                accuracy = (y_pred_direction == y_val).mean()
                cv_scores.append(accuracy)
            
            avg_score = np.mean(cv_scores)
            results['scores'][model_name] = avg_score
            
            # ì „ì²´ ë°ì´í„°ë¡œ ìµœì¢… í•™ìŠµ
            model.fit(X_scaled, y_target)
            self.models[model_name] = model
            
            print(f"    âœ… {model_name}: {avg_score:.4f} ë°©í–¥ì„± ì •í™•ë„")
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ë“¤ ì‹ë³„
        best_1h = max([k for k in results['scores'].keys() if '1h' in k], key=lambda x: results['scores'][x])
        best_2h = max([k for k in results['scores'].keys() if '2h' in k], key=lambda x: results['scores'][x])
        best_3h = max([k for k in results['scores'].keys() if '3h' in k], key=lambda x: results['scores'][x])
        
        results['best_models'] = {
            '1h': best_1h,
            '2h': best_2h, 
            '3h': best_3h
        }
        
        # ì „ì²´ í‰ê·  ì •í™•ë„
        avg_accuracy = np.mean(list(results['scores'].values()))
        results['ensemble_accuracy'] = avg_accuracy
        
        print(f"âœ… ì•™ìƒë¸” í•™ìŠµ ì™„ë£Œ!")
        print(f"   ğŸ† ìµœê³  1H ëª¨ë¸: {best_1h} ({results['scores'][best_1h]:.4f})")
        print(f"   ğŸ† ìµœê³  2H ëª¨ë¸: {best_2h} ({results['scores'][best_2h]:.4f})")
        print(f"   ğŸ† ìµœê³  3H ëª¨ë¸: {best_3h} ({results['scores'][best_3h]:.4f})")
        print(f"   ğŸ¯ ì „ì²´ ì•™ìƒë¸” ì •í™•ë„: {avg_accuracy:.4f}")
        
        return results
    
    def predict_direction(self, df: pd.DataFrame, method='ensemble') -> Dict:
        """ë°©í–¥ì„± ì˜ˆì¸¡"""
        if not self.models:
            print("âŒ ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return {}
        
        # ìµœê·¼ ë°ì´í„°ë¡œ ì˜ˆì¸¡ ì¤€ë¹„
        recent_data = df[self.feature_columns].tail(self.lookback_hours).values.flatten()
        recent_scaled = self.scalers['scaler'].transform([recent_data])
        
        predictions = {}
        confidences = {}
        
        # ê° í˜¸ë¼ì´ì¦Œë³„ ì˜ˆì¸¡
        for horizon in ['1h', '2h', '3h']:
            horizon_models = [k for k in self.models.keys() if horizon in k]
            horizon_preds = []
            
            for model_name in horizon_models:
                pred = self.models[model_name].predict(recent_scaled)[0]
                horizon_preds.append(pred)
            
            # ì•™ìƒë¸” ì˜ˆì¸¡ (í‰ê· )
            if method == 'ensemble' and len(horizon_preds) > 1:
                ensemble_pred = np.mean(horizon_preds)
                predictions[horizon] = 1 if ensemble_pred > 0 else -1
                confidences[horizon] = min(95, max(55, abs(ensemble_pred) * 50 + 50))
            elif horizon_preds:
                predictions[horizon] = 1 if horizon_preds[0] > 0 else -1
                confidences[horizon] = min(95, max(55, abs(horizon_preds[0]) * 50 + 50))
        
        current_price = df['close'].iloc[-1]
        
        result = {
            'current_price': current_price,
            'predictions': predictions,
            'confidences': confidences,
            'ensemble_confidence': np.mean(list(confidences.values())) if confidences else 50,
            'prediction_time': datetime.now().isoformat(),
            'method': method
        }
        
        print(f"ğŸ¯ ë°©í–¥ì„± ì˜ˆì¸¡ ê²°ê³¼:")
        print(f"   í˜„ì¬ê°€: ${current_price:,.2f}")
        for horizon, direction in predictions.items():
            conf = confidences.get(horizon, 50)
            direction_text = "ìƒìŠ¹" if direction == 1 else "í•˜ë½"
            print(f"   {horizon}: {direction_text} (ì‹ ë¢°ë„: {conf:.1f}%)")
        
        return result

class Advanced90PercentSystem:
    """90% ì •í™•ë„ ë„ì „ ë©”ì¸ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.feature_engineer = AdvancedFeatureEngineer()
        self.predictor = DirectionalPredictor()
        self.results = {}
        print("âœ… 90% ì •í™•ë„ ë„ì „ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def run_advanced_backtest(self, df: pd.DataFrame, test_size: int = 300) -> Dict:
        """ê³ ê¸‰ ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print(f"ğŸ† 90% ì •í™•ë„ ë„ì „ ë°±í…ŒìŠ¤íŠ¸ ì‹œì‘!")
        print(f"   í…ŒìŠ¤íŠ¸ ë°ì´í„°: {test_size}ì‹œê°„")
        print("=" * 60)
        
        # 1. ê³ ê¸‰ íŠ¹ì„±ê³µí•™
        print("1ï¸âƒ£ ê³ ê¸‰ íŠ¹ì„±ê³µí•™ ìˆ˜í–‰...")
        df_featured = self.feature_engineer.create_advanced_features(df.copy())
        
        if len(df_featured) < 500:
            print("âŒ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤ (ìµœì†Œ 500í–‰ í•„ìš”)")
            return {}
        
        # 2. í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í• 
        print("2ï¸âƒ£ ë°ì´í„° ë¶„í• ...")
        train_size = len(df_featured) - test_size
        train_df = df_featured.iloc[:train_size]
        test_df = df_featured.iloc[train_size:]
        
        print(f"   ğŸ“Š í•™ìŠµ ë°ì´í„°: {len(train_df)}ì‹œê°„")
        print(f"   ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_df)}ì‹œê°„")
        
        # 3. ë°©í–¥ì„± ì‹œí€€ìŠ¤ ìƒì„± ë° í•™ìŠµ
        print("3ï¸âƒ£ ë°©í–¥ì„± ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ...")
        X_train, y1h_train, y2h_train, y3h_train = self.predictor.create_directional_sequences(train_df)
        
        if len(X_train) < 100:
            print("âŒ í•™ìŠµ ì‹œí€€ìŠ¤ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤")
            return {}
        
        # 4. ì•™ìƒë¸” ëª¨ë¸ í•™ìŠµ
        train_results = self.predictor.train_ensemble_models(X_train, y1h_train, y2h_train, y3h_train)
        
        # 5. í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡
        print("4ï¸âƒ£ í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ ìˆ˜í–‰...")
        predictions = []
        actuals = []
        
        for i in range(len(test_df) - 10):  # ì—¬ìœ ë¶„ í™•ë³´
            # í˜„ì¬ê¹Œì§€ì˜ ë°ì´í„°ë¡œ ì˜ˆì¸¡
            current_data = pd.concat([
                train_df.tail(self.predictor.lookback_hours),
                test_df.iloc[:i+1] 
            ]).tail(len(train_df) + i + 1)
            
            if len(current_data) < self.predictor.lookback_hours + 10:
                continue
            
            # 1ì‹œê°„, 2ì‹œê°„, 3ì‹œê°„ ì˜ˆì¸¡
            pred_result = self.predictor.predict_direction(current_data)
            if pred_result and 'predictions' in pred_result:
                predictions.append(pred_result)
                
                # ì‹¤ì œê°’ ìˆ˜ì§‘ (1ì‹œê°„ í›„ ë°©í–¥ì„±)
                if i + 1 < len(test_df):
                    current_price = test_df['close'].iloc[i]
                    future_price = test_df['close'].iloc[i + 1]
                    actual_direction = 1 if future_price > current_price else -1
                    actuals.append(actual_direction)
                else:
                    break
        
        if len(predictions) < 10:
            print("âŒ ì¶©ë¶„í•œ ì˜ˆì¸¡ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤")
            return train_results
        
        # 6. ì •í™•ë„ ê³„ì‚°
        print("5ï¸âƒ£ ìµœì¢… ì •í™•ë„ ê³„ì‚°...")
        
        # 1ì‹œê°„ ë°©í–¥ì„± ì •í™•ë„
        pred_1h = [p['predictions'].get('1h', 0) for p in predictions]
        actual_1h = actuals[:len(pred_1h)]
        
        if len(pred_1h) == len(actual_1h) and len(pred_1h) > 0:
            direction_accuracy = (np.array(pred_1h) == np.array(actual_1h)).mean() * 100
        else:
            direction_accuracy = 0
        
        # ì‹ ë¢°ë„ í‰ê· 
        avg_confidence = np.mean([p.get('ensemble_confidence', 50) for p in predictions])
        
        # ìµœì¢… ê²°ê³¼
        final_results = {
            **train_results,
            'test_predictions': len(predictions),
            'direction_accuracy': direction_accuracy,
            'avg_confidence': avg_confidence,
            'final_accuracy': direction_accuracy,  # ë°©í–¥ì„±ì´ í•µì‹¬ì´ë¯€ë¡œ
            'system_type': 'Advanced90Percent',
            'feature_count': len(self.feature_engineer.scalers),
            'model_count': len(self.predictor.models)
        }
        
        print("=" * 60)
        print("ğŸ† 90% ì •í™•ë„ ë„ì „ ê²°ê³¼:")
        print(f"   ğŸ“Š í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ ìˆ˜: {len(predictions)}")
        print(f"   ğŸ¯ ë°©í–¥ì„± ì •í™•ë„: {direction_accuracy:.2f}%")
        print(f"   ğŸ”® í‰ê·  ì‹ ë¢°ë„: {avg_confidence:.1f}%")
        
        if direction_accuracy >= 90:
            print("ğŸ‰ 90% ì •í™•ë„ ë‹¬ì„±! ğŸ‰")
        elif direction_accuracy >= 80:
            print("ğŸ”¥ 80%+ ê³ ì„±ëŠ¥ ë‹¬ì„±!")
        elif direction_accuracy >= 70:
            print("âœ… 70%+ ì–‘í˜¸í•œ ì„±ëŠ¥")
        else:
            print("ğŸ“ˆ ì¶”ê°€ ê°œì„  í•„ìš”")
        
        return final_results

def load_advanced_data() -> pd.DataFrame:
    """ê³ ê¸‰ ë°ì´í„° ë¡œë”©"""
    print("ğŸ“ ê³ ê¸‰ ë°ì´í„° ë¡œë”© ì¤‘...")
    
    try:
        data_files = [
            'ai_optimized_3month_data/ai_matrix_complete.csv',
            'complete_indicators_data.csv'
        ]
        
        df = None
        for file in data_files:
            if os.path.exists(file):
                print(f"ğŸ“Š {file} ë¡œë”© ì¤‘...")
                df = pd.read_csv(file)
                break
        
        if df is None:
            print("âŒ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return None
        
        # ê°€ê²© ë° ë³¼ë¥¨ ì»¬ëŸ¼ ë§¤í•‘
        price_candidates = ['close', 'legacy_market_data_avg_price', 'market_avg_price', 'price']
        volume_candidates = ['volume', 'onchain_blockchain_info_network_stats_trade_volume_btc', 'legacy_market_data_total_volume']
        
        price_col = None
        volume_col = None
        
        for col in price_candidates:
            if col in df.columns:
                price_col = col
                break
        
        for col in volume_candidates:
            if col in df.columns:
                volume_col = col
                break
        
        if not price_col:
            print("âŒ ê°€ê²© ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return None
        
        # í‘œì¤€í™”
        if price_col != 'close':
            df['close'] = df[price_col]
        if volume_col and volume_col != 'volume':
            df['volume'] = df[volume_col]
        elif not volume_col:
            df['volume'] = 1000  # ë”ë¯¸ ë³¼ë¥¨
        
        # ì‹œê°„ ì •ë ¬
        if 'timestamp' in df.columns:
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            df = df.sort_values('timestamp')
        
        print(f"âœ… ê³ ê¸‰ ë°ì´í„° ë¡œë”© ì™„ë£Œ: {len(df)}í–‰, {len(df.columns)}ì—´")
        return df
        
    except Exception as e:
        print(f"âŒ ë°ì´í„° ë¡œë”© ì˜¤ë¥˜: {e}")
        return None

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print("ğŸ† ê³ ê¸‰ BTC 90% ì •í™•ë„ ë„ì „ ì‹œìŠ¤í…œ")
    print("   ëª©í‘œ: ë°©í–¥ì„± ì˜ˆì¸¡ 90% ì •í™•ë„ ë‹¬ì„±")
    print("=" * 60)
    
    # ë°ì´í„° ë¡œë”©
    df = load_advanced_data()
    if df is None:
        print("âŒ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨ - ì‹œìŠ¤í…œ ì¢…ë£Œ")
        return
    
    # 90% ë„ì „ ì‹œìŠ¤í…œ ì‹¤í–‰
    system = Advanced90PercentSystem()
    results = system.run_advanced_backtest(df)
    
    if not results:
        print("âŒ ë°±í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
        return
    
    # ê²°ê³¼ ì €ì¥
    results['system_version'] = 'Advanced90Percent_v1.0'
    results['timestamp'] = datetime.now().isoformat()
    
    with open('advanced_90_percent_results.json', 'w') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print("=" * 60)
    print("ğŸ† 90% ë„ì „ ë°±í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print(f"ğŸ¯ ë‹¬ì„± ì •í™•ë„: {results.get('final_accuracy', 0):.2f}%")
    
    if results.get('final_accuracy', 0) >= 90:
        print("ğŸ‰ğŸ‰ğŸ‰ 90% ì •í™•ë„ ë‹¬ì„±! ì„±ê³µ! ğŸ‰ğŸ‰ğŸ‰")
    else:
        print("ğŸš€ 90% ì •í™•ë„ ë„ì „ ì§€ì† - ë” ê³ ë„í™” í•„ìš”")
    
    print("ğŸ“„ ìƒì„¸ ê²°ê³¼: advanced_90_percent_results.json")

if __name__ == "__main__":
    main()