#!/usr/bin/env python3
"""
ğŸŒŠ ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
- Apache Kafka ê¸°ë°˜ ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° ì²˜ë¦¬
- ì‹¤ì‹œê°„ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§
- ì €ì§€ì—° ë°ì´í„° ì „ë‹¬ (<5ì´ˆ)
- ìë™ í’ˆì§ˆ ê²€ì¦ ë° ì´ìƒì¹˜ íƒì§€
- ë°±í”„ë ˆì…” ì œì–´ ë° ì˜¤ë¥˜ ë³µêµ¬
"""

import asyncio
import json
import logging
import time
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Callable, Any
from dataclasses import dataclass, asdict
import hashlib
from pathlib import Path
import queue
import threading
from concurrent.futures import ThreadPoolExecutor

# Apache Kafka
try:
    from kafka import KafkaProducer, KafkaConsumer
    from kafka.errors import KafkaError
    KAFKA_AVAILABLE = True
except ImportError:
    KAFKA_AVAILABLE = False
    print("âš ï¸ Kafka not available, using fallback message queue")

# Redis for caching and state management
import redis

# Data processing
from scipy import stats
from sklearn.preprocessing import RobustScaler
from sklearn.feature_selection import SelectKBest, f_regression

# API clients
import requests
import websocket
import ccxt

# Monitoring
from prometheus_client import Counter, Histogram, Gauge

@dataclass
class MarketData:
    """ì‹œì¥ ë°ì´í„° êµ¬ì¡°"""
    symbol: str
    timestamp: datetime
    price: float
    volume: float
    bid: Optional[float] = None
    ask: Optional[float] = None
    source: str = "unknown"
    metadata: Optional[Dict[str, Any]] = None

@dataclass
class ProcessedFeatures:
    """ì²˜ë¦¬ëœ íŠ¹ì„± ë°ì´í„°"""
    symbol: str
    timestamp: datetime
    features: Dict[str, float]
    technical_indicators: Dict[str, float]
    onchain_metrics: Dict[str, float]
    macro_data: Dict[str, float]
    quality_score: float
    processing_latency_ms: int

class DataQualityValidator:
    """ë°ì´í„° í’ˆì§ˆ ê²€ì¦ê¸°"""
    
    def __init__(self):
        self.price_bounds = {"min": 10000, "max": 200000}  # BTC ê°€ê²© ë²”ìœ„
        self.volume_bounds = {"min": 0, "max": 1e9}  # ê±°ë˜ëŸ‰ ë²”ìœ„
        self.zscore_threshold = 3.0  # ì´ìƒì¹˜ íƒì§€ ì„ê³„ê°’
        self.price_history = queue.deque(maxlen=1000)  # ìµœê·¼ ê°€ê²© ì´ë ¥
        
    def validate_market_data(self, data: MarketData) -> tuple[bool, List[str]]:
        """ì‹œì¥ ë°ì´í„° ê²€ì¦"""
        errors = []
        
        # 1. ê¸°ë³¸ ë²”ìœ„ ê²€ì¦
        if not (self.price_bounds["min"] <= data.price <= self.price_bounds["max"]):
            errors.append(f"Price {data.price} out of bounds")
            
        if not (self.volume_bounds["min"] <= data.volume <= self.volume_bounds["max"]):
            errors.append(f"Volume {data.volume} out of bounds")
            
        # 2. ì‹œê°„ ê²€ì¦ (ë„ˆë¬´ ì˜¤ë˜ëœ ë°ì´í„°)
        time_diff = datetime.now() - data.timestamp
        if time_diff.total_seconds() > 300:  # 5ë¶„ ì´ìƒ ì§€ì—°
            errors.append(f"Data too old: {time_diff}")
            
        # 3. ê°€ê²© ê¸‰ë³€ ê°ì§€
        if len(self.price_history) > 10:
            recent_prices = list(self.price_history)[-10:]
            price_changes = np.diff(recent_prices) / recent_prices[:-1]
            
            if abs(price_changes[-1]) > 0.1:  # 10% ì´ìƒ ê¸‰ë³€
                errors.append(f"Extreme price change: {price_changes[-1]:.2%}")
                
        # 4. Z-score ì´ìƒì¹˜ íƒì§€
        if len(self.price_history) > 30:
            prices_array = np.array(list(self.price_history))
            zscore = abs(stats.zscore(np.append(prices_array, data.price))[-1])
            
            if zscore > self.zscore_threshold:
                errors.append(f"Price Z-score too high: {zscore:.2f}")
                
        # ê°€ê²© ì´ë ¥ ì—…ë°ì´íŠ¸
        self.price_history.append(data.price)
        
        return len(errors) == 0, errors

class RealTimeDataCollector:
    """ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘ê¸°"""
    
    def __init__(self):
        self.setup_logging()
        self.setup_metrics()
        self.exchanges = {}
        self.websocket_connections = {}
        self.data_validator = DataQualityValidator()
        self.setup_exchanges()
        
    def setup_logging(self):
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
    def setup_metrics(self):
        """ë©”íŠ¸ë¦­ ì„¤ì •"""
        self.data_points_collected = Counter(
            'data_points_collected_total',
            'Total data points collected',
            ['source', 'symbol']
        )
        
        self.data_collection_latency = Histogram(
            'data_collection_latency_seconds',
            'Data collection latency',
            ['source']
        )
        
        self.data_quality_score = Gauge(
            'data_quality_score',
            'Data quality score',
            ['source']
        )
        
    def setup_exchanges(self):
        """ê±°ë˜ì†Œ API ì„¤ì •"""
        try:
            # Binance
            self.exchanges['binance'] = ccxt.binance({
                'apiKey': '',  # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ
                'secret': '',
                'sandbox': False,
                'enableRateLimit': True
            })
            
            # Coinbase Pro
            self.exchanges['coinbasepro'] = ccxt.coinbasepro({
                'enableRateLimit': True
            })
            
            self.logger.info(f"Exchanges initialized: {list(self.exchanges.keys())}")
            
        except Exception as e:
            self.logger.error(f"Exchange setup failed: {e}")
            
    async def collect_price_data(self) -> AsyncGenerator[MarketData, None]:
        """ì‹¤ì‹œê°„ ê°€ê²© ë°ì´í„° ìˆ˜ì§‘"""
        while True:
            try:
                for exchange_name, exchange in self.exchanges.items():
                    start_time = time.time()
                    
                    try:
                        # BTC/USDT ê°€ê²© ì¡°íšŒ
                        ticker = exchange.fetch_ticker('BTC/USDT')
                        
                        market_data = MarketData(
                            symbol='BTC/USDT',
                            timestamp=datetime.fromtimestamp(ticker['timestamp'] / 1000),
                            price=ticker['last'],
                            volume=ticker['baseVolume'],
                            bid=ticker['bid'],
                            ask=ticker['ask'],
                            source=exchange_name,
                            metadata={
                                'high': ticker['high'],
                                'low': ticker['low'],
                                'open': ticker['open']
                            }
                        )
                        
                        # ë°ì´í„° ê²€ì¦
                        is_valid, errors = self.data_validator.validate_market_data(market_data)
                        
                        if is_valid:
                            # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
                            self.data_points_collected.labels(
                                source=exchange_name,
                                symbol='BTC/USDT'
                            ).inc()
                            
                            self.data_collection_latency.labels(
                                source=exchange_name
                            ).observe(time.time() - start_time)
                            
                            yield market_data
                            
                        else:
                            self.logger.warning(
                                f"Invalid data from {exchange_name}: {errors}"
                            )
                            
                    except Exception as e:
                        self.logger.error(f"Error collecting from {exchange_name}: {e}")
                        
                await asyncio.sleep(1)  # 1ì´ˆ ê°„ê²©
                
            except Exception as e:
                self.logger.error(f"Collection loop error: {e}")
                await asyncio.sleep(5)

class FeatureProcessor:
    """ì‹¤ì‹œê°„ íŠ¹ì„± ì²˜ë¦¬ê¸°"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.price_buffer = queue.deque(maxlen=1000)  # ìµœê·¼ 1000ê°œ ê°€ê²©
        self.volume_buffer = queue.deque(maxlen=1000)
        self.feature_cache = {}
        self.scaler = RobustScaler()
        
        # ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°ì„ ìœ„í•œ íŒŒë¼ë¯¸í„°
        self.ta_params = {
            'sma_periods': [5, 10, 20, 50, 200],
            'ema_periods': [12, 26, 50],
            'rsi_periods': [14, 21],
            'bb_periods': [20, 50]
        }
        
    def process_features(self, market_data: MarketData) -> ProcessedFeatures:
        """ì‹¤ì‹œê°„ íŠ¹ì„± ì²˜ë¦¬"""
        start_time = time.time()
        
        # ë²„í¼ ì—…ë°ì´íŠ¸
        self.price_buffer.append(market_data.price)
        self.volume_buffer.append(market_data.volume)
        
        # íŠ¹ì„± ê³„ì‚°
        features = {}
        technical_indicators = {}
        
        try:
            # 1. ê¸°ë³¸ íŠ¹ì„±
            features.update(self._calculate_basic_features(market_data))
            
            # 2. ê¸°ìˆ ì  ì§€í‘œ
            technical_indicators.update(self._calculate_technical_indicators())
            
            # 3. ê±°ë˜ëŸ‰ ì§€í‘œ
            features.update(self._calculate_volume_features())
            
            # 4. ê°€ê²© íŒ¨í„´
            features.update(self._calculate_price_patterns())
            
            # 5. ë³€ë™ì„± ì§€í‘œ
            features.update(self._calculate_volatility_features())
            
            # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
            quality_score = self._calculate_quality_score(features, technical_indicators)
            
            processing_time_ms = int((time.time() - start_time) * 1000)
            
            return ProcessedFeatures(
                symbol=market_data.symbol,
                timestamp=market_data.timestamp,
                features=features,
                technical_indicators=technical_indicators,
                onchain_metrics={},  # ë³„ë„ ì²˜ë¦¬
                macro_data={},  # ë³„ë„ ì²˜ë¦¬
                quality_score=quality_score,
                processing_latency_ms=processing_time_ms
            )
            
        except Exception as e:
            self.logger.error(f"Feature processing failed: {e}")
            return self._create_fallback_features(market_data)
            
    def _calculate_basic_features(self, market_data: MarketData) -> Dict[str, float]:
        """ê¸°ë³¸ íŠ¹ì„± ê³„ì‚°"""
        features = {
            'price': market_data.price,
            'volume': market_data.volume,
            'timestamp_hour': market_data.timestamp.hour,
            'timestamp_day_of_week': market_data.timestamp.weekday(),
            'timestamp_minute': market_data.timestamp.minute
        }
        
        if market_data.bid and market_data.ask:
            features['spread'] = market_data.ask - market_data.bid
            features['spread_pct'] = features['spread'] / market_data.price
            features['mid_price'] = (market_data.bid + market_data.ask) / 2
            
        return features
        
    def _calculate_technical_indicators(self) -> Dict[str, float]:
        """ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°"""
        if len(self.price_buffer) < 50:
            return {}
            
        prices = np.array(list(self.price_buffer))
        indicators = {}
        
        try:
            # ì´ë™í‰ê· 
            for period in self.ta_params['sma_periods']:
                if len(prices) >= period:
                    indicators[f'sma_{period}'] = np.mean(prices[-period:])
                    
            # ì§€ìˆ˜ì´ë™í‰ê· 
            for period in self.ta_params['ema_periods']:
                if len(prices) >= period:
                    alpha = 2 / (period + 1)
                    ema = prices[0]
                    for price in prices[1:]:
                        ema = alpha * price + (1 - alpha) * ema
                    indicators[f'ema_{period}'] = ema
                    
            # RSI
            for period in self.ta_params['rsi_periods']:
                if len(prices) >= period + 1:
                    rsi = self._calculate_rsi(prices, period)
                    indicators[f'rsi_{period}'] = rsi
                    
            # ë³¼ë¦°ì € ë°´ë“œ
            for period in self.ta_params['bb_periods']:
                if len(prices) >= period:
                    sma = np.mean(prices[-period:])
                    std = np.std(prices[-period:])
                    indicators[f'bb_upper_{period}'] = sma + 2 * std
                    indicators[f'bb_lower_{period}'] = sma - 2 * std
                    indicators[f'bb_position_{period}'] = (prices[-1] - indicators[f'bb_lower_{period}']) / (indicators[f'bb_upper_{period}'] - indicators[f'bb_lower_{period}'])
                    
        except Exception as e:
            self.logger.error(f"Technical indicator calculation failed: {e}")
            
        return indicators
        
    def _calculate_rsi(self, prices: np.ndarray, period: int) -> float:
        """RSI ê³„ì‚°"""
        deltas = np.diff(prices)
        gains = deltas.copy()
        losses = deltas.copy()
        gains[gains < 0] = 0
        losses[losses > 0] = 0
        losses = -losses
        
        if len(gains) >= period:
            avg_gain = np.mean(gains[-period:])
            avg_loss = np.mean(losses[-period:])
            
            if avg_loss == 0:
                return 100
                
            rs = avg_gain / avg_loss
            return 100 - (100 / (1 + rs))
            
        return 50  # ì¤‘ë¦½ê°’
        
    def _calculate_volume_features(self) -> Dict[str, float]:
        """ê±°ë˜ëŸ‰ íŠ¹ì„± ê³„ì‚°"""
        if len(self.volume_buffer) < 20:
            return {}
            
        volumes = np.array(list(self.volume_buffer))
        
        return {
            'volume_sma_20': np.mean(volumes[-20:]),
            'volume_ratio': volumes[-1] / np.mean(volumes[-20:]) if np.mean(volumes[-20:]) > 0 else 1,
            'volume_std_20': np.std(volumes[-20:]),
            'volume_trend': (volumes[-1] - volumes[-5]) / volumes[-5] if volumes[-5] > 0 else 0
        }
        
    def _calculate_price_patterns(self) -> Dict[str, float]:
        """ê°€ê²© íŒ¨í„´ íŠ¹ì„±"""
        if len(self.price_buffer) < 10:
            return {}
            
        prices = np.array(list(self.price_buffer))
        
        return {
            'price_change_1': (prices[-1] - prices[-2]) / prices[-2] if len(prices) > 1 else 0,
            'price_change_5': (prices[-1] - prices[-6]) / prices[-6] if len(prices) > 5 else 0,
            'price_change_20': (prices[-1] - prices[-21]) / prices[-21] if len(prices) > 20 else 0,
            'price_momentum': np.corrcoef(range(min(10, len(prices))), prices[-min(10, len(prices)):]))[0, 1] if len(prices) > 2 else 0
        }
        
    def _calculate_volatility_features(self) -> Dict[str, float]:
        """ë³€ë™ì„± íŠ¹ì„±"""
        if len(self.price_buffer) < 20:
            return {}
            
        prices = np.array(list(self.price_buffer))
        returns = np.diff(prices) / prices[:-1]
        
        return {
            'volatility_1h': np.std(returns[-60:]) if len(returns) >= 60 else np.std(returns),
            'volatility_4h': np.std(returns[-240:]) if len(returns) >= 240 else np.std(returns),
            'volatility_24h': np.std(returns) if len(returns) >= 1440 else np.std(returns),
            'volatility_rank': stats.percentileofscore(returns, returns[-1]) if len(returns) > 10 else 50
        }
        
    def _calculate_quality_score(self, features: Dict[str, float], indicators: Dict[str, float]) -> float:
        """ë°ì´í„° í’ˆì§ˆ ì ìˆ˜"""
        try:
            # íŠ¹ì„± ì™„ì„±ë„
            expected_features = 30
            actual_features = len(features) + len(indicators)
            completeness = min(1.0, actual_features / expected_features)
            
            # NaN ê°’ ë¹„ìœ¨
            all_values = list(features.values()) + list(indicators.values())
            nan_ratio = sum(1 for v in all_values if np.isnan(v) or np.isinf(v)) / len(all_values) if all_values else 0
            
            # ìµœì¢… í’ˆì§ˆ ì ìˆ˜
            quality = completeness * (1 - nan_ratio) * 0.9  # ìµœëŒ€ 90%
            
            return max(0.0, min(1.0, quality))
            
        except Exception:
            return 0.5
            
    def _create_fallback_features(self, market_data: MarketData) -> ProcessedFeatures:
        """í´ë°± íŠ¹ì„± ìƒì„±"""
        return ProcessedFeatures(
            symbol=market_data.symbol,
            timestamp=market_data.timestamp,
            features={'price': market_data.price, 'volume': market_data.volume},
            technical_indicators={},
            onchain_metrics={},
            macro_data={},
            quality_score=0.3,
            processing_latency_ms=0
        )

class StreamProcessor:
    """ìŠ¤íŠ¸ë¦¼ ë°ì´í„° ì²˜ë¦¬ ì—”ì§„"""
    
    def __init__(self, kafka_config: Dict[str, str] = None):
        self.kafka_config = kafka_config or {
            'bootstrap_servers': 'localhost:9092',
            'client_id': 'btc-stream-processor'
        }
        self.setup_logging()
        self.setup_kafka()
        self.setup_redis()
        
        self.data_collector = RealTimeDataCollector()
        self.feature_processor = FeatureProcessor()
        
        self.processing_stats = {
            'messages_processed': 0,
            'errors': 0,
            'start_time': time.time()
        }
        
    def setup_logging(self):
        self.logger = logging.getLogger(__name__)
        
    def setup_kafka(self):
        """Kafka ì„¤ì •"""
        if not KAFKA_AVAILABLE:
            self.producer = None
            self.consumer = None
            self.logger.warning("Kafka not available, using fallback")
            return
            
        try:
            self.producer = KafkaProducer(
                **self.kafka_config,
                value_serializer=lambda v: json.dumps(v).encode('utf-8'),
                compression_type='gzip',
                batch_size=16384,
                linger_ms=100  # ë°°ì¹˜ ìµœì í™”
            )
            
            self.consumer = KafkaConsumer(
                'btc-market-data',
                **self.kafka_config,
                value_deserializer=lambda m: json.loads(m.decode('utf-8')),
                auto_offset_reset='latest',
                enable_auto_commit=True,
                group_id='btc-feature-processor'
            )
            
            self.logger.info("Kafka setup completed")
            
        except Exception as e:
            self.logger.error(f"Kafka setup failed: {e}")
            self.producer = None
            self.consumer = None
            
    def setup_redis(self):
        """Redis ì„¤ì •"""
        try:
            self.redis_client = redis.Redis(
                host='localhost',
                port=6379,
                db=0,
                decode_responses=True,
                socket_connect_timeout=5,
                socket_timeout=5
            )
            
            # ì—°ê²° í…ŒìŠ¤íŠ¸
            self.redis_client.ping()
            self.logger.info("Redis setup completed")
            
        except Exception as e:
            self.logger.error(f"Redis setup failed: {e}")
            self.redis_client = None
            
    async def run_pipeline(self):
        """ì‹¤ì‹œê°„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        self.logger.info("Starting real-time data pipeline")
        
        try:
            # ë™ì‹œ ì‹¤í–‰: ë°ì´í„° ìˆ˜ì§‘, ì²˜ë¦¬, ì „ì†¡
            await asyncio.gather(
                self.data_collection_loop(),
                self.feature_processing_loop(),
                self.monitoring_loop(),
                return_exceptions=True
            )
            
        except Exception as e:
            self.logger.error(f"Pipeline failed: {e}")
            raise
            
    async def data_collection_loop(self):
        """ë°ì´í„° ìˆ˜ì§‘ ë£¨í”„"""
        self.logger.info("Starting data collection loop")
        
        async for market_data in self.data_collector.collect_price_data():
            try:
                # Kafkaë¡œ ì›ì‹œ ë°ì´í„° ì „ì†¡
                if self.producer:
                    message = {
                        'type': 'market_data',
                        'data': asdict(market_data),
                        'timestamp': time.time()
                    }
                    
                    self.producer.send('btc-market-data', message)
                    
                # Redis ìºì‹œì— ìµœì‹  ë°ì´í„° ì €ì¥
                if self.redis_client:
                    cache_key = f"market_data:{market_data.symbol}"
                    cache_data = {
                        'price': market_data.price,
                        'volume': market_data.volume,
                        'timestamp': market_data.timestamp.isoformat(),
                        'source': market_data.source
                    }
                    
                    self.redis_client.setex(
                        cache_key, 
                        300,  # 5ë¶„ TTL
                        json.dumps(cache_data)
                    )
                    
            except Exception as e:
                self.logger.error(f"Data collection error: {e}")
                
    async def feature_processing_loop(self):
        """íŠ¹ì„± ì²˜ë¦¬ ë£¨í”„"""
        self.logger.info("Starting feature processing loop")
        
        if not self.consumer:
            self.logger.warning("No Kafka consumer, skipping feature processing")
            return
            
        try:
            for message in self.consumer:
                try:
                    # ë©”ì‹œì§€ íŒŒì‹±
                    data = message.value
                    
                    if data['type'] == 'market_data':
                        # MarketData ê°ì²´ ì¬êµ¬ì„±
                        market_data_dict = data['data']
                        market_data_dict['timestamp'] = datetime.fromisoformat(
                            market_data_dict['timestamp']
                        )
                        
                        market_data = MarketData(**market_data_dict)
                        
                        # íŠ¹ì„± ì²˜ë¦¬
                        processed_features = self.feature_processor.process_features(market_data)
                        
                        # ì²˜ë¦¬ëœ íŠ¹ì„±ì„ Redisì— ì €ì¥
                        await self.cache_processed_features(processed_features)
                        
                        # ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ì „ì†¡
                        await self.send_processed_features(processed_features)
                        
                        self.processing_stats['messages_processed'] += 1
                        
                except Exception as e:
                    self.logger.error(f"Feature processing error: {e}")
                    self.processing_stats['errors'] += 1
                    
        except Exception as e:
            self.logger.error(f"Feature processing loop failed: {e}")
            
    async def cache_processed_features(self, features: ProcessedFeatures):
        """ì²˜ë¦¬ëœ íŠ¹ì„±ì„ ìºì‹œì— ì €ì¥"""
        if not self.redis_client:
            return
            
        try:
            cache_key = f"features:{features.symbol}"
            cache_data = {
                'timestamp': features.timestamp.isoformat(),
                'features': features.features,
                'technical_indicators': features.technical_indicators,
                'quality_score': features.quality_score,
                'processing_latency_ms': features.processing_latency_ms
            }
            
            # ìµœì‹  íŠ¹ì„± ì €ì¥ (1ì‹œê°„ TTL)
            self.redis_client.setex(
                cache_key,
                3600,
                json.dumps(cache_data, default=str)
            )
            
            # ì‹œê³„ì—´ íŠ¹ì„± ì €ì¥ (24ì‹œê°„ ë³´ê´€)
            timeseries_key = f"features_timeseries:{features.symbol}"
            self.redis_client.lpush(timeseries_key, json.dumps(cache_data, default=str))
            self.redis_client.ltrim(timeseries_key, 0, 1440)  # ìµœëŒ€ 1440ê°œ (24ì‹œê°„)
            self.redis_client.expire(timeseries_key, 86400)
            
        except Exception as e:
            self.logger.error(f"Feature caching failed: {e}")
            
    async def send_processed_features(self, features: ProcessedFeatures):
        """ì²˜ë¦¬ëœ íŠ¹ì„±ì„ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ì „ì†¡"""
        if self.producer:
            try:
                message = {
                    'type': 'processed_features',
                    'data': asdict(features),
                    'timestamp': time.time()
                }
                
                self.producer.send('btc-processed-features', message)
                
            except Exception as e:
                self.logger.error(f"Feature sending failed: {e}")
                
    async def monitoring_loop(self):
        """ëª¨ë‹ˆí„°ë§ ë£¨í”„"""
        while True:
            try:
                await asyncio.sleep(60)  # 1ë¶„ë§ˆë‹¤ ëª¨ë‹ˆí„°ë§
                
                # ì²˜ë¦¬ í†µê³„
                runtime = time.time() - self.processing_stats['start_time']
                messages_per_second = self.processing_stats['messages_processed'] / runtime if runtime > 0 else 0
                error_rate = self.processing_stats['errors'] / self.processing_stats['messages_processed'] if self.processing_stats['messages_processed'] > 0 else 0
                
                self.logger.info(
                    f"Pipeline stats: {self.processing_stats['messages_processed']} messages, "
                    f"{messages_per_second:.2f} msg/s, {error_rate:.2%} error rate"
                )
                
                # Redis ì—°ê²° ìƒíƒœ í™•ì¸
                if self.redis_client:
                    try:
                        self.redis_client.ping()
                    except Exception as e:
                        self.logger.error(f"Redis connection failed: {e}")
                        
                # Kafka ì—°ê²° ìƒíƒœ í™•ì¸
                if self.producer:
                    try:
                        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€
                        self.producer.send('btc-health-check', {'timestamp': time.time()})
                    except Exception as e:
                        self.logger.error(f"Kafka producer failed: {e}")
                        
            except Exception as e:
                self.logger.error(f"Monitoring loop error: {e}")

class DataPipelineManager:
    """ë°ì´í„° íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì €"""
    
    def __init__(self, config_path: str = None):
        self.config_path = config_path
        self.stream_processor = StreamProcessor()
        self.setup_logging()
        
    def setup_logging(self):
        self.logger = logging.getLogger(__name__)
        
    async def start_pipeline(self):
        """íŒŒì´í”„ë¼ì¸ ì‹œì‘"""
        self.logger.info("ğŸŒŠ Starting real-time data pipeline")
        
        try:
            await self.stream_processor.run_pipeline()
            
        except Exception as e:
            self.logger.error(f"Pipeline startup failed: {e}")
            raise
            
    def stop_pipeline(self):
        """íŒŒì´í”„ë¼ì¸ ì¤‘ì§€"""
        self.logger.info("Stopping data pipeline")
        
        if hasattr(self.stream_processor, 'producer') and self.stream_processor.producer:
            self.stream_processor.producer.close()
            
        if hasattr(self.stream_processor, 'consumer') and self.stream_processor.consumer:
            self.stream_processor.consumer.close()

if __name__ == "__main__":
    # ì‹¤ì‹œê°„ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    async def main():
        manager = DataPipelineManager()
        
        try:
            await manager.start_pipeline()
        except KeyboardInterrupt:
            print("\nğŸ›‘ Pipeline stopped by user")
        except Exception as e:
            print(f"âŒ Pipeline failed: {e}")
        finally:
            manager.stop_pipeline()
            
    asyncio.run(main())