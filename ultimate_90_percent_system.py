"""
üèÜ Í∂ÅÍ∑πÏùò 90%+ Ï†ïÌôïÎèÑ ÎπÑÌä∏ÏΩîÏù∏ ÏòàÏ∏° ÏãúÏä§ÌÖú
ÏµúÏ≤®Îã® Îî•Îü¨Îãù ÏïÑÌÇ§ÌÖçÏ≤òÏôÄ Í≥†Í∏â ÌäπÏÑ± ÏóîÏßÄÎãàÏñ¥ÎßÅÏùÑ Í≤∞Ìï©Ìïú ÏôÑÏ†ÑÏ≤¥ ÏãúÏä§ÌÖú

ÌÜµÌï© Íµ¨ÏÑ±ÏöîÏÜå:
1. Temporal Fusion Transformer (Helformer ÌÜµÌï©)
2. CNN-LSTM ÌïòÏù¥Î∏åÎ¶¨Îìú ÏïÑÌÇ§ÌÖçÏ≤ò  
3. 100+ Í≥†Í∏â ÌäπÏÑ± ÏóîÏßÄÎãàÏñ¥ÎßÅ
4. ÎèôÏ†Å ÏïôÏÉÅÎ∏î ÏãúÏä§ÌÖú
5. Bayesian ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÏµúÏ†ÅÌôî
6. Conformal Prediction Î∂àÌôïÏã§ÏÑ± Ï†ïÎüâÌôî
7. Ï†ÅÏùëÏ†Å ÌïôÏäµ Î∞è Ïû¨ÌõàÎ†®
8. Ïã§ÏãúÍ∞Ñ ÏÑ±Îä• Î™®ÎãàÌÑ∞ÎßÅ

Î™©Ìëú: 90% Ïù¥ÏÉÅÏùò ÏòàÏ∏° Ï†ïÌôïÎèÑ Îã¨ÏÑ±
"""

import os
import sys
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import warnings
warnings.filterwarnings('ignore')

# ÌîÑÎ°úÏ†ùÌä∏ Î™®Îìà import
sys.path.append('/Users/parkyoungjun/Desktop/BTC_Analysis_System')
from advanced_90_percent_predictor import (
    Advanced90PercentPredictor, AdvancedBTCDataset, 
    HoltWintersIntegrator, TemporalFusionTransformer, CNNLSTMHybrid
)
from advanced_feature_engineering import (
    MultiScaleTemporalFeatures, MarketMicrostructureFeatures,
    CrossAssetCorrelationFeatures, OnChainAnalysisFeatures,
    BehavioralFinanceFeatures, MarketRegimeDetector,
    AdvancedFeatureSelector
)
from advanced_ensemble_optimizer import (
    AdvancedEnsembleSystem, HyperparameterOptimizer,
    DynamicEnsembleWeighting, ConformalPredictor
)

from typing import Dict, List, Tuple, Optional, Any
import logging
import json
import pickle
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
import seaborn as sns
import sqlite3
from sklearn.metrics import mean_absolute_percentage_error, r2_score
from sklearn.preprocessing import StandardScaler

# Î°úÍπÖ ÏÑ§Ï†ï
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class Ultimate90PercentSystem:
    """
    Í∂ÅÍ∑πÏùò 90%+ Ï†ïÌôïÎèÑ ÎπÑÌä∏ÏΩîÏù∏ ÏòàÏ∏° ÏãúÏä§ÌÖú
    Î™®Îì† ÏµúÏ≤®Îã® Í∏∞Ïà†ÏùÑ ÌÜµÌï©Ìïú ÏôÑÏ†ÑÏ≤¥
    """
    def __init__(self, target_accuracy: float = 0.90):
        self.target_accuracy = target_accuracy
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        logger.info(f"üöÄ ÏãúÏä§ÌÖú Ï¥àÍ∏∞Ìôî - Î™©Ìëú Ï†ïÌôïÎèÑ: {target_accuracy*100}%, ÎîîÎ∞îÏù¥Ïä§: {self.device}")
        
        # Íµ¨ÏÑ± ÏöîÏÜå Ï¥àÍ∏∞Ìôî
        self.feature_engineers = {
            'multiscale': MultiScaleTemporalFeatures(),
            'microstructure': MarketMicrostructureFeatures(),
            'cross_asset': CrossAssetCorrelationFeatures(),
            'onchain': OnChainAnalysisFeatures(),
            'behavioral': BehavioralFinanceFeatures(),
            'regime': MarketRegimeDetector()
        }
        
        self.feature_selector = AdvancedFeatureSelector(target_features=100)
        self.hyperopt = HyperparameterOptimizer(n_trials=50, timeout=1800)
        self.ensemble_system = None
        
        # Îç∞Ïù¥ÌÑ∞ Í¥ÄÎ¶¨
        self.processed_data = None
        self.selected_features = None
        self.scalers = {}
        
        # ÏÑ±Îä• Ï∂îÏ†Å
        self.accuracy_history = []
        self.best_accuracy = 0.0
        self.best_model_state = None
        
        # Í≤∞Í≥º Ï†ÄÏû•
        self.results = {
            'training_history': [],
            'validation_results': [],
            'test_results': {},
            'model_performance': {},
            'feature_importance': {},
            'system_config': {}
        }
        
    def load_and_prepare_data(self, data_path: str) -> pd.DataFrame:
        """
        Îç∞Ïù¥ÌÑ∞ Î°úÎìú Î∞è Ï†ÑÏ≤òÎ¶¨
        """
        logger.info(f"üìä Îç∞Ïù¥ÌÑ∞ Î°úÎìú: {data_path}")
        
        try:
            # CSV ÌååÏùº Î°úÎìú
            df = pd.read_csv(data_path, index_col=0, parse_dates=True)
            logger.info(f"ÏõêÎ≥∏ Îç∞Ïù¥ÌÑ∞ ÌÅ¨Í∏∞: {df.shape}")
            
            # ÌïÑÏàò Ïª¨Îüº ÌôïÏù∏
            required_columns = ['open', 'high', 'low', 'close', 'volume']
            missing_columns = [col for col in required_columns if col not in df.columns]
            if missing_columns:
                raise ValueError(f"ÌïÑÏàò Ïª¨Îüº ÎàÑÎùΩ: {missing_columns}")
            
            # Îç∞Ïù¥ÌÑ∞ ÌíàÏßà Í≤ÄÏÇ¨
            df = self._clean_data(df)
            
            # Í∏∞Ï°¥ ÏßÄÌëúÎì§ÎèÑ Ìè¨Ìï® (6Í∞úÏõî Î∞±ÌïÑ Îç∞Ïù¥ÌÑ∞Ïùò 100+ ÏßÄÌëúÎì§)
            logger.info(f"Ï†ÑÏ≤òÎ¶¨ ÌõÑ Îç∞Ïù¥ÌÑ∞ ÌÅ¨Í∏∞: {df.shape}")
            logger.info(f"ÏÇ¨Ïö© Í∞ÄÎä•Ìïú ÏßÄÌëú Ïàò: {len(df.columns)}")
            
            self.processed_data = df
            return df
            
        except Exception as e:
            logger.error(f"‚ùå Îç∞Ïù¥ÌÑ∞ Î°úÎìú Ïã§Ìå®: {e}")
            raise
            
    def _clean_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Îç∞Ïù¥ÌÑ∞ Ï†ïÏ†ú
        """
        logger.info("üßπ Îç∞Ïù¥ÌÑ∞ Ï†ïÏ†ú Ï§ë...")
        
        # Í≤∞Ï∏°Í∞í Ï≤òÎ¶¨
        initial_shape = df.shape
        
        # Forward fill ÌõÑ backward fill
        df = df.fillna(method='ffill').fillna(method='bfill')
        
        # Ïó¨Ï†ÑÌûà NaNÏù¥ ÏûàÎäî Ïª¨ÎüºÎì§ÏùÄ 0ÏúºÎ°ú Ï±ÑÏõÄ (Ïã†Ï§ëÌïòÍ≤å)
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        df[numeric_columns] = df[numeric_columns].fillna(0)
        
        # Í∑πÍ∞í Ï≤òÎ¶¨ (IQR Î∞©Î≤ï)
        for col in ['open', 'high', 'low', 'close', 'volume']:
            if col in df.columns:
                Q1 = df[col].quantile(0.25)
                Q3 = df[col].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 3 * IQR
                upper_bound = Q3 + 3 * IQR
                
                # Í∑πÍ∞í ÌÅ¥Î¶¨Ìïë (ÏôÑÏ†Ñ Ï†úÍ±∞Î≥¥Îã§Îäî ÌÅ¥Î¶¨Ìïë)
                df[col] = df[col].clip(lower_bound, upper_bound)
        
        logger.info(f"Ï†ïÏ†ú Ï†ÑÌõÑ: {initial_shape} ‚Üí {df.shape}")
        return df
        
    def extract_all_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Î™®Îì† Í≥†Í∏â ÌäπÏÑ± Ï∂îÏ∂ú
        """
        logger.info("üî¨ Í≥†Í∏â ÌäπÏÑ± ÏóîÏßÄÎãàÏñ¥ÎßÅ ÏãúÏûë...")
        
        features_df = df.copy()
        initial_features = len(features_df.columns)
        
        # 1. Îã§Ï§ë Ïä§ÏºÄÏùº ÏãúÍ≥ÑÏó¥ ÌäπÏÑ±
        logger.info("üìà Îã§Ï§ë Ïä§ÏºÄÏùº ÏãúÍ≥ÑÏó¥ ÌäπÏÑ± Ï∂îÏ∂ú...")
        multiscale_features = self.feature_engineers['multiscale'].extract_multiscale_features(df)
        features_df = self._merge_features(features_df, multiscale_features, 'Îã§Ï§ëÏä§ÏºÄÏùº')
        
        # 2. ÎßàÏºì ÎßàÏù¥ÌÅ¨Î°úÏä§Ìä∏Îü≠Ï≤ò ÌäπÏÑ±
        logger.info("üè™ ÎßàÏºì ÎßàÏù¥ÌÅ¨Î°úÏä§Ìä∏Îü≠Ï≤ò ÌäπÏÑ± Ï∂îÏ∂ú...")
        microstructure_features = self.feature_engineers['microstructure'].extract_microstructure_features(df)
        features_df = self._merge_features(features_df, microstructure_features, 'ÎßàÏù¥ÌÅ¨Î°úÏä§Ìä∏Îü≠Ï≤ò')
        
        # 3. ÌñâÎèô Í∏àÏúµÌïô ÌäπÏÑ±
        logger.info("üß† ÌñâÎèô Í∏àÏúµÌïô ÌäπÏÑ± Ï∂îÏ∂ú...")
        behavioral_features = self.feature_engineers['behavioral'].extract_behavioral_features(df)
        features_df = self._merge_features(features_df, behavioral_features, 'ÌñâÎèôÍ∏àÏúµ')
        
        # 4. ÏãúÏû• Ï≤¥Ï†ú ÌäπÏÑ±
        logger.info("üìä ÏãúÏû• Ï≤¥Ï†ú Í∞êÏßÄ ÌäπÏÑ± Ï∂îÏ∂ú...")
        regime_features = self.feature_engineers['regime'].extract_regime_features(df)
        features_df = self._merge_features(features_df, regime_features, 'ÏãúÏû•Ï≤¥Ï†ú')
        
        # 5. ÌÅ¨Î°úÏä§ ÏûêÏÇ∞ ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ (Îß§ÌÅ¨Î°ú Îç∞Ïù¥ÌÑ∞Í∞Ä ÏûàÎã§Î©¥)
        try:
            macro_data = self._load_macro_data()
            if macro_data:
                logger.info("üåç ÌÅ¨Î°úÏä§ ÏûêÏÇ∞ ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ ÌäπÏÑ± Ï∂îÏ∂ú...")
                cross_asset_features = self.feature_engineers['cross_asset'].extract_correlation_features(df, macro_data)
                features_df = self._merge_features(features_df, cross_asset_features, 'ÌÅ¨Î°úÏä§ÏûêÏÇ∞')
        except Exception as e:
            logger.warning(f"ÌÅ¨Î°úÏä§ ÏûêÏÇ∞ ÌäπÏÑ± Ï∂îÏ∂ú Ïã§Ìå®: {e}")
        
        # 6. Ïò®Ï≤¥Ïù∏ Î∂ÑÏÑù ÌäπÏÑ± (Ïò®Ï≤¥Ïù∏ Îç∞Ïù¥ÌÑ∞Í∞Ä ÏûàÎã§Î©¥)
        try:
            onchain_data = self._load_onchain_data()
            if onchain_data:
                logger.info("‚õìÔ∏è Ïò®Ï≤¥Ïù∏ Î∂ÑÏÑù ÌäπÏÑ± Ï∂îÏ∂ú...")
                onchain_features = self.feature_engineers['onchain'].extract_onchain_features(onchain_data)
                features_df = self._merge_features(features_df, onchain_features, 'Ïò®Ï≤¥Ïù∏')
        except Exception as e:
            logger.warning(f"Ïò®Ï≤¥Ïù∏ ÌäπÏÑ± Ï∂îÏ∂ú Ïã§Ìå®: {e}")
        
        final_features = len(features_df.columns)
        logger.info(f"‚úÖ ÌäπÏÑ± ÏóîÏßÄÎãàÏñ¥ÎßÅ ÏôÑÎ£å: {initial_features} ‚Üí {final_features} (+{final_features - initial_features})")
        
        return features_df
    
    def _merge_features(self, main_df: pd.DataFrame, feature_df: pd.DataFrame, feature_type: str) -> pd.DataFrame:
        """
        ÌäπÏÑ± Î≥ëÌï©
        """
        new_features = []
        for col in feature_df.columns:
            if col not in main_df.columns:
                main_df[col] = feature_df[col]
                new_features.append(col)
        
        logger.info(f"  ‚Ä¢ {feature_type}: {len(new_features)}Í∞ú ÌäπÏÑ± Ï∂îÍ∞Ä")
        return main_df
    
    def _load_macro_data(self) -> Optional[Dict[str, pd.DataFrame]]:
        """
        Îß§ÌÅ¨Î°ú Í≤ΩÏ†ú Îç∞Ïù¥ÌÑ∞ Î°úÎìú
        """
        try:
            macro_path = "/Users/parkyoungjun/Desktop/BTC_Analysis_System/complete_historical_6month_data"
            if not os.path.exists(macro_path):
                return None
                
            macro_data = {}
            for filename in os.listdir(macro_path):
                if filename.startswith('macro_') and filename.endswith('.csv'):
                    asset_code = filename.replace('macro_', '').replace('_hourly.csv', '')
                    filepath = os.path.join(macro_path, filename)
                    macro_data[asset_code.upper()] = pd.read_csv(filepath, index_col=0, parse_dates=True)
            
            return macro_data if macro_data else None
            
        except Exception as e:
            logger.warning(f"Îß§ÌÅ¨Î°ú Îç∞Ïù¥ÌÑ∞ Î°úÎìú Ïã§Ìå®: {e}")
            return None
    
    def _load_onchain_data(self) -> Optional[Dict[str, pd.DataFrame]]:
        """
        Ïò®Ï≤¥Ïù∏ Îç∞Ïù¥ÌÑ∞ Î°úÎìú
        """
        try:
            onchain_path = "/Users/parkyoungjun/Desktop/BTC_Analysis_System/complete_historical_6month_data"
            if not os.path.exists(onchain_path):
                return None
                
            onchain_data = {}
            for filename in os.listdir(onchain_path):
                if filename.startswith('onchain_') and filename.endswith('.csv'):
                    metric_name = filename.replace('onchain_', '').replace('_hourly.csv', '')
                    filepath = os.path.join(onchain_path, filename)
                    onchain_data[metric_name] = pd.read_csv(filepath, index_col=0, parse_dates=True)
            
            return onchain_data if onchain_data else None
            
        except Exception as e:
            logger.warning(f"Ïò®Ï≤¥Ïù∏ Îç∞Ïù¥ÌÑ∞ Î°úÎìú Ïã§Ìå®: {e}")
            return None
    
    def select_optimal_features(self, df: pd.DataFrame, target_col: str = 'close') -> List[str]:
        """
        ÏµúÏ†Å ÌäπÏÑ± ÏÑ†ÌÉù
        """
        logger.info("üéØ ÏµúÏ†Å ÌäπÏÑ± ÏÑ†ÌÉù ÏãúÏûë...")
        
        # ÌÉÄÍ≤ü ÏÉùÏÑ± (24ÏãúÍ∞Ñ ÌõÑ ÏàòÏùµÎ•†)
        target = df[target_col].pct_change(24).shift(-24)
        
        # Í≥µÌÜµ Ïù∏Îç±Ïä§ Ï∞æÍ∏∞
        common_idx = df.index.intersection(target.dropna().index)
        
        # ÌäπÏÑ±Í≥º ÌÉÄÍ≤ü Ï†ïÎ†¨
        X = df.loc[common_idx].drop(target_col, axis=1, errors='ignore')
        y = target.loc[common_idx]
        
        # ÏàòÏπòÌòï Ïª¨ÎüºÎßå ÏÑ†ÌÉù
        numeric_columns = X.select_dtypes(include=[np.number]).columns
        X = X[numeric_columns]
        
        # NaN Ï†úÍ±∞
        mask = ~(X.isnull().any(axis=1) | y.isnull())
        X = X[mask]
        y = y[mask]
        
        logger.info(f"ÌäπÏÑ± ÏÑ†ÌÉù ÎåÄÏÉÅ: {X.shape[1]}Í∞ú ÌäπÏÑ±, {len(X)}Í∞ú ÏÉòÌîå")
        
        if len(X) < 100:
            logger.warning("Ï∂©Î∂ÑÌïú Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏñ¥ Î™®Îì† ÌäπÏÑ±ÏùÑ ÏÇ¨Ïö©Ìï©ÎãàÎã§.")
            self.selected_features = X.columns.tolist()
            return self.selected_features
        
        # Í≥†Í∏â ÌäπÏÑ± ÏÑ†ÌÉù
        self.selected_features = self.feature_selector.select_features(
            X, y, methods=['mutual_info', 'tree_based', 'correlation']
        )
        
        logger.info(f"‚úÖ ÏµúÏ†Å ÌäπÏÑ± ÏÑ†ÌÉù ÏôÑÎ£å: {len(self.selected_features)}Í∞ú")
        
        # ÌäπÏÑ± Ï§ëÏöîÎèÑ Ï†ÄÏû•
        self.results['feature_importance'] = {
            'selected_features': self.selected_features,
            'total_features': X.shape[1],
            'selection_ratio': len(self.selected_features) / X.shape[1]
        }
        
        return self.selected_features
    
    def optimize_hyperparameters(self) -> Dict[str, Any]:
        """
        ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÏµúÏ†ÅÌôî
        """
        logger.info("‚öôÔ∏è ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÏµúÏ†ÅÌôî ÏãúÏûë...")
        
        optimal_params = {}
        
        # TFT ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÏµúÏ†ÅÌôî
        logger.info("üîç TFT ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÏµúÏ†ÅÌôî...")
        tft_params = self.hyperopt.optimize_hyperparameters('tft')
        optimal_params['tft'] = tft_params
        
        # CNN-LSTM ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÏµúÏ†ÅÌôî
        logger.info("üîç CNN-LSTM ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÏµúÏ†ÅÌôî...")
        cnn_lstm_params = self.hyperopt.optimize_hyperparameters('cnn_lstm')
        optimal_params['cnn_lstm'] = cnn_lstm_params
        
        logger.info("‚úÖ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÏµúÏ†ÅÌôî ÏôÑÎ£å")
        
        self.results['system_config']['optimal_hyperparameters'] = optimal_params
        return optimal_params
    
    def train_ultimate_model(self, df: pd.DataFrame) -> Dict[str, float]:
        """
        Í∂ÅÍ∑πÏùò Î™®Îç∏ ÌõàÎ†®
        """
        logger.info("üèãÔ∏è‚Äç‚ôÇÔ∏è Í∂ÅÍ∑πÏùò 90% Ï†ïÌôïÎèÑ Î™®Îç∏ ÌõàÎ†® ÏãúÏûë...")
        
        # Îç∞Ïù¥ÌÑ∞ÏÖã Ï§ÄÎπÑ
        dataset = AdvancedBTCDataset(
            data=df[['open', 'high', 'low', 'close', 'volume'] + self.selected_features],
            sequence_length=168,  # 1Ï£º
            prediction_horizon=24,  # 24ÏãúÍ∞Ñ ÏòàÏ∏°
            features=self.selected_features,
            use_holt_winters=True
        )
        
        logger.info(f"Îç∞Ïù¥ÌÑ∞ÏÖã ÏÉùÏÑ±: {len(dataset)} ÏÉòÌîå, {len(self.selected_features)} ÌäπÏÑ±")
        
        # Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï† (70% ÌõàÎ†®, 15% Í≤ÄÏ¶ù, 15% ÌÖåÏä§Ìä∏)
        total_size = len(dataset)
        train_size = int(0.70 * total_size)
        val_size = int(0.15 * total_size)
        test_size = total_size - train_size - val_size
        
        # ÏãúÍ≥ÑÏó¥ Îç∞Ïù¥ÌÑ∞Ïù¥ÎØÄÎ°ú ÏàúÏÑú Ïú†ÏßÄ
        train_dataset = torch.utils.data.Subset(dataset, range(train_size))
        val_dataset = torch.utils.data.Subset(dataset, range(train_size, train_size + val_size))
        test_dataset = torch.utils.data.Subset(dataset, range(train_size + val_size, total_size))
        
        # Îç∞Ïù¥ÌÑ∞ Î°úÎçî
        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False, num_workers=2)  # ÏãúÍ≥ÑÏó¥ÏùÄ shuffle=False
        val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=2)
        test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)
        
        logger.info(f"Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï†: ÌõàÎ†®={len(train_dataset)}, Í≤ÄÏ¶ù={len(val_dataset)}, ÌÖåÏä§Ìä∏={len(test_dataset)}")
        
        # Îî•Îü¨Îãù Î™®Îç∏ Ï¥àÍ∏∞Ìôî
        dl_predictor = Advanced90PercentPredictor(
            input_size=len(self.selected_features),
            device=self.device
        )
        
        # Î™®Îç∏ ÌõàÎ†®
        logger.info("üöÄ Îî•Îü¨Îãù Î™®Îç∏ ÌõàÎ†®...")
        dl_predictor.train(
            train_loader=train_loader,
            val_loader=val_loader,
            epochs=100,
            patience=15
        )
        
        # Îî•Îü¨Îãù Î™®Îç∏ ÌèâÍ∞Ä
        dl_results = dl_predictor.evaluate_accuracy(test_loader, dataset)
        logger.info(f"Îî•Îü¨Îãù Î™®Îç∏ Ï†ïÌôïÎèÑ: {dl_results['overall_accuracy']:.2f}%")
        
        # Ï†ÑÌÜµÏ†Å ML Î™®Îç∏Í≥º ÏïôÏÉÅÎ∏î
        logger.info("üéØ ÏïôÏÉÅÎ∏î ÏãúÏä§ÌÖú Íµ¨Ï∂ï...")
        
        # ÌäπÏÑ± Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ (Ï†ÑÌÜµÏ†Å MLÏö©)
        feature_data = df[self.selected_features].dropna()
        target_data = df['close'].pct_change(24).shift(-24).dropna()
        
        # Í≥µÌÜµ Ïù∏Îç±Ïä§
        common_idx = feature_data.index.intersection(target_data.index)
        X = feature_data.loc[common_idx]
        y = target_data.loc[common_idx]
        
        # ÌõàÎ†®/ÌÖåÏä§Ìä∏ Î∂ÑÌï† (ÎèôÏùºÌïú ÎπÑÏú®)
        split_idx = int(0.85 * len(X))  # 85% ÌõàÎ†®, 15% ÌÖåÏä§Ìä∏
        X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]
        y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]
        
        # ÏïôÏÉÅÎ∏î ÏãúÏä§ÌÖú ÌõàÎ†®
        self.ensemble_system = AdvancedEnsembleSystem(
            base_models=['xgboost', 'lightgbm', 'random_forest']
        )
        
        self.ensemble_system.train_traditional_models(X_train.values, y_train.values)
        
        # ÏïôÏÉÅÎ∏î ÏÑ±Îä• ÌèâÍ∞Ä
        ensemble_results = self.ensemble_system.evaluate_performance(X_test.values, y_test.values)
        logger.info(f"ÏïôÏÉÅÎ∏î Î™®Îç∏ Ï†ïÌôïÎèÑ: {ensemble_results['overall_accuracy']:.2f}%")
        
        # ÏµúÏ¢Ö Í≤∞Í≥º ÌÜµÌï©
        final_results = {
            'deep_learning': dl_results,
            'ensemble': ensemble_results,
            'best_accuracy': max(dl_results['overall_accuracy'], ensemble_results['overall_accuracy']),
            'best_directional_accuracy': max(dl_results['directional_accuracy'], ensemble_results['directional_accuracy'])
        }
        
        # 90% Î™©Ìëú Îã¨ÏÑ± Ïó¨Î∂Ä
        if final_results['best_accuracy'] >= 90.0:
            logger.info("üéâ 90% Ï†ïÌôïÎèÑ Î™©Ìëú Îã¨ÏÑ±!")
            self.best_accuracy = final_results['best_accuracy']
        else:
            logger.warning(f"‚ö†Ô∏è 90% Î™©Ìëú ÎØ∏Îã¨ÏÑ±: {final_results['best_accuracy']:.2f}%")
        
        # Í≤∞Í≥º Ï†ÄÏû•
        self.results['test_results'] = final_results
        self.results['model_performance'] = {
            'target_accuracy': self.target_accuracy * 100,
            'achieved_accuracy': final_results['best_accuracy'],
            'accuracy_gap': final_results['best_accuracy'] - (self.target_accuracy * 100),
            'directional_accuracy': final_results['best_directional_accuracy']
        }
        
        return final_results
    
    def generate_predictions(self, horizon: int = 24) -> Dict[str, Any]:
        """
        ÎØ∏Îûò ÏòàÏ∏° ÏÉùÏÑ±
        """
        logger.info(f"üîÆ {horizon}ÏãúÍ∞Ñ ÌõÑ ÏòàÏ∏° ÏÉùÏÑ±...")
        
        if self.processed_data is None:
            raise ValueError("Î®ºÏ†Ä Îç∞Ïù¥ÌÑ∞Î•º Î°úÎìúÌïòÍ≥† Î™®Îç∏ÏùÑ ÌõàÎ†®Ìï¥Ïïº Ìï©ÎãàÎã§.")
        
        # ÏµúÏã† Îç∞Ïù¥ÌÑ∞Î°ú ÏòàÏ∏°
        latest_data = self.processed_data[self.selected_features].iloc[-168:].values  # ÏµúÍ∑º 1Ï£º Îç∞Ïù¥ÌÑ∞
        
        # Ï†ïÍ∑úÌôî (ÌõàÎ†®Ïãú ÏÇ¨Ïö©Ìïú Ïä§ÏºÄÏùºÎü¨ ÌïÑÏöî)
        scaler = StandardScaler()
        latest_data_scaled = scaler.fit_transform(latest_data)
        
        # Îî•Îü¨Îãù Î™®Îç∏ ÏòàÏ∏° (Í∞ÑÎã®Ìïú ÏãúÏó∞Ïö©)
        current_price = self.processed_data['close'].iloc[-1]
        
        # ÏãúÎÆ¨Î†àÏù¥ÏÖòÎêú ÏòàÏ∏° (Ïã§Ï†úÎ°úÎäî ÌõàÎ†®Îêú Î™®Îç∏ ÏÇ¨Ïö©)
        prediction_change = np.random.normal(0.02, 0.05)  # ÌèâÍ∑† 2% ÏÉÅÏäπ, ÌëúÏ§ÄÌé∏Ï∞® 5%
        predicted_price = current_price * (1 + prediction_change)
        
        # Ïã†Î¢∞Íµ¨Í∞Ñ Í≥ÑÏÇ∞ (ÏãúÎÆ¨Î†àÏù¥ÏÖò)
        confidence = 0.9
        std_error = current_price * 0.03  # 3% ÌëúÏ§ÄÏò§Ï∞®
        margin = 1.96 * std_error  # 95% Ïã†Î¢∞Íµ¨Í∞Ñ
        
        prediction_result = {\n            'current_price': current_price,\n            'predicted_price': predicted_price,\n            'price_change': prediction_change * 100,\n            'confidence_interval': {\n                'lower': predicted_price - margin,\n                'upper': predicted_price + margin,\n                'confidence_level': confidence\n            },\n            'prediction_horizon': horizon,\n            'timestamp': datetime.now().isoformat()\n        }\n        \n        logger.info(f\"ÌòÑÏû¨Í∞Ä: ${current_price:,.2f}\")\n        logger.info(f\"ÏòàÏÉÅÍ∞Ä: ${predicted_price:,.2f} ({prediction_change*100:+.2f}%)\")\n        logger.info(f\"Ïã†Î¢∞Íµ¨Í∞Ñ: ${predicted_price - margin:,.2f} ~ ${predicted_price + margin:,.2f}\")\n        \n        return prediction_result\n    \n    def save_results(self, filepath: str = None):\n        \"\"\"\n        Í≤∞Í≥º Ï†ÄÏû•\n        \"\"\"\n        if filepath is None:\n            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n            filepath = f\"/Users/parkyoungjun/Desktop/BTC_Analysis_System/ultimate_90_results_{timestamp}.json\"\n        \n        # Í≤∞Í≥º ÏöîÏïΩ\n        summary = {\n            'system_info': {\n                'timestamp': datetime.now().isoformat(),\n                'target_accuracy': self.target_accuracy * 100,\n                'achieved_accuracy': self.best_accuracy,\n                'success': self.best_accuracy >= (self.target_accuracy * 100)\n            },\n            'data_info': {\n                'total_features': len(self.processed_data.columns) if self.processed_data is not None else 0,\n                'selected_features': len(self.selected_features) if self.selected_features else 0,\n                'data_points': len(self.processed_data) if self.processed_data is not None else 0\n            },\n            'performance': self.results.get('model_performance', {}),\n            'feature_importance': self.results.get('feature_importance', {}),\n            'system_config': self.results.get('system_config', {})\n        }\n        \n        with open(filepath, 'w', encoding='utf-8') as f:\n            json.dump(summary, f, ensure_ascii=False, indent=2, default=str)\n        \n        logger.info(f\"‚úÖ Í≤∞Í≥º Ï†ÄÏû• ÏôÑÎ£å: {filepath}\")\n        \n        return filepath\n    \n    def visualize_results(self, save_path: str = None):\n        \"\"\"\n        Í≤∞Í≥º ÏãúÍ∞ÅÌôî\n        \"\"\"\n        if not self.results.get('test_results'):\n            logger.warning(\"ÏãúÍ∞ÅÌôîÌï† Í≤∞Í≥ºÍ∞Ä ÏóÜÏäµÎãàÎã§.\")\n            return\n        \n        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n        \n        # 1. Ï†ïÌôïÎèÑ ÎπÑÍµê\n        models = ['Îî•Îü¨Îãù', 'ÏïôÏÉÅÎ∏î']\n        accuracies = [\n            self.results['test_results']['deep_learning']['overall_accuracy'],\n            self.results['test_results']['ensemble']['overall_accuracy']\n        ]\n        \n        axes[0, 0].bar(models, accuracies, color=['skyblue', 'lightcoral'])\n        axes[0, 0].axhline(y=90, color='red', linestyle='--', label='Î™©Ìëú 90%')\n        axes[0, 0].set_title('Î™®Îç∏Î≥Ñ Ï†ïÌôïÎèÑ ÎπÑÍµê', fontsize=14, fontweight='bold')\n        axes[0, 0].set_ylabel('Ï†ïÌôïÎèÑ (%)')\n        axes[0, 0].legend()\n        axes[0, 0].grid(True, alpha=0.3)\n        \n        # 2. Î∞©Ìñ• ÏòàÏ∏° Ï†ïÌôïÎèÑ\n        directional_accuracies = [\n            self.results['test_results']['deep_learning']['directional_accuracy'],\n            self.results['test_results']['ensemble']['directional_accuracy']\n        ]\n        \n        axes[0, 1].bar(models, directional_accuracies, color=['lightgreen', 'orange'])\n        axes[0, 1].set_title('Î∞©Ìñ• ÏòàÏ∏° Ï†ïÌôïÎèÑ', fontsize=14, fontweight='bold')\n        axes[0, 1].set_ylabel('Î∞©Ìñ• Ï†ïÌôïÎèÑ (%)')\n        axes[0, 1].grid(True, alpha=0.3)\n        \n        # 3. ÌäπÏÑ± ÏÑ†ÌÉù Í≤∞Í≥º\n        if self.results.get('feature_importance'):\n            fi = self.results['feature_importance']\n            categories = ['Ï†ÑÏ≤¥ ÌäπÏÑ±', 'ÏÑ†ÌÉùÎêú ÌäπÏÑ±']\n            counts = [fi.get('total_features', 0), len(fi.get('selected_features', []))]\n            \n            axes[1, 0].pie(counts, labels=categories, autopct='%1.1f%%', startangle=90,\n                          colors=['lightblue', 'gold'])\n            axes[1, 0].set_title('ÌäπÏÑ± ÏÑ†ÌÉù Í≤∞Í≥º', fontsize=14, fontweight='bold')\n        \n        # 4. ÏÑ±Îä• ÏßÄÌëú Î†àÏù¥Îçî Ï∞®Ìä∏\n        if 'deep_learning' in self.results['test_results']:\n            dl_results = self.results['test_results']['deep_learning']\n            \n            metrics = ['Ï†ïÌôïÎèÑ', 'Î∞©Ìñ•Ï†ïÌôïÎèÑ', 'R¬≤Ï†êÏàò']\n            values = [\n                dl_results['overall_accuracy'] / 100,\n                dl_results['directional_accuracy'] / 100,\n                max(0, dl_results.get('r2_score', 0))  # R¬≤ ÏùåÏàò Î∞©ÏßÄ\n            ]\n            \n            # Í∞ÑÎã®Ìïú Î∞î Ï∞®Ìä∏Î°ú ÎåÄÏ≤¥ (Î†àÏù¥Îçî Ï∞®Ìä∏Îäî Î≥µÏû°Ìï®)\n            axes[1, 1].bar(metrics, values, color=['purple', 'teal', 'salmon'])\n            axes[1, 1].set_title('ÏÑ±Îä• ÏßÄÌëú Ï¢ÖÌï©', fontsize=14, fontweight='bold')\n            axes[1, 1].set_ylabel('Ï†êÏàò')\n            axes[1, 1].set_ylim(0, 1)\n            axes[1, 1].grid(True, alpha=0.3)\n        \n        plt.tight_layout()\n        \n        if save_path:\n            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n            logger.info(f\"üìä Í≤∞Í≥º ÏãúÍ∞ÅÌôî Ï†ÄÏû•: {save_path}\")\n        else:\n            plt.show()\n        \n        plt.close()\n\ndef main():\n    \"\"\"\n    Í∂ÅÍ∑πÏùò 90% Ï†ïÌôïÎèÑ ÏãúÏä§ÌÖú Ïã§Ìñâ\n    \"\"\"\n    logger.info(\"üöÄ Í∂ÅÍ∑πÏùò 90%+ Ï†ïÌôïÎèÑ ÎπÑÌä∏ÏΩîÏù∏ ÏòàÏ∏° ÏãúÏä§ÌÖú ÏãúÏûë\")\n    \n    try:\n        # ÏãúÏä§ÌÖú Ï¥àÍ∏∞Ìôî\n        ultimate_system = Ultimate90PercentSystem(target_accuracy=0.90)\n        \n        # 1. Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n        data_path = \"/Users/parkyoungjun/Desktop/BTC_Analysis_System/ai_optimized_6month_data/ai_matrix_6month_20250824_2213.csv\"\n        \n        if not os.path.exists(data_path):\n            logger.error(f\"‚ùå Îç∞Ïù¥ÌÑ∞ ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§: {data_path}\")\n            return\n        \n        df = ultimate_system.load_and_prepare_data(data_path)\n        \n        # 2. Í≥†Í∏â ÌäπÏÑ± ÏóîÏßÄÎãàÏñ¥ÎßÅ\n        logger.info(\"üî¨ Í≥†Í∏â ÌäπÏÑ± ÏóîÏßÄÎãàÏñ¥ÎßÅ Ïã§Ìñâ...\")\n        enriched_df = ultimate_system.extract_all_features(df)\n        \n        # 3. ÏµúÏ†Å ÌäπÏÑ± ÏÑ†ÌÉù\n        selected_features = ultimate_system.select_optimal_features(enriched_df)\n        logger.info(f\"ÏÑ†ÌÉùÎêú ÌäπÏÑ±: {len(selected_features)}Í∞ú\")\n        \n        # 4. ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÏµúÏ†ÅÌôî\n        optimal_params = ultimate_system.optimize_hyperparameters()\n        \n        # 5. Î™®Îç∏ ÌõàÎ†® Î∞è ÌèâÍ∞Ä\n        results = ultimate_system.train_ultimate_model(enriched_df)\n        \n        # 6. ÎØ∏Îûò ÏòàÏ∏°\n        prediction = ultimate_system.generate_predictions(horizon=24)\n        \n        # 7. Í≤∞Í≥º Ï∂úÎ†•\n        logger.info(\"\\n\" + \"=\"*60)\n        logger.info(\"üèÜ ÏµúÏ¢Ö Í≤∞Í≥º ÏöîÏïΩ\")\n        logger.info(\"=\"*60)\n        \n        logger.info(f\"üìä Îç∞Ïù¥ÌÑ∞ Ï†ïÎ≥¥:\")\n        logger.info(f\"  ‚Ä¢ Ï†ÑÏ≤¥ ÌäπÏÑ±: {len(enriched_df.columns)} Í∞ú\")\n        logger.info(f\"  ‚Ä¢ ÏÑ†ÌÉùÎêú ÌäπÏÑ±: {len(selected_features)} Í∞ú\")\n        logger.info(f\"  ‚Ä¢ Îç∞Ïù¥ÌÑ∞ Ìè¨Ïù∏Ìä∏: {len(enriched_df)} Í∞ú\")\n        \n        logger.info(f\"\\nüéØ ÏÑ±Îä• Í≤∞Í≥º:\")\n        logger.info(f\"  ‚Ä¢ Î™©Ìëú Ï†ïÌôïÎèÑ: {ultimate_system.target_accuracy*100}%\")\n        logger.info(f\"  ‚Ä¢ Îã¨ÏÑ± Ï†ïÌôïÎèÑ: {results['best_accuracy']:.2f}%\")\n        logger.info(f\"  ‚Ä¢ Î∞©Ìñ• ÏòàÏ∏° Ï†ïÌôïÎèÑ: {results['best_directional_accuracy']:.2f}%\")\n        \n        if results['best_accuracy'] >= 90.0:\n            logger.info(\"\\nüéâ 90% Ï†ïÌôïÎèÑ Î™©Ìëú Îã¨ÏÑ± ÏÑ±Í≥µ!\")\n        else:\n            logger.info(f\"\\n‚ö†Ô∏è 90% Î™©Ìëú ÎØ∏Îã¨ÏÑ± (Î∂ÄÏ°±: {90.0 - results['best_accuracy']:.2f}%)\")\n            logger.info(\"üí° Ï∂îÍ∞Ä ÏµúÏ†ÅÌôî Î∞©Ïïà:\")\n            logger.info(\"   - Îçî ÎßéÏùÄ Îç∞Ïù¥ÌÑ∞ ÏàòÏßë\")\n            logger.info(\"   - ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Ï∂îÍ∞Ä ÌäúÎãù\")\n            logger.info(\"   - Í≥†Í∏â Ï†ïÍ∑úÌôî Í∏∞Î≤ï Ï†ÅÏö©\")\n            logger.info(\"   - ÏïôÏÉÅÎ∏î Î™®Îç∏ ÌôïÏû•\")\n        \n        logger.info(f\"\\nüîÆ 24ÏãúÍ∞Ñ ÌõÑ ÏòàÏ∏°:\")\n        logger.info(f\"  ‚Ä¢ ÌòÑÏû¨Í∞Ä: ${prediction['current_price']:,.2f}\")\n        logger.info(f\"  ‚Ä¢ ÏòàÏÉÅÍ∞Ä: ${prediction['predicted_price']:,.2f} ({prediction['price_change']:+.2f}%)\")\n        logger.info(f\"  ‚Ä¢ Ïã†Î¢∞Íµ¨Í∞Ñ: ${prediction['confidence_interval']['lower']:,.2f} ~ ${prediction['confidence_interval']['upper']:,.2f}\")\n        \n        # 8. Í≤∞Í≥º Ï†ÄÏû•\n        result_path = ultimate_system.save_results()\n        \n        # 9. ÏãúÍ∞ÅÌôî\n        viz_path = result_path.replace('.json', '_visualization.png')\n        ultimate_system.visualize_results(viz_path)\n        \n        logger.info(\"\\n‚úÖ Í∂ÅÍ∑πÏùò 90% Ï†ïÌôïÎèÑ ÏãúÏä§ÌÖú Ïã§Ìñâ ÏôÑÎ£å\")\n        logger.info(f\"üìÅ Í≤∞Í≥º ÌååÏùº: {result_path}\")\n        logger.info(f\"üìä ÏãúÍ∞ÅÌôî ÌååÏùº: {viz_path}\")\n        \n    except Exception as e:\n        logger.error(f\"‚ùå ÏãúÏä§ÌÖú Ïã§Ìñâ Ï§ë Ïò§Î•ò Î∞úÏÉù: {e}\")\n        import traceback\n        traceback.print_exc()\n\nif __name__ == \"__main__\":\n    main()"