#!/usr/bin/env python3
"""
ğŸ¯ ê³ ê¸‰ ì›Œí¬í¬ì›Œë“œ ìµœì í™” ì‹œìŠ¤í…œ
- ì ì‘í˜• ë¡¤ë§ ìœˆë„ìš° (ì‹œì¥ ë³€ë™ì„±ì— ë”°ë¼ ì¡°ì •)
- ë‹¤ì¤‘ ì‹œê°„ í”„ë ˆì„ ìµœì í™”
- ì•„ì›ƒì˜¤ë¸Œìƒ˜í”Œ ì„±ëŠ¥ ì¶”ì 
- íŒŒë¼ë¯¸í„° ì•ˆì •ì„± ë¶„ì„
- ê³¼ì í•© ê°ì§€ ë° ë°©ì§€
- ë™ì  ë¦¬ë°¸ëŸ°ì‹± ìµœì í™”
"""

import numpy as np
import pandas as pd
import warnings
import logging
from datetime import datetime, timedelta
import json
import os
from typing import Dict, List, Tuple, Optional, Callable
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
from dataclasses import dataclass
import pickle

# ML ë¼ì´ë¸ŒëŸ¬ë¦¬
from sklearn.model_selection import TimeSeriesSplit, ParameterGrid
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, Ridge, ElasticNet
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.feature_selection import SelectFromModel, RFE
import xgboost as xgb
import lightgbm as lgb

# ë² ì´ì§€ì•ˆ ìµœì í™”
try:
    from skopt import gp_minimize, forest_minimize, gbrt_minimize
    from skopt.space import Real, Integer, Categorical
    from skopt.utils import use_named_args
    from skopt.acquisition import gaussian_ei, gaussian_pi
    BAYESIAN_OPT_AVAILABLE = True
except ImportError:
    BAYESIAN_OPT_AVAILABLE = False

# ì‹œê°í™”
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.express as px

warnings.filterwarnings('ignore')

@dataclass
class OptimizationConfig:
    """ìµœì í™” ì„¤ì •"""
    # ë¡¤ë§ ìœˆë„ìš° ì„¤ì •
    min_train_size: int = 500      # ìµœì†Œ í•™ìŠµ ë°ì´í„° í¬ê¸°
    max_train_size: int = 2000     # ìµœëŒ€ í•™ìŠµ ë°ì´í„° í¬ê¸°
    test_size: int = 48            # í…ŒìŠ¤íŠ¸ ë°ì´í„° í¬ê¸° (48ì‹œê°„)
    step_size: int = 24            # ì´ë™ ë‹¨ê³„ (24ì‹œê°„)
    
    # ì ì‘í˜• ìœˆë„ìš° ì„¤ì •
    volatility_lookback: int = 168  # ë³€ë™ì„± ê³„ì‚° ê¸°ê°„ (1ì£¼ì¼)
    volatility_threshold: float = 0.05  # ë†’ì€ ë³€ë™ì„± ì„ê³„ê°’
    adaptive_window: bool = True    # ì ì‘í˜• ìœˆë„ìš° ì‚¬ìš©
    
    # ìµœì í™” ì„¤ì •
    optimization_method: str = 'bayesian'  # 'grid', 'random', 'bayesian'
    n_optimization_trials: int = 100   # ìµœì í™” ì‹œë„ íšŸìˆ˜
    cv_folds: int = 5              # êµì°¨ ê²€ì¦ fold ìˆ˜
    
    # ì„±ëŠ¥ ì„ê³„ê°’
    min_r2_threshold: float = 0.1   # ìµœì†Œ RÂ² ì„ê³„ê°’
    overfitting_threshold: float = 0.3  # ê³¼ì í•© ê°ì§€ ì„ê³„ê°’
    stability_threshold: float = 0.2    # íŒŒë¼ë¯¸í„° ì•ˆì •ì„± ì„ê³„ê°’
    
    # ë³‘ë ¬ ì²˜ë¦¬
    n_jobs: int = -1               # ë³‘ë ¬ ì²˜ë¦¬ í”„ë¡œì„¸ìŠ¤ ìˆ˜
    
    # ë¦¬ë°¸ëŸ°ì‹± ì„¤ì •
    rebalance_methods: List[str] = None  # ë¦¬ë°¸ëŸ°ì‹± ë°©ë²•ë“¤
    
    def __post_init__(self):
        if self.rebalance_methods is None:
            self.rebalance_methods = ['daily', 'weekly', 'volatility_based', 'performance_based']

class AdvancedWalkForwardOptimizer:
    """ê³ ê¸‰ ì›Œí¬í¬ì›Œë“œ ìµœì í™” ì‹œìŠ¤í…œ"""
    
    def __init__(self, config: OptimizationConfig = None):
        self.config = config or OptimizationConfig()
        self.data_path = "/Users/parkyoungjun/Desktop/BTC_Analysis_System"
        
        # ê²°ê³¼ ì €ì¥
        self.optimization_history = []
        self.parameter_stability_tracker = {}
        self.overfitting_detector = {}
        self.performance_tracker = {}
        
        # ë¡œê¹… ì„¤ì •
        self.setup_logging()
        
    def setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(os.path.join(self.data_path, 'walkforward_optimizer.log'), encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def adaptive_window_sizing(self, data: pd.DataFrame, current_idx: int) -> Dict[str, int]:
        """ì ì‘í˜• ìœˆë„ìš° í¬ê¸° ê²°ì •"""
        if not self.config.adaptive_window:
            return {
                'train_size': self.config.max_train_size,
                'test_size': self.config.test_size
            }
        
        # í˜„ì¬ ì‹œì  ê¸°ì¤€ ë³€ë™ì„± ê³„ì‚°
        lookback_start = max(0, current_idx - self.config.volatility_lookback)
        recent_data = data.iloc[lookback_start:current_idx]
        
        if len(recent_data) < 10:  # ë°ì´í„° ë¶€ì¡±ì‹œ ê¸°ë³¸ê°’
            return {
                'train_size': self.config.min_train_size,
                'test_size': self.config.test_size
            }
        
        # ê°€ê²© ë³€ë™ì„± ê³„ì‚°
        if 'price' in recent_data.columns:
            returns = recent_data['price'].pct_change().dropna()
        else:
            returns = recent_data.iloc[:, 0].pct_change().dropna()
        
        volatility = returns.std()
        
        # ë³€ë™ì„±ì— ë”°ë¥¸ ìœˆë„ìš° í¬ê¸° ì¡°ì •
        if volatility > self.config.volatility_threshold:
            # ë†’ì€ ë³€ë™ì„±: ì§§ì€ í•™ìŠµ ìœˆë„ìš°, ë” ìì£¼ ì—…ë°ì´íŠ¸
            train_size = self.config.min_train_size
            test_size = max(12, self.config.test_size // 2)  # ë” ì§§ì€ í…ŒìŠ¤íŠ¸ ê¸°ê°„
        else:
            # ë‚®ì€ ë³€ë™ì„±: ê¸´ í•™ìŠµ ìœˆë„ìš°, ì•ˆì •ì ì¸ ì˜ˆì¸¡
            train_size = self.config.max_train_size
            test_size = self.config.test_size
        
        return {
            'train_size': min(train_size, current_idx),  # ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ë¡œ ì œí•œ
            'test_size': test_size,
            'volatility': volatility
        }
    
    def comprehensive_walkforward_analysis(self, data: pd.DataFrame, model_configs: List[Dict]) -> Dict:
        """ì¢…í•©ì ì¸ ì›Œí¬í¬ì›Œë“œ ë¶„ì„"""
        self.logger.info("ğŸš€ ê³ ê¸‰ ì›Œí¬í¬ì›Œë“œ ìµœì í™” ì‹œì‘...")
        self.logger.info(f"ë°ì´í„° í¬ê¸°: {data.shape}, ê¸°ê°„: {data.index[0]} ~ {data.index[-1]}")
        
        results = []
        n_samples = len(data)
        
        # ì´ˆê¸° í•™ìŠµ ë°ì´í„° í¬ê¸°
        current_pos = self.config.min_train_size
        
        while current_pos < n_samples - self.config.test_size:
            # ì ì‘í˜• ìœˆë„ìš° í¬ê¸° ê²°ì •
            window_config = self.adaptive_window_sizing(data, current_pos)
            train_size = window_config['train_size']
            test_size = window_config['test_size']
            
            # ë°ì´í„° ë¶„í• 
            train_start = max(0, current_pos - train_size)
            train_data = data.iloc[train_start:current_pos]
            test_data = data.iloc[current_pos:current_pos + test_size]
            
            self.logger.info(f"Period {len(results)+1}: Train {len(train_data)} â†’ Test {len(test_data)}")
            
            # ë‹¤ì¤‘ ëª¨ë¸ ìµœì í™” ë° í‰ê°€
            period_results = self.optimize_multiple_models(
                train_data, test_data, model_configs, len(results)+1
            )
            
            # ê²°ê³¼ì— ìœˆë„ìš° ì •ë³´ ì¶”ê°€
            period_results.update(window_config)
            period_results['train_period'] = (train_data.index[0], train_data.index[-1])
            period_results['test_period'] = (test_data.index[0], test_data.index[-1])
            
            results.append(period_results)
            
            # ë‹¤ìŒ ìœ„ì¹˜ë¡œ ì´ë™
            current_pos += self.config.step_size
        
        # ê²°ê³¼ ë¶„ì„
        comprehensive_analysis = self.analyze_walkforward_results(results)
        
        # ì €ì¥
        self.save_optimization_results(results, comprehensive_analysis)
        
        return {
            'period_results': results,
            'comprehensive_analysis': comprehensive_analysis,
            'parameter_stability': self.parameter_stability_tracker,
            'overfitting_analysis': self.overfitting_detector
        }
    
    def optimize_multiple_models(self, train_data: pd.DataFrame, test_data: pd.DataFrame, 
                               model_configs: List[Dict], period_num: int) -> Dict:
        """ë‹¤ì¤‘ ëª¨ë¸ ìµœì í™”"""
        model_results = {}
        best_model = None
        best_score = -np.inf
        best_config = None
        
        # ê° ëª¨ë¸ì— ëŒ€í•´ ìµœì í™” ìˆ˜í–‰
        for model_config in model_configs:
            model_name = model_config['name']
            model_class = model_config['class']
            param_space = model_config['param_space']
            
            self.logger.info(f"   ğŸ”§ {model_name} ìµœì í™” ì¤‘...")
            
            # ëª¨ë¸ë³„ ìµœì í™”
            optimal_params = self.optimize_model_parameters(
                train_data, model_class, param_space
            )
            
            # ì•„ì›ƒì˜¤ë¸Œìƒ˜í”Œ í‰ê°€
            oos_performance = self.evaluate_out_of_sample(
                model_class, optimal_params, train_data, test_data
            )
            
            # ê³¼ì í•© ê²€ì‚¬
            overfitting_score = self.detect_overfitting(
                model_class, optimal_params, train_data, test_data
            )
            
            model_results[model_name] = {
                'optimal_params': optimal_params,
                'oos_performance': oos_performance,
                'overfitting_score': overfitting_score,
                'is_overfitted': overfitting_score > self.config.overfitting_threshold
            }
            
            # ìµœì  ëª¨ë¸ ì„ íƒ
            if (oos_performance['r2'] > best_score and 
                oos_performance['r2'] > self.config.min_r2_threshold and
                not model_results[model_name]['is_overfitted']):
                best_score = oos_performance['r2']
                best_model = model_name
                best_config = optimal_params
        
        # íŒŒë¼ë¯¸í„° ì•ˆì •ì„± ì¶”ì 
        self.track_parameter_stability(period_num, model_results)
        
        return {
            'period': period_num,
            'model_results': model_results,
            'best_model': best_model,
            'best_config': best_config,
            'best_score': best_score
        }
    
    def optimize_model_parameters(self, train_data: pd.DataFrame, model_class, 
                                param_space: Dict) -> Dict:
        """ëª¨ë¸ íŒŒë¼ë¯¸í„° ìµœì í™”"""
        if self.config.optimization_method == 'bayesian' and BAYESIAN_OPT_AVAILABLE:
            return self.bayesian_parameter_optimization(train_data, model_class, param_space)
        elif self.config.optimization_method == 'random':
            return self.random_search_optimization(train_data, model_class, param_space)
        else:
            return self.grid_search_optimization(train_data, model_class, param_space)
    
    def bayesian_parameter_optimization(self, train_data: pd.DataFrame, model_class, 
                                      param_space: Dict) -> Dict:
        """ë² ì´ì§€ì•ˆ ìµœì í™”"""
        # íŒŒë¼ë¯¸í„° ê³µê°„ ë³€í™˜
        dimensions = []
        param_names = []
        
        for param_name, param_range in param_space.items():
            param_names.append(param_name)
            
            if isinstance(param_range, tuple) and len(param_range) == 2:
                # ì—°ì†í˜• íŒŒë¼ë¯¸í„°
                dimensions.append(Real(param_range[0], param_range[1], name=param_name))
            elif isinstance(param_range, list):
                # ì´ì‚°í˜• íŒŒë¼ë¯¸í„°
                if all(isinstance(x, (int, float)) for x in param_range):
                    if all(isinstance(x, int) for x in param_range):
                        dimensions.append(Integer(min(param_range), max(param_range), name=param_name))
                    else:
                        dimensions.append(Real(min(param_range), max(param_range), name=param_name))
                else:
                    dimensions.append(Categorical(param_range, name=param_name))
        
        @use_named_args(dimensions)
        def objective(**params):
            try:
                score = self.cross_validate_model(train_data, model_class, params)
                return -score  # ìµœì†Œí™”ë¥¼ ìœ„í•´ ìŒìˆ˜ ë°˜í™˜
            except Exception:
                return 1000  # ì‹¤íŒ¨ì‹œ í° ê°’ ë°˜í™˜
        
        # ë² ì´ì§€ì•ˆ ìµœì í™” ì‹¤í–‰
        result = gp_minimize(
            objective,
            dimensions,
            n_calls=self.config.n_optimization_trials,
            n_initial_points=10,
            acquisition_func='EI',
            random_state=42
        )
        
        # ìµœì  íŒŒë¼ë¯¸í„° ì¶”ì¶œ
        optimal_params = {}
        for i, param_name in enumerate(param_names):
            optimal_params[param_name] = result.x[i]
        
        return optimal_params
    
    def random_search_optimization(self, train_data: pd.DataFrame, model_class, 
                                 param_space: Dict) -> Dict:
        """ëœë¤ ì„œì¹˜ ìµœì í™”"""
        best_score = -np.inf
        best_params = {}
        
        for _ in range(self.config.n_optimization_trials):
            # ëœë¤ íŒŒë¼ë¯¸í„° ìƒì„±
            params = {}
            for param_name, param_range in param_space.items():
                if isinstance(param_range, tuple):
                    # ì—°ì†í˜•
                    params[param_name] = np.random.uniform(param_range[0], param_range[1])
                elif isinstance(param_range, list):
                    # ì´ì‚°í˜•
                    params[param_name] = np.random.choice(param_range)
            
            # ì •ìˆ˜í˜• íŒŒë¼ë¯¸í„° ì²˜ë¦¬
            if 'n_estimators' in params:
                params['n_estimators'] = int(params['n_estimators'])
            if 'max_depth' in params:
                params['max_depth'] = int(params['max_depth'])
            if 'min_samples_split' in params:
                params['min_samples_split'] = int(params['min_samples_split'])
            
            try:
                score = self.cross_validate_model(train_data, model_class, params)
                if score > best_score:
                    best_score = score
                    best_params = params.copy()
            except Exception:
                continue
        
        return best_params
    
    def grid_search_optimization(self, train_data: pd.DataFrame, model_class, 
                               param_space: Dict) -> Dict:
        """ê·¸ë¦¬ë“œ ì„œì¹˜ ìµœì í™”"""
        # íŒŒë¼ë¯¸í„° ì¡°í•© ìƒì„±
        param_combinations = list(ParameterGrid(param_space))
        
        # ì¡°í•©ì´ ë„ˆë¬´ ë§ìœ¼ë©´ ìƒ˜í”Œë§
        if len(param_combinations) > self.config.n_optimization_trials:
            param_combinations = np.random.choice(
                param_combinations, 
                self.config.n_optimization_trials, 
                replace=False
            ).tolist()
        
        best_score = -np.inf
        best_params = {}
        
        for params in param_combinations:
            try:
                score = self.cross_validate_model(train_data, model_class, params)
                if score > best_score:
                    best_score = score
                    best_params = params
            except Exception:
                continue
        
        return best_params
    
    def cross_validate_model(self, train_data: pd.DataFrame, model_class, params: Dict) -> float:
        """ì‹œê³„ì—´ êµì°¨ ê²€ì¦"""
        if 'target' not in train_data.columns:
            return 0.0
        
        X = train_data.drop(columns=['target'])
        y = train_data['target']
        
        # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë§Œ ì„ íƒ
        numeric_cols = X.select_dtypes(include=[np.number]).columns
        X = X[numeric_cols]
        
        # NaN ì²˜ë¦¬
        X = X.fillna(0)
        y = y.fillna(0)
        
        if len(X) < self.config.cv_folds * 2:
            # ë°ì´í„°ê°€ ë¶€ì¡±í•˜ë©´ ë‹¨ìˆœ ë¶„í• 
            split_idx = len(X) // 2
            X_train, X_val = X.iloc[:split_idx], X.iloc[split_idx:]
            y_train, y_val = y.iloc[:split_idx], y.iloc[split_idx:]
            
            try:
                model = model_class(**params)
                model.fit(X_train, y_train)
                pred = model.predict(X_val)
                return r2_score(y_val, pred)
            except Exception:
                return 0.0
        
        # ì‹œê³„ì—´ êµì°¨ ê²€ì¦
        tscv = TimeSeriesSplit(n_splits=self.config.cv_folds)
        scores = []
        
        for train_idx, val_idx in tscv.split(X):
            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
            
            try:
                model = model_class(**params)
                model.fit(X_train, y_train)
                pred = model.predict(X_val)
                score = r2_score(y_val, pred)
                scores.append(score)
            except Exception:
                scores.append(0.0)
        
        return np.mean(scores) if scores else 0.0
    
    def evaluate_out_of_sample(self, model_class, params: Dict, train_data: pd.DataFrame, 
                             test_data: pd.DataFrame) -> Dict:
        """ì•„ì›ƒì˜¤ë¸Œìƒ˜í”Œ í‰ê°€"""
        if 'target' not in train_data.columns or 'target' not in test_data.columns:
            return {'r2': 0, 'mae': np.inf, 'rmse': np.inf}
        
        # ë°ì´í„° ì¤€ë¹„
        X_train = train_data.drop(columns=['target'])
        y_train = train_data['target']
        X_test = test_data.drop(columns=['target'])
        y_test = test_data['target']
        
        # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë§Œ ì„ íƒ
        numeric_cols = X_train.select_dtypes(include=[np.number]).columns
        X_train = X_train[numeric_cols].fillna(0)
        X_test = X_test[numeric_cols].fillna(0)
        y_train = y_train.fillna(0)
        y_test = y_test.fillna(0)
        
        # ê³µí†µ ì»¬ëŸ¼ë§Œ ì‚¬ìš©
        common_cols = X_train.columns.intersection(X_test.columns)
        X_train = X_train[common_cols]
        X_test = X_test[common_cols]
        
        try:
            # ëª¨ë¸ í›ˆë ¨
            model = model_class(**params)
            model.fit(X_train, y_train)
            
            # ì˜ˆì¸¡
            predictions = model.predict(X_test)
            
            # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
            r2 = r2_score(y_test, predictions)
            mae = mean_absolute_error(y_test, predictions)
            rmse = np.sqrt(mean_squared_error(y_test, predictions))
            
            return {
                'r2': r2,
                'mae': mae,
                'rmse': rmse,
                'predictions': predictions.tolist(),
                'actuals': y_test.tolist()
            }
        except Exception:
            return {'r2': 0, 'mae': np.inf, 'rmse': np.inf}
    
    def detect_overfitting(self, model_class, params: Dict, train_data: pd.DataFrame, 
                          test_data: pd.DataFrame) -> float:
        """ê³¼ì í•© ê²€ì‚¬"""
        # í›ˆë ¨ ë°ì´í„° ì„±ëŠ¥
        train_score = self.cross_validate_model(train_data, model_class, params)
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„±ëŠ¥
        test_performance = self.evaluate_out_of_sample(model_class, params, train_data, test_data)
        test_score = test_performance['r2']
        
        # ê³¼ì í•© ì ìˆ˜ ê³„ì‚° (í›ˆë ¨ ì„±ëŠ¥ê³¼ í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ì˜ ì°¨ì´)
        if train_score > 0:
            overfitting_score = max(0, (train_score - test_score) / train_score)
        else:
            overfitting_score = 1.0  # í›ˆë ¨ë„ ì‹¤íŒ¨í–ˆìœ¼ë©´ ì™„ì „ ê³¼ì í•©ìœ¼ë¡œ ê°„ì£¼
        
        return overfitting_score
    
    def track_parameter_stability(self, period_num: int, model_results: Dict):
        """íŒŒë¼ë¯¸í„° ì•ˆì •ì„± ì¶”ì """
        for model_name, result in model_results.items():
            if model_name not in self.parameter_stability_tracker:
                self.parameter_stability_tracker[model_name] = {}
            
            params = result['optimal_params']
            for param_name, param_value in params.items():
                if param_name not in self.parameter_stability_tracker[model_name]:
                    self.parameter_stability_tracker[model_name][param_name] = []
                
                self.parameter_stability_tracker[model_name][param_name].append({
                    'period': period_num,
                    'value': param_value
                })
    
    def analyze_walkforward_results(self, results: List[Dict]) -> Dict:
        """ì›Œí¬í¬ì›Œë“œ ê²°ê³¼ ì¢…í•© ë¶„ì„"""
        self.logger.info("ğŸ“Š ì›Œí¬í¬ì›Œë“œ ê²°ê³¼ ë¶„ì„ ì¤‘...")
        
        # ê¸°ë³¸ í†µê³„
        best_scores = [r['best_score'] for r in results if r['best_score'] is not None]
        model_usage = {}
        parameter_evolution = {}
        
        for result in results:
            # ëª¨ë¸ ì‚¬ìš© ë¹ˆë„
            best_model = result.get('best_model')
            if best_model:
                model_usage[best_model] = model_usage.get(best_model, 0) + 1
        
        # íŒŒë¼ë¯¸í„° ì•ˆì •ì„± ë¶„ì„
        stability_analysis = {}
        for model_name, param_history in self.parameter_stability_tracker.items():
            stability_analysis[model_name] = {}
            
            for param_name, values in param_history.items():
                param_values = [v['value'] for v in values]
                if len(param_values) > 1:
                    stability_analysis[model_name][param_name] = {
                        'mean': np.mean(param_values),
                        'std': np.std(param_values),
                        'cv': np.std(param_values) / (np.mean(param_values) + 1e-8),
                        'stability_score': 1 / (1 + np.std(param_values))
                    }
        
        # ì„±ëŠ¥ ì¶”ì´ ë¶„ì„
        performance_trend = self.analyze_performance_trend(results)
        
        # ì ì‘í˜• ìœˆë„ìš° íš¨ê³¼ ë¶„ì„
        adaptive_window_analysis = self.analyze_adaptive_window_effect(results)
        
        analysis = {
            'summary_stats': {
                'total_periods': len(results),
                'avg_performance': np.mean(best_scores) if best_scores else 0,
                'std_performance': np.std(best_scores) if best_scores else 0,
                'best_performance': max(best_scores) if best_scores else 0,
                'worst_performance': min(best_scores) if best_scores else 0
            },
            'model_usage': model_usage,
            'parameter_stability': stability_analysis,
            'performance_trend': performance_trend,
            'adaptive_window_analysis': adaptive_window_analysis,
            'overfitting_summary': self.summarize_overfitting_detection(results)
        }
        
        return analysis
    
    def analyze_performance_trend(self, results: List[Dict]) -> Dict:
        """ì„±ëŠ¥ ì¶”ì´ ë¶„ì„"""
        periods = [r['period'] for r in results]
        scores = [r['best_score'] if r['best_score'] is not None else 0 for r in results]
        
        if len(scores) < 2:
            return {'trend': 'insufficient_data'}
        
        # ì„ í˜• ì¶”ì„¸ ë¶„ì„
        from scipy import stats
        slope, intercept, r_value, p_value, std_err = stats.linregress(periods, scores)
        
        # ì´ë™ í‰ê· 
        window_size = min(5, len(scores) // 2)
        if window_size > 0:
            moving_avg = pd.Series(scores).rolling(window=window_size).mean().tolist()
        else:
            moving_avg = scores
        
        return {
            'trend': 'improving' if slope > 0 else 'declining',
            'slope': slope,
            'r_squared': r_value ** 2,
            'p_value': p_value,
            'moving_average': moving_avg,
            'volatility': np.std(scores)
        }
    
    def analyze_adaptive_window_effect(self, results: List[Dict]) -> Dict:
        """ì ì‘í˜• ìœˆë„ìš° íš¨ê³¼ ë¶„ì„"""
        if not self.config.adaptive_window:
            return {'enabled': False}
        
        train_sizes = [r.get('train_size', 0) for r in results]
        test_sizes = [r.get('test_size', 0) for r in results]
        volatilities = [r.get('volatility', 0) for r in results if r.get('volatility') is not None]
        scores = [r['best_score'] if r['best_score'] is not None else 0 for r in results]
        
        analysis = {
            'enabled': True,
            'train_size_stats': {
                'mean': np.mean(train_sizes),
                'std': np.std(train_sizes),
                'min': min(train_sizes),
                'max': max(train_sizes)
            },
            'test_size_stats': {
                'mean': np.mean(test_sizes),
                'std': np.std(test_sizes),
                'min': min(test_sizes),
                'max': max(test_sizes)
            }
        }
        
        # ë³€ë™ì„±ê³¼ ì„±ëŠ¥ì˜ ê´€ê³„
        if len(volatilities) > 1 and len(scores) > 1:
            corr_vol_perf = np.corrcoef(volatilities, scores[:len(volatilities)])[0, 1]
            analysis['volatility_performance_correlation'] = corr_vol_perf
        
        return analysis
    
    def summarize_overfitting_detection(self, results: List[Dict]) -> Dict:
        """ê³¼ì í•© ê²€ì¶œ ìš”ì•½"""
        overfitted_counts = {}
        total_counts = {}
        
        for result in results:
            for model_name, model_result in result.get('model_results', {}).items():
                total_counts[model_name] = total_counts.get(model_name, 0) + 1
                if model_result.get('is_overfitted', False):
                    overfitted_counts[model_name] = overfitted_counts.get(model_name, 0) + 1
        
        overfitting_rates = {}
        for model_name, total in total_counts.items():
            overfitted = overfitted_counts.get(model_name, 0)
            overfitting_rates[model_name] = overfitted / total if total > 0 else 0
        
        return {
            'overfitting_rates': overfitting_rates,
            'total_evaluations': total_counts,
            'overfitted_evaluations': overfitted_counts
        }
    
    def save_optimization_results(self, results: List[Dict], analysis: Dict):
        """ìµœì í™” ê²°ê³¼ ì €ì¥"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # JSON ì €ì¥
        output_data = {
            'timestamp': timestamp,
            'config': self.config.__dict__,
            'results': results,
            'analysis': analysis
        }
        
        json_path = os.path.join(self.data_path, f'walkforward_optimization_{timestamp}.json')
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False, default=str)
        
        self.logger.info(f"ìµœì í™” ê²°ê³¼ ì €ì¥: {json_path}")
    
    def create_optimization_dashboard(self, results: List[Dict], analysis: Dict) -> str:
        """ìµœì í™” ëŒ€ì‹œë³´ë“œ ìƒì„±"""
        fig = make_subplots(
            rows=3, cols=3,
            subplot_titles=(
                "ì„±ëŠ¥ ì¶”ì´", "ëª¨ë¸ ì‚¬ìš© ë¹ˆë„", "íŒŒë¼ë¯¸í„° ì•ˆì •ì„±",
                "ì ì‘í˜• ìœˆë„ìš° í¬ê¸°", "ê³¼ì í•© ê²€ì¶œë¥ ", "ì„±ëŠ¥ ë¶„í¬",
                "ë³€ë™ì„± vs ì„±ëŠ¥", "í•™ìŠµ ìœˆë„ìš° íš¨ê³¼", "ì˜ˆì¸¡ ì •í™•ë„"
            ),
            specs=[[{"type": "scatter"}, {"type": "pie"}, {"type": "heatmap"}],
                   [{"type": "scatter"}, {"type": "bar"}, {"type": "histogram"}],
                   [{"type": "scatter"}, {"type": "box"}, {"type": "scatter"}]]
        )
        
        # ì„±ëŠ¥ ì¶”ì´
        periods = [r['period'] for r in results]
        scores = [r['best_score'] if r['best_score'] is not None else 0 for r in results]
        
        fig.add_trace(
            go.Scatter(x=periods, y=scores, mode='lines+markers', name='ì„±ëŠ¥'),
            row=1, col=1
        )
        
        # ëª¨ë¸ ì‚¬ìš© ë¹ˆë„
        model_usage = analysis['model_usage']
        fig.add_trace(
            go.Pie(labels=list(model_usage.keys()), values=list(model_usage.values())),
            row=1, col=2
        )
        
        # ì¶”ê°€ ì°¨íŠ¸ë“¤...
        
        fig.update_layout(
            title="ğŸ”§ ì›Œí¬í¬ì›Œë“œ ìµœì í™” ëŒ€ì‹œë³´ë“œ",
            height=1200,
            showlegend=True,
            template='plotly_dark'
        )
        
        dashboard_path = os.path.join(self.data_path, 'walkforward_optimization_dashboard.html')
        fig.write_html(dashboard_path, include_plotlyjs=True)
        
        return dashboard_path

def create_default_model_configs() -> List[Dict]:
    """ê¸°ë³¸ ëª¨ë¸ ì„¤ì • ìƒì„±"""
    return [
        {
            'name': 'RandomForest',
            'class': RandomForestRegressor,
            'param_space': {
                'n_estimators': [50, 100, 200, 300],
                'max_depth': [5, 10, 15, 20],
                'min_samples_split': [2, 5, 10],
                'min_samples_leaf': [1, 2, 4]
            }
        },
        {
            'name': 'XGBoost',
            'class': xgb.XGBRegressor,
            'param_space': {
                'n_estimators': [50, 100, 200],
                'max_depth': [3, 6, 9],
                'learning_rate': [0.01, 0.1, 0.2],
                'subsample': [0.8, 0.9, 1.0]
            }
        },
        {
            'name': 'LightGBM',
            'class': lgb.LGBMRegressor,
            'param_space': {
                'n_estimators': [50, 100, 200],
                'max_depth': [3, 6, 9],
                'learning_rate': [0.01, 0.1, 0.2],
                'feature_fraction': [0.8, 0.9, 1.0]
            }
        },
        {
            'name': 'GradientBoosting',
            'class': GradientBoostingRegressor,
            'param_space': {
                'n_estimators': [50, 100, 200],
                'max_depth': [3, 5, 7],
                'learning_rate': [0.01, 0.1, 0.2],
                'subsample': [0.8, 0.9, 1.0]
            }
        }
    ]

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ”§ ê³ ê¸‰ ì›Œí¬í¬ì›Œë“œ ìµœì í™” ì‹œìŠ¤í…œ")
    print("="*60)
    
    # ì„¤ì •
    config = OptimizationConfig(
        min_train_size=500,
        max_train_size=1500,
        test_size=48,
        adaptive_window=True,
        optimization_method='bayesian' if BAYESIAN_OPT_AVAILABLE else 'random',
        n_optimization_trials=50
    )
    
    # ìµœì í™”ê¸° ì´ˆê¸°í™”
    optimizer = AdvancedWalkForwardOptimizer(config)
    
    # ë°ì´í„° ë¡œë“œ (ì„ì‹œ ë°ì´í„°)
    n_samples = 2000
    np.random.seed(42)
    data = pd.DataFrame({
        'feature1': np.random.randn(n_samples).cumsum(),
        'feature2': np.random.randn(n_samples),
        'feature3': np.random.randn(n_samples),
        'target': np.random.randn(n_samples) * 0.02
    })
    data.index = pd.date_range(start='2024-01-01', periods=n_samples, freq='H')
    
    # ëª¨ë¸ ì„¤ì •
    model_configs = create_default_model_configs()
    
    # ì›Œí¬í¬ì›Œë“œ ìµœì í™” ì‹¤í–‰
    results = optimizer.comprehensive_walkforward_analysis(data, model_configs)
    
    print(f"\nâœ… ìµœì í™” ì™„ë£Œ!")
    print(f"ì´ ê¸°ê°„: {len(results['period_results'])}")
    print(f"í‰ê·  ì„±ëŠ¥: {results['comprehensive_analysis']['summary_stats']['avg_performance']:.4f}")
    
    # ëŒ€ì‹œë³´ë“œ ìƒì„±
    dashboard_path = optimizer.create_optimization_dashboard(
        results['period_results'], 
        results['comprehensive_analysis']
    )
    print(f"ëŒ€ì‹œë³´ë“œ: {dashboard_path}")

if __name__ == "__main__":
    main()