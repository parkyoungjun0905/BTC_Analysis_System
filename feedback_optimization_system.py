"""
í”¼ë“œë°± ë£¨í”„ ë° ìë™ ìµœì í™” ì‹œìŠ¤í…œ v1.0
- ì‹¤ì‹œê°„ ì˜ˆì¸¡ ì˜¤ì°¨ ë¶„ì„
- ìë™ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
- ëª¨ë¸ ì„±ëŠ¥ ìµœì í™”
- ì ì‘í˜• í•™ìŠµë¥  ì¡°ì •
- íŠ¹ì„± ì¤‘ìš”ë„ ë™ì  ì—…ë°ì´íŠ¸
"""

import os
import json
import sqlite3
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Any, Callable
import asyncio
import logging
from dataclasses import dataclass, asdict
from collections import deque, defaultdict
from scipy.optimize import minimize, differential_evolution
from sklearn.model_selection import ParameterSampler
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import Matern
import optuna
import warnings
warnings.filterwarnings('ignore')

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class OptimizationTarget:
    """ìµœì í™” ëª©í‘œ"""
    name: str
    weight: float
    minimize: bool = True  # Trueë©´ ìµœì†Œí™”, Falseë©´ ìµœëŒ€í™”
    current_value: Optional[float] = None
    target_value: Optional[float] = None

@dataclass
class HyperParameter:
    """í•˜ì´í¼íŒŒë¼ë¯¸í„° ì •ì˜"""
    name: str
    param_type: str  # 'float', 'int', 'categorical'
    min_value: Optional[float] = None
    max_value: Optional[float] = None
    choices: Optional[List] = None
    current_value: Any = None
    best_value: Any = None
    importance: float = 0.0
    
@dataclass
class OptimizationResult:
    """ìµœì í™” ê²°ê³¼"""
    iteration: int
    parameters: Dict[str, Any]
    objective_value: float
    individual_metrics: Dict[str, float]
    improvement: float
    timestamp: datetime
    duration_seconds: float

class BayesianOptimizer:
    """ë² ì´ì§€ì•ˆ ìµœì í™”ê¸°"""
    
    def __init__(self, hyperparameters: List[HyperParameter], n_initial_points: int = 10):
        self.hyperparameters = {hp.name: hp for hp in hyperparameters}
        self.n_initial_points = n_initial_points
        self.gp_model = None
        self.X_observed = []
        self.y_observed = []
        self.iteration = 0
        
    def suggest_parameters(self) -> Dict[str, Any]:
        """ë‹¤ìŒ ì‹œë„í•  íŒŒë¼ë¯¸í„° ì œì•ˆ"""
        
        if len(self.X_observed) < self.n_initial_points:
            # ì´ˆê¸° íƒìƒ‰: ëœë¤ ìƒ˜í”Œë§
            return self._random_sample()
        else:
            # ë² ì´ì§€ì•ˆ ìµœì í™”: íšë“ í•¨ìˆ˜ ê¸°ë°˜
            return self._bayesian_sample()
    
    def _random_sample(self) -> Dict[str, Any]:
        """ëœë¤ íŒŒë¼ë¯¸í„° ìƒ˜í”Œë§"""
        params = {}
        
        for name, hp in self.hyperparameters.items():
            if hp.param_type == 'float':
                params[name] = np.random.uniform(hp.min_value, hp.max_value)
            elif hp.param_type == 'int':
                params[name] = np.random.randint(hp.min_value, hp.max_value + 1)
            elif hp.param_type == 'categorical':
                params[name] = np.random.choice(hp.choices)
                
        return params
    
    def _bayesian_sample(self) -> Dict[str, Any]:
        """ë² ì´ì§€ì•ˆ ìµœì í™” ê¸°ë°˜ ìƒ˜í”Œë§"""
        try:
            if self.gp_model is None:
                # ê°€ìš°ì‹œì•ˆ í”„ë¡œì„¸ìŠ¤ ëª¨ë¸ ì´ˆê¸°í™”
                kernel = Matern(length_scale=1.0, nu=2.5)
                self.gp_model = GaussianProcessRegressor(kernel=kernel, alpha=1e-6)
                
            # GP ëª¨ë¸ í•™ìŠµ
            X = np.array(self.X_observed)
            y = np.array(self.y_observed)
            self.gp_model.fit(X, y)
            
            # íšë“ í•¨ìˆ˜ ìµœì í™”
            best_params = self._optimize_acquisition_function()
            
            return self._denormalize_parameters(best_params)
            
        except Exception as e:
            logger.warning(f"ë² ì´ì§€ì•ˆ ìµœì í™” ì‹¤íŒ¨, ëœë¤ ìƒ˜í”Œë§ìœ¼ë¡œ ëŒ€ì²´: {e}")
            return self._random_sample()
    
    def _optimize_acquisition_function(self) -> np.ndarray:
        """íšë“ í•¨ìˆ˜ ìµœì í™” (Expected Improvement)"""
        def acquisition_function(x):
            x = x.reshape(1, -1)
            mu, sigma = self.gp_model.predict(x, return_std=True)
            
            # Expected Improvement
            if len(self.y_observed) > 0:
                f_best = np.min(self.y_observed)
                xi = 0.01  # exploration parameter
                
                if sigma > 0:
                    z = (f_best - mu - xi) / sigma
                    ei = (f_best - mu - xi) * norm.cdf(z) + sigma * norm.pdf(z)
                else:
                    ei = 0.0
                
                return -ei[0]  # ìµœì†Œí™” ë¬¸ì œë¡œ ë³€í™˜
            else:
                return mu[0]
        
        # ì´ˆê¸°ê°’ë“¤ ìƒì„±
        n_starts = 10
        bounds = []
        
        for hp in self.hyperparameters.values():
            if hp.param_type in ['float', 'int']:
                bounds.append((0, 1))  # ì •ê·œí™”ëœ ë²”ìœ„
        
        bounds = np.array(bounds)
        
        # ë‹¤ì¤‘ ì‹œì‘ì ìœ¼ë¡œ ìµœì í™”
        best_x = None
        best_val = float('inf')
        
        for _ in range(n_starts):
            x0 = np.random.uniform(0, 1, len(bounds))
            
            try:
                result = minimize(acquisition_function, x0, bounds=bounds, method='L-BFGS-B')
                if result.fun < best_val:
                    best_val = result.fun
                    best_x = result.x
            except:
                continue
        
        if best_x is None:
            best_x = np.random.uniform(0, 1, len(bounds))
            
        return best_x
    
    def _normalize_parameters(self, params: Dict[str, Any]) -> np.ndarray:
        """íŒŒë¼ë¯¸í„°ë¥¼ [0,1] ë²”ìœ„ë¡œ ì •ê·œí™”"""
        normalized = []
        
        for name, hp in self.hyperparameters.items():
            value = params[name]
            
            if hp.param_type == 'float':
                norm_value = (value - hp.min_value) / (hp.max_value - hp.min_value)
            elif hp.param_type == 'int':
                norm_value = (value - hp.min_value) / (hp.max_value - hp.min_value)
            elif hp.param_type == 'categorical':
                norm_value = hp.choices.index(value) / (len(hp.choices) - 1)
                
            normalized.append(norm_value)
            
        return np.array(normalized)
    
    def _denormalize_parameters(self, normalized_params: np.ndarray) -> Dict[str, Any]:
        """ì •ê·œí™”ëœ íŒŒë¼ë¯¸í„°ë¥¼ ì›ë˜ ë²”ìœ„ë¡œ ë³µì›"""
        params = {}
        
        for i, (name, hp) in enumerate(self.hyperparameters.items()):
            norm_value = normalized_params[i]
            
            if hp.param_type == 'float':
                value = hp.min_value + norm_value * (hp.max_value - hp.min_value)
            elif hp.param_type == 'int':
                value = int(hp.min_value + norm_value * (hp.max_value - hp.min_value))
            elif hp.param_type == 'categorical':
                idx = int(norm_value * (len(hp.choices) - 1))
                value = hp.choices[idx]
                
            params[name] = value
            
        return params
    
    def update_observation(self, parameters: Dict[str, Any], objective_value: float):
        """ìƒˆë¡œìš´ ê´€ì°°ê°’ìœ¼ë¡œ ëª¨ë¸ ì—…ë°ì´íŠ¸"""
        normalized_x = self._normalize_parameters(parameters)
        self.X_observed.append(normalized_x.tolist())
        self.y_observed.append(objective_value)
        self.iteration += 1
        
        # íŒŒë¼ë¯¸í„° ì¤‘ìš”ë„ ì—…ë°ì´íŠ¸
        if len(self.X_observed) > 5:
            self._update_parameter_importance()
    
    def _update_parameter_importance(self):
        """íŒŒë¼ë¯¸í„° ì¤‘ìš”ë„ ê³„ì‚°"""
        try:
            if self.gp_model is not None and len(self.X_observed) > 5:
                # íŠ¹ì„± ì¤‘ìš”ë„ë¥¼ ê¸¸ì´ ìŠ¤ì¼€ì¼ì˜ ì—­ìˆ˜ë¡œ ê³„ì‚°
                length_scales = self.gp_model.kernel_.length_scale
                
                if np.isscalar(length_scales):
                    length_scales = np.array([length_scales] * len(self.hyperparameters))
                
                # ì •ê·œí™”
                importances = 1.0 / (length_scales + 1e-10)
                importances = importances / np.sum(importances)
                
                # ì—…ë°ì´íŠ¸
                for i, (name, hp) in enumerate(self.hyperparameters.items()):
                    if i < len(importances):
                        hp.importance = float(importances[i])
                        
        except Exception as e:
            logger.debug(f"íŒŒë¼ë¯¸í„° ì¤‘ìš”ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")

class ErrorAnalyzer:
    """ì˜ˆì¸¡ ì˜¤ì°¨ ë¶„ì„ê¸°"""
    
    def __init__(self, window_size: int = 100):
        self.window_size = window_size
        self.error_history = deque(maxlen=window_size)
        self.feature_error_correlation = defaultdict(list)
        self.temporal_error_patterns = defaultdict(list)
        
    def analyze_prediction_error(self, predicted: float, actual: float, features: np.ndarray, 
                                timestamp: datetime, market_condition: str) -> Dict[str, Any]:
        """ì˜ˆì¸¡ ì˜¤ì°¨ ìƒì„¸ ë¶„ì„"""
        error = abs(predicted - actual)
        error_percent = error / actual if actual != 0 else 0
        
        # ì˜¤ì°¨ ê¸°ë¡
        error_record = {
            'error': error,
            'error_percent': error_percent,
            'predicted': predicted,
            'actual': actual,
            'features': features,
            'timestamp': timestamp,
            'market_condition': market_condition,
            'hour': timestamp.hour,
            'weekday': timestamp.weekday()
        }
        
        self.error_history.append(error_record)
        
        # ë¶„ì„ ê²°ê³¼
        analysis = {
            'current_error': error_percent,
            'error_trend': self._analyze_error_trend(),
            'feature_correlation': self._analyze_feature_error_correlation(features, error_percent),
            'temporal_patterns': self._analyze_temporal_patterns(),
            'market_condition_impact': self._analyze_market_condition_impact(market_condition, error_percent),
            'improvement_suggestions': self._generate_improvement_suggestions()
        }
        
        return analysis
    
    def _analyze_error_trend(self) -> Dict[str, float]:
        """ì˜¤ì°¨ ì¶”ì„¸ ë¶„ì„"""
        if len(self.error_history) < 10:
            return {'trend': 0.0, 'volatility': 0.0, 'recent_avg': 0.0}
        
        recent_errors = [record['error_percent'] for record in list(self.error_history)[-20:]]
        older_errors = [record['error_percent'] for record in list(self.error_history)[-40:-20]]
        
        if len(older_errors) == 0:
            trend = 0.0
        else:
            trend = np.mean(recent_errors) - np.mean(older_errors)
        
        volatility = np.std(recent_errors)
        recent_avg = np.mean(recent_errors)
        
        return {
            'trend': float(trend),
            'volatility': float(volatility),
            'recent_avg': float(recent_avg)
        }
    
    def _analyze_feature_error_correlation(self, features: np.ndarray, error: float) -> Dict[int, float]:
        """íŠ¹ì„±ê³¼ ì˜¤ì°¨ ê°„ì˜ ìƒê´€ê´€ê³„ ë¶„ì„"""
        correlations = {}
        
        if len(self.error_history) < 20:
            return correlations
        
        # ìµœê·¼ ê¸°ë¡ë“¤ë¡œ ìƒê´€ê´€ê³„ ê³„ì‚°
        recent_records = list(self.error_history)[-50:]
        
        for feature_idx in range(min(len(features), 20)):  # ìƒìœ„ 20ê°œ íŠ¹ì„±ë§Œ
            feature_values = []
            error_values = []
            
            for record in recent_records:
                if len(record['features']) > feature_idx:
                    feature_values.append(record['features'][feature_idx])
                    error_values.append(record['error_percent'])
            
            if len(feature_values) > 5:
                correlation = np.corrcoef(feature_values, error_values)[0, 1]
                if not np.isnan(correlation):
                    correlations[feature_idx] = float(correlation)
        
        return correlations
    
    def _analyze_temporal_patterns(self) -> Dict[str, Any]:
        """ì‹œê°„ë³„ ì˜¤ì°¨ íŒ¨í„´ ë¶„ì„"""
        if len(self.error_history) < 24:
            return {}
        
        # ì‹œê°„ëŒ€ë³„ ì˜¤ì°¨
        hourly_errors = defaultdict(list)
        weekday_errors = defaultdict(list)
        
        for record in self.error_history:
            hourly_errors[record['hour']].append(record['error_percent'])
            weekday_errors[record['weekday']].append(record['error_percent'])
        
        # í†µê³„ ê³„ì‚°
        patterns = {
            'worst_hours': [],
            'best_hours': [],
            'worst_weekdays': [],
            'best_weekdays': []
        }
        
        # ì‹œê°„ëŒ€ë³„ ë¶„ì„
        hour_stats = {}
        for hour, errors in hourly_errors.items():
            if len(errors) >= 3:
                hour_stats[hour] = np.mean(errors)
        
        if hour_stats:
            sorted_hours = sorted(hour_stats.items(), key=lambda x: x[1])
            patterns['best_hours'] = [h[0] for h in sorted_hours[:3]]
            patterns['worst_hours'] = [h[0] for h in sorted_hours[-3:]]
        
        # ìš”ì¼ë³„ ë¶„ì„
        weekday_stats = {}
        for weekday, errors in weekday_errors.items():
            if len(errors) >= 3:
                weekday_stats[weekday] = np.mean(errors)
        
        if weekday_stats:
            sorted_weekdays = sorted(weekday_stats.items(), key=lambda x: x[1])
            patterns['best_weekdays'] = [w[0] for w in sorted_weekdays[:2]]
            patterns['worst_weekdays'] = [w[0] for w in sorted_weekdays[-2:]]
        
        return patterns
    
    def _analyze_market_condition_impact(self, current_condition: str, current_error: float) -> Dict[str, float]:
        """ì‹œì¥ ì¡°ê±´ë³„ ì˜¤ì°¨ ì˜í–¥ ë¶„ì„"""
        condition_errors = defaultdict(list)
        
        for record in self.error_history:
            condition_errors[record['market_condition']].append(record['error_percent'])
        
        condition_stats = {}
        for condition, errors in condition_errors.items():
            if len(errors) >= 3:
                condition_stats[condition] = {
                    'avg_error': np.mean(errors),
                    'error_std': np.std(errors),
                    'sample_count': len(errors)
                }
        
        return condition_stats
    
    def _generate_improvement_suggestions(self) -> List[str]:
        """ê°œì„  ì œì•ˆ ìƒì„±"""
        suggestions = []
        
        if len(self.error_history) < 20:
            return ["ë” ë§ì€ ë°ì´í„° ìˆ˜ì§‘ í•„ìš”"]
        
        # ìµœê·¼ ì˜¤ì°¨ ì¶”ì„¸
        trend_analysis = self._analyze_error_trend()
        
        if trend_analysis['trend'] > 0.01:
            suggestions.append("ì˜¤ì°¨ ì¦ê°€ ì¶”ì„¸ - ëª¨ë¸ ì¬í•™ìŠµ ê¶Œì¥")
        
        if trend_analysis['volatility'] > 0.05:
            suggestions.append("ë†’ì€ ì˜¤ì°¨ ë³€ë™ì„± - ì •ê·œí™” ê°•í™” í•„ìš”")
        
        if trend_analysis['recent_avg'] > 0.1:
            suggestions.append("ë†’ì€ í‰ê·  ì˜¤ì°¨ - íŠ¹ì„± ì¬ì„ íƒ ê¶Œì¥")
        
        # ì‹œê°„ë³„ íŒ¨í„´ ë¶„ì„
        temporal_patterns = self._analyze_temporal_patterns()
        if temporal_patterns.get('worst_hours'):
            worst_hours = temporal_patterns['worst_hours']
            suggestions.append(f"ì‹œê°„ëŒ€ {worst_hours}ì—ì„œ ì˜¤ì°¨ ì¦ê°€ - ì‹œê°„ íŠ¹ì„± ê°•í™” í•„ìš”")
        
        return suggestions

class FeedbackOptimizationSystem:
    """í”¼ë“œë°± ê¸°ë°˜ ìµœì í™” ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.base_path = "/Users/parkyoungjun/Desktop/BTC_Analysis_System"
        self.db_path = os.path.join(self.base_path, "feedback_optimization.db")
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.error_analyzer = ErrorAnalyzer()
        self.bayesian_optimizer = None
        
        # ìµœì í™” ì„¤ì •
        self.optimization_targets = [
            OptimizationTarget("prediction_accuracy", 0.4, minimize=False),
            OptimizationTarget("error_volatility", 0.3, minimize=True),
            OptimizationTarget("convergence_speed", 0.2, minimize=False), 
            OptimizationTarget("stability", 0.1, minimize=False)
        ]
        
        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì •ì˜
        self.hyperparameters = [
            HyperParameter("learning_rate", "float", 0.0001, 0.01, current_value=0.001),
            HyperParameter("batch_size", "int", 8, 128, current_value=32),
            HyperParameter("hidden_size", "int", 32, 512, current_value=128),
            HyperParameter("dropout_rate", "float", 0.1, 0.8, current_value=0.2),
            HyperParameter("weight_decay", "float", 1e-6, 1e-2, current_value=1e-4),
            HyperParameter("patience", "int", 5, 50, current_value=10),
            HyperParameter("feature_selection_k", "int", 10, 100, current_value=50),
            HyperParameter("momentum", "float", 0.8, 0.99, current_value=0.9),
            HyperParameter("lr_scheduler", "categorical", choices=["step", "exponential", "cosine"], current_value="exponential")
        ]
        
        # ìƒíƒœ ì¶”ì 
        self.optimization_history = []
        self.current_parameters = {hp.name: hp.current_value for hp in self.hyperparameters}
        self.best_parameters = self.current_parameters.copy()
        self.best_objective_value = float('inf')
        
        # ìë™ ìµœì í™” ì„¤ì •
        self.auto_optimization_enabled = True
        self.optimization_interval = 100  # Në²ˆì˜ ì˜ˆì¸¡ë§ˆë‹¤ ìµœì í™”
        self.predictions_since_optimization = 0
        
        self.init_database()
        self.init_bayesian_optimizer()
    
    def init_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # ì˜¤ì°¨ ë¶„ì„ ê¸°ë¡
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS error_analysis (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT NOT NULL,
                    predicted_value REAL,
                    actual_value REAL,
                    error_percent REAL,
                    market_condition TEXT,
                    improvement_suggestions TEXT,
                    feature_correlations TEXT
                )
            ''')
            
            # ìµœì í™” ê¸°ë¡
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS optimization_history (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT NOT NULL,
                    iteration INTEGER,
                    parameters TEXT NOT NULL,
                    objective_value REAL,
                    individual_metrics TEXT,
                    improvement REAL,
                    duration_seconds REAL
                )
            ''')
            
            # í•˜ì´í¼íŒŒë¼ë¯¸í„° ê¸°ë¡
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS hyperparameter_tracking (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT NOT NULL,
                    parameter_name TEXT NOT NULL,
                    old_value TEXT,
                    new_value TEXT,
                    importance REAL,
                    reason TEXT
                )
            ''')
            
            conn.commit()
            conn.close()
            
            logger.info("âœ… í”¼ë“œë°± ìµœì í™” ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def init_bayesian_optimizer(self):
        """ë² ì´ì§€ì•ˆ ìµœì í™”ê¸° ì´ˆê¸°í™”"""
        self.bayesian_optimizer = BayesianOptimizer(self.hyperparameters)
    
    async def process_prediction_feedback(self, predicted: float, actual: float, 
                                        features: np.ndarray, market_condition: str) -> Dict[str, Any]:
        """ì˜ˆì¸¡ í”¼ë“œë°± ì²˜ë¦¬"""
        try:
            timestamp = datetime.now()
            
            # 1. ì˜¤ì°¨ ë¶„ì„
            error_analysis = self.error_analyzer.analyze_prediction_error(
                predicted, actual, features, timestamp, market_condition
            )
            
            # 2. ë°ì´í„°ë² ì´ìŠ¤ ê¸°ë¡
            await self.record_error_analysis(predicted, actual, error_analysis, market_condition)
            
            # 3. ìë™ ìµœì í™” ì²´í¬
            self.predictions_since_optimization += 1
            optimization_result = None
            
            if (self.auto_optimization_enabled and 
                self.predictions_since_optimization >= self.optimization_interval):
                
                optimization_result = await self.run_automatic_optimization()
                self.predictions_since_optimization = 0
            
            return {
                'error_analysis': error_analysis,
                'optimization_triggered': optimization_result is not None,
                'optimization_result': optimization_result,
                'current_parameters': self.current_parameters,
                'predictions_until_next_optimization': self.optimization_interval - self.predictions_since_optimization
            }
            
        except Exception as e:
            logger.error(f"ì˜ˆì¸¡ í”¼ë“œë°± ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {'error': str(e)}
    
    async def run_automatic_optimization(self) -> Optional[OptimizationResult]:
        """ìë™ ìµœì í™” ì‹¤í–‰"""
        try:
            logger.info("ğŸ”§ ìë™ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘")
            start_time = datetime.now()
            
            # 1. ìƒˆë¡œìš´ íŒŒë¼ë¯¸í„° ì œì•ˆ
            suggested_params = self.bayesian_optimizer.suggest_parameters()
            
            # 2. ì œì•ˆëœ íŒŒë¼ë¯¸í„°ë¡œ ì„±ëŠ¥ í‰ê°€
            objective_value, individual_metrics = await self.evaluate_parameters(suggested_params)
            
            # 3. ë² ì´ì§€ì•ˆ ì˜µí‹°ë§ˆì´ì € ì—…ë°ì´íŠ¸
            self.bayesian_optimizer.update_observation(suggested_params, objective_value)
            
            # 4. ìµœì ê°’ ì—…ë°ì´íŠ¸
            improvement = self.best_objective_value - objective_value
            if objective_value < self.best_objective_value:
                self.best_objective_value = objective_value
                self.best_parameters = suggested_params.copy()
                self.current_parameters = suggested_params.copy()
                
                # íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ ì•Œë¦¼
                logger.info(f"âœ… ìƒˆë¡œìš´ ìµœì  íŒŒë¼ë¯¸í„° ë°œê²¬! ê°œì„ ë„: {improvement:.4f}")
                await self.update_hyperparameters(suggested_params, "optimization")
            
            # 5. ê²°ê³¼ ê¸°ë¡
            duration = (datetime.now() - start_time).total_seconds()
            result = OptimizationResult(
                iteration=self.bayesian_optimizer.iteration,
                parameters=suggested_params,
                objective_value=objective_value,
                individual_metrics=individual_metrics,
                improvement=improvement,
                timestamp=start_time,
                duration_seconds=duration
            )
            
            self.optimization_history.append(result)
            await self.record_optimization_result(result)
            
            return result
            
        except Exception as e:
            logger.error(f"ìë™ ìµœì í™” ì‹¤íŒ¨: {e}")
            return None
    
    async def evaluate_parameters(self, parameters: Dict[str, Any]) -> Tuple[float, Dict[str, float]]:
        """íŒŒë¼ë¯¸í„° ì„±ëŠ¥ í‰ê°€"""
        try:
            # ê°„ë‹¨í•œ ì‹œë®¬ë ˆì´ì…˜ ê¸°ë°˜ í‰ê°€ (ì‹¤ì œë¡œëŠ” ëª¨ë¸ ì¬í•™ìŠµ í•„ìš”)
            
            # 1. ìµœê·¼ ì˜¤ì°¨ ë°ì´í„° ê¸°ë°˜ ì„±ëŠ¥ ì˜ˆì¸¡
            if len(self.error_analyzer.error_history) < 10:
                # ë°ì´í„°ê°€ ë¶€ì¡±í•˜ë©´ ê¸°ë³¸ê°’ ë°˜í™˜
                return 0.5, {'accuracy': 0.5, 'stability': 0.5, 'speed': 0.5}
            
            # 2. íŒŒë¼ë¯¸í„° ê¸°ë°˜ ì„±ëŠ¥ ëª¨ë¸ë§
            recent_errors = [record['error_percent'] for record in list(self.error_analyzer.error_history)[-20:]]
            base_error = np.mean(recent_errors)
            
            # í•™ìŠµë¥  ì˜í–¥
            lr_factor = parameters.get('learning_rate', 0.001)
            if lr_factor > 0.005:  # ë†’ì€ í•™ìŠµë¥ ì€ ë¶ˆì•ˆì •
                stability_penalty = (lr_factor - 0.005) * 10
            else:
                stability_penalty = 0
            
            # ë°°ì¹˜ í¬ê¸° ì˜í–¥
            batch_size = parameters.get('batch_size', 32)
            batch_factor = abs(batch_size - 32) / 32 * 0.1  # 32ì—ì„œ ë©€ì–´ì§ˆìˆ˜ë¡ ì„±ëŠ¥ ì €í•˜
            
            # ë“œë¡­ì•„ì›ƒ ì˜í–¥
            dropout = parameters.get('dropout_rate', 0.2)
            dropout_factor = abs(dropout - 0.3) * 0.2  # 0.3ì´ ìµœì ì´ë¼ê³  ê°€ì •
            
            # íŠ¹ì„± ì„ íƒ ê°œìˆ˜ ì˜í–¥
            feature_k = parameters.get('feature_selection_k', 50)
            feature_factor = max(0, (50 - feature_k) / 50 * 0.1)  # íŠ¹ì„±ì´ ì ì„ìˆ˜ë¡ ì„±ëŠ¥ ì €í•˜
            
            # ì¢…í•© ì„±ëŠ¥ ì ìˆ˜ ê³„ì‚°
            predicted_error = base_error + stability_penalty + batch_factor + dropout_factor + feature_factor
            accuracy = max(0.1, 1.0 - predicted_error)  # ì •í™•ë„
            stability = max(0.1, 1.0 - stability_penalty)  # ì•ˆì •ì„±
            speed = min(1.0, batch_size / 64)  # ì²˜ë¦¬ ì†ë„ (ë°°ì¹˜ í¬ê¸°ì— ë¹„ë¡€)
            
            # ê°œë³„ ë©”íŠ¸ë¦­
            individual_metrics = {
                'accuracy': accuracy,
                'stability': stability,
                'speed': speed,
                'predicted_error': predicted_error
            }
            
            # ëª©ì í•¨ìˆ˜ ê°’ (ê°€ì¤‘í•©)
            objective_value = (
                (1 - accuracy) * 0.5 +  # ì •í™•ë„ (ìµœëŒ€í™” â†’ ìµœì†Œí™”)
                (1 - stability) * 0.3 +  # ì•ˆì •ì„± (ìµœëŒ€í™” â†’ ìµœì†Œí™”)
                (1 - speed) * 0.2        # ì†ë„ (ìµœëŒ€í™” â†’ ìµœì†Œí™”)
            )
            
            return objective_value, individual_metrics
            
        except Exception as e:
            logger.error(f"íŒŒë¼ë¯¸í„° í‰ê°€ ì‹¤íŒ¨: {e}")
            return 1.0, {'error': 1.0}
    
    async def update_hyperparameters(self, new_parameters: Dict[str, Any], reason: str):
        """í•˜ì´í¼íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸"""
        try:
            for param_name, new_value in new_parameters.items():
                # ê¸°ì¡´ íŒŒë¼ë¯¸í„° ì°¾ê¸°
                hp = next((hp for hp in self.hyperparameters if hp.name == param_name), None)
                
                if hp is not None:
                    old_value = hp.current_value
                    hp.current_value = new_value
                    
                    # ìµœì ê°’ ì—…ë°ì´íŠ¸
                    if new_value != old_value:
                        hp.best_value = new_value
                        
                        # ë³€ê²½ ê¸°ë¡
                        await self.record_hyperparameter_change(param_name, old_value, new_value, hp.importance, reason)
            
        except Exception as e:
            logger.error(f"í•˜ì´í¼íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    async def record_error_analysis(self, predicted: float, actual: float, 
                                  analysis: Dict[str, Any], market_condition: str):
        """ì˜¤ì°¨ ë¶„ì„ ê¸°ë¡"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT INTO error_analysis 
                (timestamp, predicted_value, actual_value, error_percent, market_condition, 
                 improvement_suggestions, feature_correlations)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            ''', (
                datetime.now().isoformat(),
                predicted,
                actual,
                analysis['current_error'],
                market_condition,
                json.dumps(analysis['improvement_suggestions']),
                json.dumps(analysis['feature_correlation'])
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            logger.error(f"ì˜¤ì°¨ ë¶„ì„ ê¸°ë¡ ì‹¤íŒ¨: {e}")
    
    async def record_optimization_result(self, result: OptimizationResult):
        """ìµœì í™” ê²°ê³¼ ê¸°ë¡"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT INTO optimization_history 
                (timestamp, iteration, parameters, objective_value, individual_metrics, improvement, duration_seconds)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            ''', (
                result.timestamp.isoformat(),
                result.iteration,
                json.dumps(result.parameters),
                result.objective_value,
                json.dumps(result.individual_metrics),
                result.improvement,
                result.duration_seconds
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            logger.error(f"ìµœì í™” ê²°ê³¼ ê¸°ë¡ ì‹¤íŒ¨: {e}")
    
    async def record_hyperparameter_change(self, param_name: str, old_value: Any, 
                                         new_value: Any, importance: float, reason: str):
        """í•˜ì´í¼íŒŒë¼ë¯¸í„° ë³€ê²½ ê¸°ë¡"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT INTO hyperparameter_tracking 
                (timestamp, parameter_name, old_value, new_value, importance, reason)
                VALUES (?, ?, ?, ?, ?, ?)
            ''', (
                datetime.now().isoformat(),
                param_name,
                str(old_value),
                str(new_value),
                importance,
                reason
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            logger.error(f"í•˜ì´í¼íŒŒë¼ë¯¸í„° ë³€ê²½ ê¸°ë¡ ì‹¤íŒ¨: {e}")
    
    async def get_optimization_report(self) -> Dict[str, Any]:
        """ìµœì í™” ë¦¬í¬íŠ¸ ìƒì„±"""
        try:
            conn = sqlite3.connect(self.db_path)
            
            # ìµœê·¼ ìµœì í™” ê¸°ë¡
            optimization_df = pd.read_sql_query('''
                SELECT * FROM optimization_history 
                ORDER BY timestamp DESC LIMIT 20
            ''', conn)
            
            # ìµœê·¼ ì˜¤ì°¨ ë¶„ì„
            error_df = pd.read_sql_query('''
                SELECT * FROM error_analysis 
                ORDER BY timestamp DESC LIMIT 50
            ''', conn)
            
            # í•˜ì´í¼íŒŒë¼ë¯¸í„° ë³€ê²½ ê¸°ë¡
            param_df = pd.read_sql_query('''
                SELECT * FROM hyperparameter_tracking 
                ORDER BY timestamp DESC LIMIT 20
            ''', conn)
            
            conn.close()
            
            # í†µê³„ ê³„ì‚°
            report = {
                'current_parameters': self.current_parameters,
                'best_parameters': self.best_parameters,
                'best_objective_value': self.best_objective_value,
                'optimization_count': len(self.optimization_history),
                'recent_optimizations': optimization_df.to_dict('records'),
                'recent_errors': error_df.to_dict('records'),
                'parameter_changes': param_df.to_dict('records'),
                'hyperparameter_importance': {
                    hp.name: hp.importance for hp in self.hyperparameters
                }
            }
            
            # ì„±ëŠ¥ ì¶”ì„¸
            if not error_df.empty:
                recent_errors = error_df['error_percent'].tolist()
                report['error_statistics'] = {
                    'mean': np.mean(recent_errors),
                    'std': np.std(recent_errors),
                    'min': np.min(recent_errors),
                    'max': np.max(recent_errors),
                    'trend': np.polyfit(range(len(recent_errors)), recent_errors, 1)[0] if len(recent_errors) > 1 else 0
                }
            
            return report
            
        except Exception as e:
            logger.error(f"ìµœì í™” ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return {'error': str(e)}
    
    async def suggest_manual_improvements(self) -> List[str]:
        """ìˆ˜ë™ ê°œì„  ì œì•ˆ"""
        suggestions = []
        
        try:
            # 1. ì˜¤ì°¨ ë¶„ì„ ê¸°ë°˜ ì œì•ˆ
            if len(self.error_analyzer.error_history) > 20:
                trend_analysis = self.error_analyzer._analyze_error_trend()
                
                if trend_analysis['trend'] > 0.02:
                    suggestions.append("ğŸ”§ ì˜¤ì°¨ê°€ ì§€ì†ì ìœ¼ë¡œ ì¦ê°€í•˜ê³  ìˆìŠµë‹ˆë‹¤. ëª¨ë¸ ì¬í•™ìŠµì„ ê³ ë ¤í•˜ì„¸ìš”.")
                
                if trend_analysis['volatility'] > 0.08:
                    suggestions.append("ğŸ“Š ì˜¤ì°¨ ë³€ë™ì„±ì´ ë†’ìŠµë‹ˆë‹¤. ì •ê·œí™”ë‚˜ íŠ¹ì„± ìŠ¤ì¼€ì¼ë§ì„ ê°•í™”í•˜ì„¸ìš”.")
                
                if trend_analysis['recent_avg'] > 0.15:
                    suggestions.append("âš ï¸ í‰ê·  ì˜¤ì°¨ê°€ ë†’ìŠµë‹ˆë‹¤. íŠ¹ì„± ì„ íƒì„ ì¬ê²€í† í•˜ì„¸ìš”.")
            
            # 2. í•˜ì´í¼íŒŒë¼ë¯¸í„° ê¸°ë°˜ ì œì•ˆ
            current_lr = self.current_parameters.get('learning_rate', 0.001)
            if current_lr > 0.005:
                suggestions.append("ğŸ¯ í•™ìŠµë¥ ì´ ë„ˆë¬´ ë†’ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë” ì‘ì€ ê°’ì„ ì‹œë„í•´ë³´ì„¸ìš”.")
            elif current_lr < 0.0005:
                suggestions.append("ğŸŒ í•™ìŠµë¥ ì´ ë„ˆë¬´ ë‚®ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•™ìŠµ ì†ë„ê°€ ëŠë ¤ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            
            # 3. ìµœì í™” íˆìŠ¤í† ë¦¬ ê¸°ë°˜ ì œì•ˆ
            if len(self.optimization_history) > 5:
                recent_improvements = [result.improvement for result in self.optimization_history[-5:]]
                if all(imp <= 0.001 for imp in recent_improvements):
                    suggestions.append("ğŸ”„ ìµœê·¼ ìµœì í™”ì—ì„œ ê°œì„ ì´ ë¯¸ë¯¸í•©ë‹ˆë‹¤. ë‹¤ë¥¸ ì ‘ê·¼ë²•ì„ ê³ ë ¤í•´ë³´ì„¸ìš”.")
            
            if not suggestions:
                suggestions.append("âœ… í˜„ì¬ ì‹œìŠ¤í…œì´ ì˜ ì‘ë™í•˜ê³  ìˆìŠµë‹ˆë‹¤.")
            
            return suggestions
            
        except Exception as e:
            logger.error(f"ê°œì„  ì œì•ˆ ìƒì„± ì‹¤íŒ¨: {e}")
            return ["âŒ ê°œì„  ì œì•ˆ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."]

async def run_feedback_optimization_demo():
    """í”¼ë“œë°± ìµœì í™” ì‹œìŠ¤í…œ ë°ëª¨"""
    print("ğŸ”„ í”¼ë“œë°± ë£¨í”„ ë° ìë™ ìµœì í™” ì‹œìŠ¤í…œ ì‹œì‘")
    print("="*60)
    
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    feedback_system = FeedbackOptimizationSystem()
    
    print("ğŸ“Š ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°ë¡œ í”¼ë“œë°± ë£¨í”„ í…ŒìŠ¤íŠ¸ ì¤‘...")
    
    # ì‹œë®¬ë ˆì´ì…˜ ì‹œë‚˜ë¦¬ì˜¤
    base_price = 50000
    scenarios = []
    
    # 100ê°œì˜ ì‹œë®¬ë ˆì´ì…˜ ì˜ˆì¸¡-ì‹¤ì œ ìŒ ìƒì„±
    for i in range(100):
        # ì‹¤ì œ ê°€ê²© (ì‹œê°„ì— ë”°ë¥¸ ì¶”ì„¸ + ë…¸ì´ì¦ˆ)
        trend = i * 10  # ìƒìŠ¹ ì¶”ì„¸
        noise = np.random.normal(0, 500)
        actual_price = base_price + trend + noise
        
        # ì˜ˆì¸¡ ê°€ê²© (ì‹¤ì œ ê°€ê²© ê·¼ì²˜ + ëª¨ë¸ ì˜¤ì°¨)
        model_error = np.random.normal(0, 200 + i * 2)  # ì‹œê°„ì— ë”°ë¼ ì˜¤ì°¨ ì¦ê°€
        predicted_price = actual_price + model_error
        
        # íŠ¹ì„± ë²¡í„° (ëœë¤)
        features = np.random.normal(0, 1, 30)
        
        # ì‹œì¥ ì¡°ê±´
        conditions = ['bull_strong', 'bull_weak', 'sideways_stable', 'bear_weak', 'volatile']
        market_condition = np.random.choice(conditions)
        
        scenarios.append({
            'predicted': predicted_price,
            'actual': actual_price,
            'features': features,
            'market_condition': market_condition,
            'step': i
        })
    
    # í”¼ë“œë°± ë£¨í”„ ì‹¤í–‰
    optimization_count = 0
    
    for i, scenario in enumerate(scenarios):
        # í”¼ë“œë°± ì²˜ë¦¬
        result = await feedback_system.process_prediction_feedback(
            scenario['predicted'],
            scenario['actual'], 
            scenario['features'],
            scenario['market_condition']
        )
        
        # ìµœì í™” ì‹¤í–‰ ì—¬ë¶€ í™•ì¸
        if result.get('optimization_triggered'):
            optimization_count += 1
            opt_result = result.get('optimization_result')
            
            print(f"\nğŸ”§ ìµœì í™” ì‹¤í–‰ #{optimization_count} (ë‹¨ê³„ {i+1})")
            if opt_result:
                print(f"  â€¢ ëª©ì í•¨ìˆ˜ ê°’: {opt_result.objective_value:.4f}")
                print(f"  â€¢ ê°œì„ ë„: {opt_result.improvement:.4f}")
                print(f"  â€¢ ì†Œìš”ì‹œê°„: {opt_result.duration_seconds:.2f}ì´ˆ")
                
                # ì£¼ìš” íŒŒë¼ë¯¸í„° ë³€ê²½ì‚¬í•­ ì¶œë ¥
                key_params = ['learning_rate', 'batch_size', 'dropout_rate']
                for param in key_params:
                    if param in opt_result.parameters:
                        print(f"  â€¢ {param}: {opt_result.parameters[param]}")
        
        # ì§„í–‰ ìƒí™© ì¶œë ¥ (ë§¤ 20 ë‹¨ê³„ë§ˆë‹¤)
        if (i + 1) % 20 == 0:
            error_analysis = result.get('error_analysis', {})
            current_error = error_analysis.get('current_error', 0)
            trend = error_analysis.get('error_trend', {}).get('trend', 0)
            
            print(f"\nğŸ“ˆ ì§„í–‰ ìƒí™© (ë‹¨ê³„ {i+1}/100):")
            print(f"  â€¢ í˜„ì¬ ì˜¤ì°¨: {current_error:.2%}")
            print(f"  â€¢ ì˜¤ì°¨ ì¶”ì„¸: {trend:+.4f}")
            print(f"  â€¢ ë‹¤ìŒ ìµœì í™”ê¹Œì§€: {result.get('predictions_until_next_optimization', 0)}ë‹¨ê³„")
    
    # ìµœì¢… ë¦¬í¬íŠ¸
    print("\n" + "="*60)
    print("ğŸ“‹ ìµœì¢… ìµœì í™” ë¦¬í¬íŠ¸")
    
    report = await feedback_system.get_optimization_report()
    
    if 'error' not in report:
        print(f"ğŸ”§ ì´ ìµœì í™” íšŸìˆ˜: {report.get('optimization_count', 0)}")
        print(f"ğŸ¯ ìµœì  ëª©ì í•¨ìˆ˜ ê°’: {report.get('best_objective_value', 0):.4f}")
        
        # ì˜¤ì°¨ í†µê³„
        error_stats = report.get('error_statistics', {})
        if error_stats:
            print(f"\nğŸ“Š ì˜¤ì°¨ í†µê³„:")
            print(f"  â€¢ í‰ê·  ì˜¤ì°¨: {error_stats.get('mean', 0):.2%}")
            print(f"  â€¢ í‘œì¤€í¸ì°¨: {error_stats.get('std', 0):.2%}")
            print(f"  â€¢ ì¶”ì„¸: {error_stats.get('trend', 0):+.6f}")
        
        # ìµœì  íŒŒë¼ë¯¸í„°
        best_params = report.get('best_parameters', {})
        print(f"\nâš™ï¸ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°:")
        for name, value in best_params.items():
            if name in ['learning_rate', 'batch_size', 'dropout_rate', 'hidden_size']:
                print(f"  â€¢ {name}: {value}")
        
        # íŒŒë¼ë¯¸í„° ì¤‘ìš”ë„
        importance = report.get('hyperparameter_importance', {})
        if importance:
            print(f"\nğŸ“ˆ íŒŒë¼ë¯¸í„° ì¤‘ìš”ë„ (ìƒìœ„ 5ê°œ):")
            sorted_importance = sorted(importance.items(), key=lambda x: x[1], reverse=True)
            for name, imp in sorted_importance[:5]:
                print(f"  â€¢ {name}: {imp:.3f}")
    
    # ê°œì„  ì œì•ˆ
    print(f"\nğŸ’¡ ê°œì„  ì œì•ˆ:")
    suggestions = await feedback_system.suggest_manual_improvements()
    for suggestion in suggestions[:5]:
        print(f"  {suggestion}")
    
    print("\n" + "="*60)
    print("ğŸ‰ í”¼ë“œë°± ë£¨í”„ ë° ìë™ ìµœì í™” ì‹œìŠ¤í…œ ë°ëª¨ ì™„ë£Œ!")
    print("âœ… 90%+ ì •í™•ë„ ìœ ì§€ë¥¼ ìœ„í•œ ì§€ì†ì  ê°œì„  ë©”ì»¤ë‹ˆì¦˜ êµ¬ì¶• ì™„ë£Œ")

if __name__ == "__main__":
    asyncio.run(run_feedback_optimization_demo())