#!/usr/bin/env python3
"""
ğŸ’ í¬ê´„ì  ë¹„íŠ¸ì½”ì¸ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ íŒŒì´í”„ë¼ì¸ (1000+ íŠ¹ì„±)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ¯ ëª©í‘œ: ìµœê³  ì •í™•ë„ì˜ AI ë¹„íŠ¸ì½”ì¸ ê°€ê²© ì˜ˆì¸¡ì„ ìœ„í•œ 1000+ ê³ í’ˆì§ˆ ì˜ˆì¸¡ íŠ¹ì„± ìƒì„±

ğŸ“Š íŠ¹ì„± ë¶„ë¥˜:
â€¢ ê¸°ìˆ ì  ë¶„ì„ íŠ¹ì„±: 300+
â€¢ ì‹œì¥ ë¯¸ì‹œêµ¬ì¡° íŠ¹ì„±: 200+ 
â€¢ ì˜¨ì²´ì¸ ë¶„ì„ íŠ¹ì„±: 200+
â€¢ ê±°ì‹œê²½ì œ íŠ¹ì„±: 100+
â€¢ ê³ ê¸‰ ìˆ˜í•™ íŠ¹ì„±: 200+

ğŸ”§ í•µì‹¬ ê¸°ëŠ¥:
â€¢ íš¨ìœ¨ì  íŠ¹ì„± ê³„ì‚°
â€¢ ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ ë©”ì»¤ë‹ˆì¦˜
â€¢ íŠ¹ì„± ì¤‘ìš”ë„ ìˆœìœ„
â€¢ ìë™ íŠ¹ì„± ì„ íƒ
â€¢ ì„±ëŠ¥ ìµœì í™”
"""

import asyncio
import aiohttp
import pandas as pd
import numpy as np
import json
import os
import sys
import warnings
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Any, Union
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import logging
import sqlite3
from pathlib import Path

# ìˆ˜í•™ ë° ì‹ í˜¸ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from scipy import stats, signal
    from scipy.fft import fft, ifft
    import pywt  # wavelets
    from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    import talib
    ADVANCED_MATH_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ ê³ ê¸‰ ìˆ˜í•™ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¯¸ì„¤ì¹˜: {e}")
    ADVANCED_MATH_AVAILABLE = False

warnings.filterwarnings('ignore')

@dataclass
class FeatureConfig:
    """íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì„¤ì •"""
    lookback_periods: List[int] = None
    timeframes: List[str] = None
    technical_params: Dict[str, Any] = None
    enable_advanced_math: bool = True
    enable_cross_features: bool = True
    max_features: int = 1500
    feature_selection_method: str = "mutual_info"
    
    def __post_init__(self):
        if self.lookback_periods is None:
            self.lookback_periods = [5, 10, 14, 20, 50, 100, 200]
        if self.timeframes is None:
            self.timeframes = ['5m', '15m', '1h', '4h', '1d']
        if self.technical_params is None:
            self.technical_params = {
                'rsi_periods': [9, 14, 21, 25, 30],
                'ma_periods': [5, 10, 20, 50, 100, 200],
                'bb_periods': [10, 20, 50],
                'stoch_periods': [5, 14, 21]
            }

class ComprehensiveFeatureEngineer:
    """í¬ê´„ì  íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, config: FeatureConfig = None):
        self.config = config or FeatureConfig()
        self.logger = self._setup_logger()
        self.features_db_path = "features_database.db"
        self.feature_importance_cache = {}
        
        # íŠ¹ì„± ì¹´í…Œê³ ë¦¬ë³„ ìƒì„±ê¸° ì´ˆê¸°í™”
        self.technical_generator = TechnicalFeatureGenerator(self.config)
        self.microstructure_generator = MarketMicrostructureGenerator(self.config)
        self.onchain_generator = OnChainFeatureGenerator(self.config)
        self.macro_generator = MacroEconomicGenerator(self.config)
        self.math_generator = AdvancedMathFeatureGenerator(self.config)
        
        self._init_database()
        
    def _setup_logger(self) -> logging.Logger:
        """ë¡œê¹… ì„¤ì •"""
        logger = logging.getLogger(__name__)
        logger.setLevel(logging.INFO)
        handler = logging.StreamHandler()
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        return logger
        
    def _init_database(self):
        """íŠ¹ì„± ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        conn = sqlite3.connect(self.features_db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS feature_importance (
            feature_name TEXT PRIMARY KEY,
            importance_score REAL,
            category TEXT,
            last_updated TIMESTAMP,
            usage_count INTEGER DEFAULT 0
        )
        ''')
        
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS feature_values (
            timestamp TIMESTAMP,
            feature_name TEXT,
            value REAL,
            PRIMARY KEY (timestamp, feature_name)
        )
        ''')
        
        conn.commit()
        conn.close()
    
    async def generate_all_features(self, market_data: Dict[str, Any]) -> pd.DataFrame:
        """ëª¨ë“  ì¹´í…Œê³ ë¦¬ì˜ íŠ¹ì„± ìƒì„±"""
        self.logger.info("ğŸš€ 1000+ íŠ¹ì„± ìƒì„± ì‹œì‘")
        
        all_features = {}
        
        # ë³‘ë ¬ë¡œ ê° ì¹´í…Œê³ ë¦¬ íŠ¹ì„± ìƒì„±
        tasks = [
            self.technical_generator.generate_features(market_data),
            self.microstructure_generator.generate_features(market_data),
            self.onchain_generator.generate_features(market_data),
            self.macro_generator.generate_features(market_data),
            self.math_generator.generate_features(market_data)
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ê²°ê³¼ í†µí•©
        categories = ['technical', 'microstructure', 'onchain', 'macro', 'math']
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                self.logger.error(f"âŒ {categories[i]} íŠ¹ì„± ìƒì„± ì‹¤íŒ¨: {result}")
                continue
            all_features.update(result)
        
        # êµì°¨ íŠ¹ì„± ìƒì„±
        if self.config.enable_cross_features and len(all_features) > 0:
            cross_features = await self._generate_cross_features(all_features)
            all_features.update(cross_features)
        
        # DataFrame ë³€í™˜
        features_df = pd.DataFrame([all_features])
        
        # íŠ¹ì„± ì„ íƒ ì ìš©
        if len(features_df.columns) > self.config.max_features:
            features_df = await self._select_best_features(features_df, market_data)
        
        self.logger.info(f"âœ… ì´ {len(features_df.columns)}ê°œ íŠ¹ì„± ìƒì„± ì™„ë£Œ")
        
        # íŠ¹ì„±ê°’ ì €ì¥
        await self._save_features_to_db(features_df)
        
        return features_df
    
    async def _generate_cross_features(self, features: Dict[str, float]) -> Dict[str, float]:
        """êµì°¨ íŠ¹ì„± ìƒì„± (ìƒí˜¸ì‘ìš© íŠ¹ì„±)"""
        cross_features = {}
        feature_names = list(features.keys())
        
        # ì£¼ìš” íŠ¹ì„±ë“¤ ê°„ì˜ ìƒí˜¸ì‘ìš©
        important_features = [
            'btc_price', 'volume', 'rsi_14', 'macd_line', 'bb_position',
            'mvrv', 'nvt_ratio', 'hash_rate', 'active_addresses',
            'funding_rate', 'open_interest', 'fear_greed_index'
        ]
        
        available_important = [f for f in important_features if f in features]
        
        # ê³±ì…ˆ êµì°¨ íŠ¹ì„±
        for i in range(len(available_important)):
            for j in range(i+1, min(i+10, len(available_important))):  # ê³„ì‚°ëŸ‰ ì œí•œ
                f1, f2 = available_important[i], available_important[j]
                if pd.notna(features[f1]) and pd.notna(features[f2]):
                    cross_features[f"{f1}_x_{f2}"] = features[f1] * features[f2]
        
        # ë¹„ìœ¨ êµì°¨ íŠ¹ì„±
        for i in range(len(available_important)):
            for j in range(i+1, min(i+8, len(available_important))):
                f1, f2 = available_important[i], available_important[j]
                if pd.notna(features[f1]) and pd.notna(features[f2]) and features[f2] != 0:
                    cross_features[f"{f1}_div_{f2}"] = features[f1] / features[f2]
        
        # ì°¨ì´ êµì°¨ íŠ¹ì„±
        for i in range(len(available_important)):
            for j in range(i+1, min(i+6, len(available_important))):
                f1, f2 = available_important[i], available_important[j]
                if pd.notna(features[f1]) and pd.notna(features[f2]):
                    cross_features[f"{f1}_minus_{f2}"] = features[f1] - features[f2]
        
        return cross_features
    
    async def _select_best_features(self, features_df: pd.DataFrame, market_data: Dict[str, Any]) -> pd.DataFrame:
        """ìµœì  íŠ¹ì„± ì„ íƒ"""
        try:
            # ëª©í‘œ ë³€ìˆ˜ (ë‹¤ìŒ ì‹œê°„ ê°€ê²© ë³€í™”ìœ¨)
            if 'price_change_1h' in market_data:
                target = market_data['price_change_1h']
            else:
                # ê°„ë‹¨í•œ ëª©í‘œ ë³€ìˆ˜ ìƒì„±
                target = np.random.randn(len(features_df))
            
            # NaN ê°’ ì²˜ë¦¬
            features_clean = features_df.fillna(0)
            
            # íŠ¹ì„± ì„ íƒ ë°©ë²•ì— ë”°ë¼ ì„ íƒ
            if self.config.feature_selection_method == "mutual_info":
                selector = SelectKBest(score_func=mutual_info_regression, k=self.config.max_features)
            else:
                selector = SelectKBest(score_func=f_regression, k=self.config.max_features)
            
            # íƒ€ê²Ÿì´ ë‹¨ì¼ê°’ì´ë©´ ë°°ì—´ë¡œ ë³€í™˜
            if np.isscalar(target):
                target = np.array([target])
            elif len(np.array(target).shape) == 0:
                target = np.array([target])
            
            # íŠ¹ì„± ì„ íƒ ì ìš©
            if len(features_clean) == 1 and len(target) == 1:
                selected_features = selector.fit_transform(features_clean, target)
                selected_feature_names = selector.get_feature_names_out()
                
                # íŠ¹ì„± ì¤‘ìš”ë„ ì—…ë°ì´íŠ¸
                scores = selector.scores_
                for name, score in zip(selected_feature_names, scores):
                    await self._update_feature_importance(name, score)
                
                return pd.DataFrame(selected_features, columns=selected_feature_names)
            else:
                # í¬ê¸°ê°€ ë§ì§€ ì•Šìœ¼ë©´ ì „ì²´ íŠ¹ì„± ë°˜í™˜
                return features_df
                
        except Exception as e:
            self.logger.warning(f"íŠ¹ì„± ì„ íƒ ì‹¤íŒ¨, ì „ì²´ íŠ¹ì„± ì‚¬ìš©: {e}")
            return features_df
    
    async def _update_feature_importance(self, feature_name: str, importance: float):
        """íŠ¹ì„± ì¤‘ìš”ë„ ì—…ë°ì´íŠ¸"""
        conn = sqlite3.connect(self.features_db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
        INSERT OR REPLACE INTO feature_importance 
        (feature_name, importance_score, last_updated, usage_count)
        VALUES (?, ?, ?, 
                COALESCE((SELECT usage_count FROM feature_importance WHERE feature_name = ?), 0) + 1)
        ''', (feature_name, importance, datetime.now(), feature_name))
        
        conn.commit()
        conn.close()
    
    async def _save_features_to_db(self, features_df: pd.DataFrame):
        """íŠ¹ì„±ê°’ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥"""
        conn = sqlite3.connect(self.features_db_path)
        timestamp = datetime.now()
        
        for column in features_df.columns:
            value = features_df[column].iloc[0] if len(features_df) > 0 else 0
            if pd.notna(value):
                cursor = conn.cursor()
                cursor.execute('''
                INSERT OR REPLACE INTO feature_values (timestamp, feature_name, value)
                VALUES (?, ?, ?)
                ''', (timestamp, column, float(value)))
        
        conn.commit()
        conn.close()

    def get_feature_importance_ranking(self) -> pd.DataFrame:
        """íŠ¹ì„± ì¤‘ìš”ë„ ìˆœìœ„ ë°˜í™˜"""
        conn = sqlite3.connect(self.features_db_path)
        
        query = '''
        SELECT feature_name, importance_score, usage_count, last_updated
        FROM feature_importance 
        ORDER BY importance_score DESC, usage_count DESC
        '''
        
        df = pd.read_sql_query(query, conn)
        conn.close()
        
        return df

class TechnicalFeatureGenerator:
    """ê¸°ìˆ ì  ë¶„ì„ íŠ¹ì„± ìƒì„±ê¸° (300+ íŠ¹ì„±)"""
    
    def __init__(self, config: FeatureConfig):
        self.config = config
    
    async def generate_features(self, market_data: Dict[str, Any]) -> Dict[str, float]:
        """ê¸°ìˆ ì  ë¶„ì„ íŠ¹ì„± ìƒì„±"""
        features = {}
        
        # ê¸°ë³¸ ê°€ê²© ë°ì´í„° ì¶”ì¶œ
        try:
            price = float(market_data.get('btc_price', 0))
            volume = float(market_data.get('volume', 0))
            high = float(market_data.get('high', price))
            low = float(market_data.get('low', price))
            close = price
            open_price = float(market_data.get('open', price))
        except (ValueError, TypeError):
            # ê¸°ë³¸ê°’ ì„¤ì •
            price = volume = high = low = close = open_price = 0
        
        # ê°€ê²© ê¸°ë°˜ íŠ¹ì„±
        if price > 0:
            features.update({
                'price': price,
                'log_price': np.log(price) if price > 0 else 0,
                'price_normalized': price / 100000 if price > 0 else 0,
            })
        
        # RSI ë‹¤ì–‘í•œ ê¸°ê°„
        for period in self.config.technical_params['rsi_periods']:
            features[f'rsi_{period}'] = self._calculate_simple_rsi(price, period)
        
        # ì´ë™í‰ê·  (ë‹¤ì–‘í•œ ê¸°ê°„)
        for period in self.config.technical_params['ma_periods']:
            features[f'sma_{period}'] = self._calculate_simple_ma(price, period)
            features[f'ema_{period}'] = self._calculate_simple_ema(price, period)
            if price > 0:
                features[f'price_to_sma_{period}'] = price / (self._calculate_simple_ma(price, period) or price)
        
        # ë³¼ë¦°ì € ë°´ë“œ (ë‹¤ì–‘í•œ ê¸°ê°„)
        for period in self.config.technical_params['bb_periods']:
            bb_upper, bb_lower, bb_middle = self._calculate_bollinger_bands(price, period)
            features[f'bb_upper_{period}'] = bb_upper
            features[f'bb_lower_{period}'] = bb_lower
            features[f'bb_middle_{period}'] = bb_middle
            if bb_upper > bb_lower:
                features[f'bb_position_{period}'] = (price - bb_lower) / (bb_upper - bb_lower)
            features[f'bb_width_{period}'] = bb_upper - bb_lower if bb_upper >= bb_lower else 0
        
        # MACD ë³€í˜•
        for fast in [12, 8, 5]:
            for slow in [26, 21, 13]:
                if fast < slow:
                    macd_line, macd_signal = self._calculate_macd(price, fast, slow, 9)
                    features[f'macd_{fast}_{slow}'] = macd_line
                    features[f'macd_signal_{fast}_{slow}'] = macd_signal
                    features[f'macd_histogram_{fast}_{slow}'] = macd_line - macd_signal
        
        # Stochastic Oscillator ë³€í˜•
        for period in self.config.technical_params['stoch_periods']:
            stoch_k = self._calculate_stochastic_k(high, low, close, period)
            features[f'stoch_k_{period}'] = stoch_k
            features[f'stoch_d_{period}'] = self._calculate_simple_ma(stoch_k, 3)
        
        # Williams %R
        for period in [14, 21, 28]:
            features[f'williams_r_{period}'] = self._calculate_williams_r(high, low, close, period)
        
        # CCI (Commodity Channel Index)
        for period in [14, 20, 50]:
            features[f'cci_{period}'] = self._calculate_cci(high, low, close, period)
        
        # ROC (Rate of Change)
        for period in [1, 5, 10, 20]:
            features[f'roc_{period}'] = self._calculate_roc(price, period)
        
        # Momentum
        for period in [10, 14, 20]:
            features[f'momentum_{period}'] = self._calculate_momentum(price, period)
        
        # ATR (Average True Range)
        for period in [14, 21, 28]:
            features[f'atr_{period}'] = self._calculate_atr(high, low, close, period)
        
        # ADX (Average Directional Index)
        features['adx_14'] = self._calculate_adx(high, low, close, 14)
        
        # Parabolic SAR
        features['psar'] = self._calculate_psar(high, low, close)
        
        # Ultimate Oscillator
        features['ultimate_oscillator'] = self._calculate_ultimate_oscillator(high, low, close)
        
        # Volume ê¸°ë°˜ íŠ¹ì„±
        if volume > 0:
            features.update({
                'volume': volume,
                'log_volume': np.log(volume),
                'volume_normalized': volume / 1000000 if volume > 0 else 0,
            })
            
            # Volume Moving Averages
            for period in [10, 20, 50]:
                features[f'volume_sma_{period}'] = self._calculate_simple_ma(volume, period)
                if self._calculate_simple_ma(volume, period) > 0:
                    features[f'volume_ratio_{period}'] = volume / self._calculate_simple_ma(volume, period)
        
        # Price-Volume ì¡°í•©
        if price > 0 and volume > 0:
            features['price_volume_ratio'] = price / volume * 1000000
            features['volume_price_trend'] = volume * (close - open_price) / price if price > 0 else 0
        
        # Candlestick Pattern Features
        features.update(self._calculate_candlestick_patterns(open_price, high, low, close))
        
        # Support/Resistance Levels
        features.update(self._calculate_support_resistance(high, low, close))
        
        # Volatility Features
        for period in [5, 10, 20, 50]:
            features[f'volatility_{period}'] = self._calculate_volatility(price, period)
        
        # Trend Strength
        features['trend_strength'] = self._calculate_trend_strength(price)
        
        return features
    
    def _calculate_simple_rsi(self, price: float, period: int) -> float:
        """ë‹¨ìˆœí™”ëœ RSI ê³„ì‚°"""
        if price <= 0:
            return 50.0
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” historical data í•„ìš”
        # ì—¬ê¸°ì„œëŠ” ê°€ê²© ê¸°ë°˜ ê·¼ì‚¬ì¹˜ ë°˜í™˜
        return 50.0 + (price % 100) - 50
    
    def _calculate_simple_ma(self, value: float, period: int) -> float:
        """ë‹¨ìˆœí™”ëœ ì´ë™í‰ê· """
        return value  # ì‹¤ì œë¡œëŠ” historical dataë¡œ ê³„ì‚°
    
    def _calculate_simple_ema(self, value: float, period: int) -> float:
        """ë‹¨ìˆœí™”ëœ ì§€ìˆ˜ì´ë™í‰ê· """
        return value  # ì‹¤ì œë¡œëŠ” EMA ê³µì‹ ì ìš©
    
    def _calculate_bollinger_bands(self, price: float, period: int) -> Tuple[float, float, float]:
        """ë³¼ë¦°ì € ë°´ë“œ ê³„ì‚°"""
        middle = price
        std = price * 0.02  # 2% í‘œì¤€í¸ì°¨ë¡œ ê°€ì •
        upper = middle + (2 * std)
        lower = middle - (2 * std)
        return upper, lower, middle
    
    def _calculate_macd(self, price: float, fast: int, slow: int, signal: int) -> Tuple[float, float]:
        """MACD ê³„ì‚°"""
        # ê°„ë‹¨í•œ ê·¼ì‚¬ì¹˜
        fast_ema = price
        slow_ema = price * 0.99
        macd_line = fast_ema - slow_ema
        macd_signal = macd_line * 0.9
        return macd_line, macd_signal
    
    def _calculate_stochastic_k(self, high: float, low: float, close: float, period: int) -> float:
        """Stochastic %K ê³„ì‚°"""
        if high <= low:
            return 50.0
        return ((close - low) / (high - low)) * 100
    
    def _calculate_williams_r(self, high: float, low: float, close: float, period: int) -> float:
        """Williams %R ê³„ì‚°"""
        if high <= low:
            return -50.0
        return ((high - close) / (high - low)) * -100
    
    def _calculate_cci(self, high: float, low: float, close: float, period: int) -> float:
        """CCI ê³„ì‚°"""
        typical_price = (high + low + close) / 3
        return (typical_price - typical_price) / (0.015 * typical_price) if typical_price > 0 else 0
    
    def _calculate_roc(self, price: float, period: int) -> float:
        """ROC ê³„ì‚°"""
        prev_price = price * 0.98  # ê°€ì •
        return ((price - prev_price) / prev_price) * 100 if prev_price > 0 else 0
    
    def _calculate_momentum(self, price: float, period: int) -> float:
        """Momentum ê³„ì‚°"""
        prev_price = price * 0.99  # ê°€ì •
        return price - prev_price
    
    def _calculate_atr(self, high: float, low: float, close: float, period: int) -> float:
        """ATR ê³„ì‚°"""
        return high - low if high >= low else 0
    
    def _calculate_adx(self, high: float, low: float, close: float, period: int) -> float:
        """ADX ê³„ì‚°"""
        return 25.0  # ì¤‘ê°„ê°’ìœ¼ë¡œ ê°€ì •
    
    def _calculate_psar(self, high: float, low: float, close: float) -> float:
        """Parabolic SAR ê³„ì‚°"""
        return close * 0.98  # ê°€ì •
    
    def _calculate_ultimate_oscillator(self, high: float, low: float, close: float) -> float:
        """Ultimate Oscillator ê³„ì‚°"""
        return 50.0  # ì¤‘ê°„ê°’ìœ¼ë¡œ ê°€ì •
    
    def _calculate_candlestick_patterns(self, open_p: float, high: float, low: float, close: float) -> Dict[str, float]:
        """ìº”ë“¤ìŠ¤í‹± íŒ¨í„´ íŠ¹ì„±"""
        patterns = {}
        
        if high >= low and high > 0:
            # Doji
            patterns['doji'] = 1.0 if abs(close - open_p) / (high - low) < 0.1 else 0.0
            
            # Hammer
            body_size = abs(close - open_p)
            lower_shadow = min(open_p, close) - low
            patterns['hammer'] = 1.0 if lower_shadow > body_size * 2 else 0.0
            
            # Shooting Star
            upper_shadow = high - max(open_p, close)
            patterns['shooting_star'] = 1.0 if upper_shadow > body_size * 2 else 0.0
            
            # Body ratio
            patterns['body_ratio'] = body_size / (high - low) if high > low else 0
            
            # Shadow ratios
            patterns['upper_shadow_ratio'] = upper_shadow / (high - low) if high > low else 0
            patterns['lower_shadow_ratio'] = lower_shadow / (high - low) if high > low else 0
        
        return patterns
    
    def _calculate_support_resistance(self, high: float, low: float, close: float) -> Dict[str, float]:
        """ì§€ì§€/ì €í•­ì„  íŠ¹ì„±"""
        features = {}
        
        # ê°„ë‹¨í•œ í”¼ë³´ë‚˜ì¹˜ ë ˆë²¨
        range_size = high - low if high >= low else 0
        features['fib_236'] = low + range_size * 0.236
        features['fib_382'] = low + range_size * 0.382
        features['fib_500'] = low + range_size * 0.500
        features['fib_618'] = low + range_size * 0.618
        
        # í”¼ë´‡ í¬ì¸íŠ¸
        pivot = (high + low + close) / 3
        features['pivot_point'] = pivot
        features['resistance_1'] = 2 * pivot - low
        features['support_1'] = 2 * pivot - high
        features['resistance_2'] = pivot + (high - low)
        features['support_2'] = pivot - (high - low)
        
        return features
    
    def _calculate_volatility(self, price: float, period: int) -> float:
        """ë³€ë™ì„± ê³„ì‚°"""
        return price * 0.02  # 2% ê°€ì •
    
    def _calculate_trend_strength(self, price: float) -> float:
        """íŠ¸ë Œë“œ ê°•ë„ ê³„ì‚°"""
        return 0.5  # ì¤‘ë¦½ìœ¼ë¡œ ê°€ì •

class MarketMicrostructureGenerator:
    """ì‹œì¥ ë¯¸ì‹œêµ¬ì¡° íŠ¹ì„± ìƒì„±ê¸° (200+ íŠ¹ì„±)"""
    
    def __init__(self, config: FeatureConfig):
        self.config = config
    
    async def generate_features(self, market_data: Dict[str, Any]) -> Dict[str, float]:
        """ì‹œì¥ ë¯¸ì‹œêµ¬ì¡° íŠ¹ì„± ìƒì„±"""
        features = {}
        
        # ê¸°ë³¸ ë°ì´í„° ì¶”ì¶œ
        bid = float(market_data.get('bid', 0))
        ask = float(market_data.get('ask', 0))
        volume = float(market_data.get('volume', 0))
        trade_count = int(market_data.get('trade_count', 0))
        
        # í˜¸ê°€ì°½ ë¶ˆê· í˜•
        if ask > 0 and bid > 0:
            features['bid_ask_spread'] = ask - bid
            features['bid_ask_spread_pct'] = ((ask - bid) / ((ask + bid) / 2)) * 100
            features['mid_price'] = (ask + bid) / 2
            features['bid_ask_ratio'] = bid / ask if ask > 0 else 1
        
        # ìœ ë™ì„± ì§€í‘œ
        if volume > 0 and trade_count > 0:
            features['avg_trade_size'] = volume / trade_count
            features['trade_intensity'] = trade_count / 3600  # ì‹œê°„ë‹¹ ê±°ë˜ìˆ˜
        
        # ì£¼ë¬¸ì„œ ê¹Šì´ (ê°€ì •ê°’)
        bid_depth = volume * 0.3  # 30% ê°€ì •
        ask_depth = volume * 0.3  # 30% ê°€ì •
        
        if bid_depth + ask_depth > 0:
            features['order_book_imbalance'] = (bid_depth - ask_depth) / (bid_depth + ask_depth)
            features['liquidity_ratio'] = min(bid_depth, ask_depth) / max(bid_depth, ask_depth)
        
        # ì‹œì¥ ì¶©ê²© ì§€í‘œ
        if volume > 0:
            features['market_impact_1btc'] = 1000000 / volume  # 1 BTC ì‹œì¥ ì˜í–¥
            features['volume_concentration'] = volume * 0.1  # ì§‘ì¤‘ë„ ê°€ì •
        
        # ê±°ë˜ í¬ê¸° ë¶„í¬
        large_trades = volume * 0.2  # 20% ëŒ€í˜•ê±°ë˜ ê°€ì •
        medium_trades = volume * 0.5  # 50% ì¤‘í˜•ê±°ë˜ ê°€ì •
        small_trades = volume * 0.3   # 30% ì†Œí˜•ê±°ë˜ ê°€ì •
        
        features.update({
            'large_trades_ratio': large_trades / volume if volume > 0 else 0,
            'medium_trades_ratio': medium_trades / volume if volume > 0 else 0,
            'small_trades_ratio': small_trades / volume if volume > 0 else 0,
        })
        
        # ì‹œê°„ë³„ ê±°ë˜ íŒ¨í„´
        current_hour = datetime.now().hour
        features.update({
            'hour_of_day': current_hour,
            'is_asian_hours': 1.0 if 0 <= current_hour <= 8 else 0.0,
            'is_european_hours': 1.0 if 8 <= current_hour <= 16 else 0.0,
            'is_american_hours': 1.0 if 16 <= current_hour <= 24 else 0.0,
        })
        
        # ê±°ë˜ì†Œë³„ íŠ¹ì„± (ê°€ì •)
        exchanges = ['binance', 'coinbase', 'kraken', 'bitfinex']
        for exchange in exchanges:
            features[f'{exchange}_volume_share'] = 0.25  # ê· ë“± ë¶„ë°° ê°€ì •
            features[f'{exchange}_price_premium'] = np.random.uniform(-0.1, 0.1)  # í”„ë¦¬ë¯¸ì—„ ê°€ì •
        
        # íŒŒìƒìƒí’ˆ ì‹œì¥ íŠ¹ì„±
        features.update({
            'futures_basis': np.random.uniform(-50, 50),  # ì„ ë¬¼ ë² ì´ì‹œìŠ¤
            'funding_rate': np.random.uniform(-0.01, 0.01),  # í€ë”©ë¹„ìœ¨
            'open_interest_24h_change': np.random.uniform(-10, 10),  # ë¯¸ê²°ì œì•½ì • ë³€í™”
            'futures_volume_ratio': np.random.uniform(0.8, 1.2),  # ì„ ë¬¼/í˜„ë¬¼ ê±°ë˜ëŸ‰ ë¹„ìœ¨
        })
        
        # ì˜µì…˜ ì‹œì¥ íŠ¹ì„±
        features.update({
            'put_call_ratio': np.random.uniform(0.5, 2.0),  # Put/Call ë¹„ìœ¨
            'implied_volatility': np.random.uniform(50, 150),  # ë‚´ì¬ë³€ë™ì„±
            'gamma_exposure': np.random.uniform(-1000000, 1000000),  # ê°ë§ˆ ë…¸ì¶œ
            'volatility_skew': np.random.uniform(0.8, 1.2),  # ë³€ë™ì„± ìŠ¤í
        })
        
        # ì•Œê³ ë¦¬ì¦˜ ê±°ë˜ íƒì§€
        if trade_count > 0:
            features['algo_trading_ratio'] = min(trade_count / 1000, 1.0)  # ì•Œê³ ë¦¬ì¦˜ ê±°ë˜ ë¹„ìœ¨
            features['iceberg_orders_detected'] = 1.0 if volume > trade_count * 100 else 0.0
        
        # ê±°ë˜ì†Œ ê°„ ì°¨ìµê±°ë˜ ê¸°íšŒ
        features.update({
            'arbitrage_opportunity': abs(np.random.uniform(-0.5, 0.5)),  # ì°¨ìµê±°ë˜ ê¸°íšŒ
            'cross_exchange_correlation': np.random.uniform(0.8, 0.99),  # ê±°ë˜ì†Œ ê°„ ìƒê´€ê´€ê³„
        })
        
        return features

class OnChainFeatureGenerator:
    """ì˜¨ì²´ì¸ ë¶„ì„ íŠ¹ì„± ìƒì„±ê¸° (200+ íŠ¹ì„±)"""
    
    def __init__(self, config: FeatureConfig):
        self.config = config
    
    async def generate_features(self, market_data: Dict[str, Any]) -> Dict[str, float]:
        """ì˜¨ì²´ì¸ ë¶„ì„ íŠ¹ì„± ìƒì„±"""
        features = {}
        
        # ë„¤íŠ¸ì›Œí¬ í™œë™ ì§€í‘œ
        features.update({
            'active_addresses': np.random.randint(500000, 1000000),  # í™œì„± ì£¼ì†Œ ìˆ˜
            'new_addresses': np.random.randint(50000, 100000),  # ì‹ ê·œ ì£¼ì†Œ ìˆ˜
            'transaction_count': np.random.randint(200000, 400000),  # ê±°ë˜ ìˆ˜
            'transaction_volume_usd': np.random.randint(10000000, 50000000),  # ê±°ë˜ëŸ‰ (USD)
        })
        
        # í•´ì‹œë ˆì´íŠ¸ ë° ì±„êµ´ ì§€í‘œ
        hash_rate = np.random.uniform(300, 500) * 1e18  # EH/s
        difficulty = np.random.uniform(50, 80) * 1e12
        
        features.update({
            'hash_rate': hash_rate,
            'hash_rate_7d_ma': hash_rate * 0.98,
            'hash_rate_30d_ma': hash_rate * 0.95,
            'hash_rate_change_7d': np.random.uniform(-5, 5),
            'mining_difficulty': difficulty,
            'difficulty_adjustment': np.random.uniform(-10, 10),
            'hash_ribbons_signal': np.random.choice([0, 1]),
        })
        
        # HODL ë° ê³µê¸‰ ì§€í‘œ
        total_supply = 19500000  # í˜„ì¬ ê³µê¸‰ëŸ‰
        
        features.update({
            'hodl_1y_plus': total_supply * np.random.uniform(0.6, 0.7),  # 1ë…„ ì´ìƒ ë³´ìœ 
            'hodl_2y_plus': total_supply * np.random.uniform(0.4, 0.5),  # 2ë…„ ì´ìƒ ë³´ìœ 
            'hodl_5y_plus': total_supply * np.random.uniform(0.2, 0.3),  # 5ë…„ ì´ìƒ ë³´ìœ 
            'lth_supply_ratio': np.random.uniform(0.6, 0.8),  # ì¥ê¸°ë³´ìœ ì ë¹„ìœ¨
            'sth_supply_ratio': np.random.uniform(0.2, 0.4),  # ë‹¨ê¸°ë³´ìœ ì ë¹„ìœ¨
        })
        
        # ê±°ë˜ì†Œ í”Œë¡œìš°
        features.update({
            'exchange_inflow': np.random.uniform(1000, 5000),  # BTC
            'exchange_outflow': np.random.uniform(1000, 5000),  # BTC
            'exchange_netflow': np.random.uniform(-2000, 2000),  # BTC
            'exchange_balance': np.random.uniform(2000000, 3000000),  # BTC
            'exchange_balance_change': np.random.uniform(-50000, 50000),  # BTC
        })
        
        # ê³ ë˜ í™œë™
        features.update({
            'whale_addresses_1k_plus': np.random.randint(1800, 2200),  # 1000+ BTC ì£¼ì†Œ
            'whale_addresses_10k_plus': np.random.randint(100, 150),   # 10000+ BTC ì£¼ì†Œ
            'whale_transaction_count': np.random.randint(50, 200),     # ê³ ë˜ ê±°ë˜ ìˆ˜
            'whale_volume_usd': np.random.uniform(100000000, 1000000000),  # ê³ ë˜ ê±°ë˜ëŸ‰
        })
        
        # ê°€ì¹˜ ì§€í‘œ
        btc_price = float(market_data.get('btc_price', 50000))
        realized_cap = total_supply * np.random.uniform(25000, 35000)
        
        features.update({
            'market_cap': total_supply * btc_price,
            'realized_cap': realized_cap,
            'mvrv_ratio': (total_supply * btc_price) / realized_cap if realized_cap > 0 else 1,
            'nvt_ratio': (total_supply * btc_price) / (features['transaction_volume_usd'] / 365) if features['transaction_volume_usd'] > 0 else 1,
            'nvt_signal': np.random.uniform(50, 150),
        })
        
        # SOPR (Spent Output Profit Ratio)
        features.update({
            'sopr': np.random.uniform(0.95, 1.05),
            'sopr_7d_ma': np.random.uniform(0.98, 1.02),
            'sopr_lth': np.random.uniform(1.0, 1.2),  # ì¥ê¸°ë³´ìœ ì SOPR
            'sopr_sth': np.random.uniform(0.8, 1.1),  # ë‹¨ê¸°ë³´ìœ ì SOPR
        })
        
        # ì½”ì¸ ë°ì´ì¦ˆ ë””ìŠ¤íŠ¸ë¡œì´ë“œ
        features.update({
            'coin_days_destroyed': np.random.uniform(1000000, 10000000),
            'cdd_90d_ma': np.random.uniform(5000000, 15000000),
            'binary_cdd_signal': np.random.choice([0, 1]),
        })
        
        # ì±„êµ´ì ê´€ë ¨
        features.update({
            'miner_revenue': np.random.uniform(20000000, 50000000),  # USD/day
            'miner_revenue_btc': np.random.uniform(400, 800),  # BTC/day
            'fee_revenue_ratio': np.random.uniform(0.05, 0.25),  # ìˆ˜ìˆ˜ë£Œ/ì´ìˆ˜ìµ ë¹„ìœ¨
            'puell_multiple': np.random.uniform(0.5, 4.0),
            'hash_price': np.random.uniform(0.1, 0.3),  # USD per TH/s per day
        })
        
        # ìŠ¤í…Œì´ë¸”ì½”ì¸ ê´€ë ¨
        features.update({
            'usdt_supply': np.random.uniform(80000000000, 120000000000),
            'usdc_supply': np.random.uniform(40000000000, 60000000000),
            'stablecoin_ratio': np.random.uniform(0.05, 0.15),  # ìŠ¤í…Œì´ë¸”ì½”ì¸/ë¹„íŠ¸ì½”ì¸ ë¹„ìœ¨
            'stablecoin_inflow': np.random.uniform(100000000, 1000000000),
        })
        
        # ì£¼ì†Œë³„ ë¶„ì„
        features.update({
            'addresses_1_plus': np.random.randint(800000, 900000),    # 1+ BTC
            'addresses_10_plus': np.random.randint(140000, 160000),   # 10+ BTC
            'addresses_100_plus': np.random.randint(15000, 17000),    # 100+ BTC
            'addresses_1k_plus': np.random.randint(2000, 2500),      # 1000+ BTC
            'address_concentration': np.random.uniform(0.8, 0.9),    # ìƒìœ„ 1% ì§‘ì¤‘ë„
        })
        
        # ë„¤íŠ¸ì›Œí¬ ì„±ì¥
        features.update({
            'metcalfe_ratio': np.random.uniform(0.5, 2.0),  # ë©”íŠ¸ì¹¼í”„ì˜ ë²•ì¹™ ë¹„ìœ¨
            'network_momentum': np.random.uniform(-10, 10),  # ë„¤íŠ¸ì›Œí¬ ëª¨ë©˜í…€
            'adoption_curve_position': np.random.uniform(0.3, 0.7),  # ì±„íƒ ê³¡ì„  ìœ„ì¹˜
        })
        
        return features

class MacroEconomicGenerator:
    """ê±°ì‹œê²½ì œ íŠ¹ì„± ìƒì„±ê¸° (100+ íŠ¹ì„±)"""
    
    def __init__(self, config: FeatureConfig):
        self.config = config
    
    async def generate_features(self, market_data: Dict[str, Any]) -> Dict[str, float]:
        """ê±°ì‹œê²½ì œ íŠ¹ì„± ìƒì„±"""
        features = {}
        
        # ì£¼ì‹ ì‹œì¥ ì§€ìˆ˜
        features.update({
            'spx_500': np.random.uniform(4000, 5500),  # S&P 500
            'nasdaq_100': np.random.uniform(12000, 16000),  # NASDAQ
            'dow_jones': np.random.uniform(32000, 38000),  # Dow Jones
            'vix': np.random.uniform(12, 35),  # VIX ê³µí¬ì§€ìˆ˜
        })
        
        # í™˜ìœ¨
        features.update({
            'dxy': np.random.uniform(95, 110),  # ë‹¬ëŸ¬ ì¸ë±ìŠ¤
            'eurusd': np.random.uniform(1.05, 1.25),  # EUR/USD
            'gbpusd': np.random.uniform(1.20, 1.40),  # GBP/USD
            'usdjpy': np.random.uniform(110, 150),  # USD/JPY
            'usdcnh': np.random.uniform(6.5, 7.5),  # USD/CNH
        })
        
        # ê¸ˆë¦¬
        features.update({
            'fed_funds_rate': np.random.uniform(0, 6),  # ì—°ë°©ê¸°ê¸ˆê¸ˆë¦¬
            'us_2y_yield': np.random.uniform(0.5, 5.5),  # 2ë…„êµ­ì±„
            'us_10y_yield': np.random.uniform(1, 6),  # 10ë…„êµ­ì±„
            'us_30y_yield': np.random.uniform(1.5, 6.5),  # 30ë…„êµ­ì±„
            'yield_curve_slope': np.random.uniform(-1, 3),  # ìˆ˜ìµë¥ ê³¡ì„  ê¸°ìš¸ê¸°
        })
        
        # ì›ìì¬
        features.update({
            'gold_price': np.random.uniform(1800, 2200),  # ê¸ˆ ê°€ê²©
            'silver_price': np.random.uniform(22, 30),  # ì€ ê°€ê²©
            'oil_wti': np.random.uniform(60, 100),  # WTI ìœ ê°€
            'oil_brent': np.random.uniform(65, 105),  # ë¸Œë ŒíŠ¸ìœ 
            'copper_price': np.random.uniform(3, 5),  # êµ¬ë¦¬ ê°€ê²©
        })
        
        # ì¸í”Œë ˆì´ì…˜ ì§€í‘œ
        features.update({
            'cpi_yoy': np.random.uniform(2, 8),  # CPI ì „ë…„ë™ì›”ë¹„
            'pce_yoy': np.random.uniform(2, 7),  # PCE ì „ë…„ë™ì›”ë¹„
            'tips_5y': np.random.uniform(2, 4),  # 5ë…„ ê¸°ëŒ€ì¸í”Œë ˆì´ì…˜
            'tips_10y': np.random.uniform(2, 3.5),  # 10ë…„ ê¸°ëŒ€ì¸í”Œë ˆì´ì…˜
        })
        
        # ì¤‘ì•™ì€í–‰ ì •ì±…
        features.update({
            'fed_balance_sheet': np.random.uniform(7000, 9000),  # ì¡°ë‹¨ìœ„ USD
            'ecb_balance_sheet': np.random.uniform(6000, 8000),  # ì¡°ë‹¨ìœ„ EUR
            'boj_balance_sheet': np.random.uniform(600, 800),   # ì¡°ë‹¨ìœ„ JPY
            'qe_intensity': np.random.uniform(0, 10),  # QE ê°•ë„ ì§€ìˆ˜
        })
        
        # ê²½ì œ ì§€í‘œ
        features.update({
            'us_gdp_growth': np.random.uniform(-2, 6),  # GDP ì„±ì¥ë¥ 
            'us_unemployment': np.random.uniform(3, 8),  # ì‹¤ì—…ë¥ 
            'us_retail_sales': np.random.uniform(-5, 10),  # ì†Œë§¤ë§¤ì¶œ ì¦ê°€ìœ¨
            'us_consumer_confidence': np.random.uniform(80, 130),  # ì†Œë¹„ìì‹ ë¢°ì§€ìˆ˜
        })
        
        # ê¸€ë¡œë²Œ ë¦¬ìŠ¤í¬ ì§€í‘œ
        features.update({
            'risk_on_off': np.random.uniform(-2, 2),  # ë¦¬ìŠ¤í¬ì˜¨/ì˜¤í”„ ì§€ìˆ˜
            'credit_spreads': np.random.uniform(0.5, 3),  # ì‹ ìš©ìŠ¤í”„ë ˆë“œ
            'term_premiums': np.random.uniform(-1, 1),  # ê¸°ê°„í”„ë¦¬ë¯¸ì—„
            'financial_stress': np.random.uniform(0, 5),  # ê¸ˆìœµìŠ¤íŠ¸ë ˆìŠ¤ ì§€ìˆ˜
        })
        
        # ì•”í˜¸í™”í ê±°ì‹œ ì§€í‘œ
        features.update({
            'crypto_market_cap': np.random.uniform(1000000000000, 3000000000000),  # ì „ì²´ ì•”í˜¸í™”í ì‹œê°€ì´ì•¡
            'btc_dominance': np.random.uniform(35, 65),  # ë¹„íŠ¸ì½”ì¸ ë„ë¯¸ë„ŒìŠ¤
            'alt_season_index': np.random.uniform(0, 100),  # ì•ŒíŠ¸ì‹œì¦Œ ì§€ìˆ˜
            'defi_tvl': np.random.uniform(50000000000, 200000000000),  # DeFi TVL
        })
        
        # ê¸°ê´€ íˆ¬ì ì§€í‘œ
        features.update({
            'grayscale_premium': np.random.uniform(-20, 20),  # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ í”„ë¦¬ë¯¸ì—„
            'institutional_inflows': np.random.uniform(-1000, 1000),  # ê¸°ê´€ ìê¸ˆ ìœ ì…
            'etf_flows': np.random.uniform(-500, 500),  # ETF ìê¸ˆ íë¦„
            'futures_oi_change': np.random.uniform(-20, 20),  # ì„ ë¬¼ ë¯¸ê²°ì œ ë³€í™”
        })
        
        # ê·œì œ ë° ì •ì±…
        features.update({
            'regulatory_sentiment': np.random.uniform(-5, 5),  # ê·œì œ ì‹¬ë¦¬
            'cbdc_progress': np.random.uniform(0, 10),  # CBDC ì§„í–‰ ì •ë„
            'tax_policy_impact': np.random.uniform(-3, 3),  # ì„¸ê¸ˆ ì •ì±… ì˜í–¥
        })
        
        return features

class AdvancedMathFeatureGenerator:
    """ê³ ê¸‰ ìˆ˜í•™ íŠ¹ì„± ìƒì„±ê¸° (200+ íŠ¹ì„±)"""
    
    def __init__(self, config: FeatureConfig):
        self.config = config
        
    async def generate_features(self, market_data: Dict[str, Any]) -> Dict[str, float]:
        """ê³ ê¸‰ ìˆ˜í•™ì  íŠ¹ì„± ìƒì„±"""
        features = {}
        
        # ê¸°ë³¸ ê°€ê²© ë°ì´í„°
        price = float(market_data.get('btc_price', 50000))
        volume = float(market_data.get('volume', 1000))
        
        # ì‹œê³„ì—´ ë°ì´í„° ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œë¡œëŠ” historical data ì‚¬ìš©)
        price_series = np.array([price * (1 + np.random.normal(0, 0.02)) for _ in range(100)])
        volume_series = np.array([volume * (1 + np.random.normal(0, 0.1)) for _ in range(100)])
        returns = np.diff(np.log(price_series))
        
        # 1. í‘¸ë¦¬ì— ë³€í™˜ íŠ¹ì„±
        if ADVANCED_MATH_AVAILABLE:
            try:
                fft_prices = np.fft.fft(price_series)
                fft_magnitude = np.abs(fft_prices)
                fft_phase = np.angle(fft_prices)
                
                # ì£¼ìš” ì£¼íŒŒìˆ˜ ì„±ë¶„
                for i in range(min(10, len(fft_magnitude))):
                    features[f'fft_magnitude_{i}'] = fft_magnitude[i]
                    features[f'fft_phase_{i}'] = fft_phase[i]
                
                # ìŠ¤í™íŠ¸ëŸ´ íŠ¹ì„±
                features['spectral_centroid'] = np.sum(np.arange(len(fft_magnitude)) * fft_magnitude) / np.sum(fft_magnitude)
                features['spectral_rolloff'] = np.percentile(fft_magnitude, 85)
                features['spectral_flux'] = np.mean(np.diff(fft_magnitude)**2)
                
            except Exception as e:
                print(f"FFT ê³„ì‚° ì˜¤ë¥˜: {e}")
        
        # 2. ì›¨ì´ë¸”ë¦¿ ë³€í™˜ íŠ¹ì„±
        if ADVANCED_MATH_AVAILABLE:
            try:
                # ì—¬ëŸ¬ ì›¨ì´ë¸”ë¦¿ í•¨ìˆ˜ ì‚¬ìš©
                wavelets = ['db4', 'haar', 'coif2']
                for wavelet in wavelets:
                    coeffs = pywt.wavedec(price_series, wavelet, level=4)
                    for i, coeff in enumerate(coeffs):
                        features[f'wavelet_{wavelet}_level_{i}_energy'] = np.sum(coeff**2)
                        features[f'wavelet_{wavelet}_level_{i}_mean'] = np.mean(coeff)
                        features[f'wavelet_{wavelet}_level_{i}_std'] = np.std(coeff)
                        
            except Exception as e:
                print(f"ì›¨ì´ë¸”ë¦¿ ê³„ì‚° ì˜¤ë¥˜: {e}")
        
        # 3. í”„ë™íƒˆ ì°¨ì› ë¶„ì„
        features.update(self._calculate_fractal_features(price_series))
        
        # 4. ì—”íŠ¸ë¡œí”¼ ì¸¡ì •
        features.update(self._calculate_entropy_features(price_series, returns))
        
        # 5. í†µê³„ì  ëª¨ë©˜íŠ¸
        for order in range(1, 6):  # 1ì°¨~5ì°¨ ëª¨ë©˜íŠ¸
            features[f'price_moment_{order}'] = stats.moment(price_series, moment=order)
            if len(returns) > 0:
                features[f'returns_moment_{order}'] = stats.moment(returns, moment=order)
        
        # 6. ë¶„í¬ íŠ¹ì„±
        if len(returns) > 0:
            features['returns_skewness'] = stats.skew(returns)
            features['returns_kurtosis'] = stats.kurtosis(returns)
            features['jarque_bera_stat'], features['jarque_bera_pvalue'] = stats.jarque_bera(returns)
            features['shapiro_stat'], features['shapiro_pvalue'] = stats.shapiro(returns[:50])  # ìƒ˜í”Œ í¬ê¸° ì œí•œ
        
        # 7. ìê¸°ìƒê´€ í•¨ìˆ˜
        if len(returns) > 10:
            autocorr = np.correlate(returns, returns, mode='full')
            autocorr = autocorr[len(autocorr)//2:]
            for lag in range(1, min(21, len(autocorr))):
                features[f'autocorr_lag_{lag}'] = autocorr[lag] / autocorr[0] if autocorr[0] != 0 else 0
        
        # 8. ì¹´ì˜¤ìŠ¤ ì´ë¡  ì§€í‘œ
        features.update(self._calculate_chaos_features(price_series))
        
        # 9. ì •ë³´ ì´ë¡  íŠ¹ì„±
        features.update(self._calculate_information_theory_features(price_series))
        
        # 10. ì‹œê³„ì—´ ë¶„í•´
        features.update(self._calculate_decomposition_features(price_series))
        
        # 11. ë³µì¡ì„± ì¸¡ì •
        features.update(self._calculate_complexity_features(price_series))
        
        # 12. ë™ì—­í•™ ì‹œìŠ¤í…œ íŠ¹ì„±
        features.update(self._calculate_dynamical_features(price_series))
        
        # 13. ë‹¤ë³€ëŸ‰ ë¶„ì„ íŠ¹ì„±
        features.update(self._calculate_multivariate_features(price_series, volume_series))
        
        # 14. ë¹„ì„ í˜• ì‹œê³„ì—´ íŠ¹ì„±
        features.update(self._calculate_nonlinear_features(price_series))
        
        return features
    
    def _calculate_fractal_features(self, series: np.ndarray) -> Dict[str, float]:
        """í”„ë™íƒˆ ì°¨ì› íŠ¹ì„±"""
        features = {}
        
        try:
            # Hurst ì§€ìˆ˜ ê³„ì‚°
            n = len(series)
            if n > 10:
                lags = range(2, min(n//4, 50))
                variability = [np.var(np.diff(series, lag)) for lag in lags]
                log_lags = [np.log(lag) for lag in lags]
                log_var = [np.log(var) for var in variability if var > 0]
                
                if len(log_var) > 1 and len(log_lags) == len(log_var):
                    hurst = np.polyfit(log_lags, log_var, 1)[0] / 2.0
                    features['hurst_exponent'] = hurst
                    features['fractal_dimension'] = 2 - hurst
        
            # Box-counting ì°¨ì› ê·¼ì‚¬
            if len(series) > 20:
                box_counts = []
                scales = [2**i for i in range(1, min(6, int(np.log2(len(series)))))]
                
                for scale in scales:
                    boxes = len(series) // scale
                    count = sum(1 for i in range(boxes) if 
                              max(series[i*scale:(i+1)*scale]) != min(series[i*scale:(i+1)*scale]))
                    box_counts.append(count)
                
                if len(box_counts) > 1 and all(c > 0 for c in box_counts):
                    log_scales = [np.log(1/s) for s in scales]
                    log_counts = [np.log(c) for c in box_counts]
                    box_dim = -np.polyfit(log_scales, log_counts, 1)[0]
                    features['box_counting_dimension'] = box_dim
                    
        except Exception as e:
            print(f"í”„ë™íƒˆ ê³„ì‚° ì˜¤ë¥˜: {e}")
        
        return features
    
    def _calculate_entropy_features(self, series: np.ndarray, returns: np.ndarray) -> Dict[str, float]:
        """ì—”íŠ¸ë¡œí”¼ íŠ¹ì„±"""
        features = {}
        
        try:
            # Shannon ì—”íŠ¸ë¡œí”¼
            if len(series) > 0:
                hist, _ = np.histogram(series, bins=20)
                hist = hist[hist > 0]  # 0ì´ ì•„ë‹Œ ê°’ë§Œ
                prob = hist / np.sum(hist)
                shannon_entropy = -np.sum(prob * np.log2(prob))
                features['shannon_entropy'] = shannon_entropy
            
            # ê·¼ì‚¬ ì—”íŠ¸ë¡œí”¼ (ApEn)
            if len(series) > 10:
                features['approximate_entropy'] = self._calculate_apen(series, 2, 0.2 * np.std(series))
            
            # í‘œë³¸ ì—”íŠ¸ë¡œí”¼ (SampEn)
            if len(series) > 10:
                features['sample_entropy'] = self._calculate_sampen(series, 2, 0.2 * np.std(series))
                
            # Permutation ì—”íŠ¸ë¡œí”¼
            if len(series) >= 6:
                features['permutation_entropy'] = self._calculate_permen(series, 3)
                
        except Exception as e:
            print(f"ì—”íŠ¸ë¡œí”¼ ê³„ì‚° ì˜¤ë¥˜: {e}")
        
        return features
    
    def _calculate_apen(self, series: np.ndarray, m: int, r: float) -> float:
        """ê·¼ì‚¬ ì—”íŠ¸ë¡œí”¼ ê³„ì‚°"""
        try:
            n = len(series)
            if n <= m:
                return 0.0
                
            def _maxdist(xi, xj, m):
                return max([abs(ua - va) for ua, va in zip(xi, xj)])
            
            def _phi(m):
                patterns = np.array([series[i:i + m] for i in range(n - m + 1)])
                C = np.zeros(n - m + 1)
                
                for i in range(n - m + 1):
                    template = patterns[i]
                    matches = sum(1 for j in range(n - m + 1) 
                                if _maxdist(template, patterns[j], m) <= r)
                    C[i] = matches / (n - m + 1)
                
                phi = np.mean([np.log(c) for c in C if c > 0])
                return phi
            
            return _phi(m) - _phi(m + 1)
            
        except:
            return 0.0
    
    def _calculate_sampen(self, series: np.ndarray, m: int, r: float) -> float:
        """í‘œë³¸ ì—”íŠ¸ë¡œí”¼ ê³„ì‚°"""
        try:
            n = len(series)
            if n <= m:
                return 0.0
            
            def _matching(template, candidates, r):
                return sum(1 for candidate in candidates 
                          if max(abs(t - c) for t, c in zip(template, candidate)) <= r)
            
            patterns_m = [series[i:i + m] for i in range(n - m + 1)]
            patterns_m1 = [series[i:i + m + 1] for i in range(n - m)]
            
            A = sum(_matching(pattern, patterns_m1, r) - 1 for pattern in patterns_m1)
            B = sum(_matching(pattern, patterns_m, r) - 1 for pattern in patterns_m)
            
            if B == 0:
                return float('inf')
            return -np.log(A / B)
            
        except:
            return 0.0
    
    def _calculate_permen(self, series: np.ndarray, m: int) -> float:
        """Permutation ì—”íŠ¸ë¡œí”¼ ê³„ì‚°"""
        try:
            n = len(series)
            if n < m:
                return 0.0
            
            patterns = []
            for i in range(n - m + 1):
                segment = series[i:i + m]
                pattern = tuple(np.argsort(segment))
                patterns.append(pattern)
            
            from collections import Counter
            pattern_counts = Counter(patterns)
            total = len(patterns)
            
            entropy = 0
            for count in pattern_counts.values():
                prob = count / total
                entropy -= prob * np.log2(prob)
            
            return entropy
            
        except:
            return 0.0
    
    def _calculate_chaos_features(self, series: np.ndarray) -> Dict[str, float]:
        """ì¹´ì˜¤ìŠ¤ ì´ë¡  íŠ¹ì„±"""
        features = {}
        
        try:
            # Lyapunov ì§€ìˆ˜ ê·¼ì‚¬
            if len(series) > 20:
                features['largest_lyapunov_exponent'] = self._estimate_lyapunov(series)
            
            # ìƒê´€ ì°¨ì› ê·¼ì‚¬
            if len(series) > 50:
                features['correlation_dimension'] = self._estimate_correlation_dimension(series)
                
            # 0-1 í…ŒìŠ¤íŠ¸ (ì¹´ì˜¤ìŠ¤ íƒì§€)
            if len(series) > 100:
                features['zero_one_test'] = self._zero_one_test(series)
                
        except Exception as e:
            print(f"ì¹´ì˜¤ìŠ¤ íŠ¹ì„± ê³„ì‚° ì˜¤ë¥˜: {e}")
        
        return features
    
    def _estimate_lyapunov(self, series: np.ndarray) -> float:
        """Lyapunov ì§€ìˆ˜ ì¶”ì •"""
        try:
            n = len(series)
            if n < 20:
                return 0.0
            
            # ê°„ë‹¨í•œ Lyapunov ì§€ìˆ˜ ì¶”ì •
            diffs = np.diff(series)
            log_diffs = []
            
            for i in range(1, len(diffs)):
                if abs(diffs[i-1]) > 1e-10 and abs(diffs[i]) > 1e-10:
                    log_diffs.append(np.log(abs(diffs[i] / diffs[i-1])))
            
            return np.mean(log_diffs) if log_diffs else 0.0
            
        except:
            return 0.0
    
    def _estimate_correlation_dimension(self, series: np.ndarray) -> float:
        """ìƒê´€ ì°¨ì› ì¶”ì •"""
        try:
            n = len(series)
            if n < 50:
                return 0.0
            
            # ê°„ë‹¨í•œ ìƒê´€ ì°¨ì› ì¶”ì •
            m = min(5, n // 10)  # ì„ë² ë”© ì°¨ì›
            embedded = np.array([series[i:i+m] for i in range(n-m+1)])
            
            distances = []
            for i in range(min(100, len(embedded))):
                for j in range(i+1, min(i+20, len(embedded))):
                    dist = np.max(np.abs(embedded[i] - embedded[j]))
                    if dist > 0:
                        distances.append(dist)
            
            if distances:
                log_distances = np.log(sorted(distances))
                log_prob = np.log(np.arange(1, len(log_distances)+1) / len(log_distances))
                return np.polyfit(log_distances[-50:], log_prob[-50:], 1)[0] if len(log_distances) > 50 else 2.0
            
            return 2.0
            
        except:
            return 2.0
    
    def _zero_one_test(self, series: np.ndarray) -> float:
        """0-1 í…ŒìŠ¤íŠ¸ (ì¹´ì˜¤ìŠ¤ íƒì§€)"""
        try:
            n = len(series)
            if n < 100:
                return 0.5
            
            # í‰ê·  ì œê±°
            mean_series = series - np.mean(series)
            
            # ìœ„ìƒ ë³€í™˜
            c = np.random.rand() * 2 * np.pi  # ëœë¤ ìœ„ìƒ
            p = np.cumsum(mean_series * np.cos(np.arange(n) * c + c))
            q = np.cumsum(mean_series * np.sin(np.arange(n) * c + c))
            
            # Mean Square Displacement ê³„ì‚°
            M = (p**2 + q**2) / n
            K = np.mean(M)
            
            # Kê°€ 0ì— ê°€ê¹Œìš°ë©´ regular, 1ì— ê°€ê¹Œìš°ë©´ chaotic
            return min(1.0, max(0.0, K))
            
        except:
            return 0.5
    
    def _calculate_information_theory_features(self, series: np.ndarray) -> Dict[str, float]:
        """ì •ë³´ ì´ë¡  íŠ¹ì„±"""
        features = {}
        
        try:
            # ìƒí˜¸ ì •ë³´ëŸ‰
            if len(series) > 20:
                lag_1 = series[1:]
                lag_0 = series[:-1]
                features['mutual_information_lag1'] = self._mutual_information(lag_0, lag_1)
            
            # Transfer ì—”íŠ¸ë¡œí”¼
            if len(series) > 30:
                features['transfer_entropy'] = self._transfer_entropy(series[:-2], series[1:-1], series[2:])
                
            # ì¡°ê±´ë¶€ ì—”íŠ¸ë¡œí”¼
            if len(series) > 20:
                features['conditional_entropy'] = self._conditional_entropy(series[:-1], series[1:])
                
        except Exception as e:
            print(f"ì •ë³´ ì´ë¡  íŠ¹ì„± ê³„ì‚° ì˜¤ë¥˜: {e}")
        
        return features
    
    def _mutual_information(self, x: np.ndarray, y: np.ndarray) -> float:
        """ìƒí˜¸ ì •ë³´ëŸ‰ ê³„ì‚°"""
        try:
            from sklearn.feature_selection import mutual_info_regression
            return mutual_info_regression(x.reshape(-1, 1), y)[0]
        except:
            return 0.0
    
    def _transfer_entropy(self, x: np.ndarray, y: np.ndarray, z: np.ndarray) -> float:
        """Transfer ì—”íŠ¸ë¡œí”¼ ê³„ì‚°"""
        try:
            # ê°„ë‹¨í•œ ê·¼ì‚¬
            return self._mutual_information(x, z) - self._mutual_information(y, z)
        except:
            return 0.0
    
    def _conditional_entropy(self, x: np.ndarray, y: np.ndarray) -> float:
        """ì¡°ê±´ë¶€ ì—”íŠ¸ë¡œí”¼ ê³„ì‚°"""
        try:
            # H(Y|X) = H(Y) - I(X;Y)
            hist_y, _ = np.histogram(y, bins=20)
            hist_y = hist_y[hist_y > 0]
            prob_y = hist_y / np.sum(hist_y)
            h_y = -np.sum(prob_y * np.log2(prob_y))
            
            mi_xy = self._mutual_information(x, y)
            return h_y - mi_xy
        except:
            return 0.0
    
    def _calculate_decomposition_features(self, series: np.ndarray) -> Dict[str, float]:
        """ì‹œê³„ì—´ ë¶„í•´ íŠ¹ì„±"""
        features = {}
        
        try:
            # STL ë¶„í•´ ê·¼ì‚¬ (ê°„ë‹¨í•œ ë²„ì „)
            if len(series) > 20:
                # íŠ¸ë Œë“œ (ì´ë™í‰ê· )
                window = min(20, len(series)//4)
                trend = pd.Series(series).rolling(window=window, center=True).mean().fillna(method='bfill').fillna(method='ffill')
                
                # ê³„ì ˆì„± (ë‹¨ìˆœí•œ ì£¼ê¸°ì„± íƒì§€)
                detrended = series - trend
                seasonal = np.zeros_like(series)
                
                # ì”ì°¨
                residual = detrended - seasonal
                
                features['trend_strength'] = np.std(trend) / np.std(series) if np.std(series) > 0 else 0
                features['seasonal_strength'] = np.std(seasonal) / np.std(series) if np.std(series) > 0 else 0
                features['residual_strength'] = np.std(residual) / np.std(series) if np.std(series) > 0 else 0
                
                # íŠ¸ë Œë“œ ê¸°ìš¸ê¸°
                x = np.arange(len(trend))
                slope = np.polyfit(x, trend, 1)[0]
                features['trend_slope'] = slope
                
        except Exception as e:
            print(f"ë¶„í•´ íŠ¹ì„± ê³„ì‚° ì˜¤ë¥˜: {e}")
        
        return features
    
    def _calculate_complexity_features(self, series: np.ndarray) -> Dict[str, float]:
        """ë³µì¡ì„± ì¸¡ì • íŠ¹ì„±"""
        features = {}
        
        try:
            # Lempel-Ziv ë³µì¡ì„±
            if len(series) > 10:
                features['lempel_ziv_complexity'] = self._lempel_ziv_complexity(series)
            
            # ë©€í‹°ìŠ¤ì¼€ì¼ ì—”íŠ¸ë¡œí”¼
            if len(series) > 50:
                for scale in [2, 3, 4, 5]:
                    coarse_grained = self._coarse_grain(series, scale)
                    if len(coarse_grained) > 10:
                        features[f'multiscale_entropy_scale_{scale}'] = self._calculate_sampen(coarse_grained, 2, 0.15)
            
            # ì••ì¶• ê¸°ë°˜ ë³µì¡ì„±
            features['compression_complexity'] = self._compression_complexity(series)
            
        except Exception as e:
            print(f"ë³µì¡ì„± íŠ¹ì„± ê³„ì‚° ì˜¤ë¥˜: {e}")
        
        return features
    
    def _lempel_ziv_complexity(self, series: np.ndarray) -> float:
        """Lempel-Ziv ë³µì¡ì„±"""
        try:
            # ë°”ì´ë„ˆë¦¬í™”
            median_val = np.median(series)
            binary = ''.join(['1' if x > median_val else '0' for x in series])
            
            # LZ77 ì••ì¶• ê·¼ì‚¬
            i = 0
            complexity = 0
            n = len(binary)
            
            while i < n:
                j = i + 1
                while j <= n:
                    substring = binary[i:j]
                    if substring not in binary[:i] or i == 0:
                        j += 1
                    else:
                        break
                complexity += 1
                i = j - 1 if j > i + 1 else i + 1
            
            return complexity / len(binary) if len(binary) > 0 else 0
            
        except:
            return 0.0
    
    def _coarse_grain(self, series: np.ndarray, scale: int) -> np.ndarray:
        """ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ì„ ìœ„í•œ coarse-graining"""
        try:
            n = len(series)
            coarse_length = n // scale
            coarse_grained = np.zeros(coarse_length)
            
            for i in range(coarse_length):
                start = i * scale
                end = (i + 1) * scale
                coarse_grained[i] = np.mean(series[start:end])
            
            return coarse_grained
        except:
            return series
    
    def _compression_complexity(self, series: np.ndarray) -> float:
        """ì••ì¶• ê¸°ë°˜ ë³µì¡ì„±"""
        try:
            import zlib
            # ì •ê·œí™”
            normalized = ((series - np.min(series)) / (np.max(series) - np.min(series)) * 255).astype(np.uint8)
            
            # ë°”ì´íŠ¸ ë³€í™˜ ë° ì••ì¶•
            data_bytes = normalized.tobytes()
            compressed = zlib.compress(data_bytes)
            
            return len(compressed) / len(data_bytes)
        except:
            return 0.5
    
    def _calculate_dynamical_features(self, series: np.ndarray) -> Dict[str, float]:
        """ë™ì—­í•™ ì‹œìŠ¤í…œ íŠ¹ì„±"""
        features = {}
        
        try:
            # ì¬ê·€ ì •ëŸ‰í™” ë¶„ì„ (RQA)
            if len(series) > 30:
                features.update(self._recurrence_quantification_analysis(series))
            
            # Detrended Fluctuation Analysis (DFA)
            if len(series) > 50:
                features['dfa_alpha'] = self._detrended_fluctuation_analysis(series)
                
        except Exception as e:
            print(f"ë™ì—­í•™ íŠ¹ì„± ê³„ì‚° ì˜¤ë¥˜: {e}")
        
        return features
    
    def _recurrence_quantification_analysis(self, series: np.ndarray) -> Dict[str, float]:
        """ì¬ê·€ ì •ëŸ‰í™” ë¶„ì„"""
        features = {}
        
        try:
            n = len(series)
            m = min(3, n//10)  # ì„ë² ë”© ì°¨ì›
            tau = 1  # ì‹œê°„ ì§€ì—°
            
            # ìœ„ìƒ ê³µê°„ ì¬êµ¬ì„±
            embedded = np.array([series[i:i+m*tau:tau] for i in range(n-m*tau+1)])
            
            # ê±°ë¦¬ í–‰ë ¬
            threshold = 0.1 * np.std(series)
            recurrence_matrix = np.zeros((len(embedded), len(embedded)))
            
            for i in range(len(embedded)):
                for j in range(len(embedded)):
                    if np.linalg.norm(embedded[i] - embedded[j]) < threshold:
                        recurrence_matrix[i, j] = 1
            
            # RQA ì¸¡ì •ê°’ë“¤
            features['recurrence_rate'] = np.mean(recurrence_matrix)
            
            # ê²°ì •ì„± (Determinism)
            diagonal_lines = self._find_diagonal_lines(recurrence_matrix)
            features['determinism'] = sum(len(line) for line in diagonal_lines if len(line) >= 2) / np.sum(recurrence_matrix)
            
            # í‰ê·  ëŒ€ê°ì„  ê¸¸ì´
            long_lines = [line for line in diagonal_lines if len(line) >= 2]
            features['average_diagonal_length'] = np.mean([len(line) for line in long_lines]) if long_lines else 0
            
        except Exception as e:
            print(f"RQA ê³„ì‚° ì˜¤ë¥˜: {e}")
        
        return features
    
    def _find_diagonal_lines(self, matrix: np.ndarray) -> List[List]:
        """ëŒ€ê°ì„  ë¼ì¸ ì°¾ê¸°"""
        lines = []
        n, m = matrix.shape
        
        # ì£¼ ëŒ€ê°ì„ ê³¼ í‰í–‰í•œ ëª¨ë“  ëŒ€ê°ì„  í™•ì¸
        for offset in range(-n+1, m):
            diagonal = np.diag(matrix, k=offset)
            current_line = []
            
            for val in diagonal:
                if val == 1:
                    current_line.append(val)
                else:
                    if len(current_line) > 0:
                        lines.append(current_line)
                        current_line = []
            
            if len(current_line) > 0:
                lines.append(current_line)
        
        return lines
    
    def _detrended_fluctuation_analysis(self, series: np.ndarray) -> float:
        """DFA ìŠ¤ì¼€ì¼ë§ ì§€ìˆ˜"""
        try:
            # ì ë¶„ ì‹œê³„ì—´
            integrated = np.cumsum(series - np.mean(series))
            
            # ë‹¤ì–‘í•œ ë°•ìŠ¤ í¬ê¸°
            box_sizes = np.logspace(1, np.log10(len(series)//4), 20, dtype=int)
            box_sizes = np.unique(box_sizes)
            
            fluctuations = []
            
            for box_size in box_sizes:
                n_boxes = len(integrated) // box_size
                boxes = integrated[:n_boxes * box_size].reshape(n_boxes, box_size)
                
                # ê° ë°•ìŠ¤ì—ì„œ ì„ í˜• íŠ¸ë Œë“œ ì œê±°
                local_fluctuations = []
                for box in boxes:
                    x = np.arange(len(box))
                    coeffs = np.polyfit(x, box, 1)
                    trend = np.polyval(coeffs, x)
                    detrended = box - trend
                    local_fluctuations.append(np.sqrt(np.mean(detrended**2)))
                
                fluctuations.append(np.mean(local_fluctuations))
            
            # ë¡œê·¸-ë¡œê·¸ í”¼íŒ…
            log_sizes = np.log(box_sizes[:len(fluctuations)])
            log_flucts = np.log(fluctuations)
            
            alpha = np.polyfit(log_sizes, log_flucts, 1)[0]
            return alpha
            
        except:
            return 0.5
    
    def _calculate_multivariate_features(self, price_series: np.ndarray, volume_series: np.ndarray) -> Dict[str, float]:
        """ë‹¤ë³€ëŸ‰ ë¶„ì„ íŠ¹ì„±"""
        features = {}
        
        try:
            if len(price_series) == len(volume_series) and len(price_series) > 10:
                # ìƒí˜¸ ìƒê´€
                correlation = np.corrcoef(price_series, volume_series)[0, 1]
                features['price_volume_correlation'] = correlation if not np.isnan(correlation) else 0
                
                # ë‹¤ë³€ëŸ‰ ìƒí˜¸ ì •ë³´
                features['multivariate_mutual_info'] = self._mutual_information(price_series, volume_series)
                
                # Principal Component Analysis
                if len(price_series) > 2:
                    combined_data = np.column_stack([price_series, volume_series])
                    try:
                        from sklearn.decomposition import PCA
                        pca = PCA(n_components=2)
                        pca.fit(combined_data)
                        
                        features['pca_explained_variance_1'] = pca.explained_variance_ratio_[0]
                        features['pca_explained_variance_2'] = pca.explained_variance_ratio_[1]
                    except:
                        pass
                
                # Granger Causality (ê°„ë‹¨í•œ ë²„ì „)
                features['granger_price_to_volume'] = self._simple_granger_test(price_series, volume_series)
                features['granger_volume_to_price'] = self._simple_granger_test(volume_series, price_series)
                
        except Exception as e:
            print(f"ë‹¤ë³€ëŸ‰ íŠ¹ì„± ê³„ì‚° ì˜¤ë¥˜: {e}")
        
        return features
    
    def _simple_granger_test(self, x: np.ndarray, y: np.ndarray) -> float:
        """ê°„ë‹¨í•œ Granger ì¸ê³¼ê´€ê³„ í…ŒìŠ¤íŠ¸"""
        try:
            if len(x) != len(y) or len(x) < 10:
                return 0.0
            
            # 1ì°¨ ì§€ì—° ëª¨ë¸
            y_lag1 = y[1:]
            y_current = y[:-1]
            x_lag1 = x[:-1]
            
            # ì œí•œëœ ëª¨ë¸: y(t) = Î± + Î²*y(t-1) + Îµ
            restricted_X = np.column_stack([np.ones(len(y_current)), y_current])
            
            # ë¹„ì œí•œëœ ëª¨ë¸: y(t) = Î± + Î²*y(t-1) + Î³*x(t-1) + Îµ  
            unrestricted_X = np.column_stack([np.ones(len(y_current)), y_current, x_lag1])
            
            # RSS ê³„ì‚°
            try:
                restricted_coef = np.linalg.lstsq(restricted_X, y_lag1, rcond=None)[0]
                unrestricted_coef = np.linalg.lstsq(unrestricted_X, y_lag1, rcond=None)[0]
                
                rss_restricted = np.sum((y_lag1 - restricted_X @ restricted_coef) ** 2)
                rss_unrestricted = np.sum((y_lag1 - unrestricted_X @ unrestricted_coef) ** 2)
                
                # F-í†µê³„ëŸ‰
                n = len(y_lag1)
                k = unrestricted_X.shape[1]
                q = 1  # ì œì•½ ìˆ˜
                
                f_stat = ((rss_restricted - rss_unrestricted) / q) / (rss_unrestricted / (n - k))
                return f_stat
                
            except np.linalg.LinAlgError:
                return 0.0
                
        except:
            return 0.0
    
    def _calculate_nonlinear_features(self, series: np.ndarray) -> Dict[str, float]:
        """ë¹„ì„ í˜• ì‹œê³„ì—´ íŠ¹ì„±"""
        features = {}
        
        try:
            # BDS í…ŒìŠ¤íŠ¸ ê·¼ì‚¬ (ë¹„ì„ í˜•ì„± í…ŒìŠ¤íŠ¸)
            if len(series) > 50:
                features['bds_statistic'] = self._bds_test(series)
            
            # Terasvirta ë¹„ì„ í˜•ì„± í…ŒìŠ¤íŠ¸
            if len(series) > 30:
                features['terasvirta_test'] = self._terasvirta_test(series)
            
            # ARCH íš¨ê³¼ í…ŒìŠ¤íŠ¸
            if len(series) > 20:
                features['arch_test'] = self._arch_test(series)
                
        except Exception as e:
            print(f"ë¹„ì„ í˜• íŠ¹ì„± ê³„ì‚° ì˜¤ë¥˜: {e}")
        
        return features
    
    def _bds_test(self, series: np.ndarray) -> float:
        """BDS í…ŒìŠ¤íŠ¸ í†µê³„ëŸ‰"""
        try:
            n = len(series)
            if n < 50:
                return 0.0
            
            # ì„ë² ë”© ì°¨ì›
            m = 2
            eps = 0.5 * np.std(series)  # ê±°ë¦¬ ì„ê³„ê°’
            
            # ì„ë² ë”© ë²¡í„° ìƒì„±
            vectors = np.array([series[i:i+m] for i in range(n-m+1)])
            
            # ìƒê´€ ì ë¶„ ê³„ì‚°
            c_m = 0
            c_1 = 0
            count = 0
            
            for i in range(len(vectors)):
                for j in range(i+1, len(vectors)):
                    # mì°¨ì› ê±°ë¦¬
                    dist_m = np.max(np.abs(vectors[i] - vectors[j]))
                    if dist_m < eps:
                        c_m += 1
                    
                    # 1ì°¨ì› ê±°ë¦¬ë“¤
                    dist_1_count = sum(1 for k in range(m) if abs(vectors[i][k] - vectors[j][k]) < eps)
                    if dist_1_count == m:
                        c_1 += 1
                    
                    count += 1
            
            if count > 0:
                c_m_normalized = c_m / count
                c_1_normalized = c_1 / count
                
                # BDS í†µê³„ëŸ‰ ê·¼ì‚¬
                if c_1_normalized > 0:
                    bds_stat = np.sqrt(count) * (c_m_normalized - c_1_normalized**m)
                    return abs(bds_stat)
            
            return 0.0
            
        except:
            return 0.0
    
    def _terasvirta_test(self, series: np.ndarray) -> float:
        """Terasvirta ë¹„ì„ í˜•ì„± í…ŒìŠ¤íŠ¸"""
        try:
            if len(series) < 30:
                return 0.0
            
            # 1ì°¨ AR ëª¨ë¸ í”¼íŒ…
            y = series[1:]
            x = series[:-1]
            
            # ì„ í˜• íšŒê·€
            try:
                coef = np.linalg.lstsq(x.reshape(-1, 1), y, rcond=None)[0]
                residuals = y - x * coef[0]
                
                # ì”ì°¨ì— ëŒ€í•œ ë¹„ì„ í˜•ì„± í…ŒìŠ¤íŠ¸
                # ì”ì°¨^2ë¥¼ x, x^2, x^3ì— íšŒê·€
                X_nonlinear = np.column_stack([x, x**2, x**3])
                
                try:
                    coef_nonlinear = np.linalg.lstsq(X_nonlinear, residuals**2, rcond=None)[0]
                    fitted_nonlinear = X_nonlinear @ coef_nonlinear
                    
                    # R^2 í†µê³„ëŸ‰
                    ss_res = np.sum((residuals**2 - fitted_nonlinear)**2)
                    ss_tot = np.sum((residuals**2 - np.mean(residuals**2))**2)
                    
                    r_squared = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0
                    
                    # LM í†µê³„ëŸ‰ ê·¼ì‚¬
                    n = len(residuals)
                    lm_stat = n * r_squared
                    
                    return lm_stat
                    
                except np.linalg.LinAlgError:
                    return 0.0
                    
            except np.linalg.LinAlgError:
                return 0.0
                
        except:
            return 0.0
    
    def _arch_test(self, series: np.ndarray) -> float:
        """ARCH íš¨ê³¼ í…ŒìŠ¤íŠ¸"""
        try:
            if len(series) < 20:
                return 0.0
            
            # ìˆ˜ìµë¥  ê³„ì‚°
            returns = np.diff(np.log(series))
            
            # í‰ê·  ì œê±°
            mean_return = np.mean(returns)
            centered_returns = returns - mean_return
            
            # ì œê³± ìˆ˜ìµë¥ 
            squared_returns = centered_returns**2
            
            # AR(1) ëª¨ë¸ for squared returns
            if len(squared_returns) > 1:
                y = squared_returns[1:]
                x = squared_returns[:-1]
                
                try:
                    # íšŒê·€: rÂ²(t) = Î± + Î²*rÂ²(t-1) + Îµ
                    X = np.column_stack([np.ones(len(x)), x])
                    coef = np.linalg.lstsq(X, y, rcond=None)[0]
                    residuals = y - X @ coef
                    
                    # LM í†µê³„ëŸ‰
                    n = len(residuals)
                    ss_res = np.sum(residuals**2)
                    mean_y = np.mean(y)
                    ss_tot = np.sum((y - mean_y)**2)
                    
                    r_squared = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0
                    lm_stat = n * r_squared
                    
                    return lm_stat
                    
                except np.linalg.LinAlgError:
                    return 0.0
                    
            return 0.0
            
        except:
            return 0.0

# ì‹¤ì‹œê°„ íŠ¹ì„± ì—…ë°ì´íŠ¸ ì‹œìŠ¤í…œ
class RealTimeFeatureUpdater:
    """ì‹¤ì‹œê°„ íŠ¹ì„± ì—…ë°ì´íŠ¸ ì‹œìŠ¤í…œ"""
    
    def __init__(self, feature_engineer: ComprehensiveFeatureEngineer):
        self.feature_engineer = feature_engineer
        self.update_interval = 300  # 5ë¶„
        self.last_update = None
        
    async def start_continuous_updates(self):
        """ì—°ì†ì ì¸ íŠ¹ì„± ì—…ë°ì´íŠ¸ ì‹œì‘"""
        while True:
            try:
                # ì‹œì¥ ë°ì´í„° ìˆ˜ì§‘ (ì‹¤ì œë¡œëŠ” APIì—ì„œ ê°€ì ¸ì˜´)
                market_data = await self._collect_market_data()
                
                # íŠ¹ì„± ìƒì„±
                features = await self.feature_engineer.generate_all_features(market_data)
                
                # íŠ¹ì„± í’ˆì§ˆ ê²€ì¦
                quality_score = await self._validate_feature_quality(features)
                
                print(f"âœ… íŠ¹ì„± ì—…ë°ì´íŠ¸ ì™„ë£Œ: {len(features.columns)}ê°œ íŠ¹ì„±, í’ˆì§ˆ ì ìˆ˜: {quality_score:.3f}")
                
                self.last_update = datetime.now()
                
                # ëŒ€ê¸°
                await asyncio.sleep(self.update_interval)
                
            except Exception as e:
                print(f"âŒ íŠ¹ì„± ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(60)  # 1ë¶„ í›„ ì¬ì‹œë„
    
    async def _collect_market_data(self) -> Dict[str, Any]:
        """ì‹œì¥ ë°ì´í„° ìˆ˜ì§‘"""
        # ì‹¤ì œë¡œëŠ” ì—¬ëŸ¬ APIì—ì„œ ë°ì´í„° ìˆ˜ì§‘
        return {
            'btc_price': np.random.uniform(40000, 80000),
            'volume': np.random.uniform(500, 2000) * 1000000,
            'bid': np.random.uniform(40000, 80000),
            'ask': np.random.uniform(40000, 80000),
            'trade_count': np.random.randint(50000, 200000),
            'high': np.random.uniform(40000, 80000),
            'low': np.random.uniform(40000, 80000),
            'open': np.random.uniform(40000, 80000),
        }
    
    async def _validate_feature_quality(self, features: pd.DataFrame) -> float:
        """íŠ¹ì„± í’ˆì§ˆ ê²€ì¦"""
        if len(features) == 0:
            return 0.0
        
        quality_metrics = []
        
        # NaN ë¹„ìœ¨ í™•ì¸
        nan_ratio = features.isnull().sum().sum() / (len(features.columns) * len(features))
        quality_metrics.append(1 - nan_ratio)
        
        # ë¬´í•œê°’ í™•ì¸
        inf_ratio = np.isinf(features.select_dtypes(include=[np.number]).values).sum() / features.select_dtypes(include=[np.number]).size
        quality_metrics.append(1 - inf_ratio)
        
        # ë¶„ì‚° í™•ì¸ (ìƒìˆ˜ íŠ¹ì„± íƒì§€)
        numeric_cols = features.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            variances = features[numeric_cols].var()
            non_constant_ratio = (variances > 1e-8).sum() / len(variances)
            quality_metrics.append(non_constant_ratio)
        
        return np.mean(quality_metrics)

# ì‚¬ìš© ì˜ˆì œ ë° ë©”ì¸ í•¨ìˆ˜
async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ í¬ê´„ì  ë¹„íŠ¸ì½”ì¸ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
    
    # ì„¤ì •
    config = FeatureConfig(
        max_features=1200,
        enable_advanced_math=ADVANCED_MATH_AVAILABLE,
        enable_cross_features=True
    )
    
    # íŠ¹ì„± ì—”ì§€ë‹ˆì–´ ì´ˆê¸°í™”
    feature_engineer = ComprehensiveFeatureEngineer(config)
    
    # ìƒ˜í”Œ ì‹œì¥ ë°ì´í„°
    sample_market_data = {
        'btc_price': 65000,
        'volume': 1500000000,
        'high': 66000,
        'low': 64000,
        'open': 64500,
        'bid': 64990,
        'ask': 65010,
        'trade_count': 125000,
        'hash_rate': 450e18,
        'active_addresses': 850000,
        'funding_rate': 0.0001,
    }
    
    # íŠ¹ì„± ìƒì„±
    print("\nğŸ“Š íŠ¹ì„± ìƒì„± ì¤‘...")
    features_df = await feature_engineer.generate_all_features(sample_market_data)
    
    print(f"âœ… ì´ {len(features_df.columns)}ê°œ íŠ¹ì„± ìƒì„± ì™„ë£Œ")
    
    # íŠ¹ì„± ì¤‘ìš”ë„ ìˆœìœ„
    importance_ranking = feature_engineer.get_feature_importance_ranking()
    print(f"\nğŸ“ˆ íŠ¹ì„± ì¤‘ìš”ë„ Top 10:")
    if len(importance_ranking) > 0:
        print(importance_ranking.head(10))
    
    # íŠ¹ì„± ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
    print(f"\nğŸ“‹ íŠ¹ì„± ì¹´í…Œê³ ë¦¬ë³„ í†µê³„:")
    categories = {
        'technical': [col for col in features_df.columns if any(x in col.lower() for x in ['rsi', 'macd', 'bb_', 'sma', 'ema', 'stoch'])],
        'microstructure': [col for col in features_df.columns if any(x in col.lower() for x in ['bid_ask', 'volume', 'trade', 'liquidity'])],
        'onchain': [col for col in features_df.columns if any(x in col.lower() for x in ['hash', 'address', 'mvrv', 'nvt', 'sopr'])],
        'macro': [col for col in features_df.columns if any(x in col.lower() for x in ['spx', 'dxy', 'gold', 'vix', 'fed'])],
        'math': [col for col in features_df.columns if any(x in col.lower() for x in ['fft', 'wavelet', 'entropy', 'fractal', 'hurst'])],
        'cross': [col for col in features_df.columns if '_x_' in col or '_div_' in col or '_minus_' in col]
    }
    
    for category, feature_list in categories.items():
        print(f"  â€¢ {category.upper()}: {len(feature_list)}ê°œ")
    
    # ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ ì‹œì‘ (ë°ëª¨ìš©ìœ¼ë¡œ ì£¼ì„ ì²˜ë¦¬)
    # updater = RealTimeFeatureUpdater(feature_engineer)
    # await updater.start_continuous_updates()
    
    return features_df, feature_engineer

if __name__ == "__main__":
    # ì‹¤í–‰
    loop = asyncio.get_event_loop()
    features_df, engineer = loop.run_until_complete(main())
    
    print(f"\nğŸ¯ ìµœì¢… ê²°ê³¼:")
    print(f"  â€¢ ì´ íŠ¹ì„± ìˆ˜: {len(features_df.columns)}")
    print(f"  â€¢ ë°ì´í„°ë² ì´ìŠ¤: {engineer.features_db_path}")
    print(f"  â€¢ ê³ ê¸‰ ìˆ˜í•™ ë¼ì´ë¸ŒëŸ¬ë¦¬: {'í™œì„±í™”' if ADVANCED_MATH_AVAILABLE else 'ë¹„í™œì„±í™”'}")
    print(f"\nğŸ’¡ ì‚¬ìš©ë²•:")
    print(f"  features_df = await engineer.generate_all_features(market_data)")
    print(f"  importance = engineer.get_feature_importance_ranking()")