#!/usr/bin/env python3
"""
ğŸ¯ ì‹œì¥ ìƒí™©ë³„ ë¶„ì„ ëª¨ë“ˆ (Market Regime Analyzer)
- ë‹¤ì°¨ì› ì‹œì¥ ìƒí™© ì‹ë³„ (ê°•ì„¸/ì•½ì„¸/íš¡ë³´ì¥ Ã— ë†’ì€/ë‚®ì€ ë³€ë™ì„±)
- ê¸°ìˆ ì  ì§€í‘œ ê¸°ë°˜ ìƒí™© ë¶„ë¥˜ (ì¶”ì„¸, ëª¨ë©˜í…€, ë³€ë™ì„±)
- ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ìƒí™© ì˜ˆì¸¡ (HMM, Clustering)
- ìƒí™© ì „í™˜ì  ê°ì§€ ë° ì¡°ê¸° ê²½ê³ 
- ìƒí™©ë³„ ìµœì  ì „ëµ ì¶”ì²œ
- ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±
- ìœ„ê¸° ìƒí™© ë¶„ì„ ë° ëŒ€ì‘ ì „ëµ
"""

import numpy as np
import pandas as pd
import warnings
from datetime import datetime, timedelta
import json
import os
from typing import Dict, List, Tuple, Optional, Union
from dataclasses import dataclass, field
import logging
from enum import Enum

# ML ë° í†µê³„ ë¼ì´ë¸ŒëŸ¬ë¦¬
from sklearn.cluster import KMeans, GaussianMixture
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score
from scipy import stats
from scipy.signal import find_peaks

# HMM (Hidden Markov Model)
try:
    from hmmlearn import hmm
    HMM_AVAILABLE = True
except ImportError:
    HMM_AVAILABLE = False
    print("âš ï¸ hmmlearn ë¯¸ì„¤ì¹˜: HMM ê¸°ë°˜ ë¶„ì„ ë¶ˆê°€")

# ê¸°ìˆ ì  ë¶„ì„
try:
    import ta
    TA_AVAILABLE = True
except ImportError:
    TA_AVAILABLE = False
    print("âš ï¸ ta ë¯¸ì„¤ì¹˜: ì¼ë¶€ ê¸°ìˆ ì  ì§€í‘œ ì‚¬ìš© ë¶ˆê°€")

# ì‹œê°í™”
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.express as px

warnings.filterwarnings('ignore')

class MarketRegime(Enum):
    """ì‹œì¥ ìƒí™© ì—´ê±°í˜•"""
    BULL_LOW_VOL = "ê°•ì„¸ì¥_ë‚®ì€ë³€ë™ì„±"
    BULL_HIGH_VOL = "ê°•ì„¸ì¥_ë†’ì€ë³€ë™ì„±"
    BEAR_LOW_VOL = "ì•½ì„¸ì¥_ë‚®ì€ë³€ë™ì„±"
    BEAR_HIGH_VOL = "ì•½ì„¸ì¥_ë†’ì€ë³€ë™ì„±"
    SIDEWAYS_LOW_VOL = "íš¡ë³´ì¥_ë‚®ì€ë³€ë™ì„±"
    SIDEWAYS_HIGH_VOL = "íš¡ë³´ì¥_ë†’ì€ë³€ë™ì„±"
    CRISIS = "ìœ„ê¸°ìƒí™©"
    RECOVERY = "íšŒë³µìƒí™©"

@dataclass
class RegimeConfig:
    """ì‹œì¥ ìƒí™© ë¶„ì„ ì„¤ì •"""
    # ê¸°ë³¸ ì„¤ì •
    lookback_periods: Dict[str, int] = field(default_factory=lambda: {
        'short': 20,    # ë‹¨ê¸° (20ì¼)
        'medium': 60,   # ì¤‘ê¸° (2ê°œì›”)  
        'long': 252     # ì¥ê¸° (1ë…„)
    })
    
    # ë³€ë™ì„± ì„ê³„ê°’
    volatility_thresholds: Dict[str, float] = field(default_factory=lambda: {
        'low': 0.3,      # ì—°ê°„ 30% ì´í•˜
        'medium': 0.6,   # ì—°ê°„ 30-60%
        'high': 0.6      # ì—°ê°„ 60% ì´ìƒ
    })
    
    # ì¶”ì„¸ ì„ê³„ê°’
    trend_thresholds: Dict[str, float] = field(default_factory=lambda: {
        'bull': 0.15,    # 15% ì´ìƒ ìƒìŠ¹
        'bear': -0.15,   # 15% ì´ìƒ í•˜ë½
        'sideways': 0.15 # Â±15% ë‚´ íš¡ë³´
    })
    
    # í´ëŸ¬ìŠ¤í„°ë§ ì„¤ì •
    n_clusters: int = 6          # í´ëŸ¬ìŠ¤í„° ê°œìˆ˜
    cluster_features: List[str] = field(default_factory=lambda: [
        'returns', 'volatility', 'momentum', 'volume', 'rsi', 'macd'
    ])
    
    # HMM ì„¤ì •
    n_hidden_states: int = 4     # ì€ë‹‰ ìƒíƒœ ê°œìˆ˜
    hmm_covariance_type: str = "full"  # HMM ê³µë¶„ì‚° íƒ€ì…
    
    # ìœ„ê¸° ê°ì§€ ì„¤ì •
    crisis_threshold: float = -0.20  # 20% ì´ìƒ í•˜ë½ì‹œ ìœ„ê¸°
    recovery_threshold: float = 0.10 # 10% ì´ìƒ íšŒë³µì‹œ íšŒë³µ
    
    # ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸
    stress_scenarios: List[str] = field(default_factory=lambda: [
        'black_swan', 'market_crash', 'liquidity_crisis', 'regulatory_shock'
    ])

@dataclass
class RegimePeriod:
    """ì‹œì¥ ìƒí™© ê¸°ê°„ ì •ë³´"""
    regime: MarketRegime
    start_date: datetime
    end_date: datetime
    duration_days: int
    characteristics: Dict
    performance_metrics: Dict
    confidence_score: float

class MarketRegimeAnalyzer:
    """ì‹œì¥ ìƒí™©ë³„ ë¶„ì„ ì‹œìŠ¤í…œ"""
    
    def __init__(self, config: RegimeConfig = None):
        self.config = config or RegimeConfig()
        self.data_path = "/Users/parkyoungjun/Desktop/BTC_Analysis_System"
        
        # ë¶„ì„ ê²°ê³¼ ì €ì¥
        self.regime_history = []
        self.current_regime = None
        self.regime_transition_matrix = None
        self.feature_importance = {}
        self.models = {}
        
        # ë¡œê¹… ì„¤ì •
        self.setup_logging()
        
    def setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(os.path.join(self.data_path, 'market_regime.log'), encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def comprehensive_regime_analysis(self, data: pd.DataFrame) -> Dict:
        """ì¢…í•©ì ì¸ ì‹œì¥ ìƒí™© ë¶„ì„"""
        self.logger.info("ğŸ¯ ì¢…í•©ì ì¸ ì‹œì¥ ìƒí™© ë¶„ì„ ì‹œì‘...")
        
        try:
            # 1. íŠ¹ì„± ë³€ìˆ˜ ê³„ì‚°
            features_df = self.calculate_regime_features(data)
            
            # 2. ê·œì¹™ ê¸°ë°˜ ìƒí™© ë¶„ë¥˜
            rule_based_regimes = self.rule_based_classification(features_df)
            
            # 3. í´ëŸ¬ìŠ¤í„°ë§ ê¸°ë°˜ ë¶„ë¥˜
            clustering_regimes = self.clustering_based_classification(features_df)
            
            # 4. HMM ê¸°ë°˜ ë¶„ë¥˜ (ì„ íƒì )
            hmm_regimes = None
            if HMM_AVAILABLE:
                hmm_regimes = self.hmm_based_classification(features_df)
            
            # 5. ì•™ìƒë¸” ë¶„ë¥˜
            ensemble_regimes = self.ensemble_classification(
                rule_based_regimes, clustering_regimes, hmm_regimes
            )
            
            # 6. ìƒí™© ì „í™˜ì  ê°ì§€
            transition_points = self.detect_regime_transitions(ensemble_regimes)
            
            # 7. ìƒí™©ë³„ ì„±ëŠ¥ ë¶„ì„
            regime_performance = self.analyze_regime_performance(data, ensemble_regimes)
            
            # 8. ìœ„ê¸° ìƒí™© ë¶„ì„
            crisis_analysis = self.crisis_situation_analysis(data, ensemble_regimes)
            
            # 9. ì˜ˆì¸¡ ë° ì¡°ê¸° ê²½ê³ 
            regime_forecast = self.forecast_regime_changes(features_df)
            
            # 10. ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸
            stress_test_results = self.conduct_stress_tests(data, ensemble_regimes)
            
            # ì¢…í•© ê²°ê³¼
            comprehensive_results = {
                'analysis_timestamp': datetime.now().isoformat(),
                'data_period': {
                    'start': data.index[0],
                    'end': data.index[-1],
                    'total_periods': len(data)
                },
                'feature_analysis': {
                    'features_calculated': len(features_df.columns),
                    'feature_importance': self.feature_importance
                },
                'regime_classifications': {
                    'rule_based': rule_based_regimes,
                    'clustering_based': clustering_regimes,
                    'hmm_based': hmm_regimes,
                    'ensemble': ensemble_regimes
                },
                'transition_analysis': transition_points,
                'performance_analysis': regime_performance,
                'crisis_analysis': crisis_analysis,
                'regime_forecast': regime_forecast,
                'stress_test_results': stress_test_results,
                'current_regime': self.current_regime
            }
            
            # ê²°ê³¼ ì €ì¥
            self.save_analysis_results(comprehensive_results)
            
            # ì‹œê°í™”
            self.create_regime_visualization(comprehensive_results)
            
            return comprehensive_results
            
        except Exception as e:
            self.logger.error(f"ì‹œì¥ ìƒí™© ë¶„ì„ ì‹¤íŒ¨: {e}")
            raise
    
    def calculate_regime_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """ì‹œì¥ ìƒí™© ë¶„ë¥˜ë¥¼ ìœ„í•œ íŠ¹ì„± ë³€ìˆ˜ ê³„ì‚°"""
        self.logger.info("ğŸ“Š ì‹œì¥ ìƒí™© íŠ¹ì„± ë³€ìˆ˜ ê³„ì‚° ì¤‘...")
        
        features_df = pd.DataFrame(index=data.index)
        
        # ê°€ê²© ë°ì´í„° ì¶”ì¶œ
        if 'price' in data.columns:
            prices = data['price']
        else:
            # ì²« ë²ˆì§¸ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì„ ê°€ê²©ìœ¼ë¡œ ê°€ì •
            numeric_cols = data.select_dtypes(include=[np.number]).columns
            prices = data[numeric_cols[0]]
        
        # 1. ê¸°ë³¸ ìˆ˜ìµë¥  ë° ë³€ë™ì„±
        features_df['returns'] = prices.pct_change()
        features_df['abs_returns'] = features_df['returns'].abs()
        
        for period in [5, 10, 20, 60]:
            features_df[f'volatility_{period}'] = features_df['returns'].rolling(period).std() * np.sqrt(252)
            features_df[f'returns_mean_{period}'] = features_df['returns'].rolling(period).mean()
            features_df[f'returns_skew_{period}'] = features_df['returns'].rolling(period).skew()
            features_df[f'returns_kurt_{period}'] = features_df['returns'].rolling(period).kurt()
        
        # 2. ì¶”ì„¸ ì§€í‘œ
        for period in [20, 50, 100, 200]:
            features_df[f'sma_{period}'] = prices.rolling(period).mean()
            features_df[f'price_vs_sma_{period}'] = (prices / features_df[f'sma_{period}'] - 1)
            
            # ì´ë™í‰ê·  ê¸°ìš¸ê¸° (ì¶”ì„¸ ê°•ë„)
            features_df[f'sma_slope_{period}'] = features_df[f'sma_{period}'].diff(10) / features_df[f'sma_{period}'].shift(10)
        
        # 3. ëª¨ë©˜í…€ ì§€í‘œ
        for period in [5, 10, 20, 60]:
            features_df[f'momentum_{period}'] = (prices / prices.shift(period) - 1)
            features_df[f'roc_{period}'] = prices.pct_change(period)
        
        # 4. ê¸°ìˆ ì  ì§€í‘œ (TA ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©)
        if TA_AVAILABLE:
            # RSI
            features_df['rsi_14'] = ta.momentum.rsi(prices, window=14)
            features_df['rsi_30'] = ta.momentum.rsi(prices, window=30)
            
            # MACD
            macd = ta.trend.MACD(prices)
            features_df['macd'] = macd.macd()
            features_df['macd_signal'] = macd.macd_signal()
            features_df['macd_diff'] = macd.macd_diff()
            
            # ë³¼ë¦°ì € ë°´ë“œ
            bb = ta.volatility.BollingerBands(prices)
            features_df['bb_high'] = bb.bollinger_hband()
            features_df['bb_low'] = bb.bollinger_lband()
            features_df['bb_width'] = bb.bollinger_wband()
            features_df['bb_position'] = (prices - bb.bollinger_lband()) / (bb.bollinger_hband() - bb.bollinger_lband())
            
            # ATR (Average True Range)
            if 'high' in data.columns and 'low' in data.columns:
                features_df['atr_14'] = ta.volatility.AverageTrueRange(
                    high=data['high'], low=data['low'], close=prices
                ).average_true_range()
            
            # ìŠ¤í† ìºìŠ¤í‹±
            if 'high' in data.columns and 'low' in data.columns:
                stoch = ta.momentum.StochasticOscillator(
                    high=data['high'], low=data['low'], close=prices
                )
                features_df['stoch_k'] = stoch.stoch()
                features_df['stoch_d'] = stoch.stoch_signal()
        else:
            # TA ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ì´ ê¸°ë³¸ ì§€í‘œ ê³„ì‚°
            features_df['rsi_14'] = self._calculate_rsi(prices, 14)
            features_df['macd'] = self._calculate_macd(prices)
        
        # 5. ê±°ë˜ëŸ‰ ì§€í‘œ (ìˆëŠ” ê²½ìš°)
        if 'volume' in data.columns:
            volume = data['volume']
            features_df['volume'] = volume
            features_df['volume_sma_20'] = volume.rolling(20).mean()
            features_df['volume_ratio'] = volume / features_df['volume_sma_20']
            
            # ê°€ê²©-ê±°ë˜ëŸ‰ ê´€ê³„
            features_df['price_volume_trend'] = ((prices - prices.shift(1)) * volume).rolling(20).sum()
            
            for period in [5, 10, 20]:
                features_df[f'volume_std_{period}'] = volume.rolling(period).std()
        
        # 6. ê³ ì°¨ ëª¨ë©˜íŠ¸
        for period in [20, 60]:
            features_df[f'higher_moment_3_{period}'] = features_df['returns'].rolling(period).apply(lambda x: stats.moment(x, moment=3))
            features_df[f'higher_moment_4_{period}'] = features_df['returns'].rolling(period).apply(lambda x: stats.moment(x, moment=4))
        
        # 7. ë“œë¡œë‹¤ìš´ ì§€í‘œ
        cumulative_returns = (1 + features_df['returns']).cumprod()
        running_max = cumulative_returns.expanding().max()
        features_df['drawdown'] = (cumulative_returns - running_max) / running_max
        features_df['drawdown_duration'] = self._calculate_drawdown_duration(features_df['drawdown'])
        
        # 8. ë³€ë™ì„± í´ëŸ¬ìŠ¤í„°ë§
        features_df['volatility_clustering'] = self._calculate_volatility_clustering(features_df['returns'])
        
        # 9. ì‹œê°„ ê¸°ë°˜ íŠ¹ì„±
        features_df['hour'] = features_df.index.hour if hasattr(features_df.index, 'hour') else 0
        features_df['day_of_week'] = features_df.index.dayofweek if hasattr(features_df.index, 'dayofweek') else 0
        features_df['month'] = features_df.index.month if hasattr(features_df.index, 'month') else 0
        
        # NaN ì²˜ë¦¬
        features_df = features_df.fillna(method='ffill').fillna(method='bfill').fillna(0)
        
        self.logger.info(f"íŠ¹ì„± ë³€ìˆ˜ ê³„ì‚° ì™„ë£Œ: {len(features_df.columns)}ê°œ ë³€ìˆ˜")
        
        return features_df
    
    def _calculate_rsi(self, prices: pd.Series, period: int = 14) -> pd.Series:
        """RSI ê³„ì‚° (TA ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ì´)"""
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))
        return rsi
    
    def _calculate_macd(self, prices: pd.Series, fast: int = 12, slow: int = 26) -> pd.Series:
        """MACD ê³„ì‚° (TA ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ì´)"""
        ema_fast = prices.ewm(span=fast).mean()
        ema_slow = prices.ewm(span=slow).mean()
        macd = ema_fast - ema_slow
        return macd
    
    def _calculate_drawdown_duration(self, drawdown: pd.Series) -> pd.Series:
        """ë“œë¡œë‹¤ìš´ ì§€ì†ê¸°ê°„ ê³„ì‚°"""
        duration = pd.Series(0, index=drawdown.index)
        current_duration = 0
        
        for i, dd in enumerate(drawdown):
            if dd < 0:  # ë“œë¡œë‹¤ìš´ ìƒíƒœ
                current_duration += 1
            else:  # íšŒë³µ ìƒíƒœ
                current_duration = 0
            duration.iloc[i] = current_duration
        
        return duration
    
    def _calculate_volatility_clustering(self, returns: pd.Series, window: int = 20) -> pd.Series:
        """ë³€ë™ì„± í´ëŸ¬ìŠ¤í„°ë§ ê³„ì‚°"""
        abs_returns = returns.abs()
        rolling_vol = abs_returns.rolling(window).std()
        vol_ratio = abs_returns / rolling_vol
        return vol_ratio
    
    def rule_based_classification(self, features_df: pd.DataFrame) -> pd.Series:
        """ê·œì¹™ ê¸°ë°˜ ì‹œì¥ ìƒí™© ë¶„ë¥˜"""
        self.logger.info("ğŸ” ê·œì¹™ ê¸°ë°˜ ì‹œì¥ ìƒí™© ë¶„ë¥˜ ì¤‘...")
        
        regimes = pd.Series(index=features_df.index, dtype=str)
        
        # ê¸°ë³¸ ì§€í‘œ ì¶”ì¶œ
        returns_20 = features_df.get('returns_mean_20', features_df['returns'].rolling(20).mean())
        volatility_20 = features_df.get('volatility_20', features_df['returns'].rolling(20).std() * np.sqrt(252))
        momentum_20 = features_df.get('momentum_20', features_df['returns'].rolling(20).sum())
        
        # ê° ì‹œì ë³„ ë¶„ë¥˜
        for i, idx in enumerate(features_df.index):
            if pd.isna(returns_20.iloc[i]) or pd.isna(volatility_20.iloc[i]):
                regimes.iloc[i] = MarketRegime.SIDEWAYS_LOW_VOL.value
                continue
            
            ret_20 = returns_20.iloc[i]
            vol_20 = volatility_20.iloc[i]
            mom_20 = momentum_20.iloc[i] if not pd.isna(momentum_20.iloc[i]) else 0
            
            # ìœ„ê¸° ìƒí™© ê°ì§€ (ê¸‰ê²©í•œ í•˜ë½)
            if ret_20 < self.config.crisis_threshold:
                regimes.iloc[i] = MarketRegime.CRISIS.value
            # íšŒë³µ ìƒí™© ê°ì§€
            elif ret_20 > self.config.recovery_threshold and mom_20 > 0.05:
                regimes.iloc[i] = MarketRegime.RECOVERY.value
            # ì¼ë°˜ ìƒí™© ë¶„ë¥˜
            else:
                # ì¶”ì„¸ ë¶„ë¥˜
                if ret_20 > self.config.trend_thresholds['bull']:
                    trend = 'bull'
                elif ret_20 < self.config.trend_thresholds['bear']:
                    trend = 'bear'
                else:
                    trend = 'sideways'
                
                # ë³€ë™ì„± ë¶„ë¥˜
                if vol_20 > self.config.volatility_thresholds['high']:
                    vol_class = 'high'
                else:
                    vol_class = 'low'
                
                # ìƒí™© ë§¤í•‘
                if trend == 'bull' and vol_class == 'high':
                    regimes.iloc[i] = MarketRegime.BULL_HIGH_VOL.value
                elif trend == 'bull' and vol_class == 'low':
                    regimes.iloc[i] = MarketRegime.BULL_LOW_VOL.value
                elif trend == 'bear' and vol_class == 'high':
                    regimes.iloc[i] = MarketRegime.BEAR_HIGH_VOL.value
                elif trend == 'bear' and vol_class == 'low':
                    regimes.iloc[i] = MarketRegime.BEAR_LOW_VOL.value
                elif trend == 'sideways' and vol_class == 'high':
                    regimes.iloc[i] = MarketRegime.SIDEWAYS_HIGH_VOL.value
                else:
                    regimes.iloc[i] = MarketRegime.SIDEWAYS_LOW_VOL.value
        
        return regimes
    
    def clustering_based_classification(self, features_df: pd.DataFrame) -> pd.Series:
        """í´ëŸ¬ìŠ¤í„°ë§ ê¸°ë°˜ ì‹œì¥ ìƒí™© ë¶„ë¥˜"""
        self.logger.info("ğŸ¯ í´ëŸ¬ìŠ¤í„°ë§ ê¸°ë°˜ ì‹œì¥ ìƒí™© ë¶„ë¥˜ ì¤‘...")
        
        # í´ëŸ¬ìŠ¤í„°ë§ìš© íŠ¹ì„± ì„ íƒ
        cluster_features = []
        for feature in self.config.cluster_features:
            matching_cols = [col for col in features_df.columns if feature in col.lower()]
            if matching_cols:
                cluster_features.extend(matching_cols[:2])  # ê° íŠ¹ì„±ë‹¹ ìµœëŒ€ 2ê°œ ì»¬ëŸ¼
        
        if not cluster_features:
            # ê¸°ë³¸ íŠ¹ì„± ì‚¬ìš©
            cluster_features = ['returns', 'volatility_20', 'momentum_20', 'rsi_14']
            cluster_features = [f for f in cluster_features if f in features_df.columns]
        
        if len(cluster_features) < 2:
            self.logger.warning("í´ëŸ¬ìŠ¤í„°ë§ì„ ìœ„í•œ ì¶©ë¶„í•œ íŠ¹ì„± ì—†ìŒ")
            return pd.Series(MarketRegime.SIDEWAYS_LOW_VOL.value, index=features_df.index)
        
        # ë°ì´í„° ì „ì²˜ë¦¬
        X = features_df[cluster_features].fillna(0)
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        # ìµœì  í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ ì°¾ê¸°
        optimal_k = self._find_optimal_clusters(X_scaled)
        
        # K-Means í´ëŸ¬ìŠ¤í„°ë§
        kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
        cluster_labels = kmeans.fit_predict(X_scaled)
        
        # í´ëŸ¬ìŠ¤í„°ë³„ íŠ¹ì„± ë¶„ì„
        cluster_characteristics = self._analyze_cluster_characteristics(
            features_df, cluster_labels, cluster_features
        )
        
        # í´ëŸ¬ìŠ¤í„°ë¥¼ ì‹œì¥ ìƒí™©ìœ¼ë¡œ ë§¤í•‘
        regime_mapping = self._map_clusters_to_regimes(cluster_characteristics)
        
        # ê²°ê³¼ ìƒì„±
        regimes = pd.Series(index=features_df.index, dtype=str)
        for i, label in enumerate(cluster_labels):
            regimes.iloc[i] = regime_mapping.get(label, MarketRegime.SIDEWAYS_LOW_VOL.value)
        
        # ëª¨ë¸ ì €ì¥
        self.models['clustering'] = {
            'kmeans': kmeans,
            'scaler': scaler,
            'features': cluster_features,
            'regime_mapping': regime_mapping
        }
        
        return regimes
    
    def _find_optimal_clusters(self, X: np.ndarray, max_k: int = 8) -> int:
        """ìµœì  í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ ì°¾ê¸° (ì—˜ë³´ìš° ë°©ë²• + ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´)"""
        if len(X) < 10:
            return 2
        
        max_k = min(max_k, len(X) // 3)  # ìƒ˜í”Œ ìˆ˜ì— ë”°ë¼ ìµœëŒ€ k ì¡°ì •
        
        inertias = []
        silhouette_scores = []
        k_range = range(2, max_k + 1)
        
        for k in k_range:
            if k >= len(X):
                break
                
            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
            labels = kmeans.fit_predict(X)
            
            inertias.append(kmeans.inertia_)
            
            # ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´
            if len(set(labels)) > 1:  # í´ëŸ¬ìŠ¤í„°ê°€ ì‹¤ì œë¡œ ë¶„ë¦¬ë˜ì—ˆëŠ”ì§€ í™•ì¸
                sil_score = silhouette_score(X, labels)
                silhouette_scores.append(sil_score)
            else:
                silhouette_scores.append(0)
        
        # ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´ê°€ ê°€ì¥ ë†’ì€ k ì„ íƒ
        if silhouette_scores:
            optimal_idx = np.argmax(silhouette_scores)
            optimal_k = list(k_range)[optimal_idx]
        else:
            optimal_k = self.config.n_clusters
        
        return optimal_k
    
    def _analyze_cluster_characteristics(self, features_df: pd.DataFrame, 
                                       labels: np.ndarray, features: List[str]) -> Dict:
        """í´ëŸ¬ìŠ¤í„°ë³„ íŠ¹ì„± ë¶„ì„"""
        characteristics = {}
        
        for cluster_id in np.unique(labels):
            cluster_mask = labels == cluster_id
            cluster_data = features_df.loc[cluster_mask, features]
            
            characteristics[cluster_id] = {
                'size': int(cluster_mask.sum()),
                'proportion': float(cluster_mask.mean()),
                'mean_values': cluster_data.mean().to_dict(),
                'std_values': cluster_data.std().to_dict(),
                'feature_summary': {
                    'returns': cluster_data.get('returns', pd.Series(0)).mean(),
                    'volatility': cluster_data[[col for col in cluster_data.columns if 'volatility' in col]].mean().mean() if any('volatility' in col for col in cluster_data.columns) else 0,
                    'momentum': cluster_data[[col for col in cluster_data.columns if 'momentum' in col]].mean().mean() if any('momentum' in col for col in cluster_data.columns) else 0
                }
            }
        
        return characteristics
    
    def _map_clusters_to_regimes(self, cluster_characteristics: Dict) -> Dict:
        """í´ëŸ¬ìŠ¤í„°ë¥¼ ì‹œì¥ ìƒí™©ìœ¼ë¡œ ë§¤í•‘"""
        regime_mapping = {}
        
        for cluster_id, chars in cluster_characteristics.items():
            returns = chars['feature_summary']['returns']
            volatility = chars['feature_summary']['volatility']
            
            # ë§¤í•‘ ë¡œì§
            if returns > 0.1:  # ë†’ì€ ì–‘ì˜ ìˆ˜ìµë¥ 
                if volatility > 0.5:
                    regime_mapping[cluster_id] = MarketRegime.BULL_HIGH_VOL.value
                else:
                    regime_mapping[cluster_id] = MarketRegime.BULL_LOW_VOL.value
            elif returns < -0.1:  # ë†’ì€ ìŒì˜ ìˆ˜ìµë¥ 
                if volatility > 0.5:
                    regime_mapping[cluster_id] = MarketRegime.BEAR_HIGH_VOL.value
                else:
                    regime_mapping[cluster_id] = MarketRegime.BEAR_LOW_VOL.value
            else:  # ì¤‘ë¦½ì  ìˆ˜ìµë¥ 
                if volatility > 0.5:
                    regime_mapping[cluster_id] = MarketRegime.SIDEWAYS_HIGH_VOL.value
                else:
                    regime_mapping[cluster_id] = MarketRegime.SIDEWAYS_LOW_VOL.value
        
        return regime_mapping
    
    def hmm_based_classification(self, features_df: pd.DataFrame) -> Optional[pd.Series]:
        """HMM ê¸°ë°˜ ì‹œì¥ ìƒí™© ë¶„ë¥˜"""
        if not HMM_AVAILABLE:
            self.logger.warning("HMM ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¯¸ì‚¬ìš© - HMM ë¶„ë¥˜ ìƒëµ")
            return None
        
        self.logger.info("ğŸ”¬ HMM ê¸°ë°˜ ì‹œì¥ ìƒí™© ë¶„ë¥˜ ì¤‘...")
        
        try:
            # HMM ì…ë ¥ ë°ì´í„° ì¤€ë¹„
            hmm_features = ['returns', 'volatility_20']  # ê¸°ë³¸ íŠ¹ì„±
            hmm_features = [f for f in hmm_features if f in features_df.columns]
            
            if len(hmm_features) < 1:
                return None
            
            X = features_df[hmm_features].fillna(0).values
            
            if len(X) < 50:  # HMMì—ëŠ” ì¶©ë¶„í•œ ë°ì´í„° í•„ìš”
                return None
            
            # HMM ëª¨ë¸ í•™ìŠµ
            model = hmm.GaussianHMM(
                n_components=self.config.n_hidden_states,
                covariance_type=self.config.hmm_covariance_type,
                random_state=42
            )
            
            model.fit(X)
            hidden_states = model.predict(X)
            
            # ìƒíƒœë³„ íŠ¹ì„± ë¶„ì„
            state_characteristics = {}
            for state in range(self.config.n_hidden_states):
                state_mask = hidden_states == state
                state_data = features_df.loc[state_mask]
                
                if len(state_data) > 0:
                    state_characteristics[state] = {
                        'mean_returns': state_data['returns'].mean() if 'returns' in state_data else 0,
                        'mean_volatility': state_data.get('volatility_20', pd.Series(0)).mean()
                    }
            
            # ìƒíƒœë¥¼ ì‹œì¥ ìƒí™©ìœ¼ë¡œ ë§¤í•‘
            state_to_regime = self._map_hmm_states_to_regimes(state_characteristics)
            
            # ê²°ê³¼ ìƒì„±
            regimes = pd.Series(index=features_df.index, dtype=str)
            for i, state in enumerate(hidden_states):
                regimes.iloc[i] = state_to_regime.get(state, MarketRegime.SIDEWAYS_LOW_VOL.value)
            
            # ëª¨ë¸ ì €ì¥
            self.models['hmm'] = {
                'model': model,
                'features': hmm_features,
                'state_mapping': state_to_regime
            }
            
            return regimes
            
        except Exception as e:
            self.logger.warning(f"HMM ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
            return None
    
    def _map_hmm_states_to_regimes(self, state_characteristics: Dict) -> Dict:
        """HMM ìƒíƒœë¥¼ ì‹œì¥ ìƒí™©ìœ¼ë¡œ ë§¤í•‘"""
        state_mapping = {}
        
        # ìˆ˜ìµë¥ ê³¼ ë³€ë™ì„±ì— ë”°ë¼ ìƒíƒœ ì •ë ¬
        states_by_returns = sorted(state_characteristics.items(), 
                                 key=lambda x: x[1]['mean_returns'])
        
        n_states = len(states_by_returns)
        
        for i, (state, chars) in enumerate(states_by_returns):
            returns = chars['mean_returns']
            volatility = chars['mean_volatility']
            
            # ìˆ˜ìµë¥  ìˆœì„œì— ë”°ë¥¸ ë§¤í•‘
            if i < n_states // 3:  # í•˜ìœ„ 1/3 - ì•½ì„¸
                if volatility > 0.5:
                    state_mapping[state] = MarketRegime.BEAR_HIGH_VOL.value
                else:
                    state_mapping[state] = MarketRegime.BEAR_LOW_VOL.value
            elif i >= 2 * n_states // 3:  # ìƒìœ„ 1/3 - ê°•ì„¸
                if volatility > 0.5:
                    state_mapping[state] = MarketRegime.BULL_HIGH_VOL.value
                else:
                    state_mapping[state] = MarketRegime.BULL_LOW_VOL.value
            else:  # ì¤‘ê°„ 1/3 - íš¡ë³´
                if volatility > 0.5:
                    state_mapping[state] = MarketRegime.SIDEWAYS_HIGH_VOL.value
                else:
                    state_mapping[state] = MarketRegime.SIDEWAYS_LOW_VOL.value
        
        return state_mapping
    
    def ensemble_classification(self, rule_based: pd.Series, 
                              clustering: pd.Series, 
                              hmm_based: Optional[pd.Series] = None) -> pd.Series:
        """ì•™ìƒë¸” ì‹œì¥ ìƒí™© ë¶„ë¥˜"""
        self.logger.info("ğŸ¯ ì•™ìƒë¸” ì‹œì¥ ìƒí™© ë¶„ë¥˜ ì¤‘...")
        
        # ê°€ì¤‘ì¹˜ ì„¤ì •
        weights = {'rule': 0.4, 'clustering': 0.4, 'hmm': 0.2}
        
        if hmm_based is None:
            weights = {'rule': 0.6, 'clustering': 0.4}
        
        ensemble_regimes = pd.Series(index=rule_based.index, dtype=str)
        
        for i in range(len(rule_based)):
            votes = {}
            
            # ê·œì¹™ ê¸°ë°˜ íˆ¬í‘œ
            rule_vote = rule_based.iloc[i]
            votes[rule_vote] = votes.get(rule_vote, 0) + weights['rule']
            
            # í´ëŸ¬ìŠ¤í„°ë§ ê¸°ë°˜ íˆ¬í‘œ
            clustering_vote = clustering.iloc[i]
            votes[clustering_vote] = votes.get(clustering_vote, 0) + weights['clustering']
            
            # HMM ê¸°ë°˜ íˆ¬í‘œ
            if hmm_based is not None:
                hmm_vote = hmm_based.iloc[i]
                votes[hmm_vote] = votes.get(hmm_vote, 0) + weights['hmm']
            
            # ìµœë‹¤ ë“í‘œ ìƒí™© ì„ íƒ
            ensemble_regimes.iloc[i] = max(votes.items(), key=lambda x: x[1])[0]
        
        return ensemble_regimes
    
    def detect_regime_transitions(self, regimes: pd.Series) -> Dict:
        """ì‹œì¥ ìƒí™© ì „í™˜ì  ê°ì§€"""
        self.logger.info("ğŸ”„ ì‹œì¥ ìƒí™© ì „í™˜ì  ê°ì§€ ì¤‘...")
        
        transitions = []
        current_regime = regimes.iloc[0] if len(regimes) > 0 else None
        regime_start = regimes.index[0] if len(regimes) > 0 else None
        
        for i, (timestamp, regime) in enumerate(regimes.items()):
            if regime != current_regime:
                # ì „í™˜ì  ê°ì§€
                if current_regime is not None and regime_start is not None:
                    transitions.append({
                        'from_regime': current_regime,
                        'to_regime': regime,
                        'transition_date': timestamp,
                        'previous_duration': i - regimes.index.get_loc(regime_start) if regime_start in regimes.index else 0,
                        'transition_type': self._classify_transition_type(current_regime, regime)
                    })
                
                current_regime = regime
                regime_start = timestamp
        
        # ì „í™˜ í†µê³„
        transition_stats = self._calculate_transition_statistics(transitions)
        
        return {
            'transitions': transitions,
            'transition_statistics': transition_stats,
            'total_transitions': len(transitions),
            'avg_regime_duration': np.mean([t['previous_duration'] for t in transitions]) if transitions else 0
        }
    
    def _classify_transition_type(self, from_regime: str, to_regime: str) -> str:
        """ì „í™˜ ìœ í˜• ë¶„ë¥˜"""
        # ìœ„ê¸° ê´€ë ¨ ì „í™˜
        if 'CRISIS' in to_regime:
            return 'crisis_onset'
        elif 'CRISIS' in from_regime:
            return 'crisis_recovery'
        
        # ì¶”ì„¸ ì „í™˜
        if 'BULL' in from_regime and 'BEAR' in to_regime:
            return 'bull_to_bear'
        elif 'BEAR' in from_regime and 'BULL' in to_regime:
            return 'bear_to_bull'
        elif 'SIDEWAYS' in from_regime and ('BULL' in to_regime or 'BEAR' in to_regime):
            return 'breakout'
        elif ('BULL' in from_regime or 'BEAR' in from_regime) and 'SIDEWAYS' in to_regime:
            return 'consolidation'
        
        # ë³€ë™ì„± ì „í™˜
        if 'LOW_VOL' in from_regime and 'HIGH_VOL' in to_regime:
            return 'volatility_increase'
        elif 'HIGH_VOL' in from_regime and 'LOW_VOL' in to_regime:
            return 'volatility_decrease'
        
        return 'other'
    
    def _calculate_transition_statistics(self, transitions: List[Dict]) -> Dict:
        """ì „í™˜ í†µê³„ ê³„ì‚°"""
        if not transitions:
            return {}
        
        # ì „í™˜ ìœ í˜•ë³„ ë¹ˆë„
        transition_types = [t['transition_type'] for t in transitions]
        type_counts = {t_type: transition_types.count(t_type) for t_type in set(transition_types)}
        
        # ì§€ì† ê¸°ê°„ ë¶„ì„
        durations = [t['previous_duration'] for t in transitions if t['previous_duration'] > 0]
        
        return {
            'transition_type_frequency': type_counts,
            'duration_statistics': {
                'mean': np.mean(durations) if durations else 0,
                'median': np.median(durations) if durations else 0,
                'std': np.std(durations) if durations else 0,
                'min': np.min(durations) if durations else 0,
                'max': np.max(durations) if durations else 0
            }
        }
    
    def analyze_regime_performance(self, data: pd.DataFrame, 
                                 regimes: pd.Series) -> Dict:
        """ì‹œì¥ ìƒí™©ë³„ ì„±ëŠ¥ ë¶„ì„"""
        self.logger.info("ğŸ“ˆ ì‹œì¥ ìƒí™©ë³„ ì„±ëŠ¥ ë¶„ì„ ì¤‘...")
        
        performance_analysis = {}
        
        # ê°€ê²© ë°ì´í„°
        if 'price' in data.columns:
            prices = data['price']
        else:
            numeric_cols = data.select_dtypes(include=[np.number]).columns
            prices = data[numeric_cols[0]]
        
        returns = prices.pct_change().dropna()
        
        # ê° ìƒí™©ë³„ ì„±ëŠ¥ ë¶„ì„
        for regime_type in regimes.unique():
            if pd.isna(regime_type):
                continue
                
            regime_mask = regimes == regime_type
            regime_returns = returns[regime_mask]
            
            if len(regime_returns) == 0:
                continue
            
            # ê¸°ë³¸ í†µê³„
            performance_analysis[regime_type] = {
                'observations': len(regime_returns),
                'total_periods': regime_mask.sum(),
                'proportion': float(regime_mask.mean()),
                'returns_statistics': {
                    'mean': float(regime_returns.mean()),
                    'median': float(regime_returns.median()),
                    'std': float(regime_returns.std()),
                    'skewness': float(stats.skew(regime_returns)) if len(regime_returns) > 3 else 0,
                    'kurtosis': float(stats.kurtosis(regime_returns)) if len(regime_returns) > 3 else 0
                },
                'annual_metrics': {
                    'annual_return': float(regime_returns.mean() * 252),
                    'annual_volatility': float(regime_returns.std() * np.sqrt(252)),
                    'sharpe_ratio': float((regime_returns.mean() * 252) / (regime_returns.std() * np.sqrt(252))) if regime_returns.std() > 0 else 0
                },
                'risk_metrics': {
                    'var_95': float(np.percentile(regime_returns, 5)),
                    'max_loss': float(regime_returns.min()),
                    'max_gain': float(regime_returns.max()),
                    'positive_periods_ratio': float((regime_returns > 0).mean())
                }
            }
        
        return performance_analysis
    
    def crisis_situation_analysis(self, data: pd.DataFrame, regimes: pd.Series) -> Dict:
        """ìœ„ê¸° ìƒí™© ë¶„ì„"""
        self.logger.info("ğŸš¨ ìœ„ê¸° ìƒí™© ë¶„ì„ ì¤‘...")
        
        # ìœ„ê¸° ìƒí™© ì‹ë³„
        crisis_periods = regimes == MarketRegime.CRISIS.value
        
        if not crisis_periods.any():
            return {'no_crisis_detected': True}
        
        # ê°€ê²© ë°ì´í„°
        if 'price' in data.columns:
            prices = data['price']
        else:
            numeric_cols = data.select_dtypes(include=[np.number]).columns
            prices = data[numeric_cols[0]]
        
        returns = prices.pct_change()
        
        # ìœ„ê¸° ê¸°ê°„ ë¶„ì„
        crisis_analysis = {
            'crisis_periods_detected': int(crisis_periods.sum()),
            'crisis_proportion': float(crisis_periods.mean()),
            'crisis_statistics': {
                'avg_return': float(returns[crisis_periods].mean()),
                'volatility': float(returns[crisis_periods].std() * np.sqrt(252)),
                'max_drawdown': self._calculate_crisis_max_drawdown(prices, crisis_periods),
                'recovery_analysis': self._analyze_crisis_recovery(prices, regimes)
            }
        }
        
        return crisis_analysis
    
    def _calculate_crisis_max_drawdown(self, prices: pd.Series, crisis_mask: pd.Series) -> float:
        """ìœ„ê¸° ê¸°ê°„ ìµœëŒ€ ë“œë¡œë‹¤ìš´ ê³„ì‚°"""
        crisis_prices = prices[crisis_mask]
        if len(crisis_prices) == 0:
            return 0.0
        
        cumulative = (1 + crisis_prices.pct_change()).cumprod()
        running_max = cumulative.expanding().max()
        drawdowns = (cumulative - running_max) / running_max
        return float(drawdowns.min())
    
    def _analyze_crisis_recovery(self, prices: pd.Series, regimes: pd.Series) -> Dict:
        """ìœ„ê¸° íšŒë³µ ë¶„ì„"""
        recovery_periods = regimes == MarketRegime.RECOVERY.value
        
        if not recovery_periods.any():
            return {'no_recovery_detected': True}
        
        recovery_returns = prices.pct_change()[recovery_periods]
        
        return {
            'recovery_periods': int(recovery_periods.sum()),
            'avg_recovery_return': float(recovery_returns.mean()),
            'recovery_volatility': float(recovery_returns.std() * np.sqrt(252))
        }
    
    def forecast_regime_changes(self, features_df: pd.DataFrame, 
                               forecast_horizon: int = 30) -> Dict:
        """ì‹œì¥ ìƒí™© ë³€í™” ì˜ˆì¸¡ ë° ì¡°ê¸° ê²½ê³ """
        self.logger.info("ğŸ”® ì‹œì¥ ìƒí™© ë³€í™” ì˜ˆì¸¡ ì¤‘...")
        
        # í˜„ì¬ ìƒí™© í™•ì¸
        if len(features_df) < 50:
            return {'insufficient_data': True}
        
        # ìµœê·¼ íŠ¹ì„± ë³€í™” ë¶„ì„
        recent_features = features_df.tail(30)  # ìµœê·¼ 30ì¼
        
        # ë³€í™” ì§€í‘œ ê³„ì‚°
        trend_signals = self._calculate_trend_signals(recent_features)
        volatility_signals = self._calculate_volatility_signals(recent_features)
        momentum_signals = self._calculate_momentum_signals(recent_features)
        
        # ì „í™˜ í™•ë¥  ì˜ˆì¸¡
        transition_probability = self._predict_transition_probability(
            trend_signals, volatility_signals, momentum_signals
        )
        
        # ì¡°ê¸° ê²½ê³  ì‹œìŠ¤í…œ
        early_warnings = self._generate_early_warnings(
            trend_signals, volatility_signals, momentum_signals
        )
        
        return {
            'forecast_horizon_days': forecast_horizon,
            'current_trend_signals': trend_signals,
            'volatility_signals': volatility_signals,
            'momentum_signals': momentum_signals,
            'transition_probability': transition_probability,
            'early_warnings': early_warnings,
            'forecast_confidence': self._calculate_forecast_confidence(recent_features)
        }
    
    def _calculate_trend_signals(self, features_df: pd.DataFrame) -> Dict:
        """ì¶”ì„¸ ì‹ í˜¸ ê³„ì‚°"""
        signals = {}
        
        # ì´ë™í‰ê·  ì‹ í˜¸
        if 'price_vs_sma_20' in features_df.columns:
            sma_signal = features_df['price_vs_sma_20'].iloc[-1]
            signals['sma_signal'] = 'bullish' if sma_signal > 0.05 else 'bearish' if sma_signal < -0.05 else 'neutral'
        
        # ì¶”ì„¸ ê¸°ìš¸ê¸°
        if 'sma_slope_50' in features_df.columns:
            slope = features_df['sma_slope_50'].iloc[-5:].mean()  # ìµœê·¼ 5ì¼ í‰ê· 
            signals['trend_slope'] = 'increasing' if slope > 0.01 else 'decreasing' if slope < -0.01 else 'flat'
        
        return signals
    
    def _calculate_volatility_signals(self, features_df: pd.DataFrame) -> Dict:
        """ë³€ë™ì„± ì‹ í˜¸ ê³„ì‚°"""
        signals = {}
        
        # ë³€ë™ì„± ì¶”ì„¸
        if 'volatility_20' in features_df.columns:
            vol_recent = features_df['volatility_20'].iloc[-5:].mean()
            vol_historical = features_df['volatility_20'].iloc[-30:-5].mean()
            
            if vol_recent > vol_historical * 1.2:
                signals['volatility_trend'] = 'increasing'
            elif vol_recent < vol_historical * 0.8:
                signals['volatility_trend'] = 'decreasing'
            else:
                signals['volatility_trend'] = 'stable'
        
        return signals
    
    def _calculate_momentum_signals(self, features_df: pd.DataFrame) -> Dict:
        """ëª¨ë©˜í…€ ì‹ í˜¸ ê³„ì‚°"""
        signals = {}
        
        # RSI ì‹ í˜¸
        if 'rsi_14' in features_df.columns:
            rsi = features_df['rsi_14'].iloc[-1]
            if rsi > 70:
                signals['rsi_signal'] = 'overbought'
            elif rsi < 30:
                signals['rsi_signal'] = 'oversold'
            else:
                signals['rsi_signal'] = 'neutral'
        
        # MACD ì‹ í˜¸
        if 'macd' in features_df.columns and 'macd_signal' in features_df.columns:
            macd_diff = features_df['macd'].iloc[-1] - features_df['macd_signal'].iloc[-1]
            signals['macd_signal'] = 'bullish' if macd_diff > 0 else 'bearish'
        
        return signals
    
    def _predict_transition_probability(self, trend_signals: Dict, 
                                      volatility_signals: Dict, 
                                      momentum_signals: Dict) -> Dict:
        """ì „í™˜ í™•ë¥  ì˜ˆì¸¡"""
        # ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ í™•ë¥  ê³„ì‚°
        transition_scores = {
            'bull_to_bear': 0,
            'bear_to_bull': 0,
            'volatility_increase': 0,
            'volatility_decrease': 0
        }
        
        # ì¶”ì„¸ ì‹ í˜¸ ê¸°ì—¬
        if trend_signals.get('sma_signal') == 'bearish':
            transition_scores['bull_to_bear'] += 0.3
        elif trend_signals.get('sma_signal') == 'bullish':
            transition_scores['bear_to_bull'] += 0.3
        
        # ë³€ë™ì„± ì‹ í˜¸ ê¸°ì—¬
        if volatility_signals.get('volatility_trend') == 'increasing':
            transition_scores['volatility_increase'] += 0.4
        elif volatility_signals.get('volatility_trend') == 'decreasing':
            transition_scores['volatility_decrease'] += 0.4
        
        # ëª¨ë©˜í…€ ì‹ í˜¸ ê¸°ì—¬
        if momentum_signals.get('rsi_signal') == 'overbought':
            transition_scores['bull_to_bear'] += 0.2
        elif momentum_signals.get('rsi_signal') == 'oversold':
            transition_scores['bear_to_bull'] += 0.2
        
        return transition_scores
    
    def _generate_early_warnings(self, trend_signals: Dict, 
                               volatility_signals: Dict, 
                               momentum_signals: Dict) -> List[str]:
        """ì¡°ê¸° ê²½ê³  ìƒì„±"""
        warnings = []
        
        # ìœ„ê¸° ì¡°ê¸° ê²½ê³ 
        if (trend_signals.get('trend_slope') == 'decreasing' and 
            volatility_signals.get('volatility_trend') == 'increasing'):
            warnings.append("ì ì¬ì  ìœ„ê¸° ìƒí™© ê°ì§€ - í•˜ë½ ì¶”ì„¸ + ë³€ë™ì„± ì¦ê°€")
        
        # ê³¼ë§¤ìˆ˜/ê³¼ë§¤ë„ ê²½ê³ 
        if momentum_signals.get('rsi_signal') == 'overbought':
            warnings.append("ê³¼ë§¤ìˆ˜ ìƒí™© - ì¡°ì • ê°€ëŠ¥ì„±")
        elif momentum_signals.get('rsi_signal') == 'oversold':
            warnings.append("ê³¼ë§¤ë„ ìƒí™© - ë°˜ë“± ê°€ëŠ¥ì„±")
        
        # ì¶”ì„¸ ì „í™˜ ê²½ê³ 
        if (trend_signals.get('sma_signal') in ['bullish', 'bearish'] and 
            momentum_signals.get('macd_signal') != trend_signals.get('sma_signal')):
            warnings.append("ì¶”ì„¸ ì§€í‘œ ê°„ ë¶ˆì¼ì¹˜ - ì „í™˜ ì‹ í˜¸ ê°€ëŠ¥ì„±")
        
        return warnings
    
    def _calculate_forecast_confidence(self, features_df: pd.DataFrame) -> float:
        """ì˜ˆì¸¡ ì‹ ë¢°ë„ ê³„ì‚°"""
        # ë°ì´í„° í’ˆì§ˆ ê¸°ë°˜ ì‹ ë¢°ë„
        data_completeness = 1 - features_df.isna().mean().mean()
        
        # ìµœê·¼ ë³€ë™ì„± ê¸°ë°˜ ì¡°ì • (ë†’ì€ ë³€ë™ì„± = ë‚®ì€ ì˜ˆì¸¡ ì‹ ë¢°ë„)
        if 'volatility_20' in features_df.columns:
            recent_vol = features_df['volatility_20'].iloc[-5:].mean()
            vol_penalty = min(recent_vol / 2, 0.3)  # ìµœëŒ€ 30% íŒ¨ë„í‹°
        else:
            vol_penalty = 0
        
        confidence = max(0.1, min(0.9, data_completeness - vol_penalty))
        return float(confidence)
    
    def conduct_stress_tests(self, data: pd.DataFrame, regimes: pd.Series) -> Dict:
        """ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ì‹œí–‰"""
        self.logger.info("ğŸ§ª ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ì‹œí–‰ ì¤‘...")
        
        stress_results = {}
        
        # ê°€ê²© ë°ì´í„°
        if 'price' in data.columns:
            prices = data['price']
        else:
            numeric_cols = data.select_dtypes(include=[np.number]).columns
            prices = data[numeric_cols[0]]
        
        returns = prices.pct_change().dropna()
        
        for scenario in self.config.stress_scenarios:
            stress_results[scenario] = self._simulate_stress_scenario(scenario, returns, regimes)
        
        return stress_results
    
    def _simulate_stress_scenario(self, scenario: str, returns: pd.Series, 
                                regimes: pd.Series) -> Dict:
        """ìŠ¤íŠ¸ë ˆìŠ¤ ì‹œë‚˜ë¦¬ì˜¤ ì‹œë®¬ë ˆì´ì…˜"""
        scenario_results = {'scenario': scenario}
        
        if scenario == 'black_swan':
            # ê·¹ë‹¨ì  í•˜ë½ ì‹œë‚˜ë¦¬ì˜¤ (-30% í•˜ë½)
            shock_return = -0.30
            scenario_results.update(self._analyze_shock_impact(shock_return, returns))
            
        elif scenario == 'market_crash':
            # ì‹œì¥ í­ë½ ì‹œë‚˜ë¦¬ì˜¤ (-50% í•˜ë½)
            shock_return = -0.50
            scenario_results.update(self._analyze_shock_impact(shock_return, returns))
            
        elif scenario == 'liquidity_crisis':
            # ìœ ë™ì„± ìœ„ê¸° (ë†’ì€ ë³€ë™ì„± ì§€ì†)
            volatility_multiplier = 3.0
            scenario_results.update(self._analyze_volatility_shock(volatility_multiplier, returns))
            
        elif scenario == 'regulatory_shock':
            # ê·œì œ ì‡¼í¬ (ì¤‘ê°„ ì •ë„ í•˜ë½ + ë†’ì€ ë³€ë™ì„±)
            shock_return = -0.20
            volatility_multiplier = 2.0
            scenario_results.update(self._analyze_combined_shock(shock_return, volatility_multiplier, returns))
        
        return scenario_results
    
    def _analyze_shock_impact(self, shock_return: float, returns: pd.Series) -> Dict:
        """ì‡¼í¬ ì˜í–¥ ë¶„ì„"""
        # í˜„ì¬ í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜ ê³„ì‚°
        portfolio_value = (1 + returns).cumprod()
        
        # ì‡¼í¬ ì ìš©
        shocked_value = portfolio_value.iloc[-1] * (1 + shock_return)
        
        # ìµœëŒ€ ë“œë¡œë‹¤ìš´ ê³„ì‚°
        peak_value = portfolio_value.max()
        max_drawdown = (shocked_value - peak_value) / peak_value
        
        return {
            'shock_magnitude': shock_return,
            'portfolio_impact': shocked_value / portfolio_value.iloc[-1] - 1,
            'max_drawdown': max_drawdown,
            'recovery_time_estimate': abs(shock_return) / (returns.mean() * 252) if returns.mean() > 0 else float('inf')
        }
    
    def _analyze_volatility_shock(self, volatility_multiplier: float, returns: pd.Series) -> Dict:
        """ë³€ë™ì„± ì‡¼í¬ ë¶„ì„"""
        current_vol = returns.std() * np.sqrt(252)
        shocked_vol = current_vol * volatility_multiplier
        
        # ë³€ë™ì„± ì¦ê°€ì— ë”°ë¥¸ VaR ë³€í™”
        current_var_95 = np.percentile(returns, 5)
        shocked_var_95 = current_var_95 * volatility_multiplier
        
        return {
            'volatility_multiplier': volatility_multiplier,
            'current_volatility': current_vol,
            'shocked_volatility': shocked_vol,
            'var_95_change': shocked_var_95 / current_var_95 - 1 if current_var_95 != 0 else 0
        }
    
    def _analyze_combined_shock(self, shock_return: float, volatility_multiplier: float, 
                              returns: pd.Series) -> Dict:
        """ë³µí•© ì‡¼í¬ ë¶„ì„"""
        price_impact = self._analyze_shock_impact(shock_return, returns)
        vol_impact = self._analyze_volatility_shock(volatility_multiplier, returns)
        
        return {
            'combined_shock': True,
            'price_impact': price_impact,
            'volatility_impact': vol_impact,
            'total_risk_increase': price_impact['max_drawdown'] + vol_impact['var_95_change']
        }
    
    def create_regime_visualization(self, analysis_results: Dict) -> str:
        """ì‹œì¥ ìƒí™© ë¶„ì„ ì‹œê°í™”"""
        self.logger.info("ğŸ“Š ì‹œì¥ ìƒí™© ë¶„ì„ ì‹œê°í™” ìƒì„± ì¤‘...")
        
        fig = make_subplots(
            rows=4, cols=2,
            subplot_titles=(
                "ì‹œì¥ ìƒí™© ì‹œê³„ì—´", "ìƒí™©ë³„ ì„±ëŠ¥ ë¹„êµ",
                "ì „í™˜ì  ë¶„ì„", "ìƒí™©ë³„ ë³€ë™ì„±",
                "ìœ„ê¸° ìƒí™© ë¶„ì„", "ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ê²°ê³¼",
                "ì˜ˆì¸¡ ì‹ í˜¸", "ë¦¬ìŠ¤í¬ ì§€í‘œ"
            ),
            specs=[[{"type": "scatter"}, {"type": "bar"}],
                   [{"type": "scatter"}, {"type": "box"}],
                   [{"type": "bar"}, {"type": "radar"}],
                   [{"type": "indicator"}, {"type": "table"}]]
        )
        
        # ë°ì´í„° ì¶”ì¶œ
        ensemble_regimes = analysis_results['regime_classifications']['ensemble']
        performance_data = analysis_results.get('performance_analysis', {})
        
        if ensemble_regimes is not None and len(ensemble_regimes) > 0:
            # 1. ì‹œì¥ ìƒí™© ì‹œê³„ì—´
            regime_colors = {
                MarketRegime.BULL_LOW_VOL.value: 'green',
                MarketRegime.BULL_HIGH_VOL.value: 'lightgreen',
                MarketRegime.BEAR_LOW_VOL.value: 'red',
                MarketRegime.BEAR_HIGH_VOL.value: 'lightcoral',
                MarketRegime.SIDEWAYS_LOW_VOL.value: 'blue',
                MarketRegime.SIDEWAYS_HIGH_VOL.value: 'lightblue',
                MarketRegime.CRISIS.value: 'black',
                MarketRegime.RECOVERY.value: 'orange'
            }
            
            # ìƒí™©ë³„ ìƒ‰ìƒ ë§¤í•‘
            colors = [regime_colors.get(regime, 'gray') for regime in ensemble_regimes]
            
            fig.add_trace(
                go.Scatter(
                    x=list(range(len(ensemble_regimes))),
                    y=[1] * len(ensemble_regimes),
                    mode='markers',
                    marker=dict(color=colors, size=8),
                    name='ì‹œì¥ ìƒí™©'
                ),
                row=1, col=1
            )
        
        # 2. ìƒí™©ë³„ ì„±ëŠ¥ ë¹„êµ
        if performance_data:
            regimes = list(performance_data.keys())
            returns = [performance_data[r]['annual_metrics']['annual_return'] for r in regimes]
            
            fig.add_trace(
                go.Bar(x=regimes, y=returns, name='ì—°ê°„ ìˆ˜ìµë¥ '),
                row=1, col=2
            )
        
        # ì¶”ê°€ ì°¨íŠ¸ë“¤...
        
        fig.update_layout(
            title="ğŸ¯ ì‹œì¥ ìƒí™© ë¶„ì„ ëŒ€ì‹œë³´ë“œ",
            height=1600,
            showlegend=True,
            template='plotly_dark'
        )
        
        # ì €ì¥
        dashboard_path = os.path.join(self.data_path, 'market_regime_analysis_dashboard.html')
        fig.write_html(dashboard_path, include_plotlyjs=True)
        
        return dashboard_path
    
    def save_analysis_results(self, results: Dict):
        """ë¶„ì„ ê²°ê³¼ ì €ì¥"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # JSON ì €ì¥
        json_path = os.path.join(self.data_path, f'market_regime_analysis_{timestamp}.json')
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)
        
        self.logger.info(f"ë¶„ì„ ê²°ê³¼ ì €ì¥: {json_path}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ¯ ì‹œì¥ ìƒí™©ë³„ ë¶„ì„ ëª¨ë“ˆ")
    print("=" * 50)
    
    # ì„¤ì •
    config = RegimeConfig(
        n_clusters=6,
        n_hidden_states=4,
        crisis_threshold=-0.20,
        recovery_threshold=0.10
    )
    
    # ë¶„ì„ê¸° ì´ˆê¸°í™”
    analyzer = MarketRegimeAnalyzer(config)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    np.random.seed(42)
    n_periods = 1000
    
    # ì‹œë®¬ë ˆì´ì…˜ëœ ê°€ê²© ë°ì´í„°
    returns = np.random.normal(0.001, 0.02, n_periods)
    
    # ìƒí™©ë³„ ë³€ë™ì„± ë³€í™” ì‹œë®¬ë ˆì´ì…˜
    regime_changes = [200, 400, 600, 800]  # ìƒí™© ë³€í™” ì§€ì 
    for i, change_point in enumerate(regime_changes):
        if i % 2 == 0:  # ë†’ì€ ë³€ë™ì„± êµ¬ê°„
            returns[change_point:change_point+100] *= 2
        else:  # ë‚®ì€ ë³€ë™ì„± êµ¬ê°„
            returns[change_point:change_point+100] *= 0.5
    
    prices = (1 + pd.Series(returns)).cumprod() * 10000
    
    # ë°ì´í„°í”„ë ˆì„ ìƒì„±
    data = pd.DataFrame({
        'price': prices,
        'volume': np.random.lognormal(10, 1, n_periods)
    })
    data.index = pd.date_range('2022-01-01', periods=n_periods, freq='D')
    
    # ì¢…í•© ë¶„ì„ ì‹¤í–‰
    results = analyzer.comprehensive_regime_analysis(data)
    
    print(f"\nğŸ“Š ë¶„ì„ ì™„ë£Œ!")
    print(f"ì´ ê¸°ê°„: {len(data)} ì¼")
    
    if 'regime_classifications' in results and results['regime_classifications']['ensemble'] is not None:
        ensemble = results['regime_classifications']['ensemble']
        unique_regimes = ensemble.value_counts()
        print(f"ì‹ë³„ëœ ì‹œì¥ ìƒí™©: {len(unique_regimes)}ê°œ")
        
        for regime, count in unique_regimes.head().items():
            print(f"  â€¢ {regime}: {count}ì¼ ({count/len(ensemble)*100:.1f}%)")
    
    # ëŒ€ì‹œë³´ë“œ ê²½ë¡œ ì¶œë ¥
    if 'dashboard_path' in results:
        print(f"ëŒ€ì‹œë³´ë“œ: {results['dashboard_path']}")

if __name__ == "__main__":
    main()