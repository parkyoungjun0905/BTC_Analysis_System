#!/usr/bin/env python3
"""
ì‹¤ì‹œê°„ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì‹œìŠ¤í…œ
ë‹¤ì¤‘ ì†ŒìŠ¤ ë°ì´í„° í†µí•©, ì‹¤ì‹œê°„ ì²˜ë¦¬, ì§€ì—°ì‹œê°„ ìµœì í™”ë¡œ 90% ì˜ˆì¸¡ ì •í™•ë„ ê¸°ì—¬
"""

import asyncio
import aiohttp
import websockets
import json
import sqlite3
import redis.asyncio as redis
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any, Callable
import logging
import numpy as np
import pandas as pd
from dataclasses import dataclass, asdict
from collections import deque, defaultdict
import time
import threading
from concurrent.futures import ThreadPoolExecutor
import queue
import pickle

@dataclass
class DataPoint:
    source: str
    symbol: str
    data_type: str  # 'price', 'volume', 'orderbook', 'trade', 'sentiment', etc.
    value: Any
    timestamp: datetime
    metadata: Dict = None

@dataclass
class PipelineMetrics:
    total_messages: int
    messages_per_second: int
    average_latency_ms: float
    error_rate: float
    data_quality_score: float
    active_connections: int
    buffer_utilization: float

class RealTimeDataPipeline:
    def __init__(self, redis_url: str = "redis://localhost:6379"):
        self.logger = logging.getLogger(__name__)
        self.db_path = "pipeline_data.db"
        self.redis_url = redis_url
        
        # ë°ì´í„° íŒŒì´í”„ë¼ì¸ êµ¬ì„±ìš”ì†Œ
        self.data_sources = {}
        self.data_processors = {}
        self.data_subscribers = defaultdict(list)
        
        # ì‹¤ì‹œê°„ ë²„í¼ ë° í
        self.data_buffers = defaultdict(lambda: deque(maxlen=10000))
        self.processing_queue = queue.Queue(maxsize=50000)
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        self.metrics = {
            'total_messages': 0,
            'error_count': 0,
            'latency_samples': deque(maxlen=1000),
            'start_time': time.time()
        }
        
        # ì—°ê²° ê´€ë¦¬
        self.websocket_connections = {}
        self.http_sessions = {}
        
        # ì²˜ë¦¬ ìŠ¤ë ˆë“œí’€
        self.thread_pool = ThreadPoolExecutor(max_workers=10)
        
        # ë°ì´í„° í’ˆì§ˆ ëª¨ë‹ˆí„°ë§
        self.data_quality_monitors = {}
        
        self._init_database()
        self._init_redis()
    
    def _init_database(self):
        """íŒŒì´í”„ë¼ì¸ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # ì‹¤ì‹œê°„ ë°ì´í„° í…Œì´ë¸”
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS realtime_data (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    source TEXT NOT NULL,
                    symbol TEXT NOT NULL,
                    data_type TEXT NOT NULL,
                    value TEXT NOT NULL,
                    timestamp DATETIME NOT NULL,
                    metadata TEXT,
                    processed_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    latency_ms REAL
                )
            ''')
            
            # íŒŒì´í”„ë¼ì¸ ë©”íŠ¸ë¦­ í…Œì´ë¸”
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS pipeline_metrics (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp DATETIME NOT NULL,
                    total_messages INTEGER,
                    messages_per_second REAL,
                    average_latency_ms REAL,
                    error_rate REAL,
                    data_quality_score REAL,
                    active_connections INTEGER,
                    buffer_utilization REAL,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # ë°ì´í„° í’ˆì§ˆ ë¡œê·¸
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS data_quality_log (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    source TEXT NOT NULL,
                    data_type TEXT NOT NULL,
                    quality_score REAL NOT NULL,
                    issues TEXT,
                    timestamp DATETIME NOT NULL,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # ì¸ë±ìŠ¤ ìƒì„±
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_realtime_timestamp ON realtime_data(timestamp)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_realtime_source ON realtime_data(source)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_metrics_timestamp ON pipeline_metrics(timestamp)')
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            self.logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    async def _init_redis(self):
        """Redis ì—°ê²° ì´ˆê¸°í™”"""
        try:
            self.redis_client = await redis.from_url(self.redis_url, decode_responses=True)
            await self.redis_client.ping()
            self.logger.info("Redis ì—°ê²° ì„±ê³µ")
        except Exception as e:
            self.logger.warning(f"Redis ì—°ê²° ì‹¤íŒ¨: {e}. ë©”ëª¨ë¦¬ ë²„í¼ ì‚¬ìš©")
            self.redis_client = None
    
    async def add_data_source(self, source_name: str, source_config: Dict):
        """ë°ì´í„° ì†ŒìŠ¤ ì¶”ê°€"""
        try:
            source_type = source_config.get('type')
            
            if source_type == 'websocket':
                await self._setup_websocket_source(source_name, source_config)
            elif source_type == 'http_polling':
                await self._setup_http_polling_source(source_name, source_config)
            elif source_type == 'message_queue':
                await self._setup_mq_source(source_name, source_config)
            
            self.data_sources[source_name] = source_config
            self.logger.info(f"ë°ì´í„° ì†ŒìŠ¤ '{source_name}' ì¶”ê°€ë¨")
            
        except Exception as e:
            self.logger.error(f"ë°ì´í„° ì†ŒìŠ¤ ì¶”ê°€ ì‹¤íŒ¨ {source_name}: {e}")
    
    async def _setup_websocket_source(self, source_name: str, config: Dict):
        """WebSocket ë°ì´í„° ì†ŒìŠ¤ ì„¤ì •"""
        try:
            uri = config['uri']
            subscribe_message = config.get('subscribe_message')
            
            async def websocket_handler():
                while True:
                    try:
                        async with websockets.connect(uri) as websocket:
                            self.websocket_connections[source_name] = websocket
                            
                            if subscribe_message:
                                await websocket.send(json.dumps(subscribe_message))
                            
                            async for message in websocket:
                                await self._process_websocket_message(source_name, message, config)
                                
                    except Exception as e:
                        self.logger.error(f"WebSocket ì—°ê²° ì—ëŸ¬ {source_name}: {e}")
                        await asyncio.sleep(5)  # ì¬ì—°ê²° ëŒ€ê¸°
            
            # ë°±ê·¸ë¼ìš´ë“œì—ì„œ WebSocket í•¸ë“¤ëŸ¬ ì‹¤í–‰
            asyncio.create_task(websocket_handler())
            
        except Exception as e:
            self.logger.error(f"WebSocket ì†ŒìŠ¤ ì„¤ì • ì‹¤íŒ¨ {source_name}: {e}")
    
    async def _setup_http_polling_source(self, source_name: str, config: Dict):
        """HTTP í´ë§ ë°ì´í„° ì†ŒìŠ¤ ì„¤ì •"""
        try:
            url = config['url']
            interval = config.get('interval', 1.0)  # ê¸°ë³¸ 1ì´ˆ
            headers = config.get('headers', {})
            
            async def polling_handler():
                session = aiohttp.ClientSession(headers=headers)
                self.http_sessions[source_name] = session
                
                while True:
                    try:
                        start_time = time.time()
                        async with session.get(url) as response:
                            data = await response.text()
                            
                            if response.status == 200:
                                await self._process_http_response(source_name, data, config)
                            else:
                                self.logger.warning(f"HTTP {response.status} from {source_name}")
                        
                        # ì •í™•í•œ ê°„ê²© ìœ ì§€
                        elapsed = time.time() - start_time
                        sleep_time = max(0, interval - elapsed)
                        await asyncio.sleep(sleep_time)
                        
                    except Exception as e:
                        self.logger.error(f"HTTP í´ë§ ì—ëŸ¬ {source_name}: {e}")
                        await asyncio.sleep(interval)
            
            # ë°±ê·¸ë¼ìš´ë“œì—ì„œ í´ë§ í•¸ë“¤ëŸ¬ ì‹¤í–‰
            asyncio.create_task(polling_handler())
            
        except Exception as e:
            self.logger.error(f"HTTP í´ë§ ì†ŒìŠ¤ ì„¤ì • ì‹¤íŒ¨ {source_name}: {e}")
    
    async def _process_websocket_message(self, source_name: str, message: str, config: Dict):
        """WebSocket ë©”ì‹œì§€ ì²˜ë¦¬"""
        try:
            receive_time = time.time()
            data = json.loads(message)
            
            # ë©”ì‹œì§€ íŒŒì‹± (ì†ŒìŠ¤ë³„ ë¡œì§)
            parsed_data = await self._parse_message(source_name, data, config)
            
            for data_point in parsed_data:
                # ì§€ì—°ì‹œê°„ ê³„ì‚°
                latency_ms = (receive_time - data_point.timestamp.timestamp()) * 1000
                
                # ë²„í¼ì— ì¶”ê°€
                await self._add_to_buffer(data_point, latency_ms)
                
                # ì‹¤ì‹œê°„ ì²˜ë¦¬ íì— ì¶”ê°€
                if not self.processing_queue.full():
                    self.processing_queue.put((data_point, latency_ms))
                
        except Exception as e:
            self.logger.error(f"WebSocket ë©”ì‹œì§€ ì²˜ë¦¬ ì‹¤íŒ¨ {source_name}: {e}")
            self.metrics['error_count'] += 1
    
    async def _process_http_response(self, source_name: str, response_data: str, config: Dict):
        """HTTP ì‘ë‹µ ì²˜ë¦¬"""
        try:
            receive_time = time.time()
            data = json.loads(response_data)
            
            # ë©”ì‹œì§€ íŒŒì‹±
            parsed_data = await self._parse_message(source_name, data, config)
            
            for data_point in parsed_data:
                latency_ms = (receive_time - data_point.timestamp.timestamp()) * 1000
                await self._add_to_buffer(data_point, latency_ms)
                
                if not self.processing_queue.full():
                    self.processing_queue.put((data_point, latency_ms))
                
        except Exception as e:
            self.logger.error(f"HTTP ì‘ë‹µ ì²˜ë¦¬ ì‹¤íŒ¨ {source_name}: {e}")
            self.metrics['error_count'] += 1
    
    async def _parse_message(self, source_name: str, data: Dict, config: Dict) -> List[DataPoint]:
        """ë©”ì‹œì§€ íŒŒì‹± (ì†ŒìŠ¤ë³„ ë¡œì§)"""
        try:
            parsed_data = []
            parser_type = config.get('parser', 'generic')
            
            if parser_type == 'binance_ticker':
                # Binance í‹°ì»¤ ë°ì´í„° íŒŒì‹±
                if 'c' in data:  # í˜„ì¬ê°€
                    data_point = DataPoint(
                        source=source_name,
                        symbol=data.get('s', 'UNKNOWN'),
                        data_type='price',
                        value=float(data['c']),
                        timestamp=datetime.fromtimestamp(data.get('E', time.time()) / 1000),
                        metadata={'volume': data.get('v'), 'high': data.get('h'), 'low': data.get('l')}
                    )
                    parsed_data.append(data_point)
                    
            elif parser_type == 'coinbase_ticker':
                # Coinbase í‹°ì»¤ ë°ì´í„° íŒŒì‹±
                if data.get('type') == 'ticker':
                    data_point = DataPoint(
                        source=source_name,
                        symbol=data.get('product_id', 'UNKNOWN'),
                        data_type='price',
                        value=float(data.get('price', 0)),
                        timestamp=datetime.fromisoformat(data.get('time', '').replace('Z', '+00:00')),
                        metadata={'volume': data.get('volume_24h'), 'best_bid': data.get('best_bid')}
                    )
                    parsed_data.append(data_point)
                    
            elif parser_type == 'generic':
                # ì œë„¤ë¦­ íŒŒì‹±
                data_point = DataPoint(
                    source=source_name,
                    symbol='GENERIC',
                    data_type='raw',
                    value=data,
                    timestamp=datetime.utcnow(),
                    metadata=None
                )
                parsed_data.append(data_point)
            
            return parsed_data
            
        except Exception as e:
            self.logger.error(f"ë©”ì‹œì§€ íŒŒì‹± ì‹¤íŒ¨ {source_name}: {e}")
            return []
    
    async def _add_to_buffer(self, data_point: DataPoint, latency_ms: float):
        """ë°ì´í„°ë¥¼ ë²„í¼ì— ì¶”ê°€"""
        try:
            # ë©”ëª¨ë¦¬ ë²„í¼
            buffer_key = f"{data_point.source}_{data_point.data_type}"
            self.data_buffers[buffer_key].append((data_point, latency_ms))
            
            # Redis ìºì‹œ (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
            if self.redis_client:
                cache_key = f"pipeline:{buffer_key}:latest"
                cache_data = {
                    'value': data_point.value,
                    'timestamp': data_point.timestamp.isoformat(),
                    'latency_ms': latency_ms
                }
                await self.redis_client.setex(cache_key, 300, json.dumps(cache_data))  # 5ë¶„ TTL
            
            # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            self.metrics['total_messages'] += 1
            self.metrics['latency_samples'].append(latency_ms)
            
        except Exception as e:
            self.logger.error(f"ë²„í¼ ì¶”ê°€ ì‹¤íŒ¨: {e}")
    
    def add_data_processor(self, processor_name: str, processor_func: Callable):
        """ë°ì´í„° ì²˜ë¦¬ê¸° ì¶”ê°€"""
        self.data_processors[processor_name] = processor_func
        self.logger.info(f"ë°ì´í„° ì²˜ë¦¬ê¸° '{processor_name}' ì¶”ê°€ë¨")
    
    def subscribe_to_data(self, data_type: str, callback: Callable):
        """ë°ì´í„° êµ¬ë…ì ì¶”ê°€"""
        self.data_subscribers[data_type].append(callback)
        self.logger.info(f"'{data_type}' ë°ì´í„° êµ¬ë…ì ì¶”ê°€ë¨")
    
    async def start_processing_pipeline(self):
        """ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹œì‘"""
        try:
            # ì‹¤ì‹œê°„ ì²˜ë¦¬ ì›Œì»¤ ì‹œì‘
            for i in range(5):  # 5ê°œì˜ ì²˜ë¦¬ ì›Œì»¤
                asyncio.create_task(self._processing_worker(f"worker_{i}"))
            
            # ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì›Œì»¤
            asyncio.create_task(self._metrics_collector())
            
            # ë°ì´í„° í’ˆì§ˆ ëª¨ë‹ˆí„°
            asyncio.create_task(self._data_quality_monitor())
            
            # ë°ì´í„°ë² ì´ìŠ¤ í”ŒëŸ¬ì‹œ ì›Œì»¤
            asyncio.create_task(self._database_flusher())
            
            self.logger.info("ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹œì‘ë¨")
            
        except Exception as e:
            self.logger.error(f"íŒŒì´í”„ë¼ì¸ ì‹œì‘ ì‹¤íŒ¨: {e}")
    
    async def _processing_worker(self, worker_id: str):
        """ë°ì´í„° ì²˜ë¦¬ ì›Œì»¤"""
        while True:
            try:
                # íì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ë¹„ì°¨ë‹¨)
                try:
                    data_point, latency_ms = self.processing_queue.get_nowait()
                except queue.Empty:
                    await asyncio.sleep(0.001)  # 1ms ëŒ€ê¸°
                    continue
                
                # ë°ì´í„° ì²˜ë¦¬ê¸° ì‹¤í–‰
                for processor_name, processor_func in self.data_processors.items():
                    try:
                        processed_data = await self._run_processor(processor_func, data_point)
                        if processed_data:
                            # êµ¬ë…ìë“¤ì—ê²Œ ì•Œë¦¼
                            await self._notify_subscribers(data_point.data_type, processed_data)
                    except Exception as e:
                        self.logger.warning(f"ì²˜ë¦¬ê¸° {processor_name} ì‹¤íŒ¨: {e}")
                
                self.processing_queue.task_done()
                
            except Exception as e:
                self.logger.error(f"ì²˜ë¦¬ ì›Œì»¤ {worker_id} ì—ëŸ¬: {e}")
                await asyncio.sleep(0.1)
    
    async def _run_processor(self, processor_func: Callable, data_point: DataPoint) -> Any:
        """ë¹„ë™ê¸° ì²˜ë¦¬ê¸° ì‹¤í–‰"""
        try:
            if asyncio.iscoroutinefunction(processor_func):
                return await processor_func(data_point)
            else:
                # CPU ì§‘ì•½ì  ì‘ì—…ì€ ìŠ¤ë ˆë“œí’€ì—ì„œ ì‹¤í–‰
                loop = asyncio.get_event_loop()
                return await loop.run_in_executor(self.thread_pool, processor_func, data_point)
        except Exception as e:
            self.logger.error(f"ì²˜ë¦¬ê¸° ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return None
    
    async def _notify_subscribers(self, data_type: str, processed_data: Any):
        """êµ¬ë…ìë“¤ì—ê²Œ ì•Œë¦¼"""
        try:
            subscribers = self.data_subscribers.get(data_type, [])
            
            for callback in subscribers:
                try:
                    if asyncio.iscoroutinefunction(callback):
                        await callback(processed_data)
                    else:
                        callback(processed_data)
                except Exception as e:
                    self.logger.warning(f"êµ¬ë…ì ì•Œë¦¼ ì‹¤íŒ¨: {e}")
                    
        except Exception as e:
            self.logger.error(f"êµ¬ë…ì ì•Œë¦¼ ì—ëŸ¬: {e}")
    
    async def _metrics_collector(self):
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        while True:
            try:
                await asyncio.sleep(10)  # 10ì´ˆë§ˆë‹¤ ìˆ˜ì§‘
                
                current_time = time.time()
                elapsed_time = current_time - self.metrics['start_time']
                
                # ì´ˆë‹¹ ë©”ì‹œì§€ ìˆ˜
                messages_per_second = self.metrics['total_messages'] / max(elapsed_time, 1)
                
                # í‰ê·  ì§€ì—°ì‹œê°„
                latency_samples = list(self.metrics['latency_samples'])
                avg_latency = np.mean(latency_samples) if latency_samples else 0
                
                # ì—ëŸ¬ìœ¨
                error_rate = self.metrics['error_count'] / max(self.metrics['total_messages'], 1)
                
                # í™œì„± ì—°ê²° ìˆ˜
                active_connections = len(self.websocket_connections) + len(self.http_sessions)
                
                # ë²„í¼ ì‚¬ìš©ë¥ 
                total_buffer_items = sum(len(buffer) for buffer in self.data_buffers.values())
                max_buffer_size = len(self.data_buffers) * 10000  # maxlen per buffer
                buffer_utilization = total_buffer_items / max(max_buffer_size, 1)
                
                # ë°ì´í„° í’ˆì§ˆ ì ìˆ˜ (ë‹¨ìˆœí™”)
                data_quality_score = max(0, 1.0 - error_rate - (avg_latency / 1000))
                
                # ë©”íŠ¸ë¦­ ì €ì¥
                metrics = PipelineMetrics(
                    total_messages=self.metrics['total_messages'],
                    messages_per_second=messages_per_second,
                    average_latency_ms=avg_latency,
                    error_rate=error_rate,
                    data_quality_score=data_quality_score,
                    active_connections=active_connections,
                    buffer_utilization=buffer_utilization
                )
                
                await self._save_metrics(metrics)
                
            except Exception as e:
                self.logger.error(f"ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
    
    async def _data_quality_monitor(self):
        """ë°ì´í„° í’ˆì§ˆ ëª¨ë‹ˆí„°ë§"""
        while True:
            try:
                await asyncio.sleep(30)  # 30ì´ˆë§ˆë‹¤ í’ˆì§ˆ ê²€ì‚¬
                
                for source_name in self.data_sources.keys():
                    quality_score, issues = await self._assess_data_quality(source_name)
                    
                    if quality_score < 0.8:  # í’ˆì§ˆ ì„ê³„ê°’
                        self.logger.warning(f"ë°ì´í„° í’ˆì§ˆ ì €í•˜ {source_name}: {quality_score:.3f} - {issues}")
                    
                    await self._save_quality_log(source_name, quality_score, issues)
                
            except Exception as e:
                self.logger.error(f"ë°ì´í„° í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ì‹¤íŒ¨: {e}")
    
    async def _assess_data_quality(self, source_name: str) -> Tuple[float, List[str]]:
        """ë°ì´í„° í’ˆì§ˆ í‰ê°€"""
        try:
            issues = []
            quality_components = []
            
            # ìµœê·¼ ë°ì´í„° í™•ì¸
            recent_data = []
            for buffer_key, buffer in self.data_buffers.items():
                if source_name in buffer_key:
                    recent_data.extend(list(buffer)[-100:])  # ìµœê·¼ 100ê°œ
            
            if not recent_data:
                return 0.0, ['No recent data']
            
            # 1. ë°ì´í„° ì‹ ì„ ë„ (íƒ€ì„ìŠ¤íƒ¬í”„ ì§€ì—°)
            now = datetime.utcnow()
            timestamps = [dp.timestamp for dp, _ in recent_data]
            if timestamps:
                latest_data_age = (now - max(timestamps)).total_seconds()
                freshness_score = max(0, 1 - latest_data_age / 60)  # 1ë¶„ ê¸°ì¤€
                quality_components.append(freshness_score)
                
                if freshness_score < 0.5:
                    issues.append(f'Stale data (age: {latest_data_age:.1f}s)')
            
            # 2. ë°ì´í„° ì™„ì„±ë„ (ëˆ„ë½ê°’)
            null_count = sum(1 for dp, _ in recent_data if dp.value is None)
            completeness_score = 1 - (null_count / len(recent_data))
            quality_components.append(completeness_score)
            
            if completeness_score < 0.9:
                issues.append(f'Missing values: {null_count}/{len(recent_data)}')
            
            # 3. ë°ì´í„° ì¼ê´€ì„± (ê°’ì˜ í•©ë¦¬ì„±)
            values = [dp.value for dp, _ in recent_data if dp.value is not None and isinstance(dp.value, (int, float))]
            if values:
                value_std = np.std(values)
                value_mean = np.mean(values)
                cv = value_std / abs(value_mean) if value_mean != 0 else 0
                consistency_score = max(0, 1 - cv / 2)  # CV ê¸°ì¤€ ì¡°ì •
                quality_components.append(consistency_score)
                
                if consistency_score < 0.7:
                    issues.append(f'High variability (CV: {cv:.3f})')
            
            # 4. ì§€ì—°ì‹œê°„ í’ˆì§ˆ
            latencies = [latency for _, latency in recent_data]
            if latencies:
                avg_latency = np.mean(latencies)
                latency_score = max(0, 1 - avg_latency / 1000)  # 1ì´ˆ ê¸°ì¤€
                quality_components.append(latency_score)
                
                if latency_score < 0.8:
                    issues.append(f'High latency: {avg_latency:.1f}ms')
            
            # ì „ì²´ í’ˆì§ˆ ì ìˆ˜
            overall_quality = np.mean(quality_components) if quality_components else 0.0
            
            return overall_quality, issues
            
        except Exception as e:
            self.logger.error(f"ë°ì´í„° í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨ {source_name}: {e}")
            return 0.0, [f'Quality assessment error: {str(e)}']
    
    async def _database_flusher(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì¼ê´„ ì €ì¥"""
        batch_data = []
        batch_size = 1000
        
        while True:
            try:
                # ì²˜ë¦¬ëœ ë°ì´í„° ìˆ˜ì§‘
                for _ in range(min(batch_size, self.processing_queue.qsize())):
                    try:
                        data_point, latency_ms = self.processing_queue.get_nowait()
                        batch_data.append((data_point, latency_ms))
                    except queue.Empty:
                        break
                
                # ë°°ì¹˜ ì €ì¥
                if batch_data:
                    await self._save_batch_data(batch_data)
                    batch_data.clear()
                
                await asyncio.sleep(5)  # 5ì´ˆë§ˆë‹¤ í”ŒëŸ¬ì‹œ
                
            except Exception as e:
                self.logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ í”ŒëŸ¬ì‹œ ì‹¤íŒ¨: {e}")
                await asyncio.sleep(5)
    
    async def _save_batch_data(self, batch_data: List[Tuple[DataPoint, float]]):
        """ë°°ì¹˜ ë°ì´í„° ì €ì¥"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            for data_point, latency_ms in batch_data:
                cursor.execute('''
                    INSERT INTO realtime_data 
                    (source, symbol, data_type, value, timestamp, metadata, latency_ms)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                ''', (
                    data_point.source,
                    data_point.symbol,
                    data_point.data_type,
                    json.dumps(data_point.value) if not isinstance(data_point.value, str) else data_point.value,
                    data_point.timestamp.isoformat(),
                    json.dumps(data_point.metadata) if data_point.metadata else None,
                    latency_ms
                ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            self.logger.error(f"ë°°ì¹˜ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
    
    async def _save_metrics(self, metrics: PipelineMetrics):
        """ë©”íŠ¸ë¦­ ì €ì¥"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT INTO pipeline_metrics 
                (timestamp, total_messages, messages_per_second, average_latency_ms,
                 error_rate, data_quality_score, active_connections, buffer_utilization)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                datetime.utcnow().isoformat(),
                metrics.total_messages,
                metrics.messages_per_second,
                metrics.average_latency_ms,
                metrics.error_rate,
                metrics.data_quality_score,
                metrics.active_connections,
                metrics.buffer_utilization
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            self.logger.error(f"ë©”íŠ¸ë¦­ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    async def _save_quality_log(self, source_name: str, quality_score: float, issues: List[str]):
        """ë°ì´í„° í’ˆì§ˆ ë¡œê·¸ ì €ì¥"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT INTO data_quality_log 
                (source, data_type, quality_score, issues, timestamp)
                VALUES (?, ?, ?, ?, ?)
            ''', (
                source_name,
                'realtime_feed',
                quality_score,
                json.dumps(issues),
                datetime.utcnow().isoformat()
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            self.logger.error(f"í’ˆì§ˆ ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    async def get_latest_data(self, source: str, data_type: str, limit: int = 100) -> List[Dict]:
        """ìµœì‹  ë°ì´í„° ì¡°íšŒ"""
        try:
            buffer_key = f"{source}_{data_type}"
            buffer_data = list(self.data_buffers.get(buffer_key, []))
            
            # ìµœê·¼ ë°ì´í„° ë°˜í™˜
            latest_data = []
            for data_point, latency_ms in buffer_data[-limit:]:
                latest_data.append({
                    'source': data_point.source,
                    'symbol': data_point.symbol,
                    'data_type': data_point.data_type,
                    'value': data_point.value,
                    'timestamp': data_point.timestamp.isoformat(),
                    'latency_ms': latency_ms,
                    'metadata': data_point.metadata
                })
            
            return latest_data
            
        except Exception as e:
            self.logger.error(f"ìµœì‹  ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    async def get_pipeline_status(self) -> Dict:
        """íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì¡°íšŒ"""
        try:
            current_time = time.time()
            elapsed_time = current_time - self.metrics['start_time']
            
            # ìµœê·¼ ë©”íŠ¸ë¦­ ê³„ì‚°
            messages_per_second = self.metrics['total_messages'] / max(elapsed_time, 1)
            latency_samples = list(self.metrics['latency_samples'])
            avg_latency = np.mean(latency_samples) if latency_samples else 0
            error_rate = self.metrics['error_count'] / max(self.metrics['total_messages'], 1)
            
            # í™œì„± ì—°ê²°
            active_connections = len(self.websocket_connections) + len(self.http_sessions)
            
            # ë²„í¼ ìƒíƒœ
            buffer_status = {}
            for buffer_key, buffer in self.data_buffers.items():
                buffer_status[buffer_key] = {
                    'size': len(buffer),
                    'max_size': buffer.maxlen,
                    'utilization': len(buffer) / buffer.maxlen
                }
            
            status = {
                'uptime_seconds': elapsed_time,
                'total_messages': self.metrics['total_messages'],
                'messages_per_second': messages_per_second,
                'average_latency_ms': avg_latency,
                'error_rate': error_rate * 100,
                'active_connections': active_connections,
                'active_sources': list(self.data_sources.keys()),
                'buffer_status': buffer_status,
                'processing_queue_size': self.processing_queue.qsize(),
                'data_processors': list(self.data_processors.keys())
            }
            
            return status
            
        except Exception as e:
            self.logger.error(f"íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}
    
    async def shutdown(self):
        """íŒŒì´í”„ë¼ì¸ ì¢…ë£Œ"""
        try:
            self.logger.info("íŒŒì´í”„ë¼ì¸ ì¢…ë£Œ ì¤‘...")
            
            # WebSocket ì—°ê²° ì¢…ë£Œ
            for ws in self.websocket_connections.values():
                await ws.close()
            
            # HTTP ì„¸ì…˜ ì¢…ë£Œ
            for session in self.http_sessions.values():
                await session.close()
            
            # Redis ì—°ê²° ì¢…ë£Œ
            if self.redis_client:
                await self.redis_client.close()
            
            # ìŠ¤ë ˆë“œí’€ ì¢…ë£Œ
            self.thread_pool.shutdown(wait=True)
            
            self.logger.info("íŒŒì´í”„ë¼ì¸ ì •ìƒ ì¢…ë£Œë¨")
            
        except Exception as e:
            self.logger.error(f"íŒŒì´í”„ë¼ì¸ ì¢…ë£Œ ì‹¤íŒ¨: {e}")

# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ë“¤
async def test_realtime_pipeline():
    """ì‹¤ì‹œê°„ ë°ì´í„° íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª ì‹¤ì‹œê°„ ë°ì´í„° íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸...")
    
    pipeline = RealTimeDataPipeline()
    
    # ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ì†ŒìŠ¤ ì¶”ê°€
    await pipeline.add_data_source('binance_btc', {
        'type': 'websocket',
        'uri': 'wss://stream.binance.com:9443/ws/btcusdt@ticker',
        'parser': 'binance_ticker'
    })
    
    await pipeline.add_data_source('coinbase_btc', {
        'type': 'http_polling',
        'url': 'https://api.coinbase.com/v2/exchange-rates?currency=BTC',
        'interval': 2.0,
        'parser': 'coinbase_ticker'
    })
    
    # ë°ì´í„° ì²˜ë¦¬ê¸° ì¶”ê°€
    def price_processor(data_point: DataPoint) -> Dict:
        if data_point.data_type == 'price':
            return {
                'processed_price': float(data_point.value),
                'source': data_point.source,
                'timestamp': data_point.timestamp.isoformat()
            }
        return None
    
    pipeline.add_data_processor('price_processor', price_processor)
    
    # ë°ì´í„° êµ¬ë…ì ì¶”ê°€
    def price_subscriber(processed_data):
        if processed_data:
            print(f"ğŸ’° ê°€ê²© ì—…ë°ì´íŠ¸: {processed_data}")
    
    pipeline.subscribe_to_data('price', price_subscriber)
    
    # íŒŒì´í”„ë¼ì¸ ì‹œì‘
    await pipeline.start_processing_pipeline()
    
    # 10ì´ˆ ë™ì•ˆ ì‹¤í–‰
    print("ğŸ“Š 10ì´ˆê°„ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
    await asyncio.sleep(10)
    
    # ìƒíƒœ í™•ì¸
    status = await pipeline.get_pipeline_status()
    print("âœ… íŒŒì´í”„ë¼ì¸ ìƒíƒœ:")
    print(f"  - ê°€ë™ ì‹œê°„: {status.get('uptime_seconds', 0):.1f}ì´ˆ")
    print(f"  - ì´ ë©”ì‹œì§€: {status.get('total_messages', 0)}ê°œ")
    print(f"  - ì´ˆë‹¹ ë©”ì‹œì§€: {status.get('messages_per_second', 0):.1f}msg/s")
    print(f"  - í‰ê·  ì§€ì—°ì‹œê°„: {status.get('average_latency_ms', 0):.1f}ms")
    print(f"  - ì—ëŸ¬ìœ¨: {status.get('error_rate', 0):.2f}%")
    print(f"  - í™œì„± ì—°ê²°: {status.get('active_connections', 0)}ê°œ")
    
    # ìµœì‹  ë°ì´í„° ìƒ˜í”Œ
    latest = await pipeline.get_latest_data('binance_btc', 'price', 5)
    if latest:
        print(f"  - ìµœì‹  ë°ì´í„° (5ê°œ):")
        for data in latest[-3:]:  # ìµœê·¼ 3ê°œë§Œ í‘œì‹œ
            print(f"    * {data['symbol']}: ${data['value']} (ì§€ì—°: {data['latency_ms']:.1f}ms)")
    
    # ì •ë¦¬
    await pipeline.shutdown()
    
    return True

# ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œ
class BTCPredictionPipeline:
    """BTC ì˜ˆì¸¡ì„ ìœ„í•œ íŠ¹í™”ëœ íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self):
        self.pipeline = RealTimeDataPipeline()
        self.prediction_data = deque(maxlen=1000)
    
    async def setup_sources(self):
        """BTC ì˜ˆì¸¡ìš© ë°ì´í„° ì†ŒìŠ¤ ì„¤ì •"""
        # ê°€ê²© ë°ì´í„°
        await self.pipeline.add_data_source('binance_btc_ticker', {
            'type': 'websocket',
            'uri': 'wss://stream.binance.com:9443/ws/btcusdt@ticker',
            'parser': 'binance_ticker'
        })
        
        # ì£¼ë¬¸ì¥ ë°ì´í„°
        await self.pipeline.add_data_source('binance_btc_depth', {
            'type': 'websocket', 
            'uri': 'wss://stream.binance.com:9443/ws/btcusdt@depth20@100ms',
            'parser': 'binance_depth'
        })
        
        # ê±°ë˜ ë°ì´í„°
        await self.pipeline.add_data_source('binance_btc_trades', {
            'type': 'websocket',
            'uri': 'wss://stream.binance.com:9443/ws/btcusdt@aggTrade',
            'parser': 'binance_trades'
        })
    
    async def setup_processors(self):
        """ë°ì´í„° ì²˜ë¦¬ê¸° ì„¤ì •"""
        
        def feature_extractor(data_point: DataPoint):
            """íŠ¹ì§• ì¶”ì¶œ ì²˜ë¦¬ê¸°"""
            if data_point.data_type == 'price':
                # ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° ë“±
                features = {
                    'price': data_point.value,
                    'timestamp': data_point.timestamp,
                    'volume': data_point.metadata.get('volume', 0) if data_point.metadata else 0
                }
                return features
            return None
        
        def prediction_trigger(data_point: DataPoint):
            """ì˜ˆì¸¡ íŠ¸ë¦¬ê±°"""
            self.prediction_data.append(data_point)
            
            # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ëª¨ì´ë©´ ì˜ˆì¸¡ ì‹¤í–‰
            if len(self.prediction_data) >= 100:
                # ì˜ˆì¸¡ ë¡œì§ í˜¸ì¶œ (ë³„ë„ êµ¬í˜„)
                pass
        
        self.pipeline.add_data_processor('feature_extractor', feature_extractor)
        self.pipeline.add_data_processor('prediction_trigger', prediction_trigger)
    
    async def start(self):
        """ì˜ˆì¸¡ íŒŒì´í”„ë¼ì¸ ì‹œì‘"""
        await self.setup_sources()
        await self.setup_processors()
        await self.pipeline.start_processing_pipeline()

if __name__ == "__main__":
    asyncio.run(test_realtime_pipeline())