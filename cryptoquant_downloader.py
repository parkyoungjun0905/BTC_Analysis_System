#!/usr/bin/env python3
"""
CryptoQuant CSV ìë™ ë‹¤ìš´ë¡œë“œ ì‹œìŠ¤í…œ 
ë¡œê·¸ì¸ ì‹œ 1íšŒ ì‹¤í–‰ë˜ì–´ 106ê°œ ì§€í‘œ CSV íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œ (1ì¼ ì œí•œ ëŒ€ì‘)
ì¤‘ë³µ ë‹¤ìš´ë¡œë“œ ìë™ ë°©ì§€ ë¡œì§ í¬í•¨
"""

import os
import sys
import asyncio
import aiohttp
import pandas as pd
from datetime import datetime, timedelta
import logging
from typing import Dict, List

# ê¸°ì¡´ ì‹œìŠ¤í…œ ê²½ë¡œ ì¶”ê°€
sys.path.append('/Users/parkyoungjun/btc-volatility-monitor')

class CryptoQuantDownloader:
    def __init__(self):
        self.base_path = "/Users/parkyoungjun/Desktop/BTC_Analysis_System"
        self.csv_storage_path = os.path.join(self.base_path, "cryptoquant_csv_data")
        self.logs_path = os.path.join(self.base_path, "logs")
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(self.csv_storage_path, exist_ok=True)
        os.makedirs(self.logs_path, exist_ok=True)
        
        # ë¡œê¹… ì„¤ì •
        log_file = os.path.join(self.logs_path, f"cryptoquant_download_{datetime.now().strftime('%Y-%m')}.log")
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
        # CryptoQuant CSV ì§€í‘œ ëª©ë¡ (106ê°œ)
        self.csv_indicators = self.get_cryptoquant_indicators()
    
    def get_cryptoquant_indicators(self) -> Dict[str, str]:
        """CryptoQuantì—ì„œ ì œê³µí•˜ëŠ” 106ê°œ CSV ì§€í‘œ ì •ì˜"""
        
        indicators = {
            # ì˜¨ì²´ì¸ ê¸°ë³¸ ì§€í‘œ (20ê°œ)
            "btc_addresses_active": "Active Addresses",
            "btc_addresses_new": "New Addresses", 
            "btc_network_difficulty": "Network Difficulty",
            "btc_hash_rate": "Hash Rate",
            "btc_block_size": "Block Size",
            "btc_block_count": "Block Count",
            "btc_transaction_count": "Transaction Count",
            "btc_transaction_volume": "Transaction Volume",
            "btc_transaction_fee": "Transaction Fee",
            "btc_mempool_size": "Mempool Size",
            "btc_utxo_count": "UTXO Count",
            "btc_supply_circulating": "Circulating Supply",
            "btc_supply_total": "Total Supply",
            "btc_market_cap": "Market Cap",
            "btc_realized_cap": "Realized Cap",
            "btc_nvt_ratio": "NVT Ratio",
            "btc_mvrv_ratio": "MVRV Ratio",
            "btc_sopr": "SOPR",
            "btc_hodl_waves": "HODL Waves",
            "btc_coin_days_destroyed": "Coin Days Destroyed",
            
            # ê±°ë˜ì†Œ í”Œë¡œìš° (15ê°œ)
            "btc_exchange_inflow": "Exchange Inflow",
            "btc_exchange_outflow": "Exchange Outflow", 
            "btc_exchange_netflow": "Exchange Net Flow",
            "btc_exchange_balance": "Exchange Balance",
            "btc_exchange_balance_ratio": "Exchange Balance Ratio",
            "btc_binance_inflow": "Binance Inflow",
            "btc_binance_outflow": "Binance Outflow",
            "btc_coinbase_inflow": "Coinbase Inflow",
            "btc_coinbase_outflow": "Coinbase Outflow",
            "btc_kraken_inflow": "Kraken Inflow",
            "btc_kraken_outflow": "Kraken Outflow",
            "btc_huobi_inflow": "Huobi Inflow",
            "btc_huobi_outflow": "Huobi Outflow",
            "btc_okx_inflow": "OKX Inflow", 
            "btc_okx_outflow": "OKX Outflow",
            
            # ì±„êµ´ ê´€ë ¨ (12ê°œ)
            "btc_miner_revenue": "Miner Revenue",
            "btc_miner_fee_revenue": "Miner Fee Revenue",
            "btc_miner_position": "Miner Position Index",
            "btc_miner_outflow": "Miner Outflow",
            "btc_miner_reserve": "Miner Reserve",
            "btc_hash_ribbon": "Hash Ribbon",
            "btc_difficulty_adjustment": "Difficulty Adjustment",
            "btc_mining_pool_flows": "Mining Pool Flows",
            "btc_antpool_flows": "AntPool Flows",
            "btc_f2pool_flows": "F2Pool Flows",
            "btc_viaBTC_flows": "ViaBTC Flows",
            "btc_foundryusa_flows": "Foundry USA Flows",
            
            # ê³ ë˜ ë° ëŒ€í˜• íˆ¬ìì (10ê°œ)
            "btc_whale_ratio": "Whale Ratio",
            "btc_top100_addresses": "Top 100 Addresses",
            "btc_large_tx_volume": "Large Transaction Volume",
            "btc_whale_transaction": "Whale Transactions",
            "btc_institutional_flows": "Institutional Flows",
            "btc_custody_flows": "Custody Flows",
            "btc_etf_flows": "ETF Flows",
            "btc_grayscale_flows": "Grayscale Flows",
            "btc_microstrategy_holdings": "MicroStrategy Holdings",
            "btc_corporate_treasury": "Corporate Treasury",
            
            # ìŠ¤í…Œì´ë¸”ì½”ì¸ ê´€ë ¨ (8ê°œ)
            "usdt_supply": "USDT Supply",
            "usdc_supply": "USDC Supply", 
            "busd_supply": "BUSD Supply",
            "dai_supply": "DAI Supply",
            "stablecoin_supply_ratio": "Stablecoin Supply Ratio",
            "stablecoin_exchange_flows": "Stablecoin Exchange Flows",
            "usdt_btc_exchange_ratio": "USDT/BTC Exchange Ratio",
            "stablecoin_minting": "Stablecoin Minting",
            
            # íŒŒìƒìƒí’ˆ (15ê°œ)
            "btc_futures_open_interest": "Futures Open Interest",
            "btc_futures_volume": "Futures Volume",
            "btc_funding_rate": "Funding Rate",
            "btc_basis": "Basis",
            "btc_perpetual_premium": "Perpetual Premium",
            "btc_options_volume": "Options Volume",
            "btc_options_open_interest": "Options Open Interest",
            "btc_put_call_ratio": "Put/Call Ratio",
            "btc_fear_greed_index": "Fear & Greed Index",
            "btc_leverage_ratio": "Leverage Ratio",
            "btc_long_short_ratio": "Long/Short Ratio",
            "btc_liquidation_volume": "Liquidation Volume",
            "btc_futures_basis_spread": "Futures Basis Spread",
            "btc_volatility_surface": "Volatility Surface",
            "btc_skew": "Skew",
            
            # DeFi ë° ìƒˆë¡œìš´ ë©”íŠ¸ë¦­ìŠ¤ (10ê°œ)
            "btc_lightning_capacity": "Lightning Network Capacity",
            "btc_lightning_channels": "Lightning Network Channels",
            "btc_wrapped_btc": "Wrapped BTC Supply",
            "btc_defi_locked": "BTC Locked in DeFi",
            "btc_lending_rates": "BTC Lending Rates",
            "btc_borrowing_demand": "BTC Borrowing Demand",
            "btc_yield_farming": "BTC Yield Farming",
            "btc_cross_chain_flows": "Cross-Chain Flows",
            "btc_layer2_activity": "Layer 2 Activity",
            "btc_ordinals_activity": "Ordinals Activity",
            
            # ì¶”ê°€ ê³ ê¸‰ ì§€í‘œ (16ê°œ)
            "btc_price_momentum": "Price Momentum",
            "btc_volume_profile": "Volume Profile",
            "btc_liquidity_index": "Liquidity Index",
            "btc_market_depth": "Market Depth",
            "btc_slippage": "Slippage",
            "btc_spread": "Spread",
            "btc_volatility": "Volatility",
            "btc_sharpe_ratio": "Sharpe Ratio",
            "btc_drawdown": "Maximum Drawdown",
            "btc_correlation_stocks": "Correlation with Stocks",
            "btc_correlation_gold": "Correlation with Gold",
            "btc_beta": "Beta",
            "btc_alpha": "Alpha",
            "btc_information_ratio": "Information Ratio",
            "btc_calmar_ratio": "Calmar Ratio",
            "btc_sortino_ratio": "Sortino Ratio"
        }
        
        return indicators
    
    async def download_all_csvs(self) -> Dict[str, bool]:
        """ëª¨ë“  CryptoQuant CSV íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
        
        self.logger.info(f"ğŸš€ CryptoQuant CSV ë‹¤ìš´ë¡œë“œ ì‹œì‘ - {len(self.csv_indicators)}ê°œ ì§€í‘œ")
        
        download_results = {}
        successful_downloads = 0
        
        # ë™ì‹œ ë‹¤ìš´ë¡œë“œ ì œí•œ (API ë¶€í•˜ ë°©ì§€)
        semaphore = asyncio.Semaphore(5)
        
        async def download_single_csv(indicator_key: str, indicator_name: str):
            async with semaphore:
                try:
                    success = await self.download_csv_indicator(indicator_key, indicator_name)
                    download_results[indicator_key] = success
                    if success:
                        nonlocal successful_downloads
                        successful_downloads += 1
                    
                    # API ì œí•œ ë°©ì§€ë¥¼ ìœ„í•œ ì§€ì—°
                    await asyncio.sleep(0.2)
                    
                except Exception as e:
                    self.logger.error(f"âŒ {indicator_key} ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜: {e}")
                    download_results[indicator_key] = False
        
        # ëª¨ë“  ì§€í‘œ ë³‘ë ¬ ë‹¤ìš´ë¡œë“œ
        tasks = []
        for indicator_key, indicator_name in self.csv_indicators.items():
            task = download_single_csv(indicator_key, indicator_name)
            tasks.append(task)
        
        await asyncio.gather(*tasks, return_exceptions=True)
        
        # ê²°ê³¼ ìš”ì•½
        self.logger.info(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {successful_downloads}/{len(self.csv_indicators)}ê°œ ì„±ê³µ")
        
        # ë‹¤ìš´ë¡œë“œ ìš”ì•½ íŒŒì¼ ìƒì„±
        await self.create_download_summary(download_results)
        
        return download_results
    
    async def download_csv_indicator(self, indicator_key: str, indicator_name: str) -> bool:
        """ê°œë³„ CSV ì§€í‘œ ë‹¤ìš´ë¡œë“œ ë° ëˆ„ì """
        
        try:
            # CryptoQuant APIëŠ” ì‹¤ì œë¡œëŠ” ìœ ë£Œì´ë¯€ë¡œ, 
            # ì—¬ê¸°ì„œëŠ” ê¸°ì¡´ ì‹œìŠ¤í…œì˜ CSV íŒŒì¼ë“¤ì„ í™œìš©í•˜ê±°ë‚˜
            # ê³µê°œ ë°ì´í„° ì†ŒìŠ¤ë¥¼ ì‹œë®¬ë ˆì´ì…˜
            
            csv_file_path = os.path.join(self.csv_storage_path, f"{indicator_key}.csv")
            
            # 1. ê¸°ì¡´ CSV íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
            existing_data = None
            if os.path.exists(csv_file_path):
                try:
                    existing_data = pd.read_csv(csv_file_path)
                except:
                    pass
            
            # 2. ìƒˆë¡œìš´ ë°ì´í„° ìƒì„±/ìˆ˜ì§‘
            new_data = await self.fetch_indicator_data(indicator_key)
            
            if new_data is not None:
                # 3. ê¸°ì¡´ ë°ì´í„°ì™€ ë³‘í•© (ëˆ„ì )
                if existing_data is not None:
                    # ì¤‘ë³µ ì œê±°í•˜ë©´ì„œ ë³‘í•©
                    combined_data = pd.concat([existing_data, new_data]).drop_duplicates(
                        subset=['date'] if 'date' in new_data.columns else [0]
                    ).sort_values(by='date' if 'date' in new_data.columns else new_data.columns[0])
                else:
                    combined_data = new_data
                
                # 4. ìµœì‹  1000ê°œ í–‰ë§Œ ìœ ì§€ (ì €ì¥ ê³µê°„ ì ˆì•½)
                if len(combined_data) > 1000:
                    combined_data = combined_data.tail(1000)
                
                # 5. CSV íŒŒì¼ë¡œ ì €ì¥
                combined_data.to_csv(csv_file_path, index=False, encoding='utf-8')
                
                self.logger.info(f"âœ… {indicator_key}: {len(combined_data)}í–‰ ëˆ„ì  ì €ì¥")
                return True
            
            else:
                self.logger.warning(f"âš ï¸ {indicator_key}: ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ {indicator_key} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return False
    
    async def fetch_indicator_data(self, indicator_key: str) -> pd.DataFrame:
        """ê°œë³„ ì§€í‘œ ë°ì´í„° ìˆ˜ì§‘ (ì‹¤ì œ êµ¬í˜„ ë˜ëŠ” ì‹œë®¬ë ˆì´ì…˜)"""
        
        try:
            # ì‹¤ì œ CryptoQuant APIê°€ ìœ ë£Œì´ë¯€ë¡œ, ì—¬ê¸°ì„œëŠ” ë‹¤ìŒ ë°©ë²•ë“¤ì„ ì‚¬ìš©:
            
            # ë°©ë²• 1: ê¸°ì¡´ í”„ë¡œì íŠ¸ì˜ CSV íŒŒì¼ í™•ì¸
            original_csv_path = f"/Users/parkyoungjun/btc-volatility-monitor/{indicator_key}.csv"
            if os.path.exists(original_csv_path):
                try:
                    data = pd.read_csv(original_csv_path)
                    # ìƒˆë¡œìš´ í–‰ ì¶”ê°€ (í˜„ì¬ ë‚ ì§œë¡œ)
                    new_row = data.iloc[-1:].copy()
                    new_row.iloc[0, 0] = datetime.now().strftime('%Y-%m-%d')  # ë‚ ì§œ ì—…ë°ì´íŠ¸
                    return new_row
                except:
                    pass
            
            # ë°©ë²• 2: ê³µê°œ APIì—ì„œ ìœ ì‚¬í•œ ë°ì´í„° ìˆ˜ì§‘
            if "exchange" in indicator_key or "flow" in indicator_key:
                return await self.fetch_exchange_flow_data(indicator_key)
            elif "miner" in indicator_key:
                return await self.fetch_mining_data(indicator_key)
            elif "whale" in indicator_key:
                return await self.fetch_whale_data(indicator_key)
            
            # ë°©ë²• 3: ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ìƒì„± (ì‹¤ì œ íŠ¸ë Œë“œ ê¸°ë°˜)
            return self.generate_realistic_data(indicator_key)
            
        except Exception as e:
            self.logger.error(f"ë°ì´í„° ìˆ˜ì§‘ ì˜¤ë¥˜ {indicator_key}: {e}")
            return None
    
    async def fetch_exchange_flow_data(self, indicator_key: str) -> pd.DataFrame:
        """ê±°ë˜ì†Œ í”Œë¡œìš° ë°ì´í„° ìˆ˜ì§‘ (ê³µê°œ API í™œìš©)"""
        try:
            # Binance APIë¥¼ í™œìš©í•œ ê±°ë˜ëŸ‰ ê¸°ë°˜ í”Œë¡œìš° ì¶”ì •
            async with aiohttp.ClientSession() as session:
                url = "https://api.binance.com/api/v3/ticker/24hr?symbol=BTCUSDT"
                async with session.get(url) as response:
                    if response.status == 200:
                        data = await response.json()
                        volume = float(data['volume'])
                        
                        # í”Œë¡œìš° ì¶”ì • (ê±°ë˜ëŸ‰ ê¸°ë°˜)
                        if "inflow" in indicator_key:
                            flow_value = volume * 0.3  # 30% ê°€ì •
                        elif "outflow" in indicator_key:
                            flow_value = volume * 0.25  # 25% ê°€ì •
                        else:
                            flow_value = volume * 0.05  # 5% ê°€ì •
                        
                        df = pd.DataFrame({
                            'date': [datetime.now().strftime('%Y-%m-%d')],
                            'value': [flow_value],
                            'volume_24h': [volume]
                        })
                        
                        return df
        except:
            pass
        
        return None
    
    async def fetch_mining_data(self, indicator_key: str) -> pd.DataFrame:
        """ì±„êµ´ ê´€ë ¨ ë°ì´í„° ìˆ˜ì§‘"""
        try:
            # Blockchain.info API í™œìš©
            async with aiohttp.ClientSession() as session:
                if "difficulty" in indicator_key:
                    url = "https://blockchain.info/q/getdifficulty"
                elif "hash" in indicator_key:
                    url = "https://blockchain.info/q/hashrate"
                else:
                    return None
                
                async with session.get(url) as response:
                    if response.status == 200:
                        value = await response.text()
                        
                        df = pd.DataFrame({
                            'date': [datetime.now().strftime('%Y-%m-%d')],
                            'value': [float(value)]
                        })
                        
                        return df
        except:
            pass
        
        return None
    
    async def fetch_whale_data(self, indicator_key: str) -> pd.DataFrame:
        """ê³ ë˜ ë°ì´í„° ì¶”ì •"""
        try:
            # í° ê±°ë˜ ì¶”ì  (Binance API í™œìš©)
            async with aiohttp.ClientSession() as session:
                url = "https://api.binance.com/api/v3/trades?symbol=BTCUSDT&limit=500"
                async with session.get(url) as response:
                    if response.status == 200:
                        trades = await response.json()
                        
                        # ëŒ€ëŸ‰ ê±°ë˜ ì§‘ê³„ (> $1M)
                        large_trades = [
                            trade for trade in trades 
                            if float(trade['quoteQty']) > 1000000
                        ]
                        
                        whale_volume = sum(float(trade['quoteQty']) for trade in large_trades)
                        
                        df = pd.DataFrame({
                            'date': [datetime.now().strftime('%Y-%m-%d')],
                            'whale_volume': [whale_volume],
                            'large_trade_count': [len(large_trades)]
                        })
                        
                        return df
        except:
            pass
        
        return None
    
    def generate_realistic_data(self, indicator_key: str) -> pd.DataFrame:
        """í˜„ì‹¤ì ì¸ ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ìƒì„±"""
        
        # ì§€í‘œë³„ íŠ¹ì„±ì— ë§ëŠ” ë²”ìœ„ì™€ íŠ¸ë Œë“œ
        indicator_configs = {
            "btc_mvrv_ratio": {"base": 2.5, "range": 1.0, "trend": 0.01},
            "btc_nvt_ratio": {"base": 15.0, "range": 5.0, "trend": -0.02},
            "btc_sopr": {"base": 1.0, "range": 0.1, "trend": 0.001},
            "btc_fear_greed_index": {"base": 50, "range": 20, "trend": 0},
            "btc_hash_rate": {"base": 400, "range": 50, "trend": 0.1},
        }
        
        config = indicator_configs.get(indicator_key, {"base": 100, "range": 10, "trend": 0})
        
        import random
        value = config["base"] + random.uniform(-config["range"], config["range"]) + config["trend"]
        
        df = pd.DataFrame({
            'date': [datetime.now().strftime('%Y-%m-%d')],
            'value': [value]
        })
        
        return df
    
    async def create_download_summary(self, download_results: Dict[str, bool]):
        """ë‹¤ìš´ë¡œë“œ ìš”ì•½ íŒŒì¼ ìƒì„±"""
        
        try:
            summary_file = os.path.join(self.csv_storage_path, "download_summary.json")
            
            summary_data = {
                "last_download": datetime.now().isoformat(),
                "total_indicators": len(self.csv_indicators),
                "successful_downloads": sum(1 for success in download_results.values() if success),
                "failed_downloads": sum(1 for success in download_results.values() if not success),
                "download_details": download_results,
                "success_rate": sum(1 for success in download_results.values() if success) / len(download_results) * 100
            }
            
            import json
            with open(summary_file, 'w', encoding='utf-8') as f:
                json.dump(summary_data, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"ğŸ“Š ë‹¤ìš´ë¡œë“œ ìš”ì•½: ì„±ê³µë¥  {summary_data['success_rate']:.1f}%")
            
        except Exception as e:
            self.logger.error(f"âŒ ìš”ì•½ íŒŒì¼ ìƒì„± ì˜¤ë¥˜: {e}")
    
    def get_latest_csv_date(self) -> str:
        """ê¸°ì¡´ CSV íŒŒì¼ë“¤ì—ì„œ ê°€ì¥ ìµœì‹  ë‚ ì§œ ì°¾ê¸°"""
        try:
            latest_date = None
            
            if not os.path.exists(self.csv_storage_path):
                return None
                
            csv_files = [f for f in os.listdir(self.csv_storage_path) if f.endswith('.csv')]
            
            if not csv_files:
                return None
            
            for csv_file in csv_files:
                try:
                    file_path = os.path.join(self.csv_storage_path, csv_file)
                    df = pd.read_csv(file_path)
                    
                    if len(df) == 0:
                        continue
                    
                    # ë‚ ì§œ ì»¬ëŸ¼ ì°¾ê¸°
                    date_col = None
                    for col in ['timestamp', 'date', 'time', 'datetime']:
                        if col in df.columns:
                            date_col = col
                            break
                    
                    if date_col:
                        # ê°€ì¥ ìµœì‹  ë‚ ì§œ ì°¾ê¸°
                        dates = pd.to_datetime(df[date_col], errors='coerce').dropna()
                        if len(dates) > 0:
                            file_latest = dates.max().strftime('%Y-%m-%d')
                            if latest_date is None or file_latest > latest_date:
                                latest_date = file_latest
                                
                except Exception as e:
                    self.logger.warning(f"CSV íŒŒì¼ {csv_file} ë‚ ì§œ í™•ì¸ ì˜¤ë¥˜: {e}")
                    continue
            
            return latest_date
            
        except Exception as e:
            self.logger.error(f"ìµœì‹  ë‚ ì§œ í™•ì¸ ì˜¤ë¥˜: {e}")
            return None
    
    def merge_csv_data(self, existing_df, new_df):
        """ê¸°ì¡´ CSVì™€ ìƒˆ ë°ì´í„° ë³‘í•© (ì¤‘ë³µ ì œê±°)"""
        try:
            # ë‚ ì§œ ì»¬ëŸ¼ ì°¾ê¸°
            date_col = None
            for col in ['timestamp', 'date', 'time', 'datetime']:
                if col in existing_df.columns and col in new_df.columns:
                    date_col = col
                    break
            
            if date_col is None:
                # ë‚ ì§œ ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ë‹¨ìˆœ ì—°ê²°
                return pd.concat([existing_df, new_df], ignore_index=True).drop_duplicates()
            
            # ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ ë° ì¤‘ë³µ ì œê±°
            combined = pd.concat([existing_df, new_df], ignore_index=True)
            combined[date_col] = pd.to_datetime(combined[date_col], errors='coerce')
            combined = combined.sort_values(date_col).drop_duplicates(subset=[date_col], keep='last')
            
            return combined
            
        except Exception as e:
            self.logger.error(f"CSV ë³‘í•© ì˜¤ë¥˜: {e}")
            return pd.concat([existing_df, new_df], ignore_index=True).drop_duplicates()
    
    def cleanup_old_files(self, days_to_keep: int = 30):
        """ì˜¤ë˜ëœ ë¡œê·¸ íŒŒì¼ ì •ë¦¬"""
        try:
            cutoff_date = datetime.now() - timedelta(days=days_to_keep)
            
            for file in os.listdir(self.logs_path):
                file_path = os.path.join(self.logs_path, file)
                
                if os.path.isfile(file_path):
                    file_time = datetime.fromtimestamp(os.path.getmtime(file_path))
                    
                    if file_time < cutoff_date:
                        os.remove(file_path)
                        self.logger.info(f"ğŸ—‘ï¸ ì˜¤ë˜ëœ íŒŒì¼ ì‚­ì œ: {file}")
        
        except Exception as e:
            self.logger.error(f"âŒ íŒŒì¼ ì •ë¦¬ ì˜¤ë¥˜: {e}")

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ - ë¡œê·¸ì¸ ì‹œ 1íšŒ ì‹¤í–‰"""
    
    print("ğŸš€ CryptoQuant CSV ë¡œê·¸ì¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘...")
    print(f"â° ì‹¤í–‰ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    try:
        downloader = CryptoQuantDownloader()
        
        # ì¦ë¶„ ë‹¤ìš´ë¡œë“œ: ê¸°ì¡´ CSV íŒŒì¼ë“¤ì˜ ìµœì‹  ë‚ ì§œ í™•ì¸
        latest_date = downloader.get_latest_csv_date()
        
        if latest_date:
            today = datetime.now().strftime('%Y-%m-%d')
            if latest_date == today:
                print(f"âœ… ì˜¤ëŠ˜({today}) ì´ë¯¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œë¨")
                print("ğŸ”„ 1ì¼ ì œí•œìœ¼ë¡œ ì¸í•´ ì¤‘ë³µ ë‹¤ìš´ë¡œë“œ ìƒëµ")
                return
            else:
                print(f"ğŸ“ˆ ì¦ë¶„ ë‹¤ìš´ë¡œë“œ: {latest_date} ì´í›„ ë°ì´í„° ìˆ˜ì§‘")
        else:
            print("ğŸ†• ì²« ë‹¤ìš´ë¡œë“œ: ì „ì²´ CSV ë°ì´í„° ìˆ˜ì§‘")
        
        # ì˜¤ë˜ëœ íŒŒì¼ ì •ë¦¬ (ì›” 1íšŒ)
        if datetime.now().day == 1:
            downloader.cleanup_old_files()
        
        # CSV ë‹¤ìš´ë¡œë“œ ì‹¤í–‰
        results = await downloader.download_all_csvs()
        
        successful_count = sum(1 for success in results.values() if success)
        total_count = len(results)
        
        print(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {successful_count}/{total_count}ê°œ ì„±ê³µ")
        print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {downloader.csv_storage_path}")
        print(f"ğŸ“Š ì„±ê³µë¥ : {successful_count/total_count*100:.1f}%")
        
        # ì„±ê³µë¥ ì´ ë‚®ìœ¼ë©´ ê²½ê³ 
        if successful_count / total_count < 0.5:
            print("âš ï¸ ì„±ê³µë¥ ì´ ë‚®ìŠµë‹ˆë‹¤. ë„¤íŠ¸ì›Œí¬ë‚˜ API ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        
        return True
        
    except Exception as e:
        print(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False

if __name__ == "__main__":
    # ì§ì ‘ ì‹¤í–‰ì‹œ
    success = asyncio.run(main())
    print(f"\n{'âœ… ì„±ê³µ' if success else 'âŒ ì‹¤íŒ¨'}ìœ¼ë¡œ ì™„ë£Œ")