#!/usr/bin/env python3
"""
ğŸ›¡ï¸ ë¡œë²„ìŠ¤íŠ¸ ì‹ ë¢°ì„± ì‹œìŠ¤í…œ
ëª¨ë¸ ì‹¤íŒ¨ ê°ì§€, ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™”, ê°•ê±´í•œ ì˜ˆì¸¡ ì‹œìŠ¤í…œ

í•µì‹¬ ê¸°ëŠ¥:
- ì‹¤ì‹œê°„ ëª¨ë¸ ì‹¤íŒ¨ ê°ì§€ ë° ìë™ ë³µêµ¬
- ë² ì´ì§€ì•ˆ ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™”
- ê°•ê±´í•œ ì˜ˆì¸¡ ì§‘ê³„ (Robust Aggregation)
- ì„±ëŠ¥ ì¼ê´€ì„± ëª¨ë‹ˆí„°ë§
- ìë™ ì•Œë¦¼ ë° ë³µêµ¬ ì‹œìŠ¤í…œ
"""

import numpy as np
import pandas as pd
import json
import pickle
import warnings
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Any, Optional, Union
from pathlib import Path
import sqlite3
import logging
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass, asdict
from enum import Enum
import threading
import time

# í†µê³„ ë° ë² ì´ì§€ì•ˆ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from scipy import stats
    from scipy.stats import norm, t, chi2
    import scipy.special as special
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False
    print("âš ï¸ SciPy ë¯¸ì„¤ì¹˜ - í†µê³„ì  ê¸°ëŠ¥ ì œí•œ")

# ë¨¸ì‹ ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.preprocessing import RobustScaler
from sklearn.ensemble import IsolationForest
from sklearn.covariance import EllipticEnvelope

warnings.filterwarnings('ignore')

class ModelHealth(Enum):
    """ëª¨ë¸ ê±´ê°• ìƒíƒœ"""
    HEALTHY = "healthy"
    WARNING = "warning"
    CRITICAL = "critical"
    FAILED = "failed"

class AlertSeverity(Enum):
    """ì•Œë¦¼ ì‹¬ê°ë„"""
    INFO = "info"
    WARNING = "warning"
    CRITICAL = "critical"
    EMERGENCY = "emergency"

@dataclass
class ModelPerformanceMetrics:
    """ëª¨ë¸ ì„±ëŠ¥ ë©”íŠ¸ë¦­"""
    timestamp: datetime
    model_name: str
    accuracy: float
    mse: float
    mae: float
    r2: float
    prediction_count: int
    response_time: float
    memory_usage: float
    health_status: ModelHealth

@dataclass
class UncertaintyEstimate:
    """ë¶ˆí™•ì‹¤ì„± ì¶”ì •"""
    prediction: float
    epistemic_uncertainty: float  # ëª¨ë¸ ë¶ˆí™•ì‹¤ì„±
    aleatoric_uncertainty: float  # ë°ì´í„° ë¶ˆí™•ì‹¤ì„±
    total_uncertainty: float
    confidence_interval_lower: float
    confidence_interval_upper: float
    confidence_level: float

class BayesianUncertaintyQuantifier:
    """
    ğŸ² ë² ì´ì§€ì•ˆ ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™” ì‹œìŠ¤í…œ
    """
    
    def __init__(self, confidence_level: float = 0.95):
        self.confidence_level = confidence_level
        self.posterior_samples = {}
        self.prior_parameters = {}
        
        self.logger = logging.getLogger(__name__)

    def update_posterior(self, model_name: str, predictions: np.ndarray, 
                        targets: np.ndarray, prior_mean: float = 0.0, 
                        prior_std: float = 1.0):
        """
        ë² ì´ì§€ì•ˆ ì‚¬í›„ ë¶„í¬ ì—…ë°ì´íŠ¸
        
        Args:
            model_name: ëª¨ë¸ëª…
            predictions: ì˜ˆì¸¡ê°’ë“¤
            targets: ì‹¤ì œê°’ë“¤
            prior_mean: ì‚¬ì „ í‰ê· 
            prior_std: ì‚¬ì „ í‘œì¤€í¸ì°¨
        """
        if not SCIPY_AVAILABLE:
            return
        
        residuals = targets - predictions
        n_samples = len(residuals)
        
        if n_samples == 0:
            return
        
        # ë² ì´ì§€ì•ˆ ì„ í˜• íšŒê·€ ì¶”ì •
        sample_mean = np.mean(residuals)
        sample_var = np.var(residuals, ddof=1) if n_samples > 1 else 1.0
        
        # ì‚¬í›„ ë¶„í¬ íŒŒë¼ë¯¸í„° (ì •ê·œ-ì—­ê°ë§ˆ ëª¨ë¸)
        prior_precision = 1.0 / (prior_std ** 2)
        sample_precision = 1.0 / sample_var if sample_var > 0 else 1.0
        
        # ì‚¬í›„ í‰ê· ê³¼ ì •ë°€ë„
        posterior_precision = prior_precision + n_samples * sample_precision
        posterior_mean = (
            prior_precision * prior_mean + 
            n_samples * sample_precision * sample_mean
        ) / posterior_precision
        
        # ì‚¬í›„ ë¶„ì‚°
        posterior_var = 1.0 / posterior_precision
        
        # ì‚¬í›„ ììœ ë„ (t-ë¶„í¬ìš©)
        posterior_df = max(1, n_samples - 1)
        
        self.posterior_samples[model_name] = {
            'mean': posterior_mean,
            'var': posterior_var,
            'std': np.sqrt(posterior_var),
            'df': posterior_df,
            'n_samples': n_samples,
            'last_update': datetime.now()
        }

    def estimate_uncertainty(self, model_name: str, prediction: float,
                           feature_uncertainty: float = 0.1) -> UncertaintyEstimate:
        """
        ë¶ˆí™•ì‹¤ì„± ì¶”ì •
        
        Args:
            model_name: ëª¨ë¸ëª…
            prediction: ì˜ˆì¸¡ê°’
            feature_uncertainty: íŠ¹ì„± ë¶ˆí™•ì‹¤ì„±
            
        Returns:
            UncertaintyEstimate: ë¶ˆí™•ì‹¤ì„± ì¶”ì • ê²°ê³¼
        """
        if model_name not in self.posterior_samples:
            # ê¸°ë³¸ ë¶ˆí™•ì‹¤ì„±
            return UncertaintyEstimate(
                prediction=prediction,
                epistemic_uncertainty=0.2,
                aleatoric_uncertainty=0.1,
                total_uncertainty=0.22,
                confidence_interval_lower=prediction - 0.44,
                confidence_interval_upper=prediction + 0.44,
                confidence_level=self.confidence_level
            )
        
        posterior = self.posterior_samples[model_name]
        
        # ì¸ì‹ë¡ ì  ë¶ˆí™•ì‹¤ì„± (ëª¨ë¸ íŒŒë¼ë¯¸í„° ë¶ˆí™•ì‹¤ì„±)
        epistemic_uncertainty = posterior['std']
        
        # ìš°ì—°ì  ë¶ˆí™•ì‹¤ì„± (ë°ì´í„° ë…¸ì´ì¦ˆ)
        aleatoric_uncertainty = feature_uncertainty
        
        # ì´ ë¶ˆí™•ì‹¤ì„±
        total_uncertainty = np.sqrt(
            epistemic_uncertainty**2 + aleatoric_uncertainty**2
        )
        
        # ì‹ ë¢°êµ¬ê°„ ê³„ì‚° (t-ë¶„í¬ ì‚¬ìš©)
        if SCIPY_AVAILABLE:
            alpha = 1 - self.confidence_level
            t_value = stats.t.ppf(1 - alpha/2, posterior['df'])
            margin = t_value * total_uncertainty
        else:
            # ì •ê·œë¶„í¬ ê·¼ì‚¬
            z_value = 1.96  # 95% ì‹ ë¢°êµ¬ê°„
            margin = z_value * total_uncertainty
        
        return UncertaintyEstimate(
            prediction=prediction,
            epistemic_uncertainty=epistemic_uncertainty,
            aleatoric_uncertainty=aleatoric_uncertainty,
            total_uncertainty=total_uncertainty,
            confidence_interval_lower=prediction - margin,
            confidence_interval_upper=prediction + margin,
            confidence_level=self.confidence_level
        )

class ModelFailureDetector:
    """
    ğŸ” ëª¨ë¸ ì‹¤íŒ¨ ê°ì§€ ì‹œìŠ¤í…œ
    """
    
    def __init__(self, detection_window: int = 50):
        self.detection_window = detection_window
        self.performance_history = {}
        self.failure_thresholds = {
            'accuracy_drop': 0.15,      # 15% ì´ìƒ ì •í™•ë„ í•˜ë½
            'mse_spike': 2.0,           # MSE 2ë°° ì´ìƒ ì¦ê°€
            'response_time': 30.0,      # 30ì´ˆ ì´ìƒ ì‘ë‹µì‹œê°„
            'memory_limit': 2048,       # 2GB ë©”ëª¨ë¦¬ í•œê³„
            'nan_predictions': 0.05,    # 5% ì´ìƒ NaN ì˜ˆì¸¡
            'consecutive_failures': 5   # ì—°ì† 5íšŒ ì‹¤íŒ¨
        }
        
        # ì´ìƒì¹˜ íƒì§€ê¸°
        self.anomaly_detector = IsolationForest(contamination=0.1, random_state=42)
        self.anomaly_fitted = False
        
        self.logger = logging.getLogger(__name__)

    def add_performance_record(self, metrics: ModelPerformanceMetrics):
        """ì„±ëŠ¥ ê¸°ë¡ ì¶”ê°€"""
        model_name = metrics.model_name
        
        if model_name not in self.performance_history:
            self.performance_history[model_name] = []
        
        self.performance_history[model_name].append(metrics)
        
        # ìœˆë„ìš° í¬ê¸° ìœ ì§€
        if len(self.performance_history[model_name]) > self.detection_window:
            self.performance_history[model_name] = \
                self.performance_history[model_name][-self.detection_window:]

    def detect_model_failures(self, model_name: str) -> Dict[str, Any]:
        """
        ëª¨ë¸ ì‹¤íŒ¨ ê°ì§€
        
        Args:
            model_name: ëª¨ë¸ëª…
            
        Returns:
            Dict[str, Any]: ê°ì§€ ê²°ê³¼
        """
        if (model_name not in self.performance_history or 
            len(self.performance_history[model_name]) < 5):
            return {'status': 'insufficient_data', 'failures': []}
        
        recent_records = self.performance_history[model_name][-10:]
        older_records = self.performance_history[model_name][:-10] if len(self.performance_history[model_name]) > 10 else []
        
        failures = []
        
        # 1. ì •í™•ë„ í•˜ë½ ê°ì§€
        if older_records:
            recent_accuracy = np.mean([r.accuracy for r in recent_records])
            older_accuracy = np.mean([r.accuracy for r in older_records])
            
            accuracy_drop = (older_accuracy - recent_accuracy) / older_accuracy if older_accuracy > 0 else 0
            
            if accuracy_drop > self.failure_thresholds['accuracy_drop']:
                failures.append({
                    'type': 'accuracy_drop',
                    'severity': AlertSeverity.WARNING,
                    'value': accuracy_drop,
                    'threshold': self.failure_thresholds['accuracy_drop'],
                    'description': f'ì •í™•ë„ê°€ {accuracy_drop:.2%} í•˜ë½í–ˆìŠµë‹ˆë‹¤'
                })
        
        # 2. MSE ê¸‰ì¦ ê°ì§€
        if len(recent_records) >= 5:
            recent_mse = np.mean([r.mse for r in recent_records[-5:]])
            baseline_mse = np.mean([r.mse for r in recent_records[:-5]]) if len(recent_records) > 5 else recent_mse
            
            mse_ratio = recent_mse / baseline_mse if baseline_mse > 0 else 1.0
            
            if mse_ratio > self.failure_thresholds['mse_spike']:
                failures.append({
                    'type': 'mse_spike',
                    'severity': AlertSeverity.CRITICAL,
                    'value': mse_ratio,
                    'threshold': self.failure_thresholds['mse_spike'],
                    'description': f'MSEê°€ {mse_ratio:.1f}ë°° ì¦ê°€í–ˆìŠµë‹ˆë‹¤'
                })
        
        # 3. ì‘ë‹µ ì‹œê°„ ì§€ì—° ê°ì§€
        max_response_time = max(r.response_time for r in recent_records)
        if max_response_time > self.failure_thresholds['response_time']:
            failures.append({
                'type': 'response_time',
                'severity': AlertSeverity.WARNING,
                'value': max_response_time,
                'threshold': self.failure_thresholds['response_time'],
                'description': f'ì‘ë‹µì‹œê°„ì´ {max_response_time:.1f}ì´ˆë¡œ ì§€ì—°ë˜ì—ˆìŠµë‹ˆë‹¤'
            })
        
        # 4. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì´ˆê³¼ ê°ì§€
        max_memory = max(r.memory_usage for r in recent_records)
        if max_memory > self.failure_thresholds['memory_limit']:
            failures.append({
                'type': 'memory_limit',
                'severity': AlertSeverity.CRITICAL,
                'value': max_memory,
                'threshold': self.failure_thresholds['memory_limit'],
                'description': f'ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ {max_memory:.1f}MBë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤'
            })
        
        # 5. ì—°ì† ì‹¤íŒ¨ ê°ì§€
        failed_count = sum(1 for r in recent_records if r.health_status == ModelHealth.FAILED)
        if failed_count >= self.failure_thresholds['consecutive_failures']:
            failures.append({
                'type': 'consecutive_failures',
                'severity': AlertSeverity.EMERGENCY,
                'value': failed_count,
                'threshold': self.failure_thresholds['consecutive_failures'],
                'description': f'{failed_count}íšŒ ì—°ì† ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤'
            })
        
        # 6. ì´ìƒì¹˜ íƒì§€ (ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°)
        if len(self.performance_history[model_name]) >= 20:
            try:
                # íŠ¹ì„± ë²¡í„° ìƒì„±
                features = np.array([
                    [r.accuracy, r.mse, r.response_time, r.memory_usage]
                    for r in self.performance_history[model_name]
                ])
                
                # ì´ìƒì¹˜ íƒì§€ ëª¨ë¸ í›ˆë ¨ (ì²˜ìŒì¸ ê²½ìš°)
                if not self.anomaly_fitted:
                    self.anomaly_detector.fit(features[:-5])  # ìµœê·¼ 5ê°œ ì œì™¸í•˜ê³  í›ˆë ¨
                    self.anomaly_fitted = True
                
                # ìµœê·¼ ë°ì´í„°ì—ì„œ ì´ìƒì¹˜ ê°ì§€
                recent_features = features[-5:]
                anomaly_scores = self.anomaly_detector.decision_function(recent_features)
                is_anomaly = self.anomaly_detector.predict(recent_features) == -1
                
                if np.any(is_anomaly):
                    anomaly_count = np.sum(is_anomaly)
                    failures.append({
                        'type': 'anomaly_detection',
                        'severity': AlertSeverity.WARNING,
                        'value': anomaly_count,
                        'threshold': 1,
                        'description': f'{anomaly_count}ê°œ ì´ìƒì¹˜ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤'
                    })
                    
            except Exception as e:
                self.logger.warning(f"âš ï¸ ì´ìƒì¹˜ íƒì§€ ì‹¤íŒ¨: {e}")
        
        # ì „ì²´ ìƒíƒœ í‰ê°€
        if not failures:
            status = 'healthy'
        elif any(f['severity'] == AlertSeverity.EMERGENCY for f in failures):
            status = 'emergency'
        elif any(f['severity'] == AlertSeverity.CRITICAL for f in failures):
            status = 'critical'
        else:
            status = 'warning'
        
        return {
            'model_name': model_name,
            'status': status,
            'failures': failures,
            'total_failures': len(failures),
            'detection_time': datetime.now(),
            'recent_records_count': len(recent_records)
        }

class RobustAggregator:
    """
    ğŸ›¡ï¸ ê°•ê±´í•œ ì˜ˆì¸¡ ì§‘ê³„ ì‹œìŠ¤í…œ
    """
    
    def __init__(self):
        self.aggregation_methods = {
            'weighted_median': self._weighted_median,
            'trimmed_mean': self._trimmed_mean,
            'winsorized_mean': self._winsorized_mean,
            'huber_aggregation': self._huber_aggregation,
            'robust_average': self._robust_average
        }
        
        self.logger = logging.getLogger(__name__)

    def _weighted_median(self, predictions: np.ndarray, weights: np.ndarray) -> float:
        """ê°€ì¤‘ ì¤‘ìœ„ìˆ˜"""
        if len(predictions) == 0:
            return 0.0
        
        # ì •ë ¬ëœ ì¸ë±ìŠ¤
        sorted_indices = np.argsort(predictions)
        sorted_predictions = predictions[sorted_indices]
        sorted_weights = weights[sorted_indices]
        
        # ëˆ„ì  ê°€ì¤‘ì¹˜
        cumulative_weights = np.cumsum(sorted_weights)
        total_weight = np.sum(weights)
        
        # ì¤‘ìœ„ìˆ˜ ì°¾ê¸°
        median_weight = total_weight / 2
        median_idx = np.searchsorted(cumulative_weights, median_weight)
        
        if median_idx >= len(sorted_predictions):
            return sorted_predictions[-1]
        
        return sorted_predictions[median_idx]

    def _trimmed_mean(self, predictions: np.ndarray, weights: np.ndarray, 
                     trim_percent: float = 0.2) -> float:
        """ì ˆì‚¬ í‰ê· """
        if len(predictions) == 0:
            return 0.0
        
        n = len(predictions)
        trim_count = int(n * trim_percent / 2)
        
        if trim_count >= n // 2:
            return np.average(predictions, weights=weights)
        
        # ì •ë ¬
        sorted_indices = np.argsort(predictions)
        
        # ìƒí•˜ìœ„ ì ˆì‚¬
        keep_indices = sorted_indices[trim_count:n-trim_count]
        
        if len(keep_indices) == 0:
            return np.average(predictions, weights=weights)
        
        trimmed_predictions = predictions[keep_indices]
        trimmed_weights = weights[keep_indices]
        
        return np.average(trimmed_predictions, weights=trimmed_weights)

    def _winsorized_mean(self, predictions: np.ndarray, weights: np.ndarray,
                        limits: Tuple[float, float] = (0.1, 0.1)) -> float:
        """ìœˆì €í™” í‰ê· """
        if len(predictions) == 0:
            return 0.0
        
        # ë¶„ìœ„ìˆ˜ ê³„ì‚°
        lower_percentile = limits[0] * 100
        upper_percentile = (1 - limits[1]) * 100
        
        lower_bound = np.percentile(predictions, lower_percentile)
        upper_bound = np.percentile(predictions, upper_percentile)
        
        # ìœˆì €í™”
        winsorized_predictions = np.clip(predictions, lower_bound, upper_bound)
        
        return np.average(winsorized_predictions, weights=weights)

    def _huber_aggregation(self, predictions: np.ndarray, weights: np.ndarray,
                          delta: float = 1.35) -> float:
        """Huber ì†ì‹¤ ê¸°ë°˜ ê°•ê±´í•œ ì§‘ê³„"""
        if len(predictions) == 0:
            return 0.0
        
        # ì´ˆê¸° ì¶”ì •ì¹˜ (ê°€ì¤‘ ì¤‘ìœ„ìˆ˜)
        estimate = self._weighted_median(predictions, weights)
        
        # ë°˜ë³µì  ê°œì„ 
        for _ in range(10):  # ìµœëŒ€ 10íšŒ ë°˜ë³µ
            residuals = predictions - estimate
            
            # Huber ê°€ì¤‘ì¹˜ ê³„ì‚°
            huber_weights = np.where(
                np.abs(residuals) <= delta,
                1.0,
                delta / np.abs(residuals)
            )
            
            # ì „ì²´ ê°€ì¤‘ì¹˜ (ì›ë˜ ê°€ì¤‘ì¹˜ * Huber ê°€ì¤‘ì¹˜)
            total_weights = weights * huber_weights
            
            # ìƒˆë¡œìš´ ì¶”ì •ì¹˜
            new_estimate = np.average(predictions, weights=total_weights)
            
            # ìˆ˜ë ´ í™•ì¸
            if abs(new_estimate - estimate) < 1e-6:
                break
            
            estimate = new_estimate
        
        return estimate

    def _robust_average(self, predictions: np.ndarray, weights: np.ndarray) -> float:
        """ë‹¤ì¤‘ ë°©ë²• ê²°í•© ê°•ê±´í•œ í‰ê· """
        if len(predictions) == 0:
            return 0.0
        
        # ì—¬ëŸ¬ ë°©ë²•ì˜ ê²°ê³¼
        methods_results = []
        
        methods_results.append(self._weighted_median(predictions, weights))
        methods_results.append(self._trimmed_mean(predictions, weights))
        methods_results.append(self._winsorized_mean(predictions, weights))
        methods_results.append(self._huber_aggregation(predictions, weights))
        
        # ë°©ë²•ë“¤ì˜ í‰ê·  (ë” ê°•ê±´)
        return np.mean(methods_results)

    def robust_ensemble_prediction(self, model_predictions: Dict[str, float],
                                 model_weights: Dict[str, float],
                                 model_health: Dict[str, ModelHealth],
                                 method: str = 'robust_average') -> Dict[str, Any]:
        """
        ê°•ê±´í•œ ì•™ìƒë¸” ì˜ˆì¸¡
        
        Args:
            model_predictions: ëª¨ë¸ë³„ ì˜ˆì¸¡ê°’
            model_weights: ëª¨ë¸ë³„ ê°€ì¤‘ì¹˜
            model_health: ëª¨ë¸ë³„ ê±´ê°• ìƒíƒœ
            method: ì§‘ê³„ ë°©ë²•
            
        Returns:
            Dict[str, Any]: ê°•ê±´í•œ ì˜ˆì¸¡ ê²°ê³¼
        """
        # ê±´ê°•í•œ ëª¨ë¸ë§Œ ì„ íƒ
        healthy_models = {
            name: pred for name, pred in model_predictions.items()
            if model_health.get(name, ModelHealth.FAILED) in [ModelHealth.HEALTHY, ModelHealth.WARNING]
        }
        
        if not healthy_models:
            # ëª¨ë“  ëª¨ë¸ì´ ì‹¤íŒ¨í•œ ê²½ìš° - ë§ˆì§€ë§‰ ì•ˆì „í•œ ì˜ˆì¸¡ ì‚¬ìš©
            self.logger.error("âŒ ëª¨ë“  ëª¨ë¸ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")
            return {
                'prediction': 0.0,
                'method': 'fallback',
                'confidence': 0.0,
                'models_used': [],
                'total_models': len(model_predictions),
                'healthy_models': 0
            }
        
        # ì˜ˆì¸¡ê°’ê³¼ ê°€ì¤‘ì¹˜ ë°°ì—´ ìƒì„±
        predictions = np.array(list(healthy_models.values()))
        weights = np.array([
            model_weights.get(name, 1.0) for name in healthy_models.keys()
        ])
        
        # ê°€ì¤‘ì¹˜ ì •ê·œí™”
        if np.sum(weights) > 0:
            weights = weights / np.sum(weights)
        else:
            weights = np.ones(len(predictions)) / len(predictions)
        
        # ì„ íƒëœ ë°©ë²•ìœ¼ë¡œ ì§‘ê³„
        if method in self.aggregation_methods:
            robust_prediction = self.aggregation_methods[method](predictions, weights)
        else:
            robust_prediction = self._robust_average(predictions, weights)
        
        # ì‹ ë¢°ë„ ê³„ì‚° (ì˜ˆì¸¡ì˜ ì¼ì¹˜ë„)
        prediction_std = np.std(predictions) if len(predictions) > 1 else 0.0
        confidence = max(0.0, 1.0 - prediction_std / (np.abs(robust_prediction) + 1e-6))
        
        return {
            'prediction': float(robust_prediction),
            'method': method,
            'confidence': float(confidence),
            'models_used': list(healthy_models.keys()),
            'total_models': len(model_predictions),
            'healthy_models': len(healthy_models),
            'prediction_std': float(prediction_std),
            'individual_predictions': dict(healthy_models)
        }

class PerformanceConsistencyMonitor:
    """
    ğŸ“Š ì„±ëŠ¥ ì¼ê´€ì„± ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ
    """
    
    def __init__(self, monitoring_window: int = 100):
        self.monitoring_window = monitoring_window
        self.consistency_metrics = {}
        
        self.logger = logging.getLogger(__name__)

    def update_performance(self, model_name: str, accuracy: float, 
                          timestamp: datetime = None):
        """ì„±ëŠ¥ ì—…ë°ì´íŠ¸"""
        if timestamp is None:
            timestamp = datetime.now()
        
        if model_name not in self.consistency_metrics:
            self.consistency_metrics[model_name] = {
                'accuracies': [],
                'timestamps': [],
                'rolling_mean': [],
                'rolling_std': []
            }
        
        metrics = self.consistency_metrics[model_name]
        
        # ìƒˆ ë°ì´í„° ì¶”ê°€
        metrics['accuracies'].append(accuracy)
        metrics['timestamps'].append(timestamp)
        
        # ìœˆë„ìš° í¬ê¸° ìœ ì§€
        if len(metrics['accuracies']) > self.monitoring_window:
            metrics['accuracies'] = metrics['accuracies'][-self.monitoring_window:]
            metrics['timestamps'] = metrics['timestamps'][-self.monitoring_window:]
        
        # ë¡¤ë§ í†µê³„ ê³„ì‚°
        if len(metrics['accuracies']) >= 10:
            recent_accuracies = metrics['accuracies'][-10:]
            metrics['rolling_mean'].append(np.mean(recent_accuracies))
            metrics['rolling_std'].append(np.std(recent_accuracies))
            
            # ë¡¤ë§ í†µê³„ë„ ìœˆë„ìš° í¬ê¸° ìœ ì§€
            if len(metrics['rolling_mean']) > self.monitoring_window:
                metrics['rolling_mean'] = metrics['rolling_mean'][-self.monitoring_window:]
                metrics['rolling_std'] = metrics['rolling_std'][-self.monitoring_window:]

    def assess_consistency(self, model_name: str) -> Dict[str, Any]:
        """
        ì¼ê´€ì„± í‰ê°€
        
        Args:
            model_name: ëª¨ë¸ëª…
            
        Returns:
            Dict[str, Any]: ì¼ê´€ì„± í‰ê°€ ê²°ê³¼
        """
        if (model_name not in self.consistency_metrics or 
            len(self.consistency_metrics[model_name]['accuracies']) < 20):
            return {
                'status': 'insufficient_data',
                'consistency_score': 0.0,
                'trend': 'unknown'
            }
        
        metrics = self.consistency_metrics[model_name]
        accuracies = np.array(metrics['accuracies'])
        
        # 1. ë³€ë™ì„± ì ìˆ˜ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
        volatility = np.std(accuracies) / np.mean(accuracies) if np.mean(accuracies) > 0 else float('inf')
        
        # 2. íŠ¸ë Œë“œ ë¶„ì„
        if len(accuracies) >= 20:
            # ì„ í˜• íšŒê·€ë¥¼ í†µí•œ íŠ¸ë Œë“œ
            x = np.arange(len(accuracies))
            if SCIPY_AVAILABLE:
                slope, intercept, r_value, p_value, std_err = stats.linregress(x, accuracies)
                trend_strength = abs(r_value)
                
                if slope > 0.001 and p_value < 0.05:
                    trend = 'improving'
                elif slope < -0.001 and p_value < 0.05:
                    trend = 'declining'
                else:
                    trend = 'stable'
            else:
                # ê°„ë‹¨í•œ íŠ¸ë Œë“œ ê³„ì‚°
                first_half_mean = np.mean(accuracies[:len(accuracies)//2])
                second_half_mean = np.mean(accuracies[len(accuracies)//2:])
                
                if second_half_mean > first_half_mean * 1.05:
                    trend = 'improving'
                elif second_half_mean < first_half_mean * 0.95:
                    trend = 'declining'
                else:
                    trend = 'stable'
                
                trend_strength = abs(second_half_mean - first_half_mean) / first_half_mean
        else:
            trend = 'unknown'
            trend_strength = 0.0
        
        # 3. ì´ìƒê°’ ë¹„ìœ¨
        if len(accuracies) >= 10:
            q1, q3 = np.percentile(accuracies, [25, 75])
            iqr = q3 - q1
            lower_bound = q1 - 1.5 * iqr
            upper_bound = q3 + 1.5 * iqr
            
            outliers = np.sum((accuracies < lower_bound) | (accuracies > upper_bound))
            outlier_ratio = outliers / len(accuracies)
        else:
            outlier_ratio = 0.0
        
        # 4. ì¢…í•© ì¼ê´€ì„± ì ìˆ˜ (0-1, ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
        volatility_score = max(0, 1 - volatility * 10)  # ë³€ë™ì„±ì´ ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ
        outlier_score = 1 - outlier_ratio  # ì´ìƒê°’ì´ ì ì„ìˆ˜ë¡ ì¢‹ìŒ
        trend_score = 1 - trend_strength if trend == 'declining' else 1  # í•˜ë½ íŠ¸ë Œë“œ í˜ë„í‹°
        
        consistency_score = np.mean([volatility_score, outlier_score, trend_score])
        
        # 5. ìƒíƒœ ë¶„ë¥˜
        if consistency_score >= 0.8:
            status = 'excellent'
        elif consistency_score >= 0.6:
            status = 'good'
        elif consistency_score >= 0.4:
            status = 'fair'
        else:
            status = 'poor'
        
        return {
            'status': status,
            'consistency_score': float(consistency_score),
            'volatility': float(volatility),
            'trend': trend,
            'trend_strength': float(trend_strength),
            'outlier_ratio': float(outlier_ratio),
            'recent_performance': {
                'mean': float(np.mean(accuracies[-10:])),
                'std': float(np.std(accuracies[-10:])),
                'min': float(np.min(accuracies[-10:])),
                'max': float(np.max(accuracies[-10:]))
            },
            'data_points': len(accuracies)
        }

class ReliabilitySystemManager:
    """
    ğŸ›¡ï¸ ì¢…í•© ì‹ ë¢°ì„± ì‹œìŠ¤í…œ ê´€ë¦¬ì
    """
    
    def __init__(self):
        self.uncertainty_quantifier = BayesianUncertaintyQuantifier()
        self.failure_detector = ModelFailureDetector()
        self.robust_aggregator = RobustAggregator()
        self.consistency_monitor = PerformanceConsistencyMonitor()
        
        # ì‹œìŠ¤í…œ ìƒíƒœ
        self.system_health = {}
        self.alerts = []
        self.recovery_actions = []
        
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('reliability_system.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)

    def comprehensive_reliability_check(self, 
                                      model_predictions: Dict[str, float],
                                      model_weights: Dict[str, float],
                                      recent_targets: Dict[str, np.ndarray] = None) -> Dict[str, Any]:
        """
        ì¢…í•© ì‹ ë¢°ì„± ê²€ì‚¬
        
        Args:
            model_predictions: ëª¨ë¸ë³„ ì˜ˆì¸¡ê°’
            model_weights: ëª¨ë¸ë³„ ê°€ì¤‘ì¹˜
            recent_targets: ìµœê·¼ ì‹¤ì œê°’ë“¤ (ì„ íƒì )
            
        Returns:
            Dict[str, Any]: ì¢…í•© ì‹ ë¢°ì„± ë¶„ì„ ê²°ê³¼
        """
        print("ğŸ›¡ï¸ ì¢…í•© ì‹ ë¢°ì„± ì‹œìŠ¤í…œ ë¶„ì„ ì‹œì‘...")
        
        analysis_results = {
            'timestamp': datetime.now(),
            'models_analyzed': list(model_predictions.keys()),
            'uncertainty_estimates': {},
            'failure_detections': {},
            'consistency_assessments': {},
            'robust_prediction': {},
            'system_alerts': [],
            'recommendations': []
        }
        
        # 1. ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™”
        print("ğŸ² ë² ì´ì§€ì•ˆ ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™”...")
        for model_name, prediction in model_predictions.items():
            if recent_targets and model_name in recent_targets:
                # ì‚¬í›„ ë¶„í¬ ì—…ë°ì´íŠ¸
                dummy_predictions = np.full(len(recent_targets[model_name]), prediction)
                self.uncertainty_quantifier.update_posterior(
                    model_name, dummy_predictions, recent_targets[model_name]
                )
            
            # ë¶ˆí™•ì‹¤ì„± ì¶”ì •
            uncertainty = self.uncertainty_quantifier.estimate_uncertainty(
                model_name, prediction
            )
            analysis_results['uncertainty_estimates'][model_name] = asdict(uncertainty)
        
        # 2. ëª¨ë¸ ì‹¤íŒ¨ ê°ì§€
        print("ğŸ” ëª¨ë¸ ì‹¤íŒ¨ ê°ì§€ ë¶„ì„...")
        model_health = {}
        
        for model_name in model_predictions.keys():
            failure_analysis = self.failure_detector.detect_model_failures(model_name)
            analysis_results['failure_detections'][model_name] = failure_analysis
            
            # ê±´ê°• ìƒíƒœ ê²°ì •
            if failure_analysis['status'] == 'emergency':
                model_health[model_name] = ModelHealth.FAILED
            elif failure_analysis['status'] == 'critical':
                model_health[model_name] = ModelHealth.CRITICAL
            elif failure_analysis['status'] == 'warning':
                model_health[model_name] = ModelHealth.WARNING
            else:
                model_health[model_name] = ModelHealth.HEALTHY
            
            # ì•Œë¦¼ ìƒì„±
            for failure in failure_analysis.get('failures', []):
                alert = {
                    'model_name': model_name,
                    'type': failure['type'],
                    'severity': failure['severity'].value,
                    'description': failure['description'],
                    'timestamp': datetime.now()
                }
                analysis_results['system_alerts'].append(alert)
        
        # 3. ì¼ê´€ì„± í‰ê°€
        print("ğŸ“Š ì„±ëŠ¥ ì¼ê´€ì„± ëª¨ë‹ˆí„°ë§...")
        for model_name in model_predictions.keys():
            consistency = self.consistency_monitor.assess_consistency(model_name)
            analysis_results['consistency_assessments'][model_name] = consistency
        
        # 4. ê°•ê±´í•œ ì˜ˆì¸¡ ì§‘ê³„
        print("ğŸ›¡ï¸ ê°•ê±´í•œ ì•™ìƒë¸” ì˜ˆì¸¡...")
        robust_result = self.robust_aggregator.robust_ensemble_prediction(
            model_predictions, model_weights, model_health
        )
        analysis_results['robust_prediction'] = robust_result
        
        # 5. ì „ì²´ ì‹œìŠ¤í…œ ê±´ê°•ë„ í‰ê°€
        healthy_models = sum(1 for health in model_health.values() 
                           if health in [ModelHealth.HEALTHY, ModelHealth.WARNING])
        
        total_models = len(model_health)
        system_health_ratio = healthy_models / total_models if total_models > 0 else 0
        
        if system_health_ratio >= 0.8:
            system_status = 'excellent'
        elif system_health_ratio >= 0.6:
            system_status = 'good'
        elif system_health_ratio >= 0.4:
            system_status = 'degraded'
        else:
            system_status = 'critical'
        
        # 6. ê¶Œì¥ì‚¬í•­ ìƒì„±
        recommendations = []
        
        if system_health_ratio < 0.6:
            recommendations.append("ê¸´ê¸‰: ì‹¤íŒ¨í•œ ëª¨ë¸ë“¤ì„ ì¬í›ˆë ¨í•˜ê±°ë‚˜ êµì²´í•˜ì„¸ìš”")
        
        critical_models = [name for name, health in model_health.items() 
                          if health == ModelHealth.CRITICAL]
        if critical_models:
            recommendations.append(f"ìœ„í—˜: {', '.join(critical_models)} ëª¨ë¸ë“¤ì´ ìœ„í—˜ ìƒíƒœì…ë‹ˆë‹¤")
        
        low_confidence = [name for name, pred in analysis_results['uncertainty_estimates'].items()
                         if pred['total_uncertainty'] > 0.3]
        if low_confidence:
            recommendations.append(f"ì£¼ì˜: {', '.join(low_confidence)} ëª¨ë¸ë“¤ì˜ ë¶ˆí™•ì‹¤ì„±ì´ ë†’ìŠµë‹ˆë‹¤")
        
        if robust_result['confidence'] < 0.7:
            recommendations.append("ì•™ìƒë¸” ì‹ ë¢°ë„ê°€ ë‚®ìŠµë‹ˆë‹¤. ëª¨ë¸ ë‹¤ì–‘ì„±ì„ ê°œì„ í•˜ì„¸ìš”")
        
        analysis_results['recommendations'] = recommendations
        
        # 7. ì‹œìŠ¤í…œ ì „ì²´ ìš”ì•½
        analysis_results['system_summary'] = {
            'status': system_status,
            'health_ratio': system_health_ratio,
            'total_models': total_models,
            'healthy_models': healthy_models,
            'critical_models': len(critical_models),
            'total_alerts': len(analysis_results['system_alerts']),
            'ensemble_confidence': robust_result['confidence']
        }
        
        # ê²°ê³¼ ì¶œë ¥
        print("\n" + "="*50)
        print("ğŸ›¡ï¸ ì‹ ë¢°ì„± ì‹œìŠ¤í…œ ë¶„ì„ ì™„ë£Œ!")
        print("="*50)
        print(f"ğŸ¥ ì‹œìŠ¤í…œ ìƒíƒœ: {system_status.upper()}")
        print(f"ğŸ’ª ê±´ê°•í•œ ëª¨ë¸: {healthy_models}/{total_models} ({system_health_ratio:.1%})")
        print(f"ğŸ¯ ì•™ìƒë¸” ì˜ˆì¸¡: {robust_result['prediction']:.6f}")
        print(f"ğŸ”’ ì•™ìƒë¸” ì‹ ë¢°ë„: {robust_result['confidence']:.3f}")
        print(f"âš ï¸  ì´ ì•Œë¦¼: {len(analysis_results['system_alerts'])}ê°œ")
        
        if recommendations:
            print("\nğŸ“‹ ê¶Œì¥ì‚¬í•­:")
            for i, rec in enumerate(recommendations, 1):
                print(f"  {i}. {rec}")
        
        return analysis_results

    def save_reliability_analysis(self, analysis_results: Dict, 
                                file_path: str = None) -> str:
        """ì‹ ë¢°ì„± ë¶„ì„ ê²°ê³¼ ì €ì¥"""
        if file_path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            file_path = f"/Users/parkyoungjun/Desktop/BTC_Analysis_System/reliability_analysis_{timestamp}.json"
        
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(analysis_results, f, indent=2, ensure_ascii=False, default=str)
        
        self.logger.info(f"ğŸ’¾ ì‹ ë¢°ì„± ë¶„ì„ ê²°ê³¼ ì €ì¥: {file_path}")
        return file_path

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ›¡ï¸ ë¡œë²„ìŠ¤íŠ¸ ì‹ ë¢°ì„± ì‹œìŠ¤í…œ ì´ˆê¸°í™”")
    
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    reliability_system = ReliabilitySystemManager()
    
    print("âœ… ì‹ ë¢°ì„± ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    print("ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ê¸°ëŠ¥:")
    print("  ğŸ² ë² ì´ì§€ì•ˆ ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™”")
    print("  ğŸ” ì‹¤ì‹œê°„ ëª¨ë¸ ì‹¤íŒ¨ ê°ì§€")
    print("  ğŸ›¡ï¸ ê°•ê±´í•œ ì˜ˆì¸¡ ì§‘ê³„")
    print("  ğŸ“Š ì„±ëŠ¥ ì¼ê´€ì„± ëª¨ë‹ˆí„°ë§")
    print("  âš ï¸  ìë™ ì•Œë¦¼ ë° ë³µêµ¬")
    
    return reliability_system

if __name__ == "__main__":
    system = main()