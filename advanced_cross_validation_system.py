#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ê³ ê¸‰ ì‹œê³„ì—´ êµì°¨ ê²€ì¦ ì‹œìŠ¤í…œ
- ì›Œí¬í¬ì›Œë“œ ê²€ì¦
- í¼ì§€ë“œ ê·¸ë£¹ ì‹œê³„ì—´ ë¶„í• 
- ë¸”ë¡í‚¹ ì‹œê³„ì—´ ê²€ì¦
- ìƒ˜í”Œ ì™¸ í…ŒìŠ¤íŠ¸ í”„ë¡œí† ì½œ
"""

import numpy as np
import pandas as pd
import warnings
warnings.filterwarnings('ignore')

import os
import sys
import json
import sqlite3
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Union, Iterator, Generator
import logging
import pickle
from dataclasses import dataclass
from abc import ABC, abstractmethod

# ê³¼í•™ ê³„ì‚°
from scipy import stats
from scipy.signal import savgol_filter
from scipy.interpolate import interp1d
from scipy.ndimage import gaussian_filter1d

# ë¨¸ì‹ ëŸ¬ë‹
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import TimeSeriesSplit, BaseCrossValidator
from sklearn.base import BaseEstimator
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression

# í†µê³„ ë° ì‹œê°í™”
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.dates import DateFormatter
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class ValidationResult:
    """ê²€ì¦ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    train_scores: List[float]
    test_scores: List[float]
    train_indices: List[np.ndarray]
    test_indices: List[np.ndarray]
    fold_metadata: List[Dict]
    overall_metrics: Dict[str, float]
    timestamp: datetime

class CustomTimeSeriesSplit(BaseCrossValidator):
    """ì»¤ìŠ¤í…€ ì‹œê³„ì—´ ë¶„í•  ê¸°ë³¸ í´ë˜ìŠ¤"""
    
    def __init__(self, n_splits: int = 5):
        self.n_splits = n_splits
    
    def get_n_splits(self, X=None, y=None, groups=None) -> int:
        return self.n_splits

class WalkForwardValidation(CustomTimeSeriesSplit):
    """
    ì›Œí¬í¬ì›Œë“œ ê²€ì¦
    - ì‹œê°„ì— ë”°ë¼ í›ˆë ¨ ìœˆë„ìš°ê°€ í™•ì¥ë˜ëŠ” ë°©ì‹
    - ì‹¤ì œ ê±°ë˜ í™˜ê²½ì„ ê°€ì¥ ì˜ ëª¨ë°©
    """
    
    def __init__(self, 
                 n_splits: int = 5,
                 min_train_size: int = None,
                 max_train_size: int = None,
                 test_size: int = None,
                 gap: int = 0):
        """
        Args:
            n_splits: ë¶„í•  ìˆ˜
            min_train_size: ìµœì†Œ í›ˆë ¨ í¬ê¸°
            max_train_size: ìµœëŒ€ í›ˆë ¨ í¬ê¸°
            test_size: í…ŒìŠ¤íŠ¸ í¬ê¸°
            gap: í›ˆë ¨ê³¼ í…ŒìŠ¤íŠ¸ ê°„ ê°„ê²© (ë°ì´í„° ìœ ì¶œ ë°©ì§€)
        """
        super().__init__(n_splits)
        self.min_train_size = min_train_size
        self.max_train_size = max_train_size
        self.test_size = test_size
        self.gap = gap
    
    def split(self, X, y=None, groups=None) -> Generator[Tuple[np.ndarray, np.ndarray], None, None]:
        """
        ë°ì´í„°ë¥¼ ì›Œí¬í¬ì›Œë“œ ë°©ì‹ìœ¼ë¡œ ë¶„í• 
        
        Yields:
            (train_indices, test_indices) íŠœí”Œ
        """
        n_samples = len(X)
        
        # ê¸°ë³¸ê°’ ì„¤ì •
        test_size = self.test_size or max(1, n_samples // (self.n_splits + 1))
        min_train_size = self.min_train_size or max(50, n_samples // 5)
        
        # ì²« ë²ˆì§¸ ë¶„í• ì˜ ì‹œì‘ì 
        start_idx = min_train_size
        
        for i in range(self.n_splits):
            # í…ŒìŠ¤íŠ¸ ì‹œì‘ì ê³¼ ëì 
            test_start = start_idx + i * test_size + self.gap
            test_end = min(test_start + test_size, n_samples)
            
            if test_end - test_start < test_size // 2:
                break
            
            # í›ˆë ¨ ë°ì´í„° ë²”ìœ„
            train_end = test_start - self.gap
            train_start = 0
            
            # ìµœëŒ€ í›ˆë ¨ í¬ê¸° ì œí•œ
            if self.max_train_size:
                train_start = max(0, train_end - self.max_train_size)
            
            if train_end - train_start < min_train_size:
                break
            
            train_indices = np.arange(train_start, train_end)
            test_indices = np.arange(test_start, test_end)
            
            yield train_indices, test_indices

class PurgedGroupTimeSeriesSplit(CustomTimeSeriesSplit):
    """
    í¼ì§€ë“œ ê·¸ë£¹ ì‹œê³„ì—´ ë¶„í• 
    - ê·¸ë£¹ ê¸°ë°˜ ë¶„í• ë¡œ ë°ì´í„° ìœ ì¶œ ë°©ì§€
    - ì‹œê°„ì  ì¢…ì†ì„±ì„ ê³ ë ¤í•œ purge ì ìš©
    """
    
    def __init__(self, 
                 n_splits: int = 5,
                 group_gap: timedelta = None,
                 purge_gap: timedelta = None):
        """
        Args:
            n_splits: ë¶„í•  ìˆ˜
            group_gap: ê·¸ë£¹ ê°„ ìµœì†Œ ê°„ê²©
            purge_gap: í¼ì§€ ê°„ê²©
        """
        super().__init__(n_splits)
        self.group_gap = group_gap or timedelta(hours=24)
        self.purge_gap = purge_gap or timedelta(hours=6)
    
    def _create_groups(self, timestamps: pd.DatetimeIndex) -> np.ndarray:
        """íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê·¸ë£¹ ìƒì„±"""
        groups = np.zeros(len(timestamps), dtype=int)
        current_group = 0
        
        for i, ts in enumerate(timestamps):
            if i == 0:
                groups[i] = current_group
            else:
                if ts - timestamps[i-1] > self.group_gap:
                    current_group += 1
                groups[i] = current_group
        
        return groups
    
    def split(self, X, y=None, groups=None) -> Generator[Tuple[np.ndarray, np.ndarray], None, None]:
        """
        ê·¸ë£¹ ê¸°ë°˜ í¼ì§€ë“œ ë¶„í• 
        
        Yields:
            (train_indices, test_indices) íŠœí”Œ
        """
        if isinstance(X, pd.DataFrame) and isinstance(X.index, pd.DatetimeIndex):
            timestamps = X.index
        else:
            # ì¸ë±ìŠ¤ê°€ íƒ€ì„ìŠ¤íƒ¬í”„ê°€ ì•„ë‹ˆë©´ ìˆœì°¨ì  ê·¸ë£¹ ìƒì„±
            timestamps = pd.date_range('2024-01-01', periods=len(X), freq='H')
        
        if groups is None:
            groups = self._create_groups(timestamps)
        
        unique_groups = np.unique(groups)
        n_groups = len(unique_groups)
        
        test_size = max(1, n_groups // (self.n_splits + 1))
        
        for i in range(self.n_splits):
            # í…ŒìŠ¤íŠ¸ ê·¸ë£¹ ì„ íƒ
            test_start_group = i * test_size
            test_end_group = min((i + 1) * test_size, n_groups)
            
            if test_end_group <= test_start_group:
                break
            
            test_groups = unique_groups[test_start_group:test_end_group]
            test_indices = np.where(np.isin(groups, test_groups))[0]
            
            # í›ˆë ¨ ê·¸ë£¹ (í…ŒìŠ¤íŠ¸ ì´ì „)
            train_groups = unique_groups[:test_start_group]
            train_indices = np.where(np.isin(groups, train_groups))[0]
            
            # í¼ì§€ ì ìš© (í…ŒìŠ¤íŠ¸ ì‹œì‘ ì „ ì¼ì • ì‹œê°„ ì œê±°)
            if len(test_indices) > 0 and len(train_indices) > 0:
                test_start_time = timestamps[test_indices[0]]
                purge_cutoff = test_start_time - self.purge_gap
                
                train_indices = train_indices[timestamps[train_indices] <= purge_cutoff]
            
            if len(train_indices) > 0 and len(test_indices) > 0:
                yield train_indices, test_indices

class BlockingTimeSeriesCV(CustomTimeSeriesSplit):
    """
    ë¸”ë¡í‚¹ ì‹œê³„ì—´ êµì°¨ ê²€ì¦
    - ì‹œê°„ ë¸”ë¡ ë‹¨ìœ„ë¡œ ë¶„í• 
    - ê° ë¸”ë¡ ë‚´ ë…ë¦½ì„± ë³´ì¥
    """
    
    def __init__(self, 
                 n_splits: int = 5,
                 block_size: int = None,
                 separation_size: int = None):
        """
        Args:
            n_splits: ë¶„í•  ìˆ˜
            block_size: ë¸”ë¡ í¬ê¸°
            separation_size: ë¸”ë¡ ê°„ ë¶„ë¦¬ í¬ê¸°
        """
        super().__init__(n_splits)
        self.block_size = block_size
        self.separation_size = separation_size or 0
    
    def split(self, X, y=None, groups=None) -> Generator[Tuple[np.ndarray, np.ndarray], None, None]:
        """
        ë¸”ë¡í‚¹ ë¶„í• 
        
        Yields:
            (train_indices, test_indices) íŠœí”Œ
        """
        n_samples = len(X)
        
        # ê¸°ë³¸ ë¸”ë¡ í¬ê¸° ì„¤ì •
        if self.block_size is None:
            self.block_size = max(50, n_samples // (self.n_splits * 3))
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ì´ ê¸¸ì´ (ë¶„ë¦¬ ê³ ë ¤)
        total_used = self.n_splits * self.block_size + (self.n_splits - 1) * self.separation_size
        
        if total_used > n_samples:
            # ë¸”ë¡ í¬ê¸° ìë™ ì¡°ì •
            self.block_size = (n_samples - (self.n_splits - 1) * self.separation_size) // self.n_splits
        
        for i in range(self.n_splits):
            # í…ŒìŠ¤íŠ¸ ë¸”ë¡ ìœ„ì¹˜
            test_start = i * (self.block_size + self.separation_size)
            test_end = test_start + self.block_size
            
            if test_end > n_samples:
                break
            
            test_indices = np.arange(test_start, test_end)
            
            # í›ˆë ¨ ë¸”ë¡ë“¤ (í…ŒìŠ¤íŠ¸ ë¸”ë¡ ì œì™¸)
            train_indices = []
            
            for j in range(self.n_splits):
                if j != i:
                    block_start = j * (self.block_size + self.separation_size)
                    block_end = block_start + self.block_size
                    
                    if block_end <= n_samples:
                        train_indices.extend(range(block_start, block_end))
            
            train_indices = np.array(train_indices)
            
            if len(train_indices) > 0:
                yield train_indices, test_indices

class OutOfSampleTestProtocol:
    """
    ìƒ˜í”Œ ì™¸ í…ŒìŠ¤íŠ¸ í”„ë¡œí† ì½œ
    - ì™„ì „íˆ ë¯¸ë˜ ë°ì´í„°ë¡œ ìµœì¢… ê²€ì¦
    - ì‹œê°„ì  ì¼ê´€ì„± ë³´ì¥
    """
    
    def __init__(self, 
                 holdout_ratio: float = 0.2,
                 validation_ratio: float = 0.2,
                 purge_gap: timedelta = None):
        """
        Args:
            holdout_ratio: í™€ë“œì•„ì›ƒ ë¹„ìœ¨
            validation_ratio: ê²€ì¦ ì„¸íŠ¸ ë¹„ìœ¨
            purge_gap: í¼ì§€ ê°„ê²©
        """
        self.holdout_ratio = holdout_ratio
        self.validation_ratio = validation_ratio
        self.purge_gap = purge_gap or timedelta(hours=1)
    
    def split_temporal_holdout(self, X: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """
        ì‹œê°„ì  í™€ë“œì•„ì›ƒ ë¶„í• 
        
        Args:
            X: ì „ì²´ ë°ì´í„°
            
        Returns:
            (train, validation, test) ë°ì´í„°í”„ë ˆì„ íŠœí”Œ
        """
        n_samples = len(X)
        
        # ë¶„í•  ì§€ì  ê³„ì‚°
        test_size = int(n_samples * self.holdout_ratio)
        val_size = int(n_samples * self.validation_ratio)
        train_size = n_samples - test_size - val_size
        
        # ì‹œê°„ìˆœ ë¶„í• 
        train_data = X.iloc[:train_size]
        val_data = X.iloc[train_size:train_size + val_size]
        test_data = X.iloc[train_size + val_size:]
        
        return train_data, val_data, test_data

class AdvancedCrossValidationSystem:
    """
    ğŸ”¬ ê³ ê¸‰ ì‹œê³„ì—´ êµì°¨ ê²€ì¦ ì‹œìŠ¤í…œ
    
    ì£¼ìš” ê¸°ëŠ¥:
    1. ë‹¤ì–‘í•œ ì‹œê³„ì—´ êµì°¨ ê²€ì¦ ì „ëµ
    2. ë°ì´í„° ìœ ì¶œ ë°©ì§€ ë©”ì»¤ë‹ˆì¦˜
    3. ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¢…í•© í‰ê°€
    4. ê²€ì¦ ê²°ê³¼ ì‹œê°í™” ë° ë¶„ì„
    """
    
    def __init__(self):
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        self.logger = logging.getLogger(__name__)
        
        # ê²€ì¦ ì „ëµë“¤
        self.validators = {
            'walk_forward': WalkForwardValidation(n_splits=5),
            'purged_group': PurgedGroupTimeSeriesSplit(n_splits=5),
            'blocking': BlockingTimeSeriesCV(n_splits=5),
            'standard_ts': TimeSeriesSplit(n_splits=5)
        }
        
        # ê²°ê³¼ ì €ì¥ì†Œ
        self.validation_results = {}
        self.oos_protocol = OutOfSampleTestProtocol()
        
        self.logger.info("ğŸ”¬ ê³ ê¸‰ êµì°¨ ê²€ì¦ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def calculate_metrics(self, y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:
        """
        ì˜ˆì¸¡ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
        
        Args:
            y_true: ì‹¤ì œê°’
            y_pred: ì˜ˆì¸¡ê°’
            
        Returns:
            ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬
        """
        metrics = {}
        
        try:
            metrics['mse'] = mean_squared_error(y_true, y_pred)
            metrics['mae'] = mean_absolute_error(y_true, y_pred)
            metrics['rmse'] = np.sqrt(metrics['mse'])
            metrics['r2'] = r2_score(y_true, y_pred)
            
            # ë°©í–¥ì„± ì •í™•ë„ (ê¸ˆìœµì—ì„œ ì¤‘ìš”)
            y_true_diff = np.diff(y_true)
            y_pred_diff = np.diff(y_pred)
            direction_accuracy = np.mean(np.sign(y_true_diff) == np.sign(y_pred_diff))
            metrics['direction_accuracy'] = direction_accuracy
            
            # ìµœëŒ€ ì˜¤ì°¨
            metrics['max_error'] = np.max(np.abs(y_true - y_pred))
            
            # í‰ê·  ì ˆëŒ€ ë°±ë¶„ìœ¨ ì˜¤ì°¨ (MAPE)
            mask = y_true != 0
            if np.sum(mask) > 0:
                mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100
                metrics['mape'] = mape
            else:
                metrics['mape'] = float('inf')
            
        except Exception as e:
            self.logger.warning(f"ë©”íŠ¸ë¦­ ê³„ì‚° ì˜¤ë¥˜: {e}")
            metrics = {'error': str(e)}
        
        return metrics
    
    def run_cross_validation(self, 
                           X: pd.DataFrame, 
                           y: pd.Series,
                           model: BaseEstimator,
                           cv_method: str = 'walk_forward',
                           **cv_params) -> ValidationResult:
        """
        êµì°¨ ê²€ì¦ ì‹¤í–‰
        
        Args:
            X: íŠ¹ì„± ë°ì´í„°
            y: íƒ€ê²Ÿ ë°ì´í„°
            model: ëª¨ë¸ ê°ì²´
            cv_method: êµì°¨ ê²€ì¦ ë°©ë²•
            **cv_params: êµì°¨ ê²€ì¦ íŒŒë¼ë¯¸í„°
            
        Returns:
            ê²€ì¦ ê²°ê³¼
        """
        self.logger.info(f"ğŸ”¬ {cv_method} êµì°¨ ê²€ì¦ ì‹¤í–‰...")
        
        # ê²€ì¦ì ì„ íƒ
        if cv_method not in self.validators:
            raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ê²€ì¦ ë°©ë²•: {cv_method}")
        
        validator = self.validators[cv_method]
        
        # íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
        if cv_params:
            for param, value in cv_params.items():
                if hasattr(validator, param):
                    setattr(validator, param, value)
        
        # êµì°¨ ê²€ì¦ ì‹¤í–‰
        train_scores = []
        test_scores = []
        train_indices_list = []
        test_indices_list = []
        fold_metadata = []
        
        fold_num = 0
        for train_idx, test_idx in validator.split(X, y):
            fold_num += 1
            
            # ë°ì´í„° ë¶„í• 
            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]
            
            try:
                # ëª¨ë¸ í›ˆë ¨
                model_copy = pickle.loads(pickle.dumps(model))  # ë”¥ ì¹´í”¼
                model_copy.fit(X_train, y_train)
                
                # ì˜ˆì¸¡
                y_train_pred = model_copy.predict(X_train)
                y_test_pred = model_copy.predict(X_test)
                
                # ë©”íŠ¸ë¦­ ê³„ì‚°
                train_metrics = self.calculate_metrics(y_train.values, y_train_pred)
                test_metrics = self.calculate_metrics(y_test.values, y_test_pred)
                
                train_scores.append(train_metrics)
                test_scores.append(test_metrics)
                train_indices_list.append(train_idx)
                test_indices_list.append(test_idx)
                
                # í´ë“œ ë©”íƒ€ë°ì´í„°
                metadata = {
                    'fold': fold_num,
                    'train_period': (X_train.index[0], X_train.index[-1]),
                    'test_period': (X_test.index[0], X_test.index[-1]),
                    'train_size': len(train_idx),
                    'test_size': len(test_idx)
                }
                fold_metadata.append(metadata)
                
                self.logger.info(f"í´ë“œ {fold_num}: í›ˆë ¨ RÂ² = {train_metrics.get('r2', 0):.3f}, "
                               f"í…ŒìŠ¤íŠ¸ RÂ² = {test_metrics.get('r2', 0):.3f}")
                
            except Exception as e:
                self.logger.error(f"í´ë“œ {fold_num} ì‹¤í–‰ ì˜¤ë¥˜: {e}")
                continue
        
        # ì „ì²´ ë©”íŠ¸ë¦­ ê³„ì‚°
        overall_metrics = self._calculate_overall_metrics(train_scores, test_scores)
        
        # ê²°ê³¼ ê°ì²´ ìƒì„±
        result = ValidationResult(
            train_scores=train_scores,
            test_scores=test_scores,
            train_indices=train_indices_list,
            test_indices=test_indices_list,
            fold_metadata=fold_metadata,
            overall_metrics=overall_metrics,
            timestamp=datetime.now()
        )
        
        self.validation_results[cv_method] = result
        self.logger.info(f"âœ… {cv_method} êµì°¨ ê²€ì¦ ì™„ë£Œ")
        
        return result
    
    def _calculate_overall_metrics(self, 
                                 train_scores: List[Dict], 
                                 test_scores: List[Dict]) -> Dict[str, float]:
        """ì „ì²´ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        overall = {}
        
        if not train_scores or not test_scores:
            return overall
        
        # ë©”íŠ¸ë¦­ ì´ë¦„ ì¶”ì¶œ
        metric_names = set()
        for score_dict in train_scores + test_scores:
            metric_names.update(score_dict.keys())
        
        metric_names.discard('error')  # ì˜¤ë¥˜ ì œì™¸
        
        for metric in metric_names:
            # í›ˆë ¨ ë©”íŠ¸ë¦­
            train_values = [s.get(metric, np.nan) for s in train_scores]
            train_values = [v for v in train_values if not np.isnan(v) and v != float('inf')]
            
            if train_values:
                overall[f'train_{metric}_mean'] = np.mean(train_values)
                overall[f'train_{metric}_std'] = np.std(train_values)
            
            # í…ŒìŠ¤íŠ¸ ë©”íŠ¸ë¦­
            test_values = [s.get(metric, np.nan) for s in test_scores]
            test_values = [v for v in test_values if not np.isnan(v) and v != float('inf')]
            
            if test_values:
                overall[f'test_{metric}_mean'] = np.mean(test_values)
                overall[f'test_{metric}_std'] = np.std(test_values)
                
                # ì˜¤ë²„í”¼íŒ… ê°ì§€
                if train_values:
                    if metric in ['r2', 'direction_accuracy']:
                        # ë†’ì„ìˆ˜ë¡ ì¢‹ì€ ë©”íŠ¸ë¦­
                        overall[f'{metric}_overfitting'] = np.mean(train_values) - np.mean(test_values)
                    else:
                        # ë‚®ì„ìˆ˜ë¡ ì¢‹ì€ ë©”íŠ¸ë¦­
                        overall[f'{metric}_overfitting'] = np.mean(test_values) - np.mean(train_values)
        
        return overall
    
    def compare_validation_methods(self, 
                                 X: pd.DataFrame, 
                                 y: pd.Series,
                                 model: BaseEstimator) -> Dict[str, ValidationResult]:
        """
        ì—¬ëŸ¬ ê²€ì¦ ë°©ë²• ë¹„êµ ì‹¤í–‰
        
        Args:
            X: íŠ¹ì„± ë°ì´í„°
            y: íƒ€ê²Ÿ ë°ì´í„°
            model: ëª¨ë¸ ê°ì²´
            
        Returns:
            ë°©ë²•ë³„ ê²€ì¦ ê²°ê³¼
        """
        self.logger.info("ğŸ”¬ ì—¬ëŸ¬ ê²€ì¦ ë°©ë²• ë¹„êµ ì‹¤í–‰...")
        
        results = {}
        
        for method_name in self.validators.keys():
            try:
                self.logger.info(f"ğŸ“Š {method_name} ì‹¤í–‰ ì¤‘...")
                result = self.run_cross_validation(X, y, model, method_name)
                results[method_name] = result
            except Exception as e:
                self.logger.error(f"{method_name} ì‹¤í–‰ ì˜¤ë¥˜: {e}")
                continue
        
        self.logger.info(f"âœ… {len(results)}ê°œ ë°©ë²• ë¹„êµ ì™„ë£Œ")
        return results
    
    def out_of_sample_test(self, 
                         X: pd.DataFrame, 
                         y: pd.Series,
                         model: BaseEstimator) -> Dict[str, float]:
        """
        ìƒ˜í”Œ ì™¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        
        Args:
            X: ì „ì²´ íŠ¹ì„± ë°ì´í„°
            y: ì „ì²´ íƒ€ê²Ÿ ë°ì´í„°
            model: ëª¨ë¸ ê°ì²´
            
        Returns:
            OOS í…ŒìŠ¤íŠ¸ ê²°ê³¼
        """
        self.logger.info("ğŸ¯ ìƒ˜í”Œ ì™¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰...")
        
        # ë°ì´í„° ë¶„í• 
        combined_data = pd.concat([X, y], axis=1)
        train_data, val_data, test_data = self.oos_protocol.split_temporal_holdout(combined_data)
        
        # ë¶„í•  í›„ X, y ì¬êµ¬ì„±
        X_train = train_data.iloc[:, :-1]
        y_train = train_data.iloc[:, -1]
        X_val = val_data.iloc[:, :-1]
        y_val = val_data.iloc[:, -1]
        X_test = test_data.iloc[:, :-1]
        y_test = test_data.iloc[:, -1]
        
        # ëª¨ë¸ í›ˆë ¨ (í›ˆë ¨ + ê²€ì¦ ë°ì´í„°)
        X_train_val = pd.concat([X_train, X_val])
        y_train_val = pd.concat([y_train, y_val])
        
        model.fit(X_train_val, y_train_val)
        
        # OOS ì˜ˆì¸¡
        y_test_pred = model.predict(X_test)
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        oos_metrics = self.calculate_metrics(y_test.values, y_test_pred)
        
        self.logger.info(f"âœ… OOS í…ŒìŠ¤íŠ¸ ì™„ë£Œ - RÂ²: {oos_metrics.get('r2', 0):.3f}")
        
        return oos_metrics
    
    def visualize_cv_results(self, 
                           results: Dict[str, ValidationResult],
                           save_path: str = None) -> str:
        """
        êµì°¨ ê²€ì¦ ê²°ê³¼ ì‹œê°í™”
        
        Args:
            results: ê²€ì¦ ê²°ê³¼ë“¤
            save_path: ì €ì¥ ê²½ë¡œ
            
        Returns:
            HTML ì°¨íŠ¸ ë¬¸ìì—´
        """
        if not results:
            return "<p>ì‹œê°í™”í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.</p>"
        
        # Plotly ì„œë¸Œí”Œë¡¯ ìƒì„±
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=('RÂ² ì ìˆ˜', 'ë°©í–¥ì„± ì •í™•ë„', 'RMSE', 'MAE'),
            specs=[[{"secondary_y": False}, {"secondary_y": False}],
                   [{"secondary_y": False}, {"secondary_y": False}]]
        )
        
        colors = ['blue', 'red', 'green', 'orange', 'purple']
        
        for i, (method, result) in enumerate(results.items()):
            color = colors[i % len(colors)]
            
            # RÂ² ì ìˆ˜
            test_r2_scores = [s.get('r2', 0) for s in result.test_scores]
            fig.add_trace(
                go.Box(y=test_r2_scores, name=f'{method} RÂ²', 
                      marker_color=color, boxpoints='all'),
                row=1, col=1
            )
            
            # ë°©í–¥ì„± ì •í™•ë„
            direction_scores = [s.get('direction_accuracy', 0) for s in result.test_scores]
            fig.add_trace(
                go.Box(y=direction_scores, name=f'{method} ë°©í–¥ì„±', 
                      marker_color=color, boxpoints='all'),
                row=1, col=2
            )
            
            # RMSE
            rmse_scores = [s.get('rmse', 0) for s in result.test_scores]
            fig.add_trace(
                go.Box(y=rmse_scores, name=f'{method} RMSE', 
                      marker_color=color, boxpoints='all'),
                row=2, col=1
            )
            
            # MAE
            mae_scores = [s.get('mae', 0) for s in result.test_scores]
            fig.add_trace(
                go.Box(y=mae_scores, name=f'{method} MAE', 
                      marker_color=color, boxpoints='all'),
                row=2, col=2
            )
        
        fig.update_layout(
            title="êµì°¨ ê²€ì¦ ë°©ë²• ë¹„êµ",
            height=800,
            showlegend=True
        )
        
        # HTML ìƒì„±
        html_str = fig.to_html(include_plotlyjs='cdn')
        
        if save_path:
            with open(save_path, 'w', encoding='utf-8') as f:
                f.write(html_str)
            self.logger.info(f"ì‹œê°í™” ê²°ê³¼ ì €ì¥: {save_path}")
        
        return html_str
    
    def generate_validation_report(self, 
                                 results: Dict[str, ValidationResult],
                                 oos_results: Dict[str, float] = None) -> str:
        """
        ê²€ì¦ ë³´ê³ ì„œ ìƒì„±
        
        Args:
            results: ê²€ì¦ ê²°ê³¼ë“¤
            oos_results: OOS í…ŒìŠ¤íŠ¸ ê²°ê³¼
            
        Returns:
            HTML ë³´ê³ ì„œ
        """
        html_report = """
        <!DOCTYPE html>
        <html>
        <head>
            <title>ê³ ê¸‰ êµì°¨ ê²€ì¦ ë³´ê³ ì„œ</title>
            <meta charset="utf-8">
            <style>
                body { font-family: Arial, sans-serif; margin: 20px; }
                .header { background: #2c3e50; color: white; padding: 20px; border-radius: 5px; }
                .section { margin: 20px 0; padding: 15px; border: 1px solid #ddd; border-radius: 5px; }
                .metric { display: inline-block; margin: 10px; padding: 10px; background: #f8f9fa; border-radius: 3px; }
                .good { background: #d4edda; color: #155724; }
                .medium { background: #fff3cd; color: #856404; }
                .poor { background: #f8d7da; color: #721c24; }
                table { width: 100%; border-collapse: collapse; margin: 10px 0; }
                th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
                th { background: #f2f2f2; }
            </style>
        </head>
        <body>
            <div class="header">
                <h1>ğŸ”¬ ê³ ê¸‰ êµì°¨ ê²€ì¦ ì‹œìŠ¤í…œ ë³´ê³ ì„œ</h1>
                <p>ìƒì„± ì‹œê°„: {timestamp}</p>
            </div>
            
            <div class="section">
                <h2>ğŸ“Š ê²€ì¦ ë°©ë²• ë¹„êµ ìš”ì•½</h2>
                <table>
                    <tr>
                        <th>ê²€ì¦ ë°©ë²•</th>
                        <th>í‰ê·  RÂ²</th>
                        <th>í‰ê·  RMSE</th>
                        <th>ë°©í–¥ì„± ì •í™•ë„</th>
                        <th>ì˜¤ë²„í”¼íŒ… ì •ë„</th>
                        <th>ì•ˆì •ì„±</th>
                    </tr>
                    {method_comparison_rows}
                </table>
            </div>
            
            <div class="section">
                <h2>ğŸ¯ ìƒ˜í”Œ ì™¸ í…ŒìŠ¤íŠ¸ ê²°ê³¼</h2>
                {oos_results_section}
            </div>
            
            <div class="section">
                <h2>ğŸ“ˆ ìƒì„¸ ë¶„ì„</h2>
                {detailed_analysis}
            </div>
            
            <div class="section">
                <h2>âœ… ê¶Œì¥ì‚¬í•­</h2>
                <ul>
                    {recommendations}
                </ul>
            </div>
        </body>
        </html>
        """.format(
            timestamp=datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            method_comparison_rows=self._generate_method_comparison_rows(results),
            oos_results_section=self._generate_oos_section(oos_results),
            detailed_analysis=self._generate_detailed_analysis(results),
            recommendations=self._generate_recommendations(results, oos_results)
        )
        
        return html_report
    
    def _generate_method_comparison_rows(self, results: Dict[str, ValidationResult]) -> str:
        """ë°©ë²• ë¹„êµ í…Œì´ë¸” í–‰ ìƒì„±"""
        rows = []
        
        for method, result in results.items():
            overall = result.overall_metrics
            
            r2_mean = overall.get('test_r2_mean', 0)
            rmse_mean = overall.get('test_rmse_mean', 0)
            direction_mean = overall.get('test_direction_accuracy_mean', 0)
            overfitting = overall.get('r2_overfitting', 0)
            r2_std = overall.get('test_r2_std', 0)
            
            row = f"""
            <tr>
                <td>{method}</td>
                <td>{r2_mean:.3f}</td>
                <td>{rmse_mean:.2f}</td>
                <td>{direction_mean:.3f}</td>
                <td>{overfitting:.3f}</td>
                <td>{r2_std:.3f}</td>
            </tr>
            """
            rows.append(row)
        
        return "".join(rows)
    
    def _generate_oos_section(self, oos_results: Dict[str, float] = None) -> str:
        """OOS ê²°ê³¼ ì„¹ì…˜ ìƒì„±"""
        if not oos_results:
            return "<p>OOS í…ŒìŠ¤íŠ¸ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.</p>"
        
        r2 = oos_results.get('r2', 0)
        rmse = oos_results.get('rmse', 0)
        direction = oos_results.get('direction_accuracy', 0)
        mae = oos_results.get('mae', 0)
        
        quality_class = 'good' if r2 > 0.7 else 'medium' if r2 > 0.5 else 'poor'
        
        return f"""
        <div class="metric {quality_class}">RÂ²: {r2:.3f}</div>
        <div class="metric">RMSE: {rmse:.2f}</div>
        <div class="metric">MAE: {mae:.2f}</div>
        <div class="metric">ë°©í–¥ì„± ì •í™•ë„: {direction:.3f}</div>
        """
    
    def _generate_detailed_analysis(self, results: Dict[str, ValidationResult]) -> str:
        """ìƒì„¸ ë¶„ì„ ì„¹ì…˜ ìƒì„±"""
        analysis = []
        
        if not results:
            return "<p>ë¶„ì„í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.</p>"
        
        best_method = max(results.keys(), 
                         key=lambda k: results[k].overall_metrics.get('test_r2_mean', 0))
        
        best_r2 = results[best_method].overall_metrics.get('test_r2_mean', 0)
        
        analysis.append(f"<p><strong>ìµœê³  ì„±ëŠ¥ ë°©ë²•:</strong> {best_method} (RÂ² = {best_r2:.3f})</p>")
        
        # ì•ˆì •ì„± ë¶„ì„
        stability_analysis = []
        for method, result in results.items():
            std = result.overall_metrics.get('test_r2_std', float('inf'))
            stability_analysis.append((method, std))
        
        most_stable = min(stability_analysis, key=lambda x: x[1])
        analysis.append(f"<p><strong>ê°€ì¥ ì•ˆì •ì :</strong> {most_stable[0]} (í‘œì¤€í¸ì°¨ = {most_stable[1]:.3f})</p>")
        
        return "".join(analysis)
    
    def _generate_recommendations(self, 
                                results: Dict[str, ValidationResult],
                                oos_results: Dict[str, float] = None) -> str:
        """ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        if not results:
            return "<li>ë¶„ì„í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.</li>"
        
        # ì„±ëŠ¥ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        best_method = max(results.keys(), 
                         key=lambda k: results[k].overall_metrics.get('test_r2_mean', 0))
        
        recommendations.append(f"<li><strong>ì„±ëŠ¥ ì¤‘ì‹¬:</strong> {best_method} ë°©ë²• ì‚¬ìš© ê¶Œì¥</li>")
        
        # ì•ˆì •ì„± ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        stability_analysis = [(method, result.overall_metrics.get('test_r2_std', float('inf'))) 
                            for method, result in results.items()]
        most_stable = min(stability_analysis, key=lambda x: x[1])
        
        recommendations.append(f"<li><strong>ì•ˆì •ì„± ì¤‘ì‹¬:</strong> {most_stable[0]} ë°©ë²• ê³ ë ¤</li>")
        
        # ì¼ë°˜ì ì¸ ê¶Œì¥ì‚¬í•­
        recommendations.append("<li>ì‹¤ì œ ìš´ìš©ì—ì„œëŠ” ì›Œí¬í¬ì›Œë“œ ê²€ì¦ì´ ê°€ì¥ í˜„ì‹¤ì </li>")
        recommendations.append("<li>ë°ì´í„° ìœ ì¶œ ë°©ì§€ë¥¼ ìœ„í•´ í¼ì§€ë“œ ë°©ë²• ê³ ë ¤</li>")
        recommendations.append("<li>ëª¨ë¸ ë³µì¡ë„ì— ë”°ë¼ ì ì ˆí•œ ê²€ì¦ ë°©ë²• ì„ íƒ</li>")
        
        return "".join(recommendations)
    
    def save_results(self, output_dir: str = "cv_results") -> None:
        """ê²°ê³¼ ì €ì¥"""
        os.makedirs(output_dir, exist_ok=True)
        
        # ê²€ì¦ ê²°ê³¼ ì €ì¥
        results_file = os.path.join(output_dir, "validation_results.json")
        serializable_results = {}
        
        for method, result in self.validation_results.items():
            serializable_results[method] = {
                'overall_metrics': result.overall_metrics,
                'fold_metadata': result.fold_metadata,
                'timestamp': result.timestamp.isoformat()
            }
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(serializable_results, f, indent=2, ensure_ascii=False, default=str)
        
        self.logger.info(f"âœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_dir}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ”¬ ê³ ê¸‰ êµì°¨ ê²€ì¦ ì‹œìŠ¤í…œ ì‹œì‘")
    
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    cv_system = AdvancedCrossValidationSystem()
    
    # ì˜ˆì œ ë°ì´í„° ìƒì„±
    print("\nğŸ“Š ì˜ˆì œ ë°ì´í„° ìƒì„±...")
    np.random.seed(42)
    
    # ì‹œê³„ì—´ íŠ¹ì„±ì„ ê°€ì§„ ë°ì´í„° ìƒì„±
    n_samples = 1000
    n_features = 5
    
    # íŠ¹ì„± ë°ì´í„° (ìê¸°ìƒê´€ ìˆëŠ” ì‹œê³„ì—´)
    X_data = []
    for i in range(n_features):
        series = np.cumsum(np.random.randn(n_samples) * 0.1)
        X_data.append(series)
    
    X = pd.DataFrame(np.column_stack(X_data), 
                    columns=[f'feature_{i}' for i in range(n_features)])
    X.index = pd.date_range('2024-01-01', periods=n_samples, freq='H')
    
    # íƒ€ê²Ÿ ë°ì´í„° (íŠ¹ì„±ë“¤ê³¼ ì•½ê°„ì˜ ê´€ê³„)
    y = (X.sum(axis=1) + np.random.randn(n_samples) * 0.5)
    
    print(f"ë°ì´í„° í¬ê¸°: X {X.shape}, y {y.shape}")
    
    # ëª¨ë¸ ì •ì˜
    model = RandomForestRegressor(n_estimators=10, random_state=42)  # í…ŒìŠ¤íŠ¸ìš© ì‘ì€ ëª¨ë¸
    
    # ì—¬ëŸ¬ ê²€ì¦ ë°©ë²• ë¹„êµ
    print("\nğŸ”¬ ì—¬ëŸ¬ ê²€ì¦ ë°©ë²• ë¹„êµ...")
    comparison_results = cv_system.compare_validation_methods(X, y, model)
    
    # OOS í…ŒìŠ¤íŠ¸
    print("\nğŸ¯ ìƒ˜í”Œ ì™¸ í…ŒìŠ¤íŠ¸...")
    oos_results = cv_system.out_of_sample_test(X, y, model)
    
    # ê²°ê³¼ ì‹œê°í™”
    print("\nğŸ“ˆ ê²°ê³¼ ì‹œê°í™”...")
    chart_html = cv_system.visualize_cv_results(comparison_results, "cv_comparison_chart.html")
    
    # ë³´ê³ ì„œ ìƒì„±
    print("\nğŸ“‹ ë³´ê³ ì„œ ìƒì„±...")
    report_html = cv_system.generate_validation_report(comparison_results, oos_results)
    
    with open("advanced_cv_report.html", "w", encoding="utf-8") as f:
        f.write(report_html)
    
    # ê²°ê³¼ ì €ì¥
    print("\nğŸ’¾ ê²°ê³¼ ì €ì¥...")
    cv_system.save_results()
    
    print("\nâœ… ê³ ê¸‰ êµì°¨ ê²€ì¦ ì‹œìŠ¤í…œ ì™„ë£Œ!")
    print("ğŸ“‹ ìƒì„¸ ë³´ê³ ì„œ: advanced_cv_report.html")
    print("ğŸ“Š ë¹„êµ ì°¨íŠ¸: cv_comparison_chart.html")


if __name__ == "__main__":
    main()