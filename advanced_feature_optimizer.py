#!/usr/bin/env python3
"""
âš¡ ê³ ë„í™”ëœ íŠ¹ì„± ìµœì í™” ì‹œìŠ¤í…œ
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ¯ í•µì‹¬ ê¸°ëŠ¥:
â€¢ íŠ¹ì„± ì¤‘ìš”ë„ ìë™ í•™ìŠµ ë° ìˆœìœ„
â€¢ ì‹¤ì‹œê°„ íŠ¹ì„± ì„ íƒ ìµœì í™”
â€¢ ë‹¤ì¤‘ ëª¨ë¸ ê¸°ë°˜ íŠ¹ì„± í‰ê°€
â€¢ ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ê³„ì‚°
â€¢ ë³‘ë ¬ ì²˜ë¦¬ ê°€ì†í™”

ğŸ”§ ìµœì í™” ê¸°ë²•:
â€¢ Mutual Information ê¸°ë°˜ ì„ íƒ
â€¢ Recursive Feature Elimination
â€¢ SHAP ê°’ ê¸°ë°˜ ì¤‘ìš”ë„
â€¢ ìƒê´€ê´€ê³„ ì œê±°
â€¢ ì‹œê³„ì—´ ì•ˆì •ì„± ê²€ì¦
"""

import asyncio
import pandas as pd
import numpy as np
import json
import sqlite3
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Any
from pathlib import Path
import logging
import warnings
from concurrent.futures import ProcessPoolExecutor
import joblib

# ê¸°ê³„í•™ìŠµ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from sklearn.feature_selection import (
        SelectKBest, SelectPercentile, RFE, RFECV, 
        f_regression, mutual_info_regression, chi2
    )
    from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
    from sklearn.preprocessing import StandardScaler, RobustScaler
    from sklearn.decomposition import PCA, FastICA
    from sklearn.manifold import TSNE
    from sklearn.model_selection import cross_val_score, TimeSeriesSplit
    from sklearn.metrics import mean_squared_error, r2_score
    import shap
    SKLEARN_AVAILABLE = True
except ImportError:
    print("âš ï¸ scikit-learn ë˜ëŠ” shap ë¯¸ì„¤ì¹˜")
    SKLEARN_AVAILABLE = False

warnings.filterwarnings('ignore')

class AdvancedFeatureOptimizer:
    """ê³ ë„í™”ëœ íŠ¹ì„± ìµœì í™” ì‹œìŠ¤í…œ"""
    
    def __init__(self, n_features_target: int = 1000):
        self.n_features_target = n_features_target
        self.logger = self._setup_logger()
        self.db_path = "feature_optimization.db"
        
        # ëª¨ë¸ ì•™ìƒë¸”
        self.models = {
            'rf': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),
            'gbm': GradientBoostingRegressor(n_estimators=100, random_state=42),
        } if SKLEARN_AVAILABLE else {}
        
        self.feature_scores_cache = {}
        self.stability_cache = {}
        
        self._init_database()
    
    def _setup_logger(self) -> logging.Logger:
        """ë¡œê¹… ì„¤ì •"""
        logger = logging.getLogger(f"{__name__}.FeatureOptimizer")
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    def _init_database(self):
        """ìµœì í™” ê²°ê³¼ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # íŠ¹ì„± ì ìˆ˜ í…Œì´ë¸”
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS feature_scores (
            feature_name TEXT PRIMARY KEY,
            mutual_info_score REAL,
            f_score REAL,
            shap_importance REAL,
            stability_score REAL,
            correlation_penalty REAL,
            final_score REAL,
            rank INTEGER,
            last_updated TIMESTAMP
        )
        ''')
        
        # ìµœì í™” íˆìŠ¤í† ë¦¬
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS optimization_history (
            timestamp TIMESTAMP,
            n_features_before INTEGER,
            n_features_after INTEGER,
            optimization_method TEXT,
            performance_score REAL,
            execution_time REAL
        )
        ''')
        
        conn.commit()
        conn.close()
    
    async def optimize_features(self, features_df: pd.DataFrame, 
                              target: Optional[np.ndarray] = None,
                              method: str = 'comprehensive') -> pd.DataFrame:
        """í¬ê´„ì  íŠ¹ì„± ìµœì í™”"""
        
        self.logger.info(f"ğŸš€ íŠ¹ì„± ìµœì í™” ì‹œì‘: {len(features_df.columns)}ê°œ â†’ {self.n_features_target}ê°œ ëª©í‘œ")
        start_time = datetime.now()
        
        # 1ë‹¨ê³„: ê¸°ë³¸ ì •ë¦¬ (NaN, ìƒìˆ˜, ì¤‘ë³µ)
        features_clean = await self._basic_cleanup(features_df)
        self.logger.info(f"âœ… ê¸°ë³¸ ì •ë¦¬: {len(features_clean.columns)}ê°œ íŠ¹ì„± ìœ ì§€")
        
        # 2ë‹¨ê³„: ëª©í‘œ ë³€ìˆ˜ ìƒì„± (ì œê³µë˜ì§€ ì•Šì€ ê²½ìš°)
        if target is None:
            target = await self._generate_synthetic_target(features_clean)
        
        # 3ë‹¨ê³„: ë‹¤ì¤‘ ê¸°ë²• ì ìš©
        if method == 'comprehensive':
            optimized_features = await self._comprehensive_optimization(features_clean, target)
        elif method == 'fast':
            optimized_features = await self._fast_optimization(features_clean, target)
        elif method == 'stability_focused':
            optimized_features = await self._stability_optimization(features_clean, target)
        else:
            optimized_features = await self._mutual_info_optimization(features_clean, target)
        
        # 4ë‹¨ê³„: ìµœì¢… ê²€ì¦
        final_features = await self._final_validation(optimized_features, target)
        
        # ê²°ê³¼ ì €ì¥
        execution_time = (datetime.now() - start_time).total_seconds()
        await self._save_optimization_result(
            len(features_df.columns), len(final_features.columns),
            method, execution_time
        )
        
        self.logger.info(f"ğŸ¯ ìµœì í™” ì™„ë£Œ: {len(final_features.columns)}ê°œ íŠ¹ì„±, {execution_time:.2f}ì´ˆ")
        
        return final_features
    
    async def _basic_cleanup(self, df: pd.DataFrame) -> pd.DataFrame:
        """ê¸°ë³¸ ì •ë¦¬: NaN, ìƒìˆ˜, ì¤‘ë³µ ì œê±°"""
        
        # 1. NaN ë¹„ìœ¨ì´ ë†’ì€ íŠ¹ì„± ì œê±° (50% ì´ìƒ)
        nan_threshold = 0.5
        nan_ratios = df.isnull().sum() / len(df)
        valid_features = nan_ratios[nan_ratios <= nan_threshold].index
        df = df[valid_features]
        
        # 2. ìƒìˆ˜ íŠ¹ì„± ì œê±° (ë¶„ì‚°ì´ ë§¤ìš° ë‚®ì€ íŠ¹ì„±)
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            variances = df[numeric_cols].var()
            non_constant = variances[variances > 1e-8].index
            df = df[non_constant]
        
        # 3. ë¬´í•œê°’ ì²˜ë¦¬
        df = df.replace([np.inf, -np.inf], np.nan)
        df = df.fillna(df.median())
        
        # 4. ì™„ì „ ì¤‘ë³µ íŠ¹ì„± ì œê±°
        df = df.T.drop_duplicates().T
        
        return df
    
    async def _generate_synthetic_target(self, features_df: pd.DataFrame) -> np.ndarray:
        """í•©ì„± ëª©í‘œ ë³€ìˆ˜ ìƒì„± (ì‹¤ì œë¡œëŠ” ê°€ê²© ë³€í™”ìœ¨ ì‚¬ìš©)"""
        # ê°€ê²© ê´€ë ¨ íŠ¹ì„± ì°¾ê¸°
        price_features = [col for col in features_df.columns 
                         if any(keyword in col.lower() for keyword in ['price', 'btc', 'close'])]
        
        if price_features:
            # ì£¼ìš” ê°€ê²© íŠ¹ì„±ìœ¼ë¡œ ëª©í‘œ ë³€ìˆ˜ ìƒì„±
            main_price = features_df[price_features[0]].values
            # ê°„ë‹¨í•œ ë…¸ì´ì¦ˆê°€ ìˆëŠ” ë¯¸ë˜ ê°€ê²© ì‹œë®¬ë ˆì´ì…˜
            target = main_price * (1 + np.random.normal(0, 0.02, len(main_price)))
            return (target - main_price) / main_price  # ë³€í™”ìœ¨
        else:
            # ê¸°ë³¸ ë…¸ì´ì¦ˆ ëª©í‘œ ë³€ìˆ˜
            return np.random.randn(len(features_df))
    
    async def _comprehensive_optimization(self, features_df: pd.DataFrame, 
                                        target: np.ndarray) -> pd.DataFrame:
        """í¬ê´„ì  ìµœì í™” (ëª¨ë“  ê¸°ë²• ì ìš©)"""
        
        # 1. Mutual Information ê¸°ë°˜ ì´ˆê¸° ì„ íƒ
        features_mi = await self._select_by_mutual_info(features_df, target, top_k=2000)
        
        # 2. ìƒê´€ê´€ê³„ ì œê±°
        features_uncorr = await self._remove_high_correlation(features_mi, threshold=0.95)
        
        # 3. ì•ˆì •ì„± ê¸°ë°˜ í•„í„°ë§
        features_stable = await self._filter_by_stability(features_uncorr, target)
        
        # 4. ëª¨ë¸ ê¸°ë°˜ ì¤‘ìš”ë„
        features_model = await self._model_based_selection(features_stable, target)
        
        # 5. SHAP ê¸°ë°˜ ìµœì¢… ì„ íƒ
        if SKLEARN_AVAILABLE and len(features_model.columns) > self.n_features_target:
            features_final = await self._shap_based_selection(features_model, target)
        else:
            features_final = features_model
        
        return features_final
    
    async def _fast_optimization(self, features_df: pd.DataFrame, 
                               target: np.ndarray) -> pd.DataFrame:
        """ë¹ ë¥¸ ìµœì í™” (ê³„ì‚°ëŸ‰ ìµœì†Œí™”)"""
        
        # F-ì ìˆ˜ ê¸°ë°˜ ë¹ ë¥¸ ì„ íƒ
        if SKLEARN_AVAILABLE:
            selector = SelectKBest(score_func=f_regression, k=min(self.n_features_target, len(features_df.columns)))
            
            try:
                features_selected = selector.fit_transform(features_df.fillna(0), target)
                selected_columns = selector.get_feature_names_out()
                return pd.DataFrame(features_selected, columns=selected_columns)
            except Exception as e:
                self.logger.warning(f"F-ì ìˆ˜ ì„ íƒ ì‹¤íŒ¨: {e}")
        
        # ëŒ€ì•ˆ: ë¶„ì‚° ê¸°ë°˜ ì„ íƒ
        variances = features_df.var()
        top_variance = variances.nlargest(self.n_features_target).index
        return features_df[top_variance]
    
    async def _stability_optimization(self, features_df: pd.DataFrame,
                                    target: np.ndarray) -> pd.DataFrame:
        """ì•ˆì •ì„± ì¤‘ì‹¬ ìµœì í™”"""
        
        # ì‹œê³„ì—´ ë¶„í• ë¡œ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸
        stable_features = []
        
        if len(features_df) > 10:
            # ì‹œê³„ì—´ ë¶„í• 
            n_splits = min(5, len(features_df) // 2)
            split_size = len(features_df) // n_splits
            
            feature_stability = {}
            
            for feature in features_df.columns:
                stability_scores = []
                feature_values = features_df[feature].fillna(feature_values.median())
                
                # ê° ë¶„í• ì—ì„œ í‰ê· ê³¼ ë¶„ì‚° ê³„ì‚°
                for i in range(n_splits - 1):
                    start_idx = i * split_size
                    end_idx = (i + 1) * split_size
                    
                    segment1 = feature_values.iloc[start_idx:end_idx]
                    segment2 = feature_values.iloc[end_idx:end_idx + split_size]
                    
                    if len(segment1) > 0 and len(segment2) > 0:
                        # í‰ê· ì˜ ì•ˆì •ì„±
                        mean_diff = abs(segment1.mean() - segment2.mean())
                        mean_stability = 1 / (1 + mean_diff)
                        
                        # ë¶„ì‚°ì˜ ì•ˆì •ì„±  
                        std_ratio = min(segment1.std(), segment2.std()) / max(segment1.std(), segment2.std())
                        std_stability = std_ratio if not np.isnan(std_ratio) else 0
                        
                        combined_stability = (mean_stability + std_stability) / 2
                        stability_scores.append(combined_stability)
                
                feature_stability[feature] = np.mean(stability_scores) if stability_scores else 0
            
            # ìƒìœ„ ì•ˆì •ì„± íŠ¹ì„± ì„ íƒ
            sorted_features = sorted(feature_stability.items(), key=lambda x: x[1], reverse=True)
            top_features = [f[0] for f in sorted_features[:self.n_features_target]]
            
            return features_df[top_features]
        
        return features_df.iloc[:, :self.n_features_target]
    
    async def _mutual_info_optimization(self, features_df: pd.DataFrame,
                                      target: np.ndarray) -> pd.DataFrame:
        """ìƒí˜¸ ì •ë³´ëŸ‰ ê¸°ë°˜ ìµœì í™”"""
        
        if not SKLEARN_AVAILABLE:
            return features_df.iloc[:, :self.n_features_target]
        
        try:
            # ìƒí˜¸ ì •ë³´ëŸ‰ ê³„ì‚°
            features_array = features_df.fillna(0).values
            
            # ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í–¥ìƒ
            mi_scores = {}
            batch_size = 100
            
            for i in range(0, len(features_df.columns), batch_size):
                batch_end = min(i + batch_size, len(features_df.columns))
                batch_features = features_array[:, i:batch_end]
                batch_columns = features_df.columns[i:batch_end]
                
                batch_scores = mutual_info_regression(batch_features, target, random_state=42)
                
                for j, col in enumerate(batch_columns):
                    mi_scores[col] = batch_scores[j]
            
            # ìƒìœ„ ì ìˆ˜ íŠ¹ì„± ì„ íƒ
            sorted_features = sorted(mi_scores.items(), key=lambda x: x[1], reverse=True)
            top_features = [f[0] for f in sorted_features[:self.n_features_target]]
            
            return features_df[top_features]
            
        except Exception as e:
            self.logger.error(f"ìƒí˜¸ ì •ë³´ëŸ‰ ìµœì í™” ì‹¤íŒ¨: {e}")
            return features_df.iloc[:, :self.n_features_target]
    
    async def _select_by_mutual_info(self, features_df: pd.DataFrame, 
                                   target: np.ndarray, top_k: int) -> pd.DataFrame:
        """ìƒí˜¸ ì •ë³´ëŸ‰ ê¸°ë°˜ íŠ¹ì„± ì„ íƒ"""
        
        if not SKLEARN_AVAILABLE:
            return features_df.iloc[:, :top_k]
        
        try:
            selector = SelectKBest(score_func=mutual_info_regression, k=min(top_k, len(features_df.columns)))
            features_selected = selector.fit_transform(features_df.fillna(0), target)
            selected_columns = selector.get_feature_names_out()
            
            return pd.DataFrame(features_selected, columns=selected_columns, index=features_df.index)
            
        except Exception as e:
            self.logger.warning(f"ìƒí˜¸ ì •ë³´ëŸ‰ ì„ íƒ ì‹¤íŒ¨: {e}")
            return features_df.iloc[:, :top_k]
    
    async def _remove_high_correlation(self, features_df: pd.DataFrame, 
                                     threshold: float = 0.95) -> pd.DataFrame:
        """ë†’ì€ ìƒê´€ê´€ê³„ íŠ¹ì„± ì œê±°"""
        
        # ìƒê´€ê´€ê³„ í–‰ë ¬ ê³„ì‚°
        corr_matrix = features_df.corr().abs()
        
        # ìƒì‚¼ê°í–‰ë ¬ì—ì„œ ë†’ì€ ìƒê´€ê´€ê³„ ìŒ ì°¾ê¸°
        upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
        
        # ë†’ì€ ìƒê´€ê´€ê³„ë¥¼ ê°€ì§„ íŠ¹ì„± ì œê±°
        to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column] > threshold)]
        
        features_filtered = features_df.drop(columns=to_drop)
        
        self.logger.info(f"ğŸ”„ ìƒê´€ê´€ê³„ í•„í„°ë§: {len(to_drop)}ê°œ íŠ¹ì„± ì œê±°")
        
        return features_filtered
    
    async def _filter_by_stability(self, features_df: pd.DataFrame,
                                 target: np.ndarray) -> pd.DataFrame:
        """ì•ˆì •ì„± ê¸°ë°˜ í•„í„°ë§"""
        
        if len(features_df) < 20:
            return features_df
        
        stable_features = []
        
        for feature in features_df.columns:
            feature_values = features_df[feature].fillna(features_df[feature].median())
            
            # ì´ë™ í†µê³„ì˜ ì•ˆì •ì„± ì²´í¬
            window_size = max(5, len(feature_values) // 10)
            rolling_mean = feature_values.rolling(window=window_size).mean()
            rolling_std = feature_values.rolling(window=window_size).std()
            
            # ì•ˆì •ì„± ì ìˆ˜ (ë³€ë™ê³„ìˆ˜ì˜ ì—­ìˆ˜)
            if rolling_std.mean() > 0 and rolling_mean.mean() > 0:
                cv = rolling_std.mean() / abs(rolling_mean.mean())
                stability_score = 1 / (1 + cv)
            else:
                stability_score = 0
            
            if stability_score > 0.3:  # ì„ê³„ê°’
                stable_features.append(feature)
        
        if len(stable_features) > 0:
            return features_df[stable_features]
        else:
            return features_df
    
    async def _model_based_selection(self, features_df: pd.DataFrame,
                                   target: np.ndarray) -> pd.DataFrame:
        """ëª¨ë¸ ê¸°ë°˜ íŠ¹ì„± ì„ íƒ"""
        
        if not SKLEARN_AVAILABLE or len(features_df.columns) == 0:
            return features_df
        
        try:
            # Random Forest ì¤‘ìš”ë„
            rf_model = RandomForestRegressor(n_estimators=50, random_state=42, n_jobs=-1)
            rf_model.fit(features_df.fillna(0), target)
            
            # ì¤‘ìš”ë„ ì ìˆ˜
            importances = rf_model.feature_importances_
            feature_importance = pd.Series(importances, index=features_df.columns)
            
            # ìƒìœ„ ì¤‘ìš”ë„ íŠ¹ì„± ì„ íƒ
            n_select = min(self.n_features_target, len(features_df.columns))
            top_features = feature_importance.nlargest(n_select).index
            
            return features_df[top_features]
            
        except Exception as e:
            self.logger.warning(f"ëª¨ë¸ ê¸°ë°˜ ì„ íƒ ì‹¤íŒ¨: {e}")
            return features_df.iloc[:, :min(self.n_features_target, len(features_df.columns))]
    
    async def _shap_based_selection(self, features_df: pd.DataFrame,
                                  target: np.ndarray) -> pd.DataFrame:
        """SHAP ê°’ ê¸°ë°˜ íŠ¹ì„± ì„ íƒ"""
        
        try:
            # SHAP explainer ìƒì„± (Random Forest ê¸°ë°˜)
            model = RandomForestRegressor(n_estimators=50, random_state=42)
            model.fit(features_df.fillna(0), target)
            
            explainer = shap.TreeExplainer(model)
            shap_values = explainer.shap_values(features_df.fillna(0))
            
            # SHAP ì¤‘ìš”ë„ ê³„ì‚° (ì ˆëŒ“ê°’ì˜ í‰ê· )
            shap_importance = pd.Series(
                np.abs(shap_values).mean(axis=0),
                index=features_df.columns
            )
            
            # ìƒìœ„ SHAP ì¤‘ìš”ë„ íŠ¹ì„± ì„ íƒ
            top_features = shap_importance.nlargest(self.n_features_target).index
            
            return features_df[top_features]
            
        except Exception as e:
            self.logger.warning(f"SHAP ì„ íƒ ì‹¤íŒ¨: {e}")
            return features_df.iloc[:, :self.n_features_target]
    
    async def _final_validation(self, features_df: pd.DataFrame,
                              target: np.ndarray) -> pd.DataFrame:
        """ìµœì¢… ê²€ì¦ ë° í’ˆì§ˆ í™•ì¸"""
        
        # 1. ìµœì¢… í¬ê¸° ì¡°ì •
        if len(features_df.columns) > self.n_features_target:
            # ë¶„ì‚° ê¸°ë°˜ ë§ˆì§€ë§‰ ì„ íƒ
            variances = features_df.var()
            top_variance = variances.nlargest(self.n_features_target).index
            features_df = features_df[top_variance]
        
        # 2. ë°ì´í„° í’ˆì§ˆ ìµœì¢… í™•ì¸
        final_features = features_df.copy()
        
        # NaN ì²˜ë¦¬
        final_features = final_features.fillna(final_features.median())
        
        # ë¬´í•œê°’ ì²˜ë¦¬
        final_features = final_features.replace([np.inf, -np.inf], 0)
        
        # 3. íŠ¹ì„± ì ìˆ˜ ì €ì¥
        await self._save_feature_scores(final_features, target)
        
        return final_features
    
    async def _save_feature_scores(self, features_df: pd.DataFrame, target: np.ndarray):
        """íŠ¹ì„± ì ìˆ˜ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥"""
        
        if not SKLEARN_AVAILABLE:
            return
        
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # ìƒí˜¸ ì •ë³´ëŸ‰ ê³„ì‚°
            mi_scores = mutual_info_regression(features_df.fillna(0), target, random_state=42)
            
            # F-ì ìˆ˜ ê³„ì‚°
            f_scores = f_regression(features_df.fillna(0), target)[0]
            
            for i, feature in enumerate(features_df.columns):
                cursor.execute('''
                INSERT OR REPLACE INTO feature_scores 
                (feature_name, mutual_info_score, f_score, final_score, rank, last_updated)
                VALUES (?, ?, ?, ?, ?, ?)
                ''', (
                    feature,
                    float(mi_scores[i]),
                    float(f_scores[i]),
                    float(mi_scores[i] + f_scores[i]),
                    i + 1,
                    datetime.now()
                ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            self.logger.error(f"íŠ¹ì„± ì ìˆ˜ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    async def _save_optimization_result(self, n_before: int, n_after: int,
                                      method: str, execution_time: float):
        """ìµœì í™” ê²°ê³¼ ì €ì¥"""
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
        INSERT INTO optimization_history 
        (timestamp, n_features_before, n_features_after, optimization_method, execution_time)
        VALUES (?, ?, ?, ?, ?)
        ''', (datetime.now(), n_before, n_after, method, execution_time))
        
        conn.commit()
        conn.close()
    
    def get_feature_ranking(self) -> pd.DataFrame:
        """íŠ¹ì„± ìˆœìœ„ ì¡°íšŒ"""
        
        conn = sqlite3.connect(self.db_path)
        
        query = '''
        SELECT feature_name, mutual_info_score, f_score, final_score, rank, last_updated
        FROM feature_scores 
        ORDER BY final_score DESC, rank ASC
        '''
        
        df = pd.read_sql_query(query, conn)
        conn.close()
        
        return df
    
    def get_optimization_history(self) -> pd.DataFrame:
        """ìµœì í™” íˆìŠ¤í† ë¦¬ ì¡°íšŒ"""
        
        conn = sqlite3.connect(self.db_path)
        
        query = '''
        SELECT * FROM optimization_history 
        ORDER BY timestamp DESC
        '''
        
        df = pd.read_sql_query(query, conn)
        conn.close()
        
        return df

class RealTimeFeatureMonitor:
    """ì‹¤ì‹œê°„ íŠ¹ì„± ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ"""
    
    def __init__(self, optimizer: AdvancedFeatureOptimizer):
        self.optimizer = optimizer
        self.monitoring_active = False
        self.performance_history = []
        
    async def start_monitoring(self, features_df: pd.DataFrame, 
                             target: np.ndarray, interval: int = 3600):
        """ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œì‘ (1ì‹œê°„ ê°„ê²©)"""
        
        self.monitoring_active = True
        
        while self.monitoring_active:
            try:
                # ì„±ëŠ¥ í‰ê°€
                performance = await self._evaluate_performance(features_df, target)
                
                # íˆìŠ¤í† ë¦¬ ì €ì¥
                self.performance_history.append({
                    'timestamp': datetime.now(),
                    'performance': performance,
                    'n_features': len(features_df.columns)
                })
                
                print(f"ğŸ“Š íŠ¹ì„± ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§: {performance:.4f} (íŠ¹ì„± {len(features_df.columns)}ê°œ)")
                
                # ì„±ëŠ¥ì´ í¬ê²Œ ë–¨ì–´ì§€ë©´ ì¬ìµœì í™” ì œì•ˆ
                if len(self.performance_history) > 5:
                    recent_avg = np.mean([h['performance'] for h in self.performance_history[-3:]])
                    older_avg = np.mean([h['performance'] for h in self.performance_history[-6:-3]])
                    
                    if recent_avg < older_avg * 0.9:  # 10% ì´ìƒ ì„±ëŠ¥ ì €í•˜
                        print("âš ï¸ ì„±ëŠ¥ ì €í•˜ ê°ì§€: íŠ¹ì„± ì¬ìµœì í™” í•„ìš”")
                
                await asyncio.sleep(interval)
                
            except Exception as e:
                print(f"âŒ ëª¨ë‹ˆí„°ë§ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(60)
    
    async def _evaluate_performance(self, features_df: pd.DataFrame, 
                                  target: np.ndarray) -> float:
        """íŠ¹ì„± ì„±ëŠ¥ í‰ê°€"""
        
        if not SKLEARN_AVAILABLE:
            return 0.5
        
        try:
            # ë¹ ë¥¸ ëª¨ë¸ë¡œ ì„±ëŠ¥ í‰ê°€
            model = RandomForestRegressor(n_estimators=20, random_state=42, n_jobs=-1)
            
            # ì‹œê³„ì—´ ë¶„í• ë¡œ êµì°¨ ê²€ì¦
            tscv = TimeSeriesSplit(n_splits=3)
            scores = cross_val_score(model, features_df.fillna(0), target, cv=tscv, scoring='r2')
            
            return scores.mean()
            
        except Exception as e:
            print(f"ì„±ëŠ¥ í‰ê°€ ì˜¤ë¥˜: {e}")
            return 0.0
    
    def stop_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        self.monitoring_active = False
    
    def get_performance_history(self) -> pd.DataFrame:
        """ì„±ëŠ¥ íˆìŠ¤í† ë¦¬ ë°˜í™˜"""
        return pd.DataFrame(self.performance_history)

# ì‚¬ìš© ì˜ˆì œ
async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("âš¡ ê³ ë„í™”ëœ íŠ¹ì„± ìµœì í™” ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    np.random.seed(42)
    n_samples = 1000
    n_features = 1500
    
    # ê°€ìƒì˜ íŠ¹ì„± ë°ì´í„°
    feature_names = [f"feature_{i}" for i in range(n_features)]
    features_df = pd.DataFrame(
        np.random.randn(n_samples, n_features),
        columns=feature_names
    )
    
    # ëª©í‘œ ë³€ìˆ˜ (ì¼ë¶€ íŠ¹ì„±ê³¼ ì—°ê´€)
    important_features = np.random.choice(n_features, 50, replace=False)
    target = np.sum(features_df.iloc[:, important_features] * np.random.randn(50), axis=1) + np.random.randn(n_samples) * 0.1
    
    # ìµœì í™” ì‹¤í–‰
    optimizer = AdvancedFeatureOptimizer(n_features_target=1000)
    
    print(f"\nğŸ” ìµœì í™” ì „: {len(features_df.columns)}ê°œ íŠ¹ì„±")
    
    # ë‹¤ì–‘í•œ ìµœì í™” ë°©ë²• í…ŒìŠ¤íŠ¸
    methods = ['comprehensive', 'fast', 'stability_focused']
    
    for method in methods:
        print(f"\nğŸš€ {method} ìµœì í™” ì‹¤í–‰...")
        
        optimized_features = await optimizer.optimize_features(
            features_df.copy(), 
            target, 
            method=method
        )
        
        print(f"âœ… {method} ê²°ê³¼: {len(optimized_features.columns)}ê°œ íŠ¹ì„±")
        
        # ì„±ëŠ¥ í‰ê°€
        if SKLEARN_AVAILABLE:
            model = RandomForestRegressor(n_estimators=50, random_state=42)
            model.fit(optimized_features.fillna(0), target)
            score = model.score(optimized_features.fillna(0), target)
            print(f"ğŸ“ˆ RÂ² ì ìˆ˜: {score:.4f}")
    
    # íŠ¹ì„± ìˆœìœ„ ì¡°íšŒ
    print("\nğŸ“Š ìµœê³  ì„±ëŠ¥ íŠ¹ì„± Top 10:")
    ranking = optimizer.get_feature_ranking()
    if len(ranking) > 0:
        print(ranking.head(10))
    
    # ìµœì í™” íˆìŠ¤í† ë¦¬
    print("\nğŸ“ˆ ìµœì í™” íˆìŠ¤í† ë¦¬:")
    history = optimizer.get_optimization_history()
    print(history)

if __name__ == "__main__":
    # ì‹¤í–‰
    asyncio.run(main())