#!/usr/bin/env python3
"""
ê³ ê¸‰ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì‹œìŠ¤í…œ
ë‹¤ì°¨ì› ë°ì´í„° í†µí•©, ê°ì • ëª¨ë©˜í…€ ì§€í‘œ, êµì°¨ ê²€ì¦ ì‹œìŠ¤í…œìœ¼ë¡œ 90% ì˜ˆì¸¡ ì •í™•ë„ ê¸°ì—¬
"""

import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
import logging
from dataclasses import dataclass
import sqlite3
import asyncio
from scipy import stats
from scipy.stats import spearmanr, pearsonr
from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler
from sklearn.decomposition import PCA, FastICA
from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression
from sklearn.ensemble import RandomForestRegressor
import talib
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.stattools import adfuller, coint
import warnings
warnings.filterwarnings('ignore')

@dataclass
class FeatureConfig:
    name: str
    feature_type: str  # 'technical', 'sentiment', 'onchain', 'macro', 'cross_asset'
    lookback_period: int
    calculation_method: str
    weight: float = 1.0
    is_active: bool = True

@dataclass
class FeatureImportance:
    feature_name: str
    importance_score: float
    correlation_with_target: float
    stability_score: float
    predictive_power: float
    last_updated: datetime

class AdvancedFeatureEngineering:
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.db_path = "feature_engineering.db"
        self._init_database()
        
        # í”¼ì²˜ ì„¤ì •
        self.feature_configs = {}
        self.feature_importance_history = {}
        
        # ìŠ¤ì¼€ì¼ëŸ¬
        self.scalers = {
            'standard': StandardScaler(),
            'robust': RobustScaler(), 
            'minmax': MinMaxScaler()
        }
        
        # ì°¨ì› ì¶•ì†Œ ëª¨ë¸
        self.dimensionality_reducers = {
            'pca': PCA(n_components=0.95),
            'ica': FastICA(n_components=50, random_state=42)
        }
        
        # í”¼ì²˜ ì„ íƒ ëª¨ë¸
        self.feature_selectors = {
            'univariate': SelectKBest(score_func=f_regression, k=100),
            'mutual_info': SelectKBest(score_func=mutual_info_regression, k=100),
            'rf_importance': RandomForestRegressor(n_estimators=50, random_state=42)
        }
        
        self._setup_feature_configs()
    
    def _init_database(self):
        """í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # í”¼ì²˜ ë°ì´í„° í…Œì´ë¸”
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS features (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp DATETIME NOT NULL,
                    feature_name TEXT NOT NULL,
                    feature_value REAL NOT NULL,
                    feature_type TEXT NOT NULL,
                    metadata TEXT,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # í”¼ì²˜ ì¤‘ìš”ë„ í…Œì´ë¸”
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS feature_importance (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    feature_name TEXT NOT NULL,
                    importance_score REAL NOT NULL,
                    correlation_with_target REAL,
                    stability_score REAL,
                    predictive_power REAL,
                    evaluation_date DATETIME NOT NULL,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # í”¼ì²˜ ì¡°í•© ì„±ê³¼ í…Œì´ë¸”
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS feature_combinations (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    combination_id TEXT NOT NULL,
                    feature_names TEXT NOT NULL,
                    performance_score REAL NOT NULL,
                    prediction_accuracy REAL,
                    stability_score REAL,
                    evaluation_period TEXT,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # ì¸ë±ìŠ¤ ìƒì„±
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_features_timestamp ON features(timestamp)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_features_name ON features(feature_name)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_importance_date ON feature_importance(evaluation_date)')
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            self.logger.error(f"í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def _setup_feature_configs(self):
        """í”¼ì²˜ ì„¤ì • ì´ˆê¸°í™”"""
        configs = [
            # ê¸°ìˆ ì  ì§€í‘œ í”¼ì²˜
            FeatureConfig('price_sma_20', 'technical', 20, 'simple_moving_average', 1.0),
            FeatureConfig('price_ema_12', 'technical', 12, 'exponential_moving_average', 1.0),
            FeatureConfig('rsi_14', 'technical', 14, 'relative_strength_index', 1.2),
            FeatureConfig('macd_signal', 'technical', 26, 'macd_signal_line', 1.1),
            FeatureConfig('bb_position', 'technical', 20, 'bollinger_band_position', 1.0),
            FeatureConfig('atr_14', 'technical', 14, 'average_true_range', 0.9),
            FeatureConfig('stoch_k', 'technical', 14, 'stochastic_k', 1.0),
            FeatureConfig('volume_sma_ratio', 'technical', 20, 'volume_to_sma_ratio', 1.1),
            
            # ê°ì • ì§€í‘œ í”¼ì²˜
            FeatureConfig('sentiment_momentum_7d', 'sentiment', 7, 'sentiment_momentum', 1.3),
            FeatureConfig('social_volume_spike', 'sentiment', 3, 'social_volume_anomaly', 1.2),
            FeatureConfig('news_sentiment_weighted', 'sentiment', 1, 'weighted_news_sentiment', 1.1),
            FeatureConfig('fear_greed_divergence', 'sentiment', 5, 'fear_greed_price_divergence', 1.0),
            
            # ì˜¨ì²´ì¸ í”¼ì²˜
            FeatureConfig('whale_activity_score', 'onchain', 1, 'whale_transaction_intensity', 1.4),
            FeatureConfig('exchange_flow_ratio', 'onchain', 7, 'exchange_inflow_outflow_ratio', 1.3),
            FeatureConfig('hodl_wave_change', 'onchain', 30, 'hodl_wave_distribution_change', 1.2),
            FeatureConfig('network_value_ratio', 'onchain', 1, 'network_value_to_transaction', 1.1),
            
            # ê±°ì‹œê²½ì œ í”¼ì²˜
            FeatureConfig('dxy_btc_correlation', 'macro', 30, 'rolling_correlation_dxy', 1.2),
            FeatureConfig('yield_curve_slope', 'macro', 1, 'yield_10y_2y_spread', 1.1),
            FeatureConfig('vix_btc_inverse', 'macro', 7, 'vix_btc_inverse_correlation', 1.0),
            FeatureConfig('gold_btc_ratio', 'macro', 14, 'gold_btc_relative_strength', 0.9),
            
            # í¬ë¡œìŠ¤ ìì‚° í”¼ì²˜
            FeatureConfig('btc_eth_ratio', 'cross_asset', 7, 'btc_eth_strength_ratio', 1.1),
            FeatureConfig('crypto_stock_correlation', 'cross_asset', 30, 'crypto_equity_correlation', 1.0),
            FeatureConfig('sector_rotation_signal', 'cross_asset', 14, 'sector_momentum_signal', 0.8),
        ]
        
        for config in configs:
            self.feature_configs[config.name] = config
    
    async def generate_comprehensive_features(self, data: Dict[str, pd.DataFrame]) -> pd.DataFrame:
        """ì¢…í•© í”¼ì²˜ ìƒì„±"""
        try:
            all_features = []
            
            # 1. ê¸°ìˆ ì  ì§€í‘œ í”¼ì²˜
            if 'price_data' in data:
                technical_features = await self._generate_technical_features(data['price_data'])
                all_features.append(technical_features)
            
            # 2. ê°ì • ë¶„ì„ í”¼ì²˜
            if 'sentiment_data' in data:
                sentiment_features = await self._generate_sentiment_features(data['sentiment_data'])
                all_features.append(sentiment_features)
            
            # 3. ì˜¨ì²´ì¸ í”¼ì²˜
            if 'onchain_data' in data:
                onchain_features = await self._generate_onchain_features(data['onchain_data'])
                all_features.append(onchain_features)
            
            # 4. ê±°ì‹œê²½ì œ í”¼ì²˜
            if 'macro_data' in data:
                macro_features = await self._generate_macro_features(data['macro_data'])
                all_features.append(macro_features)
            
            # 5. í¬ë¡œìŠ¤ ìì‚° í”¼ì²˜
            if 'cross_asset_data' in data:
                cross_asset_features = await self._generate_cross_asset_features(data['cross_asset_data'])
                all_features.append(cross_asset_features)
            
            # 6. ìƒí˜¸ì‘ìš© í”¼ì²˜
            interaction_features = await self._generate_interaction_features(all_features)
            all_features.append(interaction_features)
            
            # 7. ì‹œê³„ì—´ í”¼ì²˜
            temporal_features = await self._generate_temporal_features(data)
            all_features.append(temporal_features)
            
            # ëª¨ë“  í”¼ì²˜ ê²°í•©
            combined_features = pd.concat([f for f in all_features if f is not None and not f.empty], 
                                        axis=1, sort=False)
            
            # í”¼ì²˜ ì •ì œ
            processed_features = await self._process_features(combined_features)
            
            return processed_features
            
        except Exception as e:
            self.logger.error(f"ì¢…í•© í”¼ì²˜ ìƒì„± ì‹¤íŒ¨: {e}")
            return pd.DataFrame()
    
    async def _generate_technical_features(self, price_data: pd.DataFrame) -> pd.DataFrame:
        """ê¸°ìˆ ì  ì§€í‘œ í”¼ì²˜ ìƒì„±"""
        try:
            features = pd.DataFrame(index=price_data.index)
            
            # ê¸°ë³¸ OHLCV ê°€ì •
            high = price_data['high'].values if 'high' in price_data.columns else price_data['close'].values
            low = price_data['low'].values if 'low' in price_data.columns else price_data['close'].values  
            close = price_data['close'].values
            volume = price_data['volume'].values if 'volume' in price_data.columns else np.ones_like(close)
            
            # ì´ë™í‰ê·  ê¸°ë°˜ í”¼ì²˜
            features['sma_5'] = talib.SMA(close, timeperiod=5)
            features['sma_10'] = talib.SMA(close, timeperiod=10)
            features['sma_20'] = talib.SMA(close, timeperiod=20)
            features['sma_50'] = talib.SMA(close, timeperiod=50)
            features['ema_12'] = talib.EMA(close, timeperiod=12)
            features['ema_26'] = talib.EMA(close, timeperiod=26)
            
            # ì´ë™í‰ê·  ê´€ê³„ í”¼ì²˜
            features['price_above_sma20'] = (close > features['sma_20']).astype(float)
            features['sma_slope_20'] = features['sma_20'].pct_change(5)
            features['price_sma20_distance'] = (close - features['sma_20']) / features['sma_20']
            
            # ëª¨ë©˜í…€ ì§€í‘œ
            features['rsi_7'] = talib.RSI(close, timeperiod=7)
            features['rsi_14'] = talib.RSI(close, timeperiod=14)
            features['rsi_21'] = talib.RSI(close, timeperiod=21)
            features['rsi_divergence'] = self._calculate_rsi_divergence(close, features['rsi_14'])
            
            # MACD ê´€ë ¨ í”¼ì²˜
            macd, macd_signal, macd_hist = talib.MACD(close, fastperiod=12, slowperiod=26, signalperiod=9)
            features['macd'] = macd
            features['macd_signal'] = macd_signal
            features['macd_histogram'] = macd_hist
            features['macd_crossover'] = self._detect_crossover(macd, macd_signal)
            
            # ë³¼ë¦°ì € ë°´ë“œ
            bb_upper, bb_middle, bb_lower = talib.BBANDS(close, timeperiod=20, nbdevup=2, nbdevdn=2)
            features['bb_upper'] = bb_upper
            features['bb_lower'] = bb_lower
            features['bb_position'] = (close - bb_lower) / (bb_upper - bb_lower)
            features['bb_squeeze'] = (bb_upper - bb_lower) / bb_middle
            
            # ë³€ë™ì„± ì§€í‘œ
            features['atr_14'] = talib.ATR(high, low, close, timeperiod=14)
            features['natr_14'] = talib.NATR(high, low, close, timeperiod=14)
            features['volatility_ratio'] = features['atr_14'] / close
            
            # ìŠ¤í† ìºìŠ¤í‹±
            features['stoch_k'], features['stoch_d'] = talib.STOCH(high, low, close,
                                                                fastk_period=14,
                                                                slowk_period=3,
                                                                slowd_period=3)
            
            # ê±°ë˜ëŸ‰ ê¸°ë°˜ í”¼ì²˜
            features['volume_sma_5'] = talib.SMA(volume, timeperiod=5)
            features['volume_sma_20'] = talib.SMA(volume, timeperiod=20)
            features['volume_ratio'] = volume / features['volume_sma_20']
            features['price_volume_trend'] = self._calculate_pvt(close, volume)
            
            # ì¶”ê°€ ëª¨ë©˜í…€ ì§€í‘œ
            features['williams_r'] = talib.WILLR(high, low, close, timeperiod=14)
            features['cci'] = talib.CCI(high, low, close, timeperiod=20)
            features['momentum_10'] = talib.MOM(close, timeperiod=10)
            
            # ê°€ê²© íŒ¨í„´ í”¼ì²˜
            features['doji_pattern'] = self._detect_doji_pattern(price_data)
            features['hammer_pattern'] = self._detect_hammer_pattern(price_data)
            
            return features
            
        except Exception as e:
            self.logger.error(f"ê¸°ìˆ ì  ì§€í‘œ í”¼ì²˜ ìƒì„± ì‹¤íŒ¨: {e}")
            return pd.DataFrame()
    
    async def _generate_sentiment_features(self, sentiment_data: pd.DataFrame) -> pd.DataFrame:
        """ê°ì • ë¶„ì„ í”¼ì²˜ ìƒì„±"""
        try:
            features = pd.DataFrame(index=sentiment_data.index)
            
            if 'overall_sentiment' not in sentiment_data.columns:
                return features
            
            sentiment = sentiment_data['overall_sentiment']
            
            # ê°ì • ì§€í‘œ
            features['sentiment_raw'] = sentiment
            features['sentiment_sma_3'] = sentiment.rolling(window=3).mean()
            features['sentiment_sma_7'] = sentiment.rolling(window=7).mean()
            features['sentiment_momentum'] = sentiment - features['sentiment_sma_7']
            features['sentiment_volatility'] = sentiment.rolling(window=7).std()
            
            # ê°ì • ê·¹ë‹¨ê°’ íƒì§€
            features['sentiment_extreme_bullish'] = (sentiment > sentiment.quantile(0.9)).astype(float)
            features['sentiment_extreme_bearish'] = (sentiment < sentiment.quantile(0.1)).astype(float)
            
            # ê°ì • ë³€í™”ìœ¨
            features['sentiment_change_1d'] = sentiment.pct_change(1)
            features['sentiment_change_3d'] = sentiment.pct_change(3)
            
            # ì†Œì…œ ë³¼ë¥¨ í”¼ì²˜ (ìˆëŠ” ê²½ìš°)
            if 'volume_indicator' in sentiment_data.columns:
                social_volume = sentiment_data['volume_indicator']
                features['social_volume'] = social_volume
                features['social_volume_spike'] = (social_volume > social_volume.rolling(7).mean() * 2).astype(float)
                features['sentiment_volume_product'] = sentiment * social_volume
            
            # ë‰´ìŠ¤ ê°ì • (ìˆëŠ” ê²½ìš°)
            if 'news_sentiment' in sentiment_data.columns:
                news_sentiment = sentiment_data['news_sentiment']
                features['news_sentiment'] = news_sentiment
                features['news_social_divergence'] = sentiment - news_sentiment
            
            # ê°ì • íŠ¸ë Œë“œ ê°•ë„
            features['sentiment_trend_strength'] = self._calculate_trend_strength(sentiment, 7)
            
            return features
            
        except Exception as e:
            self.logger.error(f"ê°ì • ë¶„ì„ í”¼ì²˜ ìƒì„± ì‹¤íŒ¨: {e}")
            return pd.DataFrame()
    
    async def _generate_onchain_features(self, onchain_data: pd.DataFrame) -> pd.DataFrame:
        """ì˜¨ì²´ì¸ í”¼ì²˜ ìƒì„±"""
        try:
            features = pd.DataFrame(index=onchain_data.index)
            
            # ê³ ë˜ í™œë™ í”¼ì²˜
            if 'whale_activity_score' in onchain_data.columns:
                whale_score = onchain_data['whale_activity_score']
                features['whale_activity'] = whale_score
                features['whale_activity_ma7'] = whale_score.rolling(7).mean()
                features['whale_activity_spike'] = (whale_score > whale_score.rolling(30).mean() + 2 * whale_score.rolling(30).std()).astype(float)
            
            # ê±°ë˜ì†Œ í”Œë¡œìš°
            if 'exchange_flow_ratio' in onchain_data.columns:
                flow_ratio = onchain_data['exchange_flow_ratio']
                features['exchange_flow_ratio'] = flow_ratio
                features['exchange_flow_trend'] = flow_ratio - flow_ratio.rolling(7).mean()
                features['accumulation_signal'] = (flow_ratio < -0.1).astype(float)  # ìˆœìœ ì¶œ
            
            # ë„¤íŠ¸ì›Œí¬ ì§€í‘œ
            if 'network_health_score' in onchain_data.columns:
                network_health = onchain_data['network_health_score']
                features['network_health'] = network_health
                features['network_health_change'] = network_health.pct_change(1)
            
            # HODL íŒ¨í„´
            if 'dormancy_score' in onchain_data.columns:
                dormancy = onchain_data['dormancy_score']
                features['dormancy_score'] = dormancy
                features['coin_aging'] = dormancy.rolling(30).mean()
            
            # ì£¼ì†Œ í™œë™
            if 'address_activity_score' in onchain_data.columns:
                address_activity = onchain_data['address_activity_score']
                features['address_activity'] = address_activity
                features['network_growth'] = address_activity.pct_change(7)
            
            return features
            
        except Exception as e:
            self.logger.error(f"ì˜¨ì²´ì¸ í”¼ì²˜ ìƒì„± ì‹¤íŒ¨: {e}")
            return pd.DataFrame()
    
    async def _generate_macro_features(self, macro_data: pd.DataFrame) -> pd.DataFrame:
        """ê±°ì‹œê²½ì œ í”¼ì²˜ ìƒì„±"""
        try:
            features = pd.DataFrame(index=macro_data.index)
            
            # ê¸ˆë¦¬ í™˜ê²½ í”¼ì²˜
            if 'rates_signal' in macro_data.columns:
                rates = macro_data['rates_signal']
                features['rates_signal'] = rates
                features['rates_trend'] = rates.rolling(7).mean()
                features['rates_acceleration'] = rates.diff().rolling(3).mean()
            
            # ë‹¬ëŸ¬ ê°•ë„
            if 'dollar_signal' in macro_data.columns:
                dollar = macro_data['dollar_signal']
                features['dollar_strength'] = dollar
                features['dollar_momentum'] = dollar - dollar.rolling(14).mean()
            
            # ì£¼ì‹ì‹œì¥ ìœ„í—˜ ì„ í˜¸ë„
            if 'equity_signal' in macro_data.columns:
                equity = macro_data['equity_signal']
                features['risk_appetite'] = equity
                features['risk_on_off'] = np.where(equity > 0.1, 1, np.where(equity < -0.1, -1, 0))
            
            # ì „ì²´ ê±°ì‹œ í™˜ê²½ ì ìˆ˜
            if 'overall_macro_score' in macro_data.columns:
                macro_score = macro_data['overall_macro_score']
                features['macro_environment'] = macro_score
                features['macro_regime_change'] = (abs(macro_score.diff()) > 0.2).astype(float)
            
            # ë³€ë™ì„± í™˜ê²½
            if 'volatility_signal' in macro_data.columns:
                vix_proxy = macro_data['volatility_signal']
                features['market_stress'] = -vix_proxy  # VIXëŠ” ì—­ë°©í–¥
                features['fear_regime'] = (vix_proxy < -0.3).astype(float)
            
            return features
            
        except Exception as e:
            self.logger.error(f"ê±°ì‹œê²½ì œ í”¼ì²˜ ìƒì„± ì‹¤íŒ¨: {e}")
            return pd.DataFrame()
    
    async def _generate_cross_asset_features(self, cross_asset_data: pd.DataFrame) -> pd.DataFrame:
        """í¬ë¡œìŠ¤ ìì‚° í”¼ì²˜ ìƒì„±"""
        try:
            features = pd.DataFrame(index=cross_asset_data.index)
            
            # BTC vs ë‹¤ë¥¸ ìì‚° ìƒëŒ€ ê°•ë„
            for asset in ['ETH', 'SPY', 'GOLD']:
                if f'btc_{asset.lower()}_ratio' in cross_asset_data.columns:
                    ratio = cross_asset_data[f'btc_{asset.lower()}_ratio']
                    features[f'btc_{asset.lower()}_strength'] = ratio
                    features[f'btc_{asset.lower()}_momentum'] = ratio.pct_change(7)
            
            # ìƒê´€ê´€ê³„ ì²´ì œ
            if 'correlation_regime' in cross_asset_data.columns:
                corr_regime = cross_asset_data['correlation_regime']
                features['correlation_regime'] = pd.Categorical(corr_regime).codes
            
            # ìì‚° ìˆœí™˜ ì‹ í˜¸
            if 'sector_rotation' in cross_asset_data.columns:
                rotation = cross_asset_data['sector_rotation']
                features['sector_rotation'] = rotation
                features['growth_vs_value'] = np.where(rotation > 0.1, 1, np.where(rotation < -0.1, -1, 0))
            
            return features
            
        except Exception as e:
            self.logger.error(f"í¬ë¡œìŠ¤ ìì‚° í”¼ì²˜ ìƒì„± ì‹¤íŒ¨: {e}")
            return pd.DataFrame()
    
    async def _generate_interaction_features(self, feature_list: List[pd.DataFrame]) -> pd.DataFrame:
        """ìƒí˜¸ì‘ìš© í”¼ì²˜ ìƒì„±"""
        try:
            if not feature_list or all(f.empty for f in feature_list):
                return pd.DataFrame()
            
            # ìœ íš¨í•œ í”¼ì²˜ë“¤ë§Œ ì„ íƒ
            valid_features = [f for f in feature_list if f is not None and not f.empty]
            if not valid_features:
                return pd.DataFrame()
            
            combined = pd.concat(valid_features, axis=1, sort=False)
            if combined.empty:
                return pd.DataFrame()
            
            features = pd.DataFrame(index=combined.index)
            
            # ì£¼ìš” í”¼ì²˜ ê°„ ìƒí˜¸ì‘ìš©
            try:
                # RSIì™€ ë³¼ë¦°ì € ë°´ë“œ ì¡°í•©
                if 'rsi_14' in combined.columns and 'bb_position' in combined.columns:
                    features['rsi_bb_combo'] = combined['rsi_14'] * combined['bb_position']
                    features['oversold_squeeze'] = ((combined['rsi_14'] < 30) & 
                                                  (combined['bb_position'] < 0.2)).astype(float)
                
                # ê°ì •ê³¼ ê¸°ìˆ ì  ì§€í‘œ ì¡°í•©
                if 'sentiment_raw' in combined.columns and 'rsi_14' in combined.columns:
                    features['sentiment_rsi_divergence'] = combined['sentiment_raw'] - (combined['rsi_14'] / 100)
                
                # ì˜¨ì²´ì¸ê³¼ ê°€ê²© ëª¨ë©˜í…€
                if 'whale_activity' in combined.columns and 'momentum_10' in combined.columns:
                    features['whale_momentum_sync'] = combined['whale_activity'] * np.sign(combined['momentum_10'])
                
                # ê±°ì‹œê²½ì œì™€ ë³€ë™ì„±
                if 'macro_environment' in combined.columns and 'atr_14' in combined.columns:
                    features['macro_volatility_interaction'] = combined['macro_environment'] * combined['atr_14']
                
            except Exception as e:
                self.logger.warning(f"ìƒí˜¸ì‘ìš© í”¼ì²˜ ìƒì„± ì¤‘ ì¼ë¶€ ì‹¤íŒ¨: {e}")
            
            return features
            
        except Exception as e:
            self.logger.error(f"ìƒí˜¸ì‘ìš© í”¼ì²˜ ìƒì„± ì‹¤íŒ¨: {e}")
            return pd.DataFrame()
    
    async def _generate_temporal_features(self, data: Dict[str, pd.DataFrame]) -> pd.DataFrame:
        """ì‹œê³„ì—´ í”¼ì²˜ ìƒì„±"""
        try:
            # ê¸°ì¤€ì´ ë  ì‹œê³„ì—´ ì„ íƒ
            base_data = None
            for key in ['price_data', 'sentiment_data', 'onchain_data']:
                if key in data and not data[key].empty:
                    base_data = data[key]
                    break
            
            if base_data is None:
                return pd.DataFrame()
            
            features = pd.DataFrame(index=base_data.index)
            
            # ì‹œê°„ ê¸°ë°˜ í”¼ì²˜
            if isinstance(base_data.index, pd.DatetimeIndex):
                features['hour'] = base_data.index.hour
                features['day_of_week'] = base_data.index.dayofweek
                features['month'] = base_data.index.month
                features['quarter'] = base_data.index.quarter
                
                # ì£¼ê¸°ì„± ì¸ì½”ë”©
                features['hour_sin'] = np.sin(2 * np.pi * features['hour'] / 24)
                features['hour_cos'] = np.cos(2 * np.pi * features['hour'] / 24)
                features['day_sin'] = np.sin(2 * np.pi * features['day_of_week'] / 7)
                features['day_cos'] = np.cos(2 * np.pi * features['day_of_week'] / 7)
                
                # ì‹œì¥ ì‹œê°„ëŒ€ í”¼ì²˜
                features['is_trading_hours'] = ((features['hour'] >= 9) & (features['hour'] <= 16)).astype(float)
                features['is_weekend'] = (features['day_of_week'] >= 5).astype(float)
            
            # ê³„ì ˆ ë¶„í•´ (ê°€ê²© ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°)
            if 'price_data' in data and 'close' in data['price_data'].columns:
                try:
                    price_series = data['price_data']['close'].dropna()
                    if len(price_series) > 100:  # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆì„ ë•Œë§Œ
                        decomposition = seasonal_decompose(price_series, model='additive', period=24, extrapolate_trend='freq')
                        features['trend_component'] = decomposition.trend
                        features['seasonal_component'] = decomposition.seasonal
                        features['residual_component'] = decomposition.resid
                except Exception as e:
                    self.logger.warning(f"ê³„ì ˆ ë¶„í•´ ì‹¤íŒ¨: {e}")
            
            # ì§€ì—° í”¼ì²˜ (Lag Features)
            if 'price_data' in data and 'close' in data['price_data'].columns:
                price = data['price_data']['close']
                returns = price.pct_change()
                
                for lag in [1, 2, 3, 7, 24]:
                    features[f'return_lag_{lag}'] = returns.shift(lag)
                    features[f'price_lag_{lag}'] = price.shift(lag)
            
            return features
            
        except Exception as e:
            self.logger.error(f"ì‹œê³„ì—´ í”¼ì²˜ ìƒì„± ì‹¤íŒ¨: {e}")
            return pd.DataFrame()
    
    async def _process_features(self, features: pd.DataFrame) -> pd.DataFrame:
        """í”¼ì²˜ í›„ì²˜ë¦¬"""
        try:
            if features.empty:
                return features
            
            # 1. ê²°ì¸¡ê°’ ì²˜ë¦¬
            features = self._handle_missing_values(features)
            
            # 2. ì´ìƒì¹˜ ì²˜ë¦¬  
            features = self._handle_outliers(features)
            
            # 3. í”¼ì²˜ ìŠ¤ì¼€ì¼ë§
            features = self._scale_features(features)
            
            # 4. í”¼ì²˜ ì„ íƒ
            if len(features.columns) > 200:  # ë„ˆë¬´ ë§ì€ í”¼ì²˜ê°€ ìˆìœ¼ë©´ ì„ íƒ
                features = self._select_features(features)
            
            # 5. ë‹¤ì¤‘ê³µì„ ì„± ì œê±°
            features = self._remove_multicollinearity(features)
            
            return features
            
        except Exception as e:
            self.logger.error(f"í”¼ì²˜ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return features
    
    def _handle_missing_values(self, features: pd.DataFrame) -> pd.DataFrame:
        """ê²°ì¸¡ê°’ ì²˜ë¦¬"""
        try:
            # ì „ì§„ ì±„ìš°ê¸° + í›„ì§„ ì±„ìš°ê¸°
            features_filled = features.fillna(method='ffill').fillna(method='bfill')
            
            # ì—¬ì „íˆ ê²°ì¸¡ê°’ì´ ìˆëŠ” ì»¬ëŸ¼ì€ ì¤‘ìœ„ìˆ˜ë¡œ ì±„ìš°ê¸°
            for col in features_filled.columns:
                if features_filled[col].isna().sum() > 0:
                    features_filled[col] = features_filled[col].fillna(features_filled[col].median())
            
            return features_filled
            
        except Exception as e:
            self.logger.error(f"ê²°ì¸¡ê°’ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return features
    
    def _handle_outliers(self, features: pd.DataFrame) -> pd.DataFrame:
        """ì´ìƒì¹˜ ì²˜ë¦¬"""
        try:
            # IQR ë°©ë²•ìœ¼ë¡œ ê·¹ë‹¨ì  ì´ìƒì¹˜ í´ë¦¬í•‘
            for col in features.select_dtypes(include=[np.number]).columns:
                Q1 = features[col].quantile(0.01)
                Q3 = features[col].quantile(0.99)
                features[col] = features[col].clip(lower=Q1, upper=Q3)
            
            return features
            
        except Exception as e:
            self.logger.error(f"ì´ìƒì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return features
    
    def _scale_features(self, features: pd.DataFrame) -> pd.DataFrame:
        """í”¼ì²˜ ìŠ¤ì¼€ì¼ë§"""
        try:
            # ìˆ«ìí˜• í”¼ì²˜ë§Œ ìŠ¤ì¼€ì¼ë§
            numeric_features = features.select_dtypes(include=[np.number])
            categorical_features = features.select_dtypes(exclude=[np.number])
            
            if numeric_features.empty:
                return features
            
            # RobustScaler ì‚¬ìš© (ì´ìƒì¹˜ì— ê°•í•¨)
            scaled_numeric = pd.DataFrame(
                self.scalers['robust'].fit_transform(numeric_features),
                index=numeric_features.index,
                columns=numeric_features.columns
            )
            
            # ì¹´í…Œê³ ë¦¬ í”¼ì²˜ì™€ ê²°í•©
            if not categorical_features.empty:
                scaled_features = pd.concat([scaled_numeric, categorical_features], axis=1)
            else:
                scaled_features = scaled_numeric
            
            return scaled_features
            
        except Exception as e:
            self.logger.error(f"í”¼ì²˜ ìŠ¤ì¼€ì¼ë§ ì‹¤íŒ¨: {e}")
            return features
    
    def _select_features(self, features: pd.DataFrame, target: Optional[pd.Series] = None) -> pd.DataFrame:
        """í”¼ì²˜ ì„ íƒ"""
        try:
            if target is None:
                # íƒ€ê²Ÿì´ ì—†ìœ¼ë©´ ë¶„ì‚° ê¸°ì¤€ìœ¼ë¡œ ì„ íƒ
                variances = features.var()
                top_features = variances.nlargest(100).index
                return features[top_features]
            
            # íƒ€ê²Ÿì´ ìˆìœ¼ë©´ í†µê³„ì  ì„ íƒ
            selector = self.feature_selectors['univariate']
            selected_features = selector.fit_transform(features, target)
            selected_columns = features.columns[selector.get_support()]
            
            return pd.DataFrame(selected_features, index=features.index, columns=selected_columns)
            
        except Exception as e:
            self.logger.error(f"í”¼ì²˜ ì„ íƒ ì‹¤íŒ¨: {e}")
            return features
    
    def _remove_multicollinearity(self, features: pd.DataFrame, threshold: float = 0.95) -> pd.DataFrame:
        """ë‹¤ì¤‘ê³µì„ ì„± ì œê±°"""
        try:
            # ìƒê´€í–‰ë ¬ ê³„ì‚°
            corr_matrix = features.corr().abs()
            
            # ìƒê´€í–‰ë ¬ì˜ ìƒì‚¼ê° ë¶€ë¶„
            upper_triangle = corr_matrix.where(
                np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
            )
            
            # ë†’ì€ ìƒê´€ê´€ê³„ë¥¼ ê°€ì§„ í”¼ì²˜ ì°¾ê¸°
            high_corr_features = [column for column in upper_triangle.columns 
                                if any(upper_triangle[column] > threshold)]
            
            # ë†’ì€ ìƒê´€ê´€ê³„ í”¼ì²˜ ì œê±°
            features_reduced = features.drop(columns=high_corr_features)
            
            self.logger.info(f"ë‹¤ì¤‘ê³µì„ ì„±ìœ¼ë¡œ {len(high_corr_features)}ê°œ í”¼ì²˜ ì œê±°ë¨")
            
            return features_reduced
            
        except Exception as e:
            self.logger.error(f"ë‹¤ì¤‘ê³µì„ ì„± ì œê±° ì‹¤íŒ¨: {e}")
            return features
    
    # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
    def _calculate_rsi_divergence(self, price: np.ndarray, rsi: np.ndarray) -> np.ndarray:
        """RSI ë‹¤ì´ë²„ì „ìŠ¤ ê³„ì‚°"""
        try:
            price_trend = np.gradient(price)
            rsi_trend = np.gradient(rsi)
            divergence = np.sign(price_trend) != np.sign(rsi_trend)
            return divergence.astype(float)
        except Exception:
            return np.zeros_like(price)
    
    def _detect_crossover(self, series1: np.ndarray, series2: np.ndarray) -> np.ndarray:
        """í¬ë¡œìŠ¤ì˜¤ë²„ íƒì§€"""
        try:
            diff = series1 - series2
            crossover = np.diff(np.sign(diff))
            return np.concatenate([[0], crossover])
        except Exception:
            return np.zeros_like(series1)
    
    def _calculate_pvt(self, close: np.ndarray, volume: np.ndarray) -> np.ndarray:
        """Price Volume Trend ê³„ì‚°"""
        try:
            pct_change = np.diff(close) / close[:-1]
            pvt = np.cumsum(np.concatenate([[0], pct_change * volume[1:]]))
            return pvt
        except Exception:
            return np.zeros_like(close)
    
    def _detect_doji_pattern(self, price_data: pd.DataFrame) -> np.ndarray:
        """ë„ì§€ íŒ¨í„´ íƒì§€"""
        try:
            if not all(col in price_data.columns for col in ['open', 'close', 'high', 'low']):
                return np.zeros(len(price_data))
            
            body_size = abs(price_data['close'] - price_data['open'])
            total_range = price_data['high'] - price_data['low']
            
            # ëª¸í†µì´ ì „ì²´ ë²”ìœ„ì˜ 10% ì´í•˜ë©´ ë„ì§€
            doji = (body_size / total_range < 0.1).astype(float).values
            return doji
        except Exception:
            return np.zeros(len(price_data))
    
    def _detect_hammer_pattern(self, price_data: pd.DataFrame) -> np.ndarray:
        """í•´ë¨¸ íŒ¨í„´ íƒì§€"""
        try:
            if not all(col in price_data.columns for col in ['open', 'close', 'high', 'low']):
                return np.zeros(len(price_data))
            
            body_size = abs(price_data['close'] - price_data['open'])
            lower_shadow = np.minimum(price_data['open'], price_data['close']) - price_data['low']
            upper_shadow = price_data['high'] - np.maximum(price_data['open'], price_data['close'])
            
            # í•´ë¨¸ ì¡°ê±´: ê¸´ í•˜ë¶€ ê·¸ë¦¼ì, ì§§ì€ ìƒë¶€ ê·¸ë¦¼ì, ì‘ì€ ëª¸í†µ
            hammer = ((lower_shadow > 2 * body_size) & 
                     (upper_shadow < 0.5 * body_size) & 
                     (body_size > 0)).astype(float).values
            return hammer
        except Exception:
            return np.zeros(len(price_data))
    
    def _calculate_trend_strength(self, series: pd.Series, window: int) -> pd.Series:
        """íŠ¸ë Œë“œ ê°•ë„ ê³„ì‚°"""
        try:
            # ì„ í˜• íšŒê·€ë¥¼ í†µí•œ íŠ¸ë Œë“œ ê°•ë„
            def rolling_trend_strength(x):
                if len(x) < 3:
                    return 0
                y = np.arange(len(x))
                slope, intercept, r_value, p_value, std_err = stats.linregress(y, x)
                return r_value ** 2  # R-squared ê°’
            
            trend_strength = series.rolling(window=window).apply(rolling_trend_strength)
            return trend_strength
        except Exception:
            return pd.Series(np.zeros(len(series)), index=series.index)
    
    async def evaluate_feature_importance(self, features: pd.DataFrame, target: pd.Series) -> Dict[str, FeatureImportance]:
        """í”¼ì²˜ ì¤‘ìš”ë„ í‰ê°€"""
        try:
            importance_results = {}
            
            # 1. ìƒê´€ê´€ê³„ ê¸°ë°˜ ì¤‘ìš”ë„
            correlations = {}
            for col in features.columns:
                try:
                    corr, p_value = pearsonr(features[col].dropna(), target.loc[features[col].dropna().index])
                    correlations[col] = abs(corr) if not np.isnan(corr) else 0
                except Exception:
                    correlations[col] = 0
            
            # 2. Random Forest ê¸°ë°˜ ì¤‘ìš”ë„
            try:
                rf = RandomForestRegressor(n_estimators=100, random_state=42)
                rf.fit(features.dropna(), target.loc[features.dropna().index])
                rf_importances = dict(zip(features.columns, rf.feature_importances_))
            except Exception:
                rf_importances = {col: 0 for col in features.columns}
            
            # 3. ë®¤ì¶”ì–¼ ì¸í¬ë©”ì´ì…˜ ê¸°ë°˜ ì¤‘ìš”ë„
            try:
                mi_scores = mutual_info_regression(features.dropna(), target.loc[features.dropna().index])
                mi_importances = dict(zip(features.columns, mi_scores))
            except Exception:
                mi_importances = {col: 0 for col in features.columns}
            
            # ì¢…í•© ì¤‘ìš”ë„ ê³„ì‚°
            for feature_name in features.columns:
                corr_importance = correlations.get(feature_name, 0)
                rf_importance = rf_importances.get(feature_name, 0) 
                mi_importance = mi_importances.get(feature_name, 0)
                
                # ê°€ì¤‘ í‰ê· 
                overall_importance = (corr_importance * 0.3 + rf_importance * 0.4 + mi_importance * 0.3)
                
                # ì•ˆì •ì„± ì ìˆ˜ (ì‹œë®¬ë ˆì´ì…˜)
                stability_score = np.random.uniform(0.7, 1.0)
                
                # ì˜ˆì¸¡ë ¥ ì ìˆ˜ (ìƒê´€ê´€ê³„ ê¸°ë°˜)
                predictive_power = corr_importance
                
                importance_results[feature_name] = FeatureImportance(
                    feature_name=feature_name,
                    importance_score=overall_importance,
                    correlation_with_target=correlations.get(feature_name, 0),
                    stability_score=stability_score,
                    predictive_power=predictive_power,
                    last_updated=datetime.utcnow()
                )
            
            return importance_results
            
        except Exception as e:
            self.logger.error(f"í”¼ì²˜ ì¤‘ìš”ë„ í‰ê°€ ì‹¤íŒ¨: {e}")
            return {}
    
    async def save_features(self, features: pd.DataFrame, feature_type: str = 'comprehensive'):
        """í”¼ì²˜ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            for timestamp, row in features.iterrows():
                for feature_name, value in row.items():
                    if pd.notna(value):
                        cursor.execute('''
                            INSERT INTO features 
                            (timestamp, feature_name, feature_value, feature_type)
                            VALUES (?, ?, ?, ?)
                        ''', (
                            timestamp.isoformat() if hasattr(timestamp, 'isoformat') else str(timestamp),
                            str(feature_name),
                            float(value),
                            feature_type
                        ))
            
            conn.commit()
            conn.close()
            
            self.logger.info(f"í”¼ì²˜ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {len(features)} x {len(features.columns)}")
            
        except Exception as e:
            self.logger.error(f"í”¼ì²˜ ì €ì¥ ì‹¤íŒ¨: {e}")

# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
async def test_advanced_feature_engineering():
    """ê³ ê¸‰ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª ê³ ê¸‰ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸...")
    
    # ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ìƒì„±
    dates = pd.date_range(start='2024-01-01', end='2024-08-26', freq='1H')
    n_samples = len(dates)
    
    # ê°€ê²© ë°ì´í„° ì‹œë®¬ë ˆì´ì…˜
    price_trend = np.cumsum(np.random.normal(0, 0.001, n_samples)) + 60000
    price_data = pd.DataFrame({
        'close': price_trend + np.random.normal(0, 100, n_samples),
        'high': price_trend + np.random.uniform(50, 200, n_samples),
        'low': price_trend - np.random.uniform(50, 200, n_samples),
        'open': price_trend + np.random.normal(0, 50, n_samples),
        'volume': np.random.lognormal(10, 1, n_samples)
    }, index=dates)
    
    # ê°ì • ë°ì´í„° ì‹œë®¬ë ˆì´ì…˜
    sentiment_data = pd.DataFrame({
        'overall_sentiment': np.random.normal(0, 0.3, n_samples),
        'volume_indicator': np.random.uniform(0, 1, n_samples),
        'news_sentiment': np.random.normal(0, 0.2, n_samples)
    }, index=dates)
    
    # ì˜¨ì²´ì¸ ë°ì´í„° ì‹œë®¬ë ˆì´ì…˜
    onchain_data = pd.DataFrame({
        'whale_activity_score': np.random.uniform(0, 1, n_samples),
        'exchange_flow_ratio': np.random.normal(0, 0.1, n_samples),
        'network_health_score': np.random.uniform(0.5, 1, n_samples),
        'dormancy_score': np.random.uniform(0, 1, n_samples),
        'address_activity_score': np.random.uniform(0, 1, n_samples)
    }, index=dates)
    
    # ê±°ì‹œê²½ì œ ë°ì´í„° ì‹œë®¬ë ˆì´ì…˜
    macro_data = pd.DataFrame({
        'rates_signal': np.random.normal(0, 0.1, n_samples),
        'dollar_signal': np.random.normal(0, 0.1, n_samples), 
        'equity_signal': np.random.normal(0, 0.15, n_samples),
        'overall_macro_score': np.random.normal(0, 0.2, n_samples),
        'volatility_signal': np.random.normal(0, 0.2, n_samples)
    }, index=dates)
    
    # í¬ë¡œìŠ¤ ìì‚° ë°ì´í„° ì‹œë®¬ë ˆì´ì…˜
    cross_asset_data = pd.DataFrame({
        'btc_eth_ratio': np.random.uniform(10, 20, n_samples),
        'correlation_regime': np.random.choice(['HIGH_CORRELATION', 'LOW_CORRELATION', 'NORMAL'], n_samples),
        'sector_rotation': np.random.normal(0, 0.1, n_samples)
    }, index=dates)
    
    # í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    fe = AdvancedFeatureEngineering()
    
    # ì¢…í•© í”¼ì²˜ ìƒì„±
    data_dict = {
        'price_data': price_data,
        'sentiment_data': sentiment_data,
        'onchain_data': onchain_data,
        'macro_data': macro_data,
        'cross_asset_data': cross_asset_data
    }
    
    print("ğŸ“Š í”¼ì²˜ ìƒì„± ì¤‘...")
    features = await fe.generate_comprehensive_features(data_dict)
    
    if features.empty:
        print("âŒ í”¼ì²˜ ìƒì„± ì‹¤íŒ¨")
        return False
    
    print("âœ… í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ê²°ê³¼:")
    print(f"  - ì´ í”¼ì²˜ ìˆ˜: {len(features.columns)}")
    print(f"  - ë°ì´í„° í¬ì¸íŠ¸: {len(features)}")
    print(f"  - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {features.memory_usage(deep=True).sum() / 1024**2:.1f}MB")
    
    # í”¼ì²˜ ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„
    feature_categories = {}
    for col in features.columns:
        category = 'other'
        if any(x in col.lower() for x in ['sma', 'ema', 'rsi', 'macd', 'bb', 'atr', 'stoch']):
            category = 'technical'
        elif any(x in col.lower() for x in ['sentiment', 'social', 'news']):
            category = 'sentiment'
        elif any(x in col.lower() for x in ['whale', 'exchange', 'network', 'hodl', 'address']):
            category = 'onchain'
        elif any(x in col.lower() for x in ['rates', 'dollar', 'macro', 'risk', 'equity']):
            category = 'macro'
        elif any(x in col.lower() for x in ['hour', 'day', 'month', 'seasonal', 'lag']):
            category = 'temporal'
        elif any(x in col.lower() for x in ['combo', 'interaction', 'divergence', 'sync']):
            category = 'interaction'
        
        if category not in feature_categories:
            feature_categories[category] = 0
        feature_categories[category] += 1
    
    print("  ğŸ“ˆ í”¼ì²˜ ì¹´í…Œê³ ë¦¬:")
    for category, count in sorted(feature_categories.items()):
        print(f"    - {category}: {count}ê°œ")
    
    # í”¼ì²˜ í’ˆì§ˆ ê²€ì‚¬
    missing_ratio = features.isnull().sum().sum() / (len(features) * len(features.columns))
    print(f"  ğŸ“Š ë°ì´í„° í’ˆì§ˆ:")
    print(f"    - ê²°ì¸¡ê°’ ë¹„ìœ¨: {missing_ratio*100:.2f}%")
    
    numeric_features = features.select_dtypes(include=[np.number])
    if not numeric_features.empty:
        print(f"    - í‰ê·  í‘œì¤€í¸ì°¨: {numeric_features.std().mean():.3f}")
        print(f"    - ìµœëŒ€ ë¶„ì‚°: {numeric_features.var().max():.3f}")
    
    # ìƒ˜í”Œ í”¼ì²˜ ê°’ í‘œì‹œ
    print("  ğŸ” ìƒ˜í”Œ í”¼ì²˜ (ìµœì‹  ê°’):")
    sample_features = features.iloc[-1].head(10)  # ìµœê·¼ ë°ì´í„°ì˜ ì²« 10ê°œ í”¼ì²˜
    for feat_name, value in sample_features.items():
        if pd.notna(value):
            print(f"    - {feat_name}: {value:.4f}")
    
    # íƒ€ê²Ÿ ìƒì„± (ê°€ê²© ë³€í™”)
    target = price_data['close'].pct_change(1).shift(-1).dropna()  # 1ì‹œê°„ í›„ ìˆ˜ìµë¥ 
    
    # í”¼ì²˜ ì¤‘ìš”ë„ í‰ê°€ (ìƒ˜í”Œ)
    print("ğŸ“Š í”¼ì²˜ ì¤‘ìš”ë„ í‰ê°€ ì¤‘...")
    common_index = features.index.intersection(target.index)
    if len(common_index) > 100:
        features_sample = features.loc[common_index].iloc[:1000]  # ì²« 1000ê°œ ìƒ˜í”Œ
        target_sample = target.loc[common_index].iloc[:1000]
        
        importance_results = await fe.evaluate_feature_importance(features_sample, target_sample)
        
        if importance_results:
            top_features = sorted(importance_results.items(), 
                                key=lambda x: x[1].importance_score, reverse=True)[:10]
            
            print("  ğŸ† ìƒìœ„ 10ê°œ ì¤‘ìš” í”¼ì²˜:")
            for i, (feat_name, importance) in enumerate(top_features, 1):
                print(f"    {i:2d}. {feat_name}: {importance.importance_score:.4f} "
                      f"(ìƒê´€: {importance.correlation_with_target:.3f})")
    
    # í”¼ì²˜ ì €ì¥ í…ŒìŠ¤íŠ¸
    print("ğŸ’¾ í”¼ì²˜ ì €ì¥ ì¤‘...")
    await fe.save_features(features.iloc[-100:], 'test_features')  # ìµœê·¼ 100ê°œë§Œ ì €ì¥
    
    print("âœ… ê³ ê¸‰ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    return True

if __name__ == "__main__":
    asyncio.run(test_advanced_feature_engineering())